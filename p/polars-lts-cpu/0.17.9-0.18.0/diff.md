# Comparing `tmp/polars_lts_cpu-0.17.9.tar.gz` & `tmp/polars_lts_cpu-0.18.0.tar.gz`

## Comparing `polars_lts_cpu-0.17.9.tar` & `polars_lts_cpu-0.18.0.tar`

### file list

```diff
@@ -1,1114 +1,1208 @@
--rw-r--r--   0        0        0     4343 1970-01-01 00:00:00.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/Cargo.toml
--rw-r--r--   0     1001      123     1055 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/LICENSE
--rw-r--r--   0     1001      123     2383 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/avro/mod.rs
--rw-r--r--   0     1001      123     3608 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/avro/read.rs
--rw-r--r--   0     1001      123     2622 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/avro/write.rs
--rw-r--r--   0     1001      123     4505 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/cloud/adaptors.rs
--rw-r--r--   0     1001      123     9506 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/cloud/glob.rs
--rw-r--r--   0     1001      123     3089 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/cloud/mod.rs
--rw-r--r--   0     1001      123    28678 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/csv/buffer.rs
--rw-r--r--   0     1001      123     1898 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/csv/mod.rs
--rw-r--r--   0     1001      123    19712 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/csv/parser.rs
--rw-r--r--   0     1001      123    21432 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/csv/read.rs
--rw-r--r--   0     1001      123    10817 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/csv/read_impl/batched_mmap.rs
--rw-r--r--   0     1001      123    13909 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/csv/read_impl/batched_read.rs
--rw-r--r--   0     1001      123    31365 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/csv/read_impl/mod.rs
--rw-r--r--   0     1001      123    11466 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/csv/splitfields.rs
--rw-r--r--   0     1001      123    25183 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/csv/utils.rs
--rw-r--r--   0     1001      123     2796 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/csv/write.rs
--rw-r--r--   0     1001      123    13265 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/csv/write_impl.rs
--rw-r--r--   0     1001      123      184 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/export.rs
--rw-r--r--   0     1001      123     7586 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/ipc/ipc_file.rs
--rw-r--r--   0     1001      123     9222 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/ipc/ipc_stream.rs
--rw-r--r--   0     1001      123     3253 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/ipc/mmap.rs
--rw-r--r--   0     1001      123      401 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/ipc/mod.rs
--rw-r--r--   0     1001      123     8282 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/ipc/write.rs
--rw-r--r--   0     1001      123     1471 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/ipc/write_async.rs
--rw-r--r--   0     1001      123    11046 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/json.rs
--rw-r--r--   0     1001      123     4864 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/lib.rs
--rw-r--r--   0     1001      123     1969 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/mmap.rs
--rw-r--r--   0     1001      123     7215 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/ndjson_core/buffer.rs
--rw-r--r--   0     1001      123       39 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/ndjson_core/mod.rs
--rw-r--r--   0     1001      123    12172 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/ndjson_core/ndjson.rs
--rw-r--r--   0     1001      123      273 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/options.rs
--rw-r--r--   0     1001      123     7360 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/parquet/async_impl.rs
--rw-r--r--   0     1001      123     3093 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/parquet/mmap.rs
--rw-r--r--   0     1001      123     3132 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/parquet/mod.rs
--rw-r--r--   0     1001      123     4784 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/parquet/predicates.rs
--rw-r--r--   0     1001      123     9606 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/parquet/read.rs
--rw-r--r--   0     1001      123    16886 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/parquet/read_impl.rs
--rw-r--r--   0     1001      123    10106 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/parquet/write.rs
--rw-r--r--   0     1001      123     5334 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/partition.rs
--rw-r--r--   0     1001      123     1455 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/predicates.rs
--rw-r--r--   0     1001      123      628 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/prelude.rs
--rw-r--r--   0     1001      123      417 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/tests.rs
--rw-r--r--   0     1001      123     4467 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/utils.rs
--rw-r--r--   0        0        0     5450 1970-01-01 00:00:00.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/Cargo.toml
--rw-r--r--   0     1001      123     1055 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/LICENSE
--rw-r--r--   0     1001      123    19606 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/arithmetic.rs
--rw-r--r--   0     1001      123     8699 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/bitwise.rs
--rw-r--r--   0     1001      123     2298 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/builder/binary.rs
--rw-r--r--   0     1001      123     1207 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/builder/boolean.rs
--rw-r--r--   0     1001      123     1556 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/builder/from.rs
--rw-r--r--   0     1001      123    20366 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/builder/list.rs
--rw-r--r--   0     1001      123     8845 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/builder/mod.rs
--rw-r--r--   0     1001      123     1410 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/builder/primitive.rs
--rw-r--r--   0     1001      123     2291 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/builder/utf8.rs
--rw-r--r--   0     1001      123    11683 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/cast.rs
--rw-r--r--   0     1001      123    48300 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/comparison/mod.rs
--rw-r--r--   0     1001      123     9463 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/comparison/scalar.rs
--rw-r--r--   0     1001      123      551 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/drop.rs
--rw-r--r--   0     1001      123      963 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/float.rs
--rw-r--r--   0     1001      123     5069 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/from.rs
--rw-r--r--   0     1001      123    38411 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/iterator/mod.rs
--rw-r--r--   0     1001      123     1395 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/iterator/par/list.rs
--rw-r--r--   0     1001      123       28 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/iterator/par/mod.rs
--rw-r--r--   0     1001      123     1129 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/iterator/par/utf8.rs
--rw-r--r--   0     1001      123       21 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/kernels/mod.rs
--rw-r--r--   0     1001      123     2986 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/kernels/take.rs
--rw-r--r--   0     1001      123     7728 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/list/iterator.rs
--rw-r--r--   0     1001      123     2802 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/list/mod.rs
--rw-r--r--   0     1001      123    19796 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/categorical/builder.rs
--rw-r--r--   0     1001      123     3688 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/categorical/from.rs
--rw-r--r--   0     1001      123     4270 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/categorical/merge.rs
--rw-r--r--   0     1001      123    10220 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/categorical/mod.rs
--rw-r--r--   0     1001      123     1400 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/categorical/ops/append.rs
--rw-r--r--   0     1001      123      358 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/categorical/ops/full.rs
--rw-r--r--   0     1001      123      192 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/categorical/ops/mod.rs
--rw-r--r--   0     1001      123     2731 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/categorical/ops/take_random.rs
--rw-r--r--   0     1001      123     2172 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/categorical/ops/unique.rs
--rw-r--r--   0     1001      123      925 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/categorical/ops/zip.rs
--rw-r--r--   0     1001      123     6453 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/categorical/stringcache.rs
--rw-r--r--   0     1001      123     1604 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/date.rs
--rw-r--r--   0     1001      123     4105 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/datetime.rs
--rw-r--r--   0     1001      123     4443 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/decimal.rs
--rw-r--r--   0     1001      123     2434 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/duration.rs
--rw-r--r--   0     1001      123     2549 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/mod.rs
--rw-r--r--   0     1001      123      476 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/struct_/from.rs
--rw-r--r--   0     1001      123    15081 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/struct_/mod.rs
--rw-r--r--   0     1001      123     1182 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/time.rs
--rw-r--r--   0     1001      123    23524 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/mod.rs
--rw-r--r--   0     1001      123     7200 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ndarray.rs
--rw-r--r--   0     1001      123     4484 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/object/builder.rs
--rw-r--r--   0     1001      123     1547 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/object/extension/drop.rs
--rw-r--r--   0     1001      123     3124 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/object/extension/list.rs
--rw-r--r--   0     1001      123     7054 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/object/extension/mod.rs
--rw-r--r--   0     1001      123     3410 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/object/extension/polars_extension.rs
--rw-r--r--   0     1001      123      137 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/object/is_valid.rs
--rw-r--r--   0     1001      123     4419 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/object/iterator.rs
--rw-r--r--   0     1001      123     4826 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/object/mod.rs
--rw-r--r--   0     1001      123     2853 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/object/registry.rs
--rw-r--r--   0     1001      123      272 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/abs.rs
--rw-r--r--   0     1001      123    32120 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/aggregate/mod.rs
--rw-r--r--   0     1001      123     9946 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/aggregate/quantile.rs
--rw-r--r--   0     1001      123     2875 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/aggregate/var.rs
--rw-r--r--   0     1001      123     9670 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/any_value.rs
--rw-r--r--   0     1001      123     2365 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/append.rs
--rw-r--r--   0     1001      123    27269 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/apply.rs
--rw-r--r--   0     1001      123    12799 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/bit_repr.rs
--rw-r--r--   0     1001      123     6214 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/chunkops.rs
--rw-r--r--   0     1001      123    11537 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/compare_inner.rs
--rw-r--r--   0     1001      123     1737 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/concat_str.rs
--rw-r--r--   0     1001      123     4801 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/cum_agg.rs
--rw-r--r--   0     1001      123     6264 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/downcast.rs
--rw-r--r--   0     1001      123    25614 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/explode.rs
--rw-r--r--   0     1001      123     8339 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/extend.rs
--rw-r--r--   0     1001      123    13777 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/fill_null.rs
--rw-r--r--   0     1001      123     5308 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/filter.rs
--rw-r--r--   0     1001      123     4436 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/full.rs
--rw-r--r--   0     1001      123        1 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/interpolate.rs
--rw-r--r--   0     1001      123    15385 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/is_in.rs
--rw-r--r--   0     1001      123        1 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/len.rs
--rw-r--r--   0     1001      123     2658 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/min_max_binary.rs
--rw-r--r--   0     1001      123    22407 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/mod.rs
--rw-r--r--   0     1001      123     2403 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/nulls.rs
--rw-r--r--   0     1001      123      593 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/peaks.rs
--rw-r--r--   0     1001      123     2585 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/repeat_by.rs
--rw-r--r--   0     1001      123     1539 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/reverse.rs
--rw-r--r--   0     1001      123    10234 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/rolling_window.rs
--rw-r--r--   0     1001      123    12518 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/set.rs
--rw-r--r--   0     1001      123     6151 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/shift.rs
--rw-r--r--   0     1001      123     2299 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/sort/arg_sort.rs
--rw-r--r--   0     1001      123     5467 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/sort/arg_sort_multiple.rs
--rw-r--r--   0     1001      123     7430 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/sort/categorical.rs
--rw-r--r--   0     1001      123    30762 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/sort/mod.rs
--rw-r--r--   0     1001      123      380 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/sort/slice.rs
--rw-r--r--   0     1001      123    20472 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/take/mod.rs
--rw-r--r--   0     1001      123     6630 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/take/take_chunked.rs
--rw-r--r--   0     1001      123     1859 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/take/take_every.rs
--rw-r--r--   0     1001      123    16256 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/take/take_random.rs
--rw-r--r--   0     1001      123     5041 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/take/take_single.rs
--rw-r--r--   0     1001      123     6064 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/take/traits.rs
--rw-r--r--   0     1001      123    11229 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/unique/mod.rs
--rw-r--r--   0     1001      123    14620 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/unique/rank.rs
--rw-r--r--   0     1001      123     7753 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/zip.rs
--rw-r--r--   0     1001      123     9093 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/random.rs
--rw-r--r--   0     1001      123     1959 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/temporal/conversion.rs
--rw-r--r--   0     1001      123     2501 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/temporal/date.rs
--rw-r--r--   0     1001      123    12123 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/temporal/datetime.rs
--rw-r--r--   0     1001      123     3201 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/temporal/duration.rs
--rw-r--r--   0     1001      123      533 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/temporal/mod.rs
--rw-r--r--   0     1001      123     2724 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/temporal/time.rs
--rw-r--r--   0     1001      123      872 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/to_vec.rs
--rw-r--r--   0     1001      123     8113 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/trusted_len.rs
--rw-r--r--   0     1001      123    25182 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/upstream_traits.rs
--rw-r--r--   0     1001      123     7689 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/cloud.rs
--rw-r--r--   0     1001      123     1549 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/config.rs
--rw-r--r--   0     1001      123     3946 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/datatypes/_serde.rs
--rw-r--r--   0     1001      123     2650 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/datatypes/aliases.rs
--rw-r--r--   0     1001      123    42410 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/datatypes/any_value.rs
--rw-r--r--   0     1001      123    12083 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/datatypes/dtype.rs
--rw-r--r--   0     1001      123     5456 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/datatypes/field.rs
--rw-r--r--   0     1001      123     7675 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/datatypes/mod.rs
--rw-r--r--   0     1001      123     2016 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/datatypes/time_unit.rs
--rw-r--r--   0     1001      123      118 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/doc/changelog/mod.rs
--rw-r--r--   0     1001      123      898 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/doc/changelog/v0_10_0_11.rs
--rw-r--r--   0     1001      123      481 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/doc/changelog/v0_3.rs
--rw-r--r--   0     1001      123      293 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/doc/changelog/v0_4.rs
--rw-r--r--   0     1001      123      499 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/doc/changelog/v0_5.rs
--rw-r--r--   0     1001      123      288 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/doc/changelog/v0_6.rs
--rw-r--r--   0     1001      123     1071 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/doc/changelog/v0_7.rs
--rw-r--r--   0     1001      123      819 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/doc/changelog/v0_8.rs
--rw-r--r--   0     1001      123      596 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/doc/changelog/v0_9.rs
--rw-r--r--   0     1001      123       43 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/doc/mod.rs
--rw-r--r--   0     1001      123       25 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/error.rs
--rw-r--r--   0     1001      123      433 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/export.rs
--rw-r--r--   0     1001      123    36116 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/fmt.rs
--rw-r--r--   0     1001      123     5177 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/arithmetic.rs
--rw-r--r--   0     1001      123     7874 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/asof_join/asof.rs
--rw-r--r--   0     1001      123    33865 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/asof_join/groups.rs
--rw-r--r--   0     1001      123     6660 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/asof_join/mod.rs
--rw-r--r--   0     1001      123      559 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/chunks.rs
--rw-r--r--   0     1001      123     5179 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/cross_join.rs
--rw-r--r--   0     1001      123    16609 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/explode.rs
--rw-r--r--   0     1001      123     1019 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/from.rs
--rw-r--r--   0     1001      123    16518 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/groupby/aggregations/agg_list.rs
--rw-r--r--   0     1001      123     4115 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/groupby/aggregations/boolean.rs
--rw-r--r--   0     1001      123     7643 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/groupby/aggregations/dispatch.rs
--rw-r--r--   0     1001      123    39255 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/groupby/aggregations/mod.rs
--rw-r--r--   0     1001      123     5636 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/groupby/aggregations/utf8.rs
--rw-r--r--   0     1001      123      218 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/groupby/expr.rs
--rw-r--r--   0     1001      123    19976 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/groupby/hashing.rs
--rw-r--r--   0     1001      123    14075 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/groupby/into_groups.rs
--rw-r--r--   0     1001      123    39383 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/groupby/mod.rs
--rw-r--r--   0     1001      123    10593 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/groupby/perfect.rs
--rw-r--r--   0     1001      123    18842 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/groupby/proxy.rs
--rw-r--r--   0     1001      123    18264 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/hash_join/mod.rs
--rw-r--r--   0     1001      123    22392 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/hash_join/multiple_keys.rs
--rw-r--r--   0     1001      123     2413 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/hash_join/single_keys.rs
--rw-r--r--   0     1001      123    16352 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/hash_join/single_keys_dispatch.rs
--rw-r--r--   0     1001      123     4295 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/hash_join/single_keys_inner.rs
--rw-r--r--   0     1001      123     6076 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/hash_join/single_keys_left.rs
--rw-r--r--   0     1001      123     4247 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/hash_join/single_keys_outer.rs
--rw-r--r--   0     1001      123     3913 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/hash_join/single_keys_semi_anti.rs
--rw-r--r--   0     1001      123    11884 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/hash_join/sort_merge.rs
--rw-r--r--   0     1001      123   124136 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/mod.rs
--rw-r--r--   0     1001      123    27513 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/row/av_buffer.rs
--rw-r--r--   0     1001      123     3732 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/row/dataframe.rs
--rw-r--r--   0     1001      123     5976 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/row/mod.rs
--rw-r--r--   0     1001      123     8795 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/row/transpose.rs
--rw-r--r--   0     1001      123     2183 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/top_k.rs
--rw-r--r--   0     1001      123     1388 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/upstream_traits.rs
--rw-r--r--   0     1001      123    10935 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/functions.rs
--rw-r--r--   0     1001      123     2149 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/hashing/fx.rs
--rw-r--r--   0     1001      123     1503 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/hashing/identity.rs
--rw-r--r--   0     1001      123      457 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/hashing/mod.rs
--rw-r--r--   0     1001      123     2684 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/hashing/partition.rs
--rw-r--r--   0     1001      123    17653 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/hashing/vector_hasher.rs
--rw-r--r--   0     1001      123     1896 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/lib.rs
--rw-r--r--   0     1001      123    15733 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/named_from.rs
--rw-r--r--   0     1001      123     2411 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/prelude.rs
--rw-r--r--   0     1001      123    16652 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/schema.rs
--rw-r--r--   0     1001      123     4218 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/serde/chunked_array.rs
--rw-r--r--   0     1001      123     6551 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/serde/mod.rs
--rw-r--r--   0     1001      123     9929 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/serde/series.rs
--rw-r--r--   0     1001      123    18441 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/any_value.rs
--rw-r--r--   0     1001      123    28511 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/arithmetic/borrowed.rs
--rw-r--r--   0     1001      123      222 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/arithmetic/mod.rs
--rw-r--r--   0     1001      123     3546 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/arithmetic/owned.rs
--rw-r--r--   0     1001      123    13187 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/comparison.rs
--rw-r--r--   0     1001      123    25008 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/from.rs
--rw-r--r--   0     1001      123     9217 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/binary.rs
--rw-r--r--   0     1001      123    10850 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/boolean.rs
--rw-r--r--   0     1001      123    12962 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/categorical.rs
--rw-r--r--   0     1001      123    18304 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/dates_time.rs
--rw-r--r--   0     1001      123    15242 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/datetime.rs
--rw-r--r--   0     1001      123     5829 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/decimal.rs
--rw-r--r--   0     1001      123    14917 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/duration.rs
--rw-r--r--   0     1001      123    14223 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/floats.rs
--rw-r--r--   0     1001      123     6207 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/list.rs
--rw-r--r--   0     1001      123    18305 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/mod.rs
--rw-r--r--   0     1001      123     5336 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/null.rs
--rw-r--r--   0     1001      123     8004 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/object.rs
--rw-r--r--   0     1001      123    11029 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/struct_.rs
--rw-r--r--   0     1001      123     9735 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/utf8.rs
--rw-r--r--   0     1001      123     4471 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/into.rs
--rw-r--r--   0     1001      123     6297 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/iterator.rs
--rw-r--r--   0     1001      123    37684 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/mod.rs
--rw-r--r--   0     1001      123      853 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/ops/diff.rs
--rw-r--r--   0     1001      123     5204 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/ops/downcast.rs
--rw-r--r--   0     1001      123     3601 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/ops/ewm.rs
--rw-r--r--   0     1001      123      413 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/ops/extend.rs
--rw-r--r--   0     1001      123      562 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/ops/mod.rs
--rw-r--r--   0     1001      123     5974 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/ops/moment.rs
--rw-r--r--   0     1001      123     2908 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/ops/null.rs
--rw-r--r--   0     1001      123     1347 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/ops/pct_change.rs
--rw-r--r--   0     1001      123     4247 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/ops/round.rs
--rw-r--r--   0     1001      123     5072 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/ops/to_list.rs
--rw-r--r--   0     1001      123     1476 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/ops/unique.rs
--rw-r--r--   0     1001      123    18204 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/series_trait.rs
--rw-r--r--   0     1001      123     2840 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/unstable.rs
--rw-r--r--   0     1001      123     8117 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/testing.rs
--rw-r--r--   0     1001      123      508 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/tests.rs
--rw-r--r--   0     1001      123     2494 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/utils/flatten.rs
--rw-r--r--   0     1001      123    31704 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/utils/mod.rs
--rw-r--r--   0     1001      123     1646 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/utils/series.rs
--rw-r--r--   0     1001      123    13773 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/utils/supertype.rs
--rw-r--r--   0        0        0     2055 1970-01-01 00:00:00.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/Cargo.toml
--rw-r--r--   0     1001      123     1055 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/LICENSE
--rw-r--r--   0     1001      123     3565 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/date.rs
--rw-r--r--   0     1001      123     6465 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/datetime.rs
--rw-r--r--   0     1001      123     3305 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/duration.rs
--rw-r--r--   0     1001      123     5607 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/kernels.rs
--rw-r--r--   0     1001      123     1062 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/mod.rs
--rw-r--r--   0     1001      123     7302 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/rolling_window/floats.rs
--rw-r--r--   0     1001      123     2582 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/rolling_window/ints.rs
--rw-r--r--   0     1001      123    10495 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/rolling_window/mod.rs
--rw-r--r--   0     1001      123      428 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/rolling_window/rolling_kernels/mod.rs
--rw-r--r--   0     1001      123     7642 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/rolling_window/rolling_kernels/no_nulls.rs
--rw-r--r--   0     1001      123     2717 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/time.rs
--rw-r--r--   0     1001      123    21192 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/utf8/infer.rs
--rw-r--r--   0     1001      123    20108 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/utf8/mod.rs
--rw-r--r--   0     1001      123     4115 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/utf8/patterns.rs
--rw-r--r--   0     1001      123     9692 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/utf8/strptime.rs
--rw-r--r--   0     1001      123     2960 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/date_range.rs
--rw-r--r--   0     1001      123    31843 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/groupby/dynamic.rs
--rw-r--r--   0     1001      123       88 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/groupby/mod.rs
--rw-r--r--   0     1001      123      613 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/lib.rs
--rw-r--r--   0     1001      123     3080 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/month_end.rs
--rw-r--r--   0     1001      123     3468 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/month_start.rs
--rw-r--r--   0     1001      123      320 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/prelude.rs
--rw-r--r--   0     1001      123     1568 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/round.rs
--rw-r--r--   0     1001      123     4028 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/series/_trait.rs
--rw-r--r--   0     1001      123      136 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/series/implementations/boolean.rs
--rw-r--r--   0     1001      123      140 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/series/implementations/categoricals.rs
--rw-r--r--   0     1001      123      133 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/series/implementations/date.rs
--rw-r--r--   0     1001      123      137 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/series/implementations/datetime.rs
--rw-r--r--   0     1001      123      137 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/series/implementations/duration.rs
--rw-r--r--   0     1001      123     1863 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/series/implementations/floats.rs
--rw-r--r--   0     1001      123     1792 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/series/implementations/integers.rs
--rw-r--r--   0     1001      123      133 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/series/implementations/list.rs
--rw-r--r--   0     1001      123      486 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/series/implementations/mod.rs
--rw-r--r--   0     1001      123      155 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/series/implementations/object.rs
--rw-r--r--   0     1001      123      135 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/series/implementations/struct_.rs
--rw-r--r--   0     1001      123      133 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/series/implementations/time.rs
--rw-r--r--   0     1001      123      133 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/series/implementations/utf8.rs
--rw-r--r--   0     1001      123    12466 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/series/mod.rs
--rw-r--r--   0     1001      123     1630 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/truncate.rs
--rw-r--r--   0     1001      123     7308 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/upsample.rs
--rw-r--r--   0     1001      123     2567 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/utils.rs
--rw-r--r--   0     1001      123     1524 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/windows/bounds.rs
--rw-r--r--   0     1001      123     2650 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/windows/calendar.rs
--rw-r--r--   0     1001      123    25110 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/windows/duration.rs
--rw-r--r--   0     1001      123    19317 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/windows/groupby.rs
--rw-r--r--   0     1001      123      503 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/windows/mod.rs
--rw-r--r--   0     1001      123    24372 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/windows/test.rs
--rw-r--r--   0     1001      123    11624 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/windows/window.rs
--rw-r--r--   0        0        0      823 1970-01-01 00:00:00.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-algo/Cargo.toml
--rw-r--r--   0     1001      123     1055 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-algo/LICENSE
--rw-r--r--   0     1001      123     7477 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-algo/src/algo.rs
--rw-r--r--   0     1001      123       88 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-algo/src/lib.rs
--rw-r--r--   0     1001      123       28 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-algo/src/prelude.rs
--rw-r--r--   0        0        0      994 1970-01-01 00:00:00.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-sql/Cargo.toml
--rw-r--r--   0     1001      123     1055 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-sql/LICENSE
--rw-r--r--   0     1001      123    16037 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-sql/src/context.rs
--rw-r--r--   0     1001      123    20245 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-sql/src/functions.rs
--rw-r--r--   0     1001      123     1920 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-sql/src/keywords.rs
--rw-r--r--   0     1001      123      211 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-sql/src/lib.rs
--rw-r--r--   0     1001      123    15240 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-sql/src/sql_expr.rs
--rw-r--r--   0     1001      123     4572 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-sql/src/table_functions.rs
--rw-r--r--   0     1001      123     1682 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-sql/tests/functions_cumulative.rs
--rw-r--r--   0     1001      123     3063 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-sql/tests/functions_io.rs
--rw-r--r--   0     1001      123     1539 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-sql/tests/functions_math.rs
--rw-r--r--   0     1001      123     2982 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-sql/tests/functions_string.rs
--rw-r--r--   0     1001      123     1056 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-sql/tests/iss_7436.rs
--rw-r--r--   0     1001      123      888 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-sql/tests/iss_7437.rs
--rw-r--r--   0     1001      123      652 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-sql/tests/iss_7440.rs
--rw-r--r--   0     1001      123      700 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-sql/tests/iss_8395.rs
--rw-r--r--   0     1001      123     1062 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-sql/tests/iss_8419.rs
--rw-r--r--   0     1001      123    14296 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-sql/tests/simple_exprs.rs
--rw-r--r--   0        0        0     5241 1970-01-01 00:00:00.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/Cargo.toml
--rw-r--r--   0     1001      123     1055 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/LICENSE
--rw-r--r--   0     1001      123       45 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/constants.rs
--rw-r--r--   0     1001      123    17248 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dot.rs
--rw-r--r--   0     1001      123     4171 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/arithmetic.rs
--rw-r--r--   0     1001      123      935 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/binary.rs
--rw-r--r--   0     1001      123      650 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/cat.rs
--rw-r--r--   0     1001      123     9880 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/dt.rs
--rw-r--r--   0     1001      123    13169 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/expr.rs
--rw-r--r--   0     1001      123      753 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/from.rs
--rw-r--r--   0     1001      123       85 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/abs.rs
--rw-r--r--   0     1001      123     1431 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/arg_where.rs
--rw-r--r--   0     1001      123     1327 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/binary.rs
--rw-r--r--   0     1001      123     4221 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/boolean.rs
--rw-r--r--   0     1001      123     1910 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/bounds.rs
--rw-r--r--   0     1001      123     1216 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/cat.rs
--rw-r--r--   0     1001      123      344 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/clip.rs
--rw-r--r--   0     1001      123     1593 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/cum.rs
--rw-r--r--   0     1001      123    15231 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/datetime.rs
--rw-r--r--   0     1001      123      810 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/dispatch.rs
--rw-r--r--   0     1001      123     2541 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/fill_null.rs
--rw-r--r--   0     1001      123     8119 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/list.rs
--rw-r--r--   0     1001      123      581 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/log.rs
--rw-r--r--   0     1001      123    19416 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/mod.rs
--rw-r--r--   0     1001      123      462 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/nan.rs
--rw-r--r--   0     1001      123     3132 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/pow.rs
--rw-r--r--   0     1001      123      152 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/rolling.rs
--rw-r--r--   0     1001      123      260 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/round.rs
--rw-r--r--   0     1001      123      200 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/row_hash.rs
--rw-r--r--   0     1001      123    13668 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/schema.rs
--rw-r--r--   0     1001      123      306 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/search_sorted.rs
--rw-r--r--   0     1001      123     3812 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/shift_and_fill.rs
--rw-r--r--   0     1001      123     1238 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/shrink_type.rs
--rw-r--r--   0     1001      123      972 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/sign.rs
--rw-r--r--   0     1001      123    19923 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/strings.rs
--rw-r--r--   0     1001      123     1017 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/struct_.rs
--rw-r--r--   0     1001      123     3010 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/temporal.rs
--rw-r--r--   0     1001      123     5122 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/trigonometry.rs
--rw-r--r--   0     1001      123      170 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/unique.rs
--rw-r--r--   0     1001      123    43864 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/functions.rs
--rw-r--r--   0     1001      123    10196 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/list.rs
--rw-r--r--   0     1001      123     2181 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/meta.rs
--rw-r--r--   0     1001      123    64804 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/mod.rs
--rw-r--r--   0     1001      123       40 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/names.rs
--rw-r--r--   0     1001      123     2100 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/options.rs
--rw-r--r--   0     1001      123    15238 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/string.rs
--rw-r--r--   0     1001      123     2715 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/struct_.rs
--rw-r--r--   0     1001      123       38 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/frame/mod.rs
--rw-r--r--   0     1001      123      933 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/frame/opt_state.rs
--rw-r--r--   0     1001      123      466 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/global.rs
--rw-r--r--   0     1001      123      175 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/lib.rs
--rw-r--r--   0     1001      123     6825 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/aexpr/mod.rs
--rw-r--r--   0     1001      123    11393 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/aexpr/schema.rs
--rw-r--r--   0     1001      123    25656 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/alp.rs
--rw-r--r--   0     1001      123     1622 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/anonymous_scan.rs
--rw-r--r--   0     1001      123     1428 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/apply.rs
--rw-r--r--   0     1001      123    24898 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/builder.rs
--rw-r--r--   0     1001      123    29652 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/conversion.rs
--rw-r--r--   0     1001      123      301 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/debug.rs
--rw-r--r--   0     1001      123    15191 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/format.rs
--rw-r--r--   0     1001      123      895 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/functions/drop.rs
--rw-r--r--   0     1001      123      137 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/functions/explode.rs
--rw-r--r--   0     1001      123     1169 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/functions/merge_sorted.rs
--rw-r--r--   0     1001      123    11905 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/functions/mod.rs
--rw-r--r--   0     1001      123     1330 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/functions/rename.rs
--rw-r--r--   0     1001      123     9752 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/iterator.rs
--rw-r--r--   0     1001      123    10463 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/lit.rs
--rw-r--r--   0     1001      123     8115 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/mod.rs
--rw-r--r--   0     1001      123     7416 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/cache_states.rs
--rw-r--r--   0     1001      123    15277 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/cse.rs
--rw-r--r--   0     1001      123     3250 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/delay_rechunk.rs
--rw-r--r--   0     1001      123     3236 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/drop_nulls.rs
--rw-r--r--   0     1001      123     3994 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/fast_projection.rs
--rw-r--r--   0     1001      123    14479 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/file_caching.rs
--rw-r--r--   0     1001      123     1556 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/flatten_union.rs
--rw-r--r--   0     1001      123     6644 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/mod.rs
--rw-r--r--   0     1001      123     1222 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/predicate_pushdown/keys.rs
--rw-r--r--   0     1001      123    28276 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/predicate_pushdown/mod.rs
--rw-r--r--   0     1001      123     2571 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/predicate_pushdown/rename.rs
--rw-r--r--   0     1001      123    15130 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/predicate_pushdown/utils.rs
--rw-r--r--   0     1001      123     1755 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/functions/melt.rs
--rw-r--r--   0     1001      123     3930 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/functions/mod.rs
--rw-r--r--   0     1001      123     1799 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/generic.rs
--rw-r--r--   0     1001      123     3269 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/groupby.rs
--rw-r--r--   0     1001      123     2638 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/hstack.rs
--rw-r--r--   0     1001      123    15747 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/joins.rs
--rw-r--r--   0     1001      123    26502 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/mod.rs
--rw-r--r--   0     1001      123     3707 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/projection.rs
--rw-r--r--   0     1001      123     2639 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/rename.rs
--rw-r--r--   0     1001      123     3501 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/semi_anti_join.rs
--rw-r--r--   0     1001      123    27361 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/simplify_expr.rs
--rw-r--r--   0     1001      123     3492 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/slice_pushdown_expr.rs
--rw-r--r--   0     1001      123    13845 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/slice_pushdown_lp.rs
--rw-r--r--   0     1001      123     3161 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/stack_opt.rs
--rw-r--r--   0     1001      123     9725 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/type_coercion/binary.rs
--rw-r--r--   0     1001      123    19820 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/type_coercion/mod.rs
--rw-r--r--   0     1001      123    10187 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/options.rs
--rw-r--r--   0     1001      123    15276 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/projection.rs
--rw-r--r--   0     1001      123     4584 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/pyarrow.rs
--rw-r--r--   0     1001      123    13038 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/schema.rs
--rw-r--r--   0     1001      123      809 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/prelude.rs
--rw-r--r--   0     1001      123    11879 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/utils.rs
--rw-r--r--   0        0        0     1765 1970-01-01 00:00:00.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/Cargo.toml
--rw-r--r--   0     1001      123     1055 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/LICENSE
--rw-r--r--   0     1001      123       98 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/mod.rs
--rw-r--r--   0     1001      123     1219 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/operators/filter.rs
--rw-r--r--   0     1001      123     4103 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/operators/function.rs
--rw-r--r--   0     1001      123      229 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/operators/mod.rs
--rw-r--r--   0     1001      123      548 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/operators/placeholder.rs
--rw-r--r--   0     1001      123     3247 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/operators/projection.rs
--rw-r--r--   0     1001      123     2324 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/operators/reproject.rs
--rw-r--r--   0     1001      123     6479 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/file_sink.rs
--rw-r--r--   0     1001      123    10473 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/convert.rs
--rw-r--r--   0     1001      123     1207 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/count.rs
--rw-r--r--   0     1001      123     1888 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/first.rs
--rw-r--r--   0     1001      123     4554 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/interface.rs
--rw-r--r--   0     1001      123     1746 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/last.rs
--rw-r--r--   0     1001      123     5413 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/mean.rs
--rw-r--r--   0     1001      123     4951 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/min_max.rs
--rw-r--r--   0     1001      123      211 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/mod.rs
--rw-r--r--   0     1001      123      856 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/null.rs
--rw-r--r--   0     1001      123     4294 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/sum.rs
--rw-r--r--   0     1001      123     3030 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/eval.rs
--rw-r--r--   0     1001      123     6275 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/global.rs
--rw-r--r--   0     1001      123    13665 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/hash_table.rs
--rw-r--r--   0     1001      123     2681 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/mod.rs
--rw-r--r--   0     1001      123     2534 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/ooc_state.rs
--rw-r--r--   0     1001      123     5961 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/sink.rs
--rw-r--r--   0     1001      123    10673 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/thread_local.rs
--rw-r--r--   0     1001      123     2164 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/mod.rs
--rw-r--r--   0     1001      123     4666 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/ooc.rs
--rw-r--r--   0     1001      123     3906 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/ooc_state.rs
--rw-r--r--   0     1001      123    20859 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/primitive/mod.rs
--rw-r--r--   0     1001      123    23518 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/string.rs
--rw-r--r--   0     1001      123     2457 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/utils.rs
--rw-r--r--   0     1001      123     8395 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/io.rs
--rw-r--r--   0     1001      123     5485 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/joins/cross.rs
--rw-r--r--   0     1001      123    14279 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/joins/generic_build.rs
--rw-r--r--   0     1001      123    11824 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/joins/inner_left.rs
--rw-r--r--   0     1001      123      178 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/joins/mod.rs
--rw-r--r--   0     1001      123     2002 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/memory.rs
--rw-r--r--   0     1001      123      589 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/mod.rs
--rw-r--r--   0     1001      123     1492 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/ordered.rs
--rw-r--r--   0     1001      123     1824 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/reproject.rs
--rw-r--r--   0     1001      123     3108 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/slice.rs
--rw-r--r--   0     1001      123      130 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/sort/mod.rs
--rw-r--r--   0     1001      123     3808 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/sort/ooc.rs
--rw-r--r--   0     1001      123     6295 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/sort/sink.rs
--rw-r--r--   0     1001      123     5953 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/sort/sink_multiple.rs
--rw-r--r--   0     1001      123     3801 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/sort/source.rs
--rw-r--r--   0     1001      123      635 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/utils.rs
--rw-r--r--   0     1001      123     5076 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sources/csv.rs
--rw-r--r--   0     1001      123     1231 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sources/frame.rs
--rw-r--r--   0     1001      123      987 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sources/ipc_one_shot.rs
--rw-r--r--   0     1001      123      366 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sources/mod.rs
--rw-r--r--   0     1001      123     3387 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sources/parquet.rs
--rw-r--r--   0     1001      123     1146 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sources/reproject.rs
--rw-r--r--   0     1001      123     1022 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sources/union.rs
--rw-r--r--   0     1001      123      448 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/expressions.rs
--rw-r--r--   0     1001      123      272 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/lib.rs
--rw-r--r--   0     1001      123      719 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/operators/chunks.rs
--rw-r--r--   0     1001      123      474 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/operators/context.rs
--rw-r--r--   0     1001      123      223 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/operators/mod.rs
--rw-r--r--   0     1001      123      430 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/operators/operator.rs
--rw-r--r--   0     1001      123      626 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/operators/sink.rs
--rw-r--r--   0     1001      123      241 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/operators/source.rs
--rw-r--r--   0     1001      123        1 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/pipeline/config.rs
--rw-r--r--   0     1001      123    20492 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/pipeline/convert.rs
--rw-r--r--   0     1001      123    13982 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/pipeline/dispatcher.rs
--rw-r--r--   0     1001      123     1237 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/pipeline/mod.rs
--rw-r--r--   0        0        0      940 1970-01-01 00:00:00.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-row/Cargo.toml
--rw-r--r--   0     1001      123     1055 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-row/LICENSE
--rw-r--r--   0     1001      123     8985 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-row/src/encode.rs
--rw-r--r--   0     1001      123     4591 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-row/src/encodings/fixed.rs
--rw-r--r--   0     1001      123       47 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-row/src/encodings/mod.rs
--rw-r--r--   0     1001      123     4508 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-row/src/encodings/variable.rs
--rw-r--r--   0     1001      123    13678 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-row/src/lib.rs
--rw-r--r--   0     1001      123     2079 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-row/src/row.rs
--rw-r--r--   0     1001      123      682 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-row/src/utils.rs
--rw-r--r--   0        0        0    10541 1970-01-01 00:00:00.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/Cargo.toml
--rw-r--r--   0     1001      123     1055 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/LICENSE
--rw-r--r--   0     1001      123     3592 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/Makefile
--rw-r--r--   0     1001      123      215 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/build.rs
--rw-r--r--   0     1001      123       78 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/clippy.toml
--rw-r--r--   0     1001      123    17602 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/src/docs/eager.rs
--rw-r--r--   0     1001      123     8778 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/src/docs/lazy.rs
--rw-r--r--   0     1001      123       50 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/src/docs/mod.rs
--rw-r--r--   0     1001      123     3797 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/src/docs/performance.rs
--rw-r--r--   0     1001      123       59 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/src/export.rs
--rw-r--r--   0     1001      123    20207 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/src/lib.rs
--rw-r--r--   0     1001      123      387 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/src/prelude.rs
--rw-r--r--   0     1001      123       44 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/src/sql.rs
--rw-r--r--   0     1001      123     4272 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/core/date_like.rs
--rw-r--r--   0     1001      123     2401 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/core/groupby.rs
--rw-r--r--   0     1001      123    17826 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/core/joins.rs
--rw-r--r--   0     1001      123      545 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/core/list.rs
--rw-r--r--   0     1001      123      189 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/core/mod.rs
--rw-r--r--   0     1001      123     6258 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/core/pivot.rs
--rw-r--r--   0     1001      123     1102 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/core/random.rs
--rw-r--r--   0     1001      123    10844 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/core/rolling_window.rs
--rw-r--r--   0     1001      123     1093 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/core/series.rs
--rw-r--r--   0     1001      123      370 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/core/utils.rs
--rw-r--r--   0     1001      123    30013 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/io/csv.rs
--rw-r--r--   0     1001      123     4490 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/io/ipc_stream.rs
--rw-r--r--   0     1001      123     7044 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/io/json.rs
--rw-r--r--   0     1001      123      378 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/io/mod.rs
--rw-r--r--   0     1001      123      988 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/io/parquet.rs
--rw-r--r--   0     1001      123     1530 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/joins.rs
--rw-r--r--   0     1001      123     2452 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/aggregation.rs
--rw-r--r--   0     1001      123      702 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/cse.rs
--rw-r--r--   0     1001      123      500 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/explodes.rs
--rw-r--r--   0     1001      123     2279 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/expressions/apply.rs
--rw-r--r--   0     1001      123    10285 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/expressions/arity.rs
--rw-r--r--   0     1001      123     1064 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/expressions/expand.rs
--rw-r--r--   0     1001      123     1008 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/expressions/filter.rs
--rw-r--r--   0     1001      123      428 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/expressions/is_in.rs
--rw-r--r--   0     1001      123      121 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/expressions/mod.rs
--rw-r--r--   0     1001      123      659 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/expressions/slice.rs
--rw-r--r--   0     1001      123    10139 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/expressions/window.rs
--rw-r--r--   0     1001      123      579 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/folds.rs
--rw-r--r--   0     1001      123      557 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/functions.rs
--rw-r--r--   0     1001      123     4482 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/groupby.rs
--rw-r--r--   0     1001      123     1655 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/groupby_dynamic.rs
--rw-r--r--   0     1001      123      691 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/mod.rs
--rw-r--r--   0     1001      123     5381 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/predicate_queries.rs
--rw-r--r--   0     1001      123     4476 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/projection_queries.rs
--rw-r--r--   0     1001      123     6444 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/queries.rs
--rw-r--r--   0     1001      123      141 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/main.rs
--rw-r--r--   0     1001      123    12591 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/schema.rs
--rw-r--r--   0        0        0     1431 1970-01-01 00:00:00.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/Cargo.toml
--rw-r--r--   0     1001      123     1055 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/LICENSE
--rw-r--r--   0     1001      123     1975 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/array/default_arrays.rs
--rw-r--r--   0     1001      123     3160 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/array/get.rs
--rw-r--r--   0     1001      123     6459 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/array/list.rs
--rw-r--r--   0     1001      123     7771 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/array/mod.rs
--rw-r--r--   0     1001      123      878 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/array/null.rs
--rw-r--r--   0     1001      123     1125 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/array/slice.rs
--rw-r--r--   0     1001      123     1807 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/array/utf8.rs
--rw-r--r--   0     1001      123     2294 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/bit_util.rs
--rw-r--r--   0     1001      123       17 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/bitmap/mod.rs
--rw-r--r--   0     1001      123      819 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/bitmap/mutable.rs
--rw-r--r--   0     1001      123      232 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/compute/cast.rs
--rw-r--r--   0     1001      123       56 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/compute/mod.rs
--rw-r--r--   0     1001      123     2962 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/compute/take/boolean.rs
--rw-r--r--   0     1001      123    24907 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/compute/take/mod.rs
--rw-r--r--   0     1001      123     1042 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/conversion.rs
--rw-r--r--   0     1001      123     1609 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/data_types.rs
--rw-r--r--   0     1001      123       25 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/error.rs
--rw-r--r--   0     1001      123       28 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/export.rs
--rw-r--r--   0     1001      123       26 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/floats/mod.rs
--rw-r--r--   0     1001      123     2066 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/floats/ord.rs
--rw-r--r--   0     1001      123     1273 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/index.rs
--rw-r--r--   0     1001      123      915 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/is_valid.rs
--rw-r--r--   0     1001      123     4740 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/agg_mean.rs
--rw-r--r--   0     1001      123     1015 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/concatenate.rs
--rw-r--r--   0     1001      123     5161 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/ewm/average.rs
--rw-r--r--   0     1001      123     1808 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/ewm/mod.rs
--rw-r--r--   0     1001      123    25065 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/ewm/variance.rs
--rw-r--r--   0     1001      123     1406 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/float.rs
--rw-r--r--   0     1001      123     4907 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/list.rs
--rw-r--r--   0     1001      123     1895 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/list_bytes_iter.rs
--rw-r--r--   0     1001      123     9731 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/mod.rs
--rw-r--r--   0     1001      123     3703 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/mod.rs
--rw-r--r--   0     1001      123     2022 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/no_nulls/mean.rs
--rw-r--r--   0     1001      123    18684 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/no_nulls/min_max.rs
--rw-r--r--   0     1001      123     3924 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/no_nulls/mod.rs
--rw-r--r--   0     1001      123    11659 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/no_nulls/quantile.rs
--rw-r--r--   0     1001      123     5504 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/no_nulls/sum.rs
--rw-r--r--   0     1001      123     8683 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/no_nulls/variance.rs
--rw-r--r--   0     1001      123     1749 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/nulls/mean.rs
--rw-r--r--   0     1001      123    14367 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/nulls/min_max.rs
--rw-r--r--   0     1001      123     9070 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/nulls/mod.rs
--rw-r--r--   0     1001      123    11609 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/nulls/quantile.rs
--rw-r--r--   0     1001      123     4698 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/nulls/sum.rs
--rw-r--r--   0     1001      123     8335 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/nulls/variance.rs
--rw-r--r--   0     1001      123     8109 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/window.rs
--rw-r--r--   0     1001      123     4752 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/set.rs
--rw-r--r--   0     1001      123     4529 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/sort_partition.rs
--rw-r--r--   0     1001      123     2948 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/sorted_join/inner.rs
--rw-r--r--   0     1001      123     5974 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/sorted_join/left.rs
--rw-r--r--   0     1001      123      231 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/sorted_join/mod.rs
--rw-r--r--   0     1001      123      841 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/string.rs
--rw-r--r--   0     1001      123     2310 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/take_agg/boolean.rs
--rw-r--r--   0     1001      123     4343 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/take_agg/mod.rs
--rw-r--r--   0     1001      123     2606 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/take_agg/var.rs
--rw-r--r--   0     1001      123     4417 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/time.rs
--rw-r--r--   0     1001      123      341 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/lib.rs
--rw-r--r--   0     1001      123      434 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/prelude.rs
--rw-r--r--   0     1001      123      534 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/slice.rs
--rw-r--r--   0     1001      123      762 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/time_zone.rs
--rw-r--r--   0     1001      123      998 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/trusted_len/boolean.rs
--rw-r--r--   0     1001      123     2821 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/trusted_len/mod.rs
--rw-r--r--   0     1001      123     2533 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/trusted_len/push_unchecked.rs
--rw-r--r--   0     1001      123      158 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/trusted_len/rev.rs
--rw-r--r--   0     1001      123     5232 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/utils.rs
--rw-r--r--   0        0        0      476 1970-01-01 00:00:00.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-utils/Cargo.toml
--rw-r--r--   0     1001      123     1055 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-utils/LICENSE
--rw-r--r--   0     1001      123     2645 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/arena.rs
--rw-r--r--   0     1001      123     1373 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/atomic.rs
--rw-r--r--   0     1001      123     2659 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/cell.rs
--rw-r--r--   0     1001      123     1015 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/contention_pool.rs
--rw-r--r--   0     1001      123      509 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/error.rs
--rw-r--r--   0     1001      123      271 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/fmt.rs
--rw-r--r--   0     1001      123      763 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/functions.rs
--rw-r--r--   0     1001      123      514 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/hash.rs
--rw-r--r--   0     1001      123     2709 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/iter/enumerate_idx.rs
--rw-r--r--   0     1001      123       61 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/iter/mod.rs
--rw-r--r--   0     1001      123      583 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/lib.rs
--rw-r--r--   0     1001      123      573 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/macros.rs
--rw-r--r--   0     1001      123      282 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/mem.rs
--rw-r--r--   0     1001      123     1792 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/slice.rs
--rw-r--r--   0     1001      123     2467 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/sort.rs
--rw-r--r--   0     1001      123     1115 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/sync.rs
--rw-r--r--   0     1001      123      504 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/sys.rs
--rw-r--r--   0     1001      123      697 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/unwrap.rs
--rw-r--r--   0     1001      123      616 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/wasm.rs
--rw-r--r--   0        0        0     5977 1970-01-01 00:00:00.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/Cargo.toml
--rw-r--r--   0     1001      123     1055 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/LICENSE
--rw-r--r--   0     1001      123     1796 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/dot.rs
--rw-r--r--   0     1001      123     4359 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/dsl/eval.rs
--rw-r--r--   0     1001      123     4505 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/dsl/functions.rs
--rw-r--r--   0     1001      123      164 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/dsl/into.rs
--rw-r--r--   0     1001      123     6754 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/dsl/list.rs
--rw-r--r--   0     1001      123     2899 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/dsl/mod.rs
--rw-r--r--   0     1001      123     1182 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/frame/anonymous_scan.rs
--rw-r--r--   0     1001      123     9278 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/frame/csv.rs
--rw-r--r--   0     1001      123     4309 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/frame/file_list_reader.rs
--rw-r--r--   0     1001      123     2261 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/frame/ipc.rs
--rw-r--r--   0     1001      123    47176 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/frame/mod.rs
--rw-r--r--   0     1001      123     3414 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/frame/ndjson.rs
--rw-r--r--   0     1001      123     3440 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/frame/parquet.rs
--rw-r--r--   0     1001      123     2892 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/frame/pivot.rs
--rw-r--r--   0     1001      123      416 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/frame/python.rs
--rw-r--r--   0     1001      123     6376 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/lib.rs
--rw-r--r--   0     1001      123     1049 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/cache.rs
--rw-r--r--   0     1001      123      776 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/executor.rs
--rw-r--r--   0     1001      123      670 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/ext_context.rs
--rw-r--r--   0     1001      123     1284 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/filter.rs
--rw-r--r--   0     1001      123     3908 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/groupby.rs
--rw-r--r--   0     1001      123     3577 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/groupby_dynamic.rs
--rw-r--r--   0     1001      123    13439 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/groupby_partitioned.rs
--rw-r--r--   0     1001      123     4335 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/groupby_rolling.rs
--rw-r--r--   0     1001      123     6058 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/join.rs
--rw-r--r--   0     1001      123     7074 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/mod.rs
--rw-r--r--   0     1001      123     2045 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/projection.rs
--rw-r--r--   0     1001      123     1677 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/python_scan.rs
--rw-r--r--   0     1001      123     2986 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/scan/csv.rs
--rw-r--r--   0     1001      123     1963 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/scan/ipc.rs
--rw-r--r--   0     1001      123     4151 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/scan/mod.rs
--rw-r--r--   0     1001      123     1208 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/scan/ndjson.rs
--rw-r--r--   0     1001      123     2421 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/scan/parquet.rs
--rw-r--r--   0     1001      123      548 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/slice.rs
--rw-r--r--   0     1001      123     2197 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/sort.rs
--rw-r--r--   0     1001      123     1922 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/stack.rs
--rw-r--r--   0     1001      123      663 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/udf.rs
--rw-r--r--   0     1001      123     3702 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/union.rs
--rw-r--r--   0     1001      123      838 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/unique.rs
--rw-r--r--   0     1001      123     1284 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/exotic.rs
--rw-r--r--   0     1001      123    21873 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/aggregation.rs
--rw-r--r--   0     1001      123     2689 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/alias.rs
--rw-r--r--   0     1001      123    17432 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/apply.rs
--rw-r--r--   0     1001      123    17488 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/binary.rs
--rw-r--r--   0     1001      123     3153 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/cast.rs
--rw-r--r--   0     1001      123     6326 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/column.rs
--rw-r--r--   0     1001      123     2003 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/count.rs
--rw-r--r--   0     1001      123     5809 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/filter.rs
--rw-r--r--   0     1001      123     4131 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/group_iter.rs
--rw-r--r--   0     1001      123     5304 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/literal.rs
--rw-r--r--   0     1001      123    21175 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/mod.rs
--rw-r--r--   0     1001      123    10091 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/slice.rs
--rw-r--r--   0     1001      123     3929 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/sort.rs
--rw-r--r--   0     1001      123    11541 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/sortby.rs
--rw-r--r--   0     1001      123     8331 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/take.rs
--rw-r--r--   0     1001      123    14360 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/ternary.rs
--rw-r--r--   0     1001      123    32007 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/window.rs
--rw-r--r--   0     1001      123     2039 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/file_cache.rs
--rw-r--r--   0     1001      123      414 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/mod.rs
--rw-r--r--   0     1001      123     2005 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/node_timer.rs
--rw-r--r--   0     1001      123    27366 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/planner/expr.rs
--rw-r--r--   0     1001      123    18862 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/planner/lp.rs
--rw-r--r--   0     1001      123       87 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/planner/mod.rs
--rw-r--r--   0     1001      123     9425 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/state.rs
--rw-r--r--   0     1001      123    20891 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/streaming/convert.rs
--rw-r--r--   0     1001      123      219 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/streaming/mod.rs
--rw-r--r--   0     1001      123     3333 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/streaming/tree.rs
--rw-r--r--   0     1001      123      722 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/prelude.rs
--rw-r--r--   0     1001      123    14990 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/tests/aggregations.rs
--rw-r--r--   0     1001      123     2339 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/tests/arity.rs
--rw-r--r--   0     1001      123     7031 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/tests/cse.rs
--rw-r--r--   0     1001      123    12749 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/tests/io.rs
--rw-r--r--   0     1001      123     4207 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/tests/logical.rs
--rw-r--r--   0     1001      123     4288 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/tests/mod.rs
--rw-r--r--   0     1001      123    14812 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/tests/optimization_checks.rs
--rw-r--r--   0     1001      123     6799 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/tests/predicate_queries.rs
--rw-r--r--   0     1001      123     3158 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/tests/projection_queries.rs
--rw-r--r--   0     1001      123    47910 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/tests/queries.rs
--rw-r--r--   0     1001      123     8358 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/tests/streaming.rs
--rw-r--r--   0     1001      123     2886 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/tests/tpch.rs
--rw-r--r--   0     1001      123     1028 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/utils.rs
--rw-r--r--   0        0        0      879 1970-01-01 00:00:00.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-error/Cargo.toml
--rw-r--r--   0     1001      123     1055 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-error/LICENSE
--rw-r--r--   0     1001      123     6297 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-error/src/lib.rs
--rw-r--r--   0        0        0     3268 1970-01-01 00:00:00.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/Cargo.toml
--rw-r--r--   0     1001      123     1055 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/LICENSE
--rw-r--r--   0     1001      123      234 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/binary/mod.rs
--rw-r--r--   0     1001      123     3549 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/binary/namespace.rs
--rw-r--r--   0     1001      123    11023 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/interpolate.rs
--rw-r--r--   0     1001      123     1679 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/list/count.rs
--rw-r--r--   0     1001      123     2452 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/list/hash.rs
--rw-r--r--   0     1001      123     7861 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/list/min_max.rs
--rw-r--r--   0     1001      123      511 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/list/mod.rs
--rw-r--r--   0     1001      123    22005 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/list/namespace.rs
--rw-r--r--   0     1001      123     5269 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/list/sum_mean.rs
--rw-r--r--   0     1001      123     2435 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/list/to_struct.rs
--rw-r--r--   0     1001      123      489 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/mod.rs
--rw-r--r--   0     1001      123     9380 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/nan_propagating_aggregate.rs
--rw-r--r--   0     1001      123     6795 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/set.rs
--rw-r--r--   0     1001      123     7490 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/strings/case.rs
--rw-r--r--   0     1001      123     8251 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/strings/json_path.rs
--rw-r--r--   0     1001      123     2345 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/strings/justify.rs
--rw-r--r--   0     1001      123      514 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/strings/mod.rs
--rw-r--r--   0     1001      123    14731 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/strings/namespace.rs
--rw-r--r--   0     1001      123     4053 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/strings/replace.rs
--rw-r--r--   0     1001      123     2486 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/top_k.rs
--rw-r--r--   0     1001      123     7727 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/frame/join/merge_sorted.rs
--rw-r--r--   0     1001      123    18025 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/frame/join/mod.rs
--rw-r--r--   0     1001      123     4174 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/frame/mod.rs
--rw-r--r--   0     1001      123    10257 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/frame/pivot/mod.rs
--rw-r--r--   0     1001      123    13486 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/frame/pivot/positioning.rs
--rw-r--r--   0     1001      123      217 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/lib.rs
--rw-r--r--   0     1001      123      290 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/prelude.rs
--rw-r--r--   0     1001      123       25 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/mod.rs
--rw-r--r--   0     1001      123     9623 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/ops/approx_algo/hyperloglogplus.rs
--rw-r--r--   0     1001      123      118 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/ops/approx_algo/mod.rs
--rw-r--r--   0     1001      123     2016 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/ops/approx_unique.rs
--rw-r--r--   0     1001      123    11872 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/ops/arg_min_max.rs
--rw-r--r--   0     1001      123     3688 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/ops/floor_divide.rs
--rw-r--r--   0     1001      123     3423 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/ops/is_first.rs
--rw-r--r--   0     1001      123     2975 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/ops/is_unique.rs
--rw-r--r--   0     1001      123     3626 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/ops/log.rs
--rw-r--r--   0     1001      123     1106 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/ops/mod.rs
--rw-r--r--   0     1001      123     1769 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/ops/rolling.rs
--rw-r--r--   0     1001      123     7642 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/ops/search_sorted.rs
--rw-r--r--   0     1001      123     2500 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/ops/to_dummies.rs
--rw-r--r--   0     1001      123     2067 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/ops/various.rs
--rw-r--r--   0        0        0     4388 1970-01-01 00:00:00.000000 polars_lts_cpu-0.17.9/Cargo.toml
--rw-r--r--   0     1001      123       76 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/.gitignore
--rw-r--r--   0     1001      123     1055 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/LICENSE
--rw-r--r--   0     1001      123     2414 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/Makefile
--rw-r--r--   0     1001      123    12625 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/README.md
--rw-r--r--   0     1001      123      651 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/build.rs
--rw-r--r--   0     1001      123       32 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/.gitignore
--rw-r--r--   0     1001      123      679 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/Makefile
--rw-r--r--   0     1001      123      318 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/_templates/api_redirect.html
--rw-r--r--   0     1001      123      151 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/_templates/autosummary/accessor.rst
--rw-r--r--   0     1001      123      160 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/_templates/autosummary/accessor_attribute.rst
--rw-r--r--   0     1001      123      168 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/_templates/autosummary/accessor_callable.rst
--rw-r--r--   0     1001      123      157 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/_templates/autosummary/accessor_method.rst
--rw-r--r--   0     1001      123      836 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/_templates/autosummary/class.rst
--rw-r--r--   0     1001      123       94 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/_templates/autosummary/class_without_autosummary.rst
--rw-r--r--   0     1001      123      406 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/_templates/sidebar-nav-bs.html
--rw-r--r--   0     1001      123      468 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/requirements-docs.txt
--rw-r--r--   0     1001      123     1164 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/run_live_docs_server.py
--rw-r--r--   0     1001      123     1567 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/_static/css/custom.css
--rw-r--r--   0     1001      123     7326 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/conf.py
--rw-r--r--   0     1001      123       51 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/index.rst
--rw-r--r--   0     1001      123     6767 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/api.rst
--rw-r--r--   0     1001      123     1694 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/config.rst
--rw-r--r--   0     1001      123      274 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/dataframe/aggregation.rst
--rw-r--r--   0     1001      123      221 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/dataframe/attributes.rst
--rw-r--r--   0     1001      123      142 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/dataframe/computation.rst
--rw-r--r--   0     1001      123      319 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/dataframe/descriptive.rst
--rw-r--r--   0     1001      123      319 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/dataframe/export.rst
--rw-r--r--   0     1001      123      464 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/dataframe/groupby.rst
--rw-r--r--   0     1001      123      379 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/dataframe/index.rst
--rw-r--r--   0     1001      123      189 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/dataframe/miscellaneous.rst
--rw-r--r--   0     1001      123     1538 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/dataframe/modify_select.rst
--rw-r--r--   0     1001      123      663 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/datatypes.rst
--rw-r--r--   0     1001      123      421 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/exceptions.rst
--rw-r--r--   0     1001      123      391 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/expressions/aggregation.rst
--rw-r--r--   0     1001      123      309 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/expressions/binary.rst
--rw-r--r--   0     1001      123      338 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/expressions/boolean.rst
--rw-r--r--   0     1001      123      237 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/expressions/categories.rst
--rw-r--r--   0     1001      123      221 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/expressions/columns.rst
--rw-r--r--   0     1001      123     1061 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/expressions/computation.rst
--rw-r--r--   0     1001      123     1114 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/expressions/functions.rst
--rw-r--r--   0     1001      123      461 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/expressions/index.rst
--rw-r--r--   0     1001      123      695 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/expressions/list.rst
--rw-r--r--   0     1001      123      374 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/expressions/meta.rst
--rw-r--r--   0     1001      123      125 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/expressions/miscellaneous.rst
--rw-r--r--   0     1001      123      977 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/expressions/modify_select.rst
--rw-r--r--   0     1001      123      639 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/expressions/operators.rst
--rw-r--r--   0     1001      123      860 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/expressions/string.rst
--rw-r--r--   0     1001      123      254 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/expressions/struct.rst
--rw-r--r--   0     1001      123     1014 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/expressions/temporal.rst
--rw-r--r--   0     1001      123       98 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/expressions/window.rst
--rw-r--r--   0     1001      123      692 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/functions.rst
--rw-r--r--   0     1001      123      392 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/index.rst
--rw-r--r--   0     1001      123     1269 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/io.rst
--rw-r--r--   0     1001      123      252 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/lazyframe/aggregation.rst
--rw-r--r--   0     1001      123      179 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/lazyframe/attributes.rst
--rw-r--r--   0     1001      123      146 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/lazyframe/descriptive.rst
--rw-r--r--   0     1001      123      497 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/lazyframe/groupby.rst
--rw-r--r--   0     1001      123      354 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/lazyframe/index.rst
--rw-r--r--   0     1001      123      455 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/lazyframe/miscellaneous.rst
--rw-r--r--   0     1001      123     1013 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/lazyframe/modify_select.rst
--rw-r--r--   0     1001      123      358 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/series/aggregation.rst
--rw-r--r--   0     1001      123      256 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/series/attributes.rst
--rw-r--r--   0     1001      123      321 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/series/binary.rst
--rw-r--r--   0     1001      123      117 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/series/boolean.rst
--rw-r--r--   0     1001      123      241 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/series/categories.rst
--rw-r--r--   0     1001      123     1103 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/series/computation.rst
--rw-r--r--   0     1001      123      744 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/series/descriptive.rst
--rw-r--r--   0     1001      123      240 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/series/export.rst
--rw-r--r--   0     1001      123      428 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/series/index.rst
--rw-r--r--   0     1001      123      749 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/series/list.rst
--rw-r--r--   0     1001      123      236 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/series/miscellaneous.rst
--rw-r--r--   0     1001      123     1077 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/series/modify_select.rst
--rw-r--r--   0     1001      123      922 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/series/string.rst
--rw-r--r--   0     1001      123      396 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/series/struct.rst
--rw-r--r--   0     1001      123     1168 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/series/temporal.rst
--rw-r--r--   0     1001      123      302 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/sql.rst
--rw-r--r--   0     1001      123     8067 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/testing.rst
--rw-r--r--   0     1001      123      168 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/docs/source/reference/utils.rst
--rw-r--r--   0     1001      123     6335 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/__init__.py
--rw-r--r--   0     1001      123    13396 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/api.py
--rw-r--r--   0     1001      123    25956 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/config.py
--rw-r--r--   0     1001      123    27169 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/convert.py
--rw-r--r--   0     1001      123       77 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/dataframe/__init__.py
--rw-r--r--   0     1001      123     5237 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/dataframe/_html.py
--rw-r--r--   0     1001      123   305755 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/dataframe/frame.py
--rw-r--r--   0     1001      123    33240 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/dataframe/groupby.py
--rw-r--r--   0     1001      123     2668 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/datatypes/__init__.py
--rw-r--r--   0     1001      123    12133 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/datatypes/classes.py
--rw-r--r--   0     1001      123     1603 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/datatypes/constants.py
--rw-r--r--   0     1001      123     4430 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/datatypes/constructor.py
--rw-r--r--   0     1001      123    15109 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/datatypes/convert.py
--rw-r--r--   0     1001      123     7338 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/dependencies.py
--rw-r--r--   0     1001      123     3436 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/exceptions.py
--rw-r--r--   0     1001      123       61 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/expr/__init__.py
--rw-r--r--   0     1001      123     2730 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/expr/binary.py
--rw-r--r--   0     1001      123     1708 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/expr/categorical.py
--rw-r--r--   0     1001      123    73000 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/expr/datetime.py
--rw-r--r--   0     1001      123   252403 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/expr/expr.py
--rw-r--r--   0     1001      123    22899 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/expr/list.py
--rw-r--r--   0     1001      123     2059 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/expr/meta.py
--rw-r--r--   0     1001      123    44648 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/expr/string.py
--rw-r--r--   0     1001      123     5436 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/expr/struct.py
--rw-r--r--   0     1001      123     1981 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/functions/__init__.py
--rw-r--r--   0     1001      123    29637 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/functions/eager.py
--rw-r--r--   0     1001      123    90827 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/functions/lazy.py
--rw-r--r--   0     1001      123     6293 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/functions/whenthen.py
--rw-r--r--   0     1001      123      280 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/internals.py
--rw-r--r--   0     1001      123      978 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/io/__init__.py
--rw-r--r--   0     1001      123     6264 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/io/_utils.py
--rw-r--r--   0     1001      123      878 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/io/avro.py
--rw-r--r--   0     1001      123      144 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/io/csv/__init__.py
--rw-r--r--   0     1001      123     1082 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/io/csv/_utils.py
--rw-r--r--   0     1001      123     4691 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/io/csv/batched_reader.py
--rw-r--r--   0     1001      123    35533 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/io/csv/functions.py
--rw-r--r--   0     1001      123     8655 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/io/database.py
--rw-r--r--   0     1001      123    10988 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/io/delta.py
--rw-r--r--   0     1001      123       75 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/io/excel/__init__.py
--rw-r--r--   0     1001      123    18459 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/io/excel/_write_utils.py
--rw-r--r--   0     1001      123     6476 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/io/excel/functions.py
--rw-r--r--   0     1001      123      142 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/io/ipc/__init__.py
--rw-r--r--   0     1001      123     1271 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/io/ipc/anonymous_scan.py
--rw-r--r--   0     1001      123     5840 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/io/ipc/functions.py
--rw-r--r--   0     1001      123      519 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/io/json.py
--rw-r--r--   0     1001      123     2257 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/io/ndjson.py
--rw-r--r--   0     1001      123      170 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/io/parquet/__init__.py
--rw-r--r--   0     1001      123     1299 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/io/parquet/anonymous_scan.py
--rw-r--r--   0     1001      123     7212 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/io/parquet/functions.py
--rw-r--r--   0     1001      123      136 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/io/pyarrow_dataset/__init__.py
--rw-r--r--   0     1001      123     2331 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/io/pyarrow_dataset/anonymous_scan.py
--rw-r--r--   0     1001      123     3611 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/io/pyarrow_dataset/functions.py
--rw-r--r--   0     1001      123       77 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/lazyframe/__init__.py
--rw-r--r--   0     1001      123   168293 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/lazyframe/frame.py
--rw-r--r--   0     1001      123    24178 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/lazyframe/groupby.py
--rw-r--r--   0     1001      123        0 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/py.typed
--rw-r--r--   0     1001      123       69 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/series/__init__.py
--rw-r--r--   0     1001      123     1579 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/series/_numpy.py
--rw-r--r--   0     1001      123     1920 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/series/binary.py
--rw-r--r--   0     1001      123     1699 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/series/categorical.py
--rw-r--r--   0     1001      123    49664 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/series/datetime.py
--rw-r--r--   0     1001      123    12385 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/series/list.py
--rw-r--r--   0     1001      123   164038 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/series/series.py
--rw-r--r--   0     1001      123    27401 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/series/string.py
--rw-r--r--   0     1001      123     2287 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/series/struct.py
--rw-r--r--   0     1001      123     5375 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/series/utils.py
--rw-r--r--   0     1001      123     7638 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/slice.py
--rw-r--r--   0     1001      123       75 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/sql/__init__.py
--rw-r--r--   0     1001      123     1351 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/sql/context.py
--rw-r--r--   0     1001      123     4764 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/string_cache.py
--rw-r--r--   0     1001      123      362 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/testing/__init__.py
--rw-r--r--   0     1001      123      929 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/testing/_private.py
--rw-r--r--   0     1001      123     3689 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/testing/_tempdir.py
--rw-r--r--   0     1001      123    13597 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/testing/asserts.py
--rw-r--r--   0     1001      123      800 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/testing/parametric/__init__.py
--rw-r--r--   0     1001      123    26499 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/testing/parametric/primitives.py
--rw-r--r--   0     1001      123     3409 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/testing/parametric/profiles.py
--rw-r--r--   0     1001      123     7711 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/testing/parametric/strategies.py
--rw-r--r--   0     1001      123     5824 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/type_aliases.py
--rw-r--r--   0     1001      123     1211 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/utils/__init__.py
--rw-r--r--   0     1001      123    50687 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/utils/_construction.py
--rw-r--r--   0     1001      123     2782 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/utils/_parse_expr_input.py
--rw-r--r--   0     1001      123      721 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/utils/_scan.py
--rw-r--r--   0     1001      123      687 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/utils/_wrap.py
--rw-r--r--   0     1001      123      683 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/utils/build_info.py
--rw-r--r--   0     1001      123     8812 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/utils/convert.py
--rw-r--r--   0     1001      123     5789 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/utils/decorators.py
--rw-r--r--   0     1001      123     1660 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/utils/meta.py
--rw-r--r--   0     1001      123      514 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/utils/polars_version.py
--rw-r--r--   0     1001      123     2339 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/utils/show_versions.py
--rw-r--r--   0     1001      123    11592 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/polars/utils/various.py
--rw-r--r--   0     1001      123     5378 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/pyproject.toml
--rw-r--r--   0     1001      123      699 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/requirements-dev.txt
--rw-r--r--   0     1001      123       70 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/requirements-lint.txt
--rw-r--r--   0     1001      123     1640 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/scripts/check_stacklevels.py
--rw-r--r--   0     1001      123    11023 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/apply/dataframe.rs
--rw-r--r--   0     1001      123     8388 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/apply/mod.rs
--rw-r--r--   0     1001      123    71480 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/apply/series.rs
--rw-r--r--   0     1001      123       32 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/arrow_interop/mod.rs
--rw-r--r--   0     1001      123     1306 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/arrow_interop/to_py.rs
--rw-r--r--   0     1001      123     3902 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/arrow_interop/to_rust.rs
--rw-r--r--   0     1001      123     5250 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/batched_csv.rs
--rw-r--r--   0     1001      123    46606 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/conversion.rs
--rw-r--r--   0     1001      123    45472 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/dataframe.rs
--rw-r--r--   0     1001      123     3799 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/datatypes.rs
--rw-r--r--   0     1001      123     3288 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/error.rs
--rw-r--r--   0     1001      123     9482 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/file.rs
--rw-r--r--   0     1001      123     7468 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/lazy/apply.rs
--rw-r--r--   0     1001      123    33439 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/lazy/dataframe.rs
--rw-r--r--   0     1001      123    62598 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/lazy/dsl.rs
--rw-r--r--   0     1001      123     1082 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/lazy/meta.rs
--rw-r--r--   0     1001      123      727 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/lazy/mod.rs
--rw-r--r--   0     1001      123      212 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/lazy/utils.rs
--rw-r--r--   0     1001      123    20992 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/lib.rs
--rw-r--r--   0     1001      123     8642 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/npy.rs
--rw-r--r--   0     1001      123     1029 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/object.rs
--rw-r--r--   0     1001      123      122 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/prelude.rs
--rw-r--r--   0     1001      123      435 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/py_modules.rs
--rw-r--r--   0     1001      123    54535 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/series.rs
--rw-r--r--   0     1001      123     3478 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/set.rs
--rw-r--r--   0     1001      123      843 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/sql.rs
--rw-r--r--   0     1001      123     2335 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/src/utils.rs
--rw-r--r--   0     1001      123     6165 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/README.md
--rw-r--r--   0     1001      123     2189 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/benchmark/groupby-datagen.R
--rw-r--r--   0     1001      123     7945 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/benchmark/run_h2oai_benchmark.py
--rw-r--r--   0     1001      123     6530 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/benchmark/test_release.py
--rw-r--r--   0     1001      123     4589 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/docs/run_doctest.py
--rw-r--r--   0     1001      123      179 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/parametric/conftest.py
--rw-r--r--   0     1001      123     3823 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/parametric/test_dataframe.py
--rw-r--r--   0     1001      123     1692 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/parametric/test_lazyframe.py
--rw-r--r--   0     1001      123     6863 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/parametric/test_series.py
--rw-r--r--   0     1001      123     8299 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/parametric/test_testing.py
--rw-r--r--   0     1001      123        0 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/__init__.py
--rw-r--r--   0     1001      123     3382 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/conftest.py
--rw-r--r--   0     1001      123       86 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/datatypes/__init__.py
--rw-r--r--   0     1001      123      351 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/datatypes/test_binary.py
--rw-r--r--   0     1001      123     1420 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/datatypes/test_bool.py
--rw-r--r--   0     1001      123    12683 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/datatypes/test_categorical.py
--rw-r--r--   0     1001      123     2826 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/datatypes/test_decimal.py
--rw-r--r--   0     1001      123      280 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/datatypes/test_duration.py
--rw-r--r--   0     1001      123    16033 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/datatypes/test_list.py
--rw-r--r--   0     1001      123      284 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/datatypes/test_null.py
--rw-r--r--   0     1001      123     2801 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/datatypes/test_object.py
--rw-r--r--   0     1001      123    29644 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/datatypes/test_struct.py
--rw-r--r--   0     1001      123    92213 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/datatypes/test_temporal.py
--rw-r--r--   0     1001      123      418 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/datatypes/test_time.py
--rw-r--r--   0     1001      123      218 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/conftest.py
--rw-r--r--   0     1001      123       16 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/files/delta-table/.part-00000-e42312d7-60e5-454d-acbc-db192d220e73-c000.snappy.parquet.crc
--rw-r--r--   0     1001      123       16 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/files/delta-table/.part-00000-e4a999da-df45-4fb0-bdc4-d999fc0f58aa-c000.snappy.parquet.crc
--rw-r--r--   0     1001      123       16 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/files/delta-table/_delta_log/.00000000000000000000.json.crc
--rw-r--r--   0     1001      123       16 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/files/delta-table/_delta_log/.00000000000000000001.json.crc
--rw-r--r--   0     1001      123      905 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/files/delta-table/_delta_log/00000000000000000000.json
--rw-r--r--   0     1001      123      936 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/files/delta-table/_delta_log/00000000000000000001.json
--rw-r--r--   0     1001      123      972 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/files/delta-table/part-00000-e42312d7-60e5-454d-acbc-db192d220e73-c000.snappy.parquet
--rw-r--r--   0     1001      123      690 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/files/delta-table/part-00000-e4a999da-df45-4fb0-bdc4-d999fc0f58aa-c000.snappy.parquet
--rw-r--r--   0     1001      123        0 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/files/empty.csv
--rw-r--r--   0     1001      123     5959 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/files/example.xlsx
--rw-r--r--   0     1001      123      457 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/files/foods1.csv
--rw-r--r--   0     1001      123     2351 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/files/foods1.ipc
--rw-r--r--   0     1001      123     1713 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/files/foods1.ndjson
--rw-r--r--   0     1001      123     1427 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/files/foods1.parquet
--rw-r--r--   0     1001      123      455 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/files/foods2.csv
--rw-r--r--   0     1001      123     2351 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/files/foods2.ipc
--rw-r--r--   0     1001      123     1711 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/files/foods2.ndjson
--rw-r--r--   0     1001      123     1916 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/files/foods2.parquet
--rw-r--r--   0     1001      123      455 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/files/foods3.csv
--rw-r--r--   0     1001      123      457 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/files/foods4.csv
--rw-r--r--   0     1001      123      452 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/files/foods5.csv
--rw-r--r--   0     1001      123       49 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/files/gzipped.csv
--rw-r--r--   0     1001      123       57 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/files/small.csv
--rw-r--r--   0     1001      123      756 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/files/small.parquet
--rw-r--r--   0     1001      123     1937 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/test_avro.py
--rw-r--r--   0     1001      123    39498 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/test_csv.py
--rw-r--r--   0     1001      123     6360 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/test_database.py
--rw-r--r--   0     1001      123     3456 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/test_delta.py
--rw-r--r--   0     1001      123    11169 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/test_excel.py
--rw-r--r--   0     1001      123     5919 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/test_ipc.py
--rw-r--r--   0     1001      123     3720 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/test_json.py
--rw-r--r--   0     1001      123     7232 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/test_lazy_csv.py
--rw-r--r--   0     1001      123     2060 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/test_lazy_ipc.py
--rw-r--r--   0     1001      123     2881 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/test_lazy_json.py
--rw-r--r--   0     1001      123    11849 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/test_lazy_parquet.py
--rw-r--r--   0     1001      123     2012 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/test_other.py
--rw-r--r--   0     1001      123    13735 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/test_parquet.py
--rw-r--r--   0     1001      123      612 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/test_pickle.py
--rw-r--r--   0     1001      123     3259 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/io/test_pyarrow_dataset.py
--rw-r--r--   0     1001      123      509 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/namespaces/__init__.py
--rw-r--r--   0     1001      123     3218 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/namespaces/test_binary.py
--rw-r--r--   0     1001      123     2489 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/namespaces/test_categorical.py
--rw-r--r--   0     1001      123    18949 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/namespaces/test_datetime.py
--rw-r--r--   0     1001      123    12750 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/namespaces/test_list.py
--rw-r--r--   0     1001      123     1748 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/namespaces/test_meta.py
--rw-r--r--   0     1001      123    23698 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/namespaces/test_string.py
--rw-r--r--   0     1001      123    16930 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/namespaces/test_strptime.py
--rw-r--r--   0     1001      123      982 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/namespaces/test_struct.py
--rw-r--r--   0     1001      123       85 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/operations/__init__.py
--rw-r--r--   0     1001      123     6238 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/operations/test_aggregations.py
--rw-r--r--   0     1001      123     9412 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/operations/test_apply.py
--rw-r--r--   0     1001      123     4390 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/operations/test_arithmetic.py
--rw-r--r--   0     1001      123      956 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/operations/test_comparison.py
--rw-r--r--   0     1001      123     2906 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/operations/test_drop.py
--rw-r--r--   0     1001      123     8252 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/operations/test_explode.py
--rw-r--r--   0     1001      123     3664 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/operations/test_filter.py
--rw-r--r--   0     1001      123     1801 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/operations/test_folds.py
--rw-r--r--   0     1001      123    22867 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/operations/test_groupby.py
--rw-r--r--   0     1001      123     2959 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/operations/test_is_in.py
--rw-r--r--   0     1001      123    16937 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/operations/test_join.py
--rw-r--r--   0     1001      123    10891 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/operations/test_join_asof.py
--rw-r--r--   0     1001      123      643 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/operations/test_melt.py
--rw-r--r--   0     1001      123    10253 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/operations/test_pivot.py
--rw-r--r--   0     1001      123    18016 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/operations/test_rolling.py
--rw-r--r--   0     1001      123    19550 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/operations/test_sort.py
--rw-r--r--   0     1001      123     4038 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/operations/test_statistics.py
--rw-r--r--   0     1001      123     3631 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/operations/test_transpose.py
--rw-r--r--   0     1001      123      771 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/operations/test_unique.py
--rw-r--r--   0     1001      123     9814 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/operations/test_window.py
--rw-r--r--   0     1001      123     4775 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_api.py
--rw-r--r--   0     1001      123     1077 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_arity.py
--rw-r--r--   0     1001      123    19856 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_cfg.py
--rw-r--r--   0     1001      123    39286 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_constructors.py
--rw-r--r--   0     1001      123      454 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_context.py
--rw-r--r--   0     1001      123     1628 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_cse.py
--rw-r--r--   0     1001      123     3817 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_datatypes.py
--rw-r--r--   0     1001      123   121217 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_df.py
--rw-r--r--   0     1001      123     1594 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_empty.py
--rw-r--r--   0     1001      123    16914 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_errors.py
--rw-r--r--   0     1001      123     2387 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_expr_multi_cols.py
--rw-r--r--   0     1001      123    32643 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_exprs.py
--rw-r--r--   0     1001      123     3305 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_fmt.py
--rw-r--r--   0     1001      123    11420 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_functions.py
--rw-r--r--   0     1001      123     3763 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_interchange.py
--rw-r--r--   0     1001      123    33952 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_interop.py
--rw-r--r--   0     1001      123    49534 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_lazy.py
--rw-r--r--   0     1001      123     2369 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_polars_import.py
--rw-r--r--   0     1001      123     4222 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_predicates.py
--rw-r--r--   0     1001      123     7102 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_projections.py
--rw-r--r--   0     1001      123    11550 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_queries.py
--rw-r--r--   0     1001      123     4743 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_rows.py
--rw-r--r--   0     1001      123    13202 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_schema.py
--rw-r--r--   0     1001      123     2634 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_serde.py
--rw-r--r--   0     1001      123    84354 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_series.py
--rw-r--r--   0     1001      123      657 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_single.py
--rw-r--r--   0     1001      123     2561 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_sql.py
--rw-r--r--   0     1001      123    13877 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_streaming.py
--rw-r--r--   0     1001      123    10700 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/test_testing.py
--rw-r--r--   0     1001      123       41 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/utils/__init__.py
--rw-r--r--   0     1001      123      306 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/utils/test_build_info.py
--rw-r--r--   0     1001      123      247 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/utils/test_show_versions.py
--rw-r--r--   0     1001      123     4307 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/tests/unit/utils/test_utils.py
--rw-r--r--   0     1001      123    65631 2023-04-25 13:58:36.000000 polars_lts_cpu-0.17.9/Cargo.lock
--rw-r--r--   0        0        0    15163 1970-01-01 00:00:00.000000 polars_lts_cpu-0.17.9/PKG-INFO
+-rw-r--r--   0        0        0     5256 1970-01-01 00:00:00.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/Cargo.toml
+-rw-r--r--   0     1001      123     1055 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/LICENSE
+-rw-r--r--   0     1001      123       45 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/constants.rs
+-rw-r--r--   0     1001      123    17248 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dot.rs
+-rw-r--r--   0     1001      123     4171 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/arithmetic.rs
+-rw-r--r--   0     1001      123     3992 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/arity.rs
+-rw-r--r--   0     1001      123      758 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/array.rs
+-rw-r--r--   0     1001      123      935 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/binary.rs
+-rw-r--r--   0     1001      123      650 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/cat.rs
+-rw-r--r--   0     1001      123    10228 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/dt.rs
+-rw-r--r--   0     1001      123    13410 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/expr.rs
+-rw-r--r--   0     1001      123      753 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/from.rs
+-rw-r--r--   0     1001      123       85 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/abs.rs
+-rw-r--r--   0     1001      123     1431 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/arg_where.rs
+-rw-r--r--   0     1001      123      780 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/array.rs
+-rw-r--r--   0     1001      123     1327 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/binary.rs
+-rw-r--r--   0     1001      123     4221 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/boolean.rs
+-rw-r--r--   0     1001      123     1910 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/bounds.rs
+-rw-r--r--   0     1001      123     1216 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/cat.rs
+-rw-r--r--   0     1001      123      344 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/clip.rs
+-rw-r--r--   0     1001      123     1593 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/cum.rs
+-rw-r--r--   0     1001      123     9529 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/datetime.rs
+-rw-r--r--   0     1001      123      673 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/dispatch.rs
+-rw-r--r--   0     1001      123     2567 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/fill_null.rs
+-rw-r--r--   0     1001      123      992 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/fused.rs
+-rw-r--r--   0     1001      123     8119 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/list.rs
+-rw-r--r--   0     1001      123      581 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/log.rs
+-rw-r--r--   0     1001      123    20488 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/mod.rs
+-rw-r--r--   0     1001      123      462 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/nan.rs
+-rw-r--r--   0     1001      123     3132 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/pow.rs
+-rw-r--r--   0     1001      123      152 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/rolling.rs
+-rw-r--r--   0     1001      123      260 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/round.rs
+-rw-r--r--   0     1001      123      200 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/row_hash.rs
+-rw-r--r--   0     1001      123    13744 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/schema.rs
+-rw-r--r--   0     1001      123      306 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/search_sorted.rs
+-rw-r--r--   0     1001      123     3812 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/shift_and_fill.rs
+-rw-r--r--   0     1001      123     1238 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/shrink_type.rs
+-rw-r--r--   0     1001      123      972 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/sign.rs
+-rw-r--r--   0     1001      123    21037 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/strings.rs
+-rw-r--r--   0     1001      123     1017 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/struct_.rs
+-rw-r--r--   0     1001      123     5406 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/temporal.rs
+-rw-r--r--   0     1001      123     5122 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/trigonometry.rs
+-rw-r--r--   0     1001      123      170 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/unique.rs
+-rw-r--r--   0     1001      123    48049 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/functions.rs
+-rw-r--r--   0     1001      123    10213 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/list.rs
+-rw-r--r--   0     1001      123     2181 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/meta.rs
+-rw-r--r--   0     1001      123    60990 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/mod.rs
+-rw-r--r--   0     1001      123       40 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/names.rs
+-rw-r--r--   0     1001      123     2786 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/options.rs
+-rw-r--r--   0     1001      123    17095 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/string.rs
+-rw-r--r--   0     1001      123     2715 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/struct_.rs
+-rw-r--r--   0     1001      123       38 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/frame/mod.rs
+-rw-r--r--   0     1001      123      933 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/frame/opt_state.rs
+-rw-r--r--   0     1001      123      466 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/global.rs
+-rw-r--r--   0     1001      123      175 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/lib.rs
+-rw-r--r--   0     1001      123     8318 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/aexpr/mod.rs
+-rw-r--r--   0     1001      123    11653 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/aexpr/schema.rs
+-rw-r--r--   0     1001      123    25683 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/alp.rs
+-rw-r--r--   0     1001      123     1622 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/anonymous_scan.rs
+-rw-r--r--   0     1001      123     1428 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/apply.rs
+-rw-r--r--   0     1001      123    24898 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/builder.rs
+-rw-r--r--   0     1001      123    29916 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/conversion.rs
+-rw-r--r--   0     1001      123      301 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/debug.rs
+-rw-r--r--   0     1001      123    15415 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/format.rs
+-rw-r--r--   0     1001      123      895 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/functions/drop.rs
+-rw-r--r--   0     1001      123      137 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/functions/explode.rs
+-rw-r--r--   0     1001      123     1169 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/functions/merge_sorted.rs
+-rw-r--r--   0     1001      123    11905 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/functions/mod.rs
+-rw-r--r--   0     1001      123     1330 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/functions/rename.rs
+-rw-r--r--   0     1001      123     9849 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/iterator.rs
+-rw-r--r--   0     1001      123    10559 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/lit.rs
+-rw-r--r--   0     1001      123     8115 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/mod.rs
+-rw-r--r--   0     1001      123     7416 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/cache_states.rs
+-rw-r--r--   0     1001      123    15277 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/cse.rs
+-rw-r--r--   0     1001      123     3250 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/delay_rechunk.rs
+-rw-r--r--   0     1001      123     3236 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/drop_nulls.rs
+-rw-r--r--   0     1001      123     3994 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/fast_projection.rs
+-rw-r--r--   0     1001      123    14479 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/file_caching.rs
+-rw-r--r--   0     1001      123     1556 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/flatten_union.rs
+-rw-r--r--   0     1001      123     5827 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/fused.rs
+-rw-r--r--   0     1001      123     6774 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/mod.rs
+-rw-r--r--   0     1001      123     1222 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/predicate_pushdown/keys.rs
+-rw-r--r--   0     1001      123    28871 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/predicate_pushdown/mod.rs
+-rw-r--r--   0     1001      123     2571 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/predicate_pushdown/rename.rs
+-rw-r--r--   0     1001      123    15756 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/predicate_pushdown/utils.rs
+-rw-r--r--   0     1001      123     1755 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/functions/melt.rs
+-rw-r--r--   0     1001      123     3930 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/functions/mod.rs
+-rw-r--r--   0     1001      123     1799 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/generic.rs
+-rw-r--r--   0     1001      123     3269 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/groupby.rs
+-rw-r--r--   0     1001      123     2638 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/hstack.rs
+-rw-r--r--   0     1001      123    15747 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/joins.rs
+-rw-r--r--   0     1001      123    26502 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/mod.rs
+-rw-r--r--   0     1001      123     3707 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/projection.rs
+-rw-r--r--   0     1001      123     2639 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/rename.rs
+-rw-r--r--   0     1001      123     3501 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/semi_anti_join.rs
+-rw-r--r--   0     1001      123    27269 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/simplify_expr.rs
+-rw-r--r--   0     1001      123     3492 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/slice_pushdown_expr.rs
+-rw-r--r--   0     1001      123    13781 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/slice_pushdown_lp.rs
+-rw-r--r--   0     1001      123     4181 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/stack_opt.rs
+-rw-r--r--   0     1001      123     9725 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/type_coercion/binary.rs
+-rw-r--r--   0     1001      123    20215 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/type_coercion/mod.rs
+-rw-r--r--   0     1001      123     9838 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/options.rs
+-rw-r--r--   0     1001      123    15313 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/projection.rs
+-rw-r--r--   0     1001      123     5551 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/pyarrow.rs
+-rw-r--r--   0     1001      123    13042 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/schema.rs
+-rw-r--r--   0     1001      123      832 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/prelude.rs
+-rw-r--r--   0     1001      123    11872 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/utils.rs
+-rw-r--r--   0        0        0     4447 1970-01-01 00:00:00.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/Cargo.toml
+-rw-r--r--   0     1001      123     1055 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/LICENSE
+-rw-r--r--   0     1001      123      138 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/README.md
+-rw-r--r--   0     1001      123     2383 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/avro/mod.rs
+-rw-r--r--   0     1001      123     3608 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/avro/read.rs
+-rw-r--r--   0     1001      123     2622 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/avro/write.rs
+-rw-r--r--   0     1001      123     4505 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/cloud/adaptors.rs
+-rw-r--r--   0     1001      123     9506 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/cloud/glob.rs
+-rw-r--r--   0     1001      123     3089 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/cloud/mod.rs
+-rw-r--r--   0     1001      123    28419 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/csv/buffer.rs
+-rw-r--r--   0     1001      123     1898 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/csv/mod.rs
+-rw-r--r--   0     1001      123    19446 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/csv/parser.rs
+-rw-r--r--   0     1001      123    21432 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/csv/read.rs
+-rw-r--r--   0     1001      123    10846 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/csv/read_impl/batched_mmap.rs
+-rw-r--r--   0     1001      123    13938 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/csv/read_impl/batched_read.rs
+-rw-r--r--   0     1001      123    30724 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/csv/read_impl/mod.rs
+-rw-r--r--   0     1001      123    11466 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/csv/splitfields.rs
+-rw-r--r--   0     1001      123    24531 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/csv/utils.rs
+-rw-r--r--   0     1001      123     2796 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/csv/write.rs
+-rw-r--r--   0     1001      123    14759 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/csv/write_impl.rs
+-rw-r--r--   0     1001      123      184 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/export.rs
+-rw-r--r--   0     1001      123     7586 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/ipc/ipc_file.rs
+-rw-r--r--   0     1001      123     9227 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/ipc/ipc_stream.rs
+-rw-r--r--   0     1001      123     3253 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/ipc/mmap.rs
+-rw-r--r--   0     1001      123      401 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/ipc/mod.rs
+-rw-r--r--   0     1001      123     8287 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/ipc/write.rs
+-rw-r--r--   0     1001      123     1471 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/ipc/write_async.rs
+-rw-r--r--   0     1001      123    11044 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/json/mod.rs
+-rw-r--r--   0     1001      123     4859 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/lib.rs
+-rw-r--r--   0     1001      123     1969 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/mmap.rs
+-rw-r--r--   0     1001      123     7155 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/ndjson/buffer.rs
+-rw-r--r--   0     1001      123    12013 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/ndjson/core.rs
+-rw-r--r--   0     1001      123       37 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/ndjson/mod.rs
+-rw-r--r--   0     1001      123      273 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/options.rs
+-rw-r--r--   0     1001      123     7360 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/parquet/async_impl.rs
+-rw-r--r--   0     1001      123     3093 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/parquet/mmap.rs
+-rw-r--r--   0     1001      123     3132 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/parquet/mod.rs
+-rw-r--r--   0     1001      123     4784 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/parquet/predicates.rs
+-rw-r--r--   0     1001      123     9623 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/parquet/read.rs
+-rw-r--r--   0     1001      123    16886 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/parquet/read_impl.rs
+-rw-r--r--   0     1001      123    10052 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/parquet/write.rs
+-rw-r--r--   0     1001      123     5334 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/partition.rs
+-rw-r--r--   0     1001      123     1455 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/predicates.rs
+-rw-r--r--   0     1001      123      621 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/prelude.rs
+-rw-r--r--   0     1001      123      417 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/tests.rs
+-rw-r--r--   0     1001      123     4374 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/utils.rs
+-rw-r--r--   0        0        0    10716 1970-01-01 00:00:00.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/Cargo.toml
+-rw-r--r--   0     1001      123     1055 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/LICENSE
+-rw-r--r--   0     1001      123     3590 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/Makefile
+-rw-r--r--   0     1001      123      215 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/build.rs
+-rw-r--r--   0     1001      123       78 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/clippy.toml
+-rw-r--r--   0     1001      123    17602 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/src/docs/eager.rs
+-rw-r--r--   0     1001      123     8794 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/src/docs/lazy.rs
+-rw-r--r--   0     1001      123       50 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/src/docs/mod.rs
+-rw-r--r--   0     1001      123     3797 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/src/docs/performance.rs
+-rw-r--r--   0     1001      123       59 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/src/export.rs
+-rw-r--r--   0     1001      123    20213 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/src/lib.rs
+-rw-r--r--   0     1001      123      387 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/src/prelude.rs
+-rw-r--r--   0     1001      123       44 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/src/sql.rs
+-rw-r--r--   0     1001      123     4272 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/core/date_like.rs
+-rw-r--r--   0     1001      123     2401 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/core/groupby.rs
+-rw-r--r--   0     1001      123    17826 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/core/joins.rs
+-rw-r--r--   0     1001      123      545 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/core/list.rs
+-rw-r--r--   0     1001      123      198 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/core/mod.rs
+-rw-r--r--   0     1001      123       24 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/core/ops/mod.rs
+-rw-r--r--   0     1001      123      457 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/core/ops/take.rs
+-rw-r--r--   0     1001      123     6259 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/core/pivot.rs
+-rw-r--r--   0     1001      123     1102 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/core/random.rs
+-rw-r--r--   0     1001      123    10844 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/core/rolling_window.rs
+-rw-r--r--   0     1001      123     1093 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/core/series.rs
+-rw-r--r--   0     1001      123      370 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/core/utils.rs
+-rw-r--r--   0     1001      123    30423 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/io/csv.rs
+-rw-r--r--   0     1001      123     4490 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/io/ipc_stream.rs
+-rw-r--r--   0     1001      123     7043 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/io/json.rs
+-rw-r--r--   0     1001      123      378 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/io/mod.rs
+-rw-r--r--   0     1001      123      531 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/io/parquet.rs
+-rw-r--r--   0     1001      123     1530 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/joins.rs
+-rw-r--r--   0     1001      123     2452 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/aggregation.rs
+-rw-r--r--   0     1001      123      702 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/cse.rs
+-rw-r--r--   0     1001      123      500 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/explodes.rs
+-rw-r--r--   0     1001      123     2279 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/expressions/apply.rs
+-rw-r--r--   0     1001      123    10285 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/expressions/arity.rs
+-rw-r--r--   0     1001      123     1065 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/expressions/expand.rs
+-rw-r--r--   0     1001      123     1008 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/expressions/filter.rs
+-rw-r--r--   0     1001      123      428 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/expressions/is_in.rs
+-rw-r--r--   0     1001      123      121 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/expressions/mod.rs
+-rw-r--r--   0     1001      123      659 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/expressions/slice.rs
+-rw-r--r--   0     1001      123    10657 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/expressions/window.rs
+-rw-r--r--   0     1001      123      579 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/folds.rs
+-rw-r--r--   0     1001      123      557 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/functions.rs
+-rw-r--r--   0     1001      123     4482 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/groupby.rs
+-rw-r--r--   0     1001      123     1635 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/groupby_dynamic.rs
+-rw-r--r--   0     1001      123      691 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/mod.rs
+-rw-r--r--   0     1001      123     5614 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/predicate_queries.rs
+-rw-r--r--   0     1001      123     4476 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/projection_queries.rs
+-rw-r--r--   0     1001      123     6577 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/queries.rs
+-rw-r--r--   0     1001      123      141 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/main.rs
+-rw-r--r--   0     1001      123    12591 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/schema.rs
+-rw-r--r--   0        0        0     1523 1970-01-01 00:00:00.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/Cargo.toml
+-rw-r--r--   0     1001      123     1055 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/LICENSE
+-rw-r--r--   0     1001      123      144 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/README.md
+-rw-r--r--   0     1001      123     1975 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/array/default_arrays.rs
+-rw-r--r--   0     1001      123     1791 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/array/fixed_size_list.rs
+-rw-r--r--   0     1001      123     3773 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/array/get.rs
+-rw-r--r--   0     1001      123     6664 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/array/list.rs
+-rw-r--r--   0     1001      123     8165 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/array/mod.rs
+-rw-r--r--   0     1001      123      878 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/array/null.rs
+-rw-r--r--   0     1001      123     1125 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/array/slice.rs
+-rw-r--r--   0     1001      123     2252 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/array/utf8.rs
+-rw-r--r--   0     1001      123     2294 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/bit_util.rs
+-rw-r--r--   0     1001      123       17 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/bitmap/mod.rs
+-rw-r--r--   0     1001      123      819 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/bitmap/mutable.rs
+-rw-r--r--   0     1001      123      727 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/compute/bitwise.rs
+-rw-r--r--   0     1001      123     1159 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/compute/cast.rs
+-rw-r--r--   0     1001      123     4163 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/compute/decimal.rs
+-rw-r--r--   0     1001      123      138 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/compute/mod.rs
+-rw-r--r--   0     1001      123      391 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/compute/take/bitmap.rs
+-rw-r--r--   0     1001      123     2767 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/compute/take/boolean.rs
+-rw-r--r--   0     1001      123     3487 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/compute/take/fixed_size_list.rs
+-rw-r--r--   0     1001      123    25289 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/compute/take/mod.rs
+-rw-r--r--   0     1001      123      797 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/compute/tile.rs
+-rw-r--r--   0     1001      123     1042 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/conversion.rs
+-rw-r--r--   0     1001      123     1609 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/data_types.rs
+-rw-r--r--   0     1001      123       25 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/error.rs
+-rw-r--r--   0     1001      123       28 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/export.rs
+-rw-r--r--   0     1001      123       26 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/floats/mod.rs
+-rw-r--r--   0     1001      123     2066 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/floats/ord.rs
+-rw-r--r--   0     1001      123     1273 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/index.rs
+-rw-r--r--   0     1001      123      984 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/is_valid.rs
+-rw-r--r--   0     1001      123     4740 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/agg_mean.rs
+-rw-r--r--   0     1001      123     1074 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/comparison.rs
+-rw-r--r--   0     1001      123     1015 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/concatenate.rs
+-rw-r--r--   0     1001      123     5161 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/ewm/average.rs
+-rw-r--r--   0     1001      123     1808 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/ewm/mod.rs
+-rw-r--r--   0     1001      123    25065 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/ewm/variance.rs
+-rw-r--r--   0     1001      123     1406 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/float.rs
+-rw-r--r--   0     1001      123     4907 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/list.rs
+-rw-r--r--   0     1001      123     1885 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/list_bytes_iter.rs
+-rw-r--r--   0     1001      123     9783 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/mod.rs
+-rw-r--r--   0     1001      123     3703 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/mod.rs
+-rw-r--r--   0     1001      123     2022 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/no_nulls/mean.rs
+-rw-r--r--   0     1001      123    18684 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/no_nulls/min_max.rs
+-rw-r--r--   0     1001      123     3924 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/no_nulls/mod.rs
+-rw-r--r--   0     1001      123    11659 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/no_nulls/quantile.rs
+-rw-r--r--   0     1001      123     5504 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/no_nulls/sum.rs
+-rw-r--r--   0     1001      123     8683 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/no_nulls/variance.rs
+-rw-r--r--   0     1001      123     1749 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/nulls/mean.rs
+-rw-r--r--   0     1001      123    14367 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/nulls/min_max.rs
+-rw-r--r--   0     1001      123     9070 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/nulls/mod.rs
+-rw-r--r--   0     1001      123    11609 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/nulls/quantile.rs
+-rw-r--r--   0     1001      123     4698 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/nulls/sum.rs
+-rw-r--r--   0     1001      123     8335 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/nulls/variance.rs
+-rw-r--r--   0     1001      123     8109 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/window.rs
+-rw-r--r--   0     1001      123     4752 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/set.rs
+-rw-r--r--   0     1001      123     4529 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/sort_partition.rs
+-rw-r--r--   0     1001      123     2948 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/sorted_join/inner.rs
+-rw-r--r--   0     1001      123     5974 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/sorted_join/left.rs
+-rw-r--r--   0     1001      123      231 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/sorted_join/mod.rs
+-rw-r--r--   0     1001      123      841 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/string.rs
+-rw-r--r--   0     1001      123     2310 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/take_agg/boolean.rs
+-rw-r--r--   0     1001      123     4344 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/take_agg/mod.rs
+-rw-r--r--   0     1001      123     2606 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/take_agg/var.rs
+-rw-r--r--   0     1001      123     3672 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/time.rs
+-rw-r--r--   0     1001      123      341 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/lib.rs
+-rw-r--r--   0     1001      123      434 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/prelude.rs
+-rw-r--r--   0     1001      123      534 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/slice.rs
+-rw-r--r--   0     1001      123      183 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/time_zone.rs
+-rw-r--r--   0     1001      123      998 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/trusted_len/boolean.rs
+-rw-r--r--   0     1001      123     2821 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/trusted_len/mod.rs
+-rw-r--r--   0     1001      123     2052 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/trusted_len/push_unchecked.rs
+-rw-r--r--   0     1001      123      158 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/trusted_len/rev.rs
+-rw-r--r--   0     1001      123     5232 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/utils.rs
+-rw-r--r--   0        0        0      943 1970-01-01 00:00:00.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-row/Cargo.toml
+-rw-r--r--   0     1001      123     1055 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-row/LICENSE
+-rw-r--r--   0     1001      123      137 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-row/README.md
+-rw-r--r--   0     1001      123     8985 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-row/src/encode.rs
+-rw-r--r--   0     1001      123     4591 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-row/src/encodings/fixed.rs
+-rw-r--r--   0     1001      123       47 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-row/src/encodings/mod.rs
+-rw-r--r--   0     1001      123     4508 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-row/src/encodings/variable.rs
+-rw-r--r--   0     1001      123    13678 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-row/src/lib.rs
+-rw-r--r--   0     1001      123     2079 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-row/src/row.rs
+-rw-r--r--   0     1001      123      682 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-row/src/utils.rs
+-rw-r--r--   0        0        0     1104 1970-01-01 00:00:00.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-sql/Cargo.toml
+-rw-r--r--   0     1001      123     1055 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-sql/LICENSE
+-rw-r--r--   0     1001      123      466 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-sql/README.md
+-rw-r--r--   0     1001      123    22886 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-sql/src/context.rs
+-rw-r--r--   0     1001      123    20710 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-sql/src/functions.rs
+-rw-r--r--   0     1001      123     2098 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-sql/src/keywords.rs
+-rw-r--r--   0     1001      123      211 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-sql/src/lib.rs
+-rw-r--r--   0     1001      123    18227 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-sql/src/sql_expr.rs
+-rw-r--r--   0     1001      123     4572 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-sql/src/table_functions.rs
+-rw-r--r--   0     1001      123     1682 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-sql/tests/functions_cumulative.rs
+-rw-r--r--   0     1001      123     3063 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-sql/tests/functions_io.rs
+-rw-r--r--   0     1001      123     1539 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-sql/tests/functions_math.rs
+-rw-r--r--   0     1001      123      860 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-sql/tests/functions_meta.rs
+-rw-r--r--   0     1001      123     2982 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-sql/tests/functions_string.rs
+-rw-r--r--   0     1001      123     1056 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-sql/tests/iss_7436.rs
+-rw-r--r--   0     1001      123      888 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-sql/tests/iss_7437.rs
+-rw-r--r--   0     1001      123      652 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-sql/tests/iss_7440.rs
+-rw-r--r--   0     1001      123      700 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-sql/tests/iss_8395.rs
+-rw-r--r--   0     1001      123     1062 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-sql/tests/iss_8419.rs
+-rw-r--r--   0     1001      123      982 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-sql/tests/ops_distinct_on.rs
+-rw-r--r--   0     1001      123    15110 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-sql/tests/simple_exprs.rs
+-rw-r--r--   0     1001      123     2985 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-sql/tests/statements.rs
+-rw-r--r--   0        0        0     1948 1970-01-01 00:00:00.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/Cargo.toml
+-rw-r--r--   0     1001      123     1055 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/LICENSE
+-rw-r--r--   0     1001      123      165 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/README.md
+-rw-r--r--   0     1001      123       98 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/mod.rs
+-rw-r--r--   0     1001      123     1219 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/operators/filter.rs
+-rw-r--r--   0     1001      123     4103 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/operators/function.rs
+-rw-r--r--   0     1001      123      266 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/operators/mod.rs
+-rw-r--r--   0     1001      123      682 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/operators/pass.rs
+-rw-r--r--   0     1001      123      548 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/operators/placeholder.rs
+-rw-r--r--   0     1001      123     3247 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/operators/projection.rs
+-rw-r--r--   0     1001      123     2324 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/operators/reproject.rs
+-rw-r--r--   0     1001      123     6479 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/file_sink.rs
+-rw-r--r--   0     1001      123    11288 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/convert.rs
+-rw-r--r--   0     1001      123     1207 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/count.rs
+-rw-r--r--   0     1001      123     1888 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/first.rs
+-rw-r--r--   0     1001      123     4554 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/interface.rs
+-rw-r--r--   0     1001      123     1746 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/last.rs
+-rw-r--r--   0     1001      123     5413 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/mean.rs
+-rw-r--r--   0     1001      123     4951 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/min_max.rs
+-rw-r--r--   0     1001      123      211 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/mod.rs
+-rw-r--r--   0     1001      123      856 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/null.rs
+-rw-r--r--   0     1001      123     4294 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/sum.rs
+-rw-r--r--   0     1001      123     3030 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/eval.rs
+-rw-r--r--   0     1001      123     7500 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/global.rs
+-rw-r--r--   0     1001      123    13819 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/hash_table.rs
+-rw-r--r--   0     1001      123     3243 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/mod.rs
+-rw-r--r--   0     1001      123     2767 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/ooc_state.rs
+-rw-r--r--   0     1001      123     6311 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/sink.rs
+-rw-r--r--   0     1001      123     3116 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/source.rs
+-rw-r--r--   0     1001      123    11009 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/thread_local.rs
+-rw-r--r--   0     1001      123     2119 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/mod.rs
+-rw-r--r--   0     1001      123     4695 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/ooc.rs
+-rw-r--r--   0     1001      123     1887 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/ooc_state.rs
+-rw-r--r--   0     1001      123    20800 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/primitive/mod.rs
+-rw-r--r--   0     1001      123    23420 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/string.rs
+-rw-r--r--   0     1001      123     2457 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/utils.rs
+-rw-r--r--   0     1001      123     9239 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/io.rs
+-rw-r--r--   0     1001      123     5485 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/joins/cross.rs
+-rw-r--r--   0     1001      123    14279 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/joins/generic_build.rs
+-rw-r--r--   0     1001      123    11824 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/joins/inner_left.rs
+-rw-r--r--   0     1001      123      178 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/joins/mod.rs
+-rw-r--r--   0     1001      123     2241 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/memory.rs
+-rw-r--r--   0     1001      123      589 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/mod.rs
+-rw-r--r--   0     1001      123     1492 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/ordered.rs
+-rw-r--r--   0     1001      123     1824 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/reproject.rs
+-rw-r--r--   0     1001      123     3108 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/slice.rs
+-rw-r--r--   0     1001      123      130 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/sort/mod.rs
+-rw-r--r--   0     1001      123     6787 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/sort/ooc.rs
+-rw-r--r--   0     1001      123     7279 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/sort/sink.rs
+-rw-r--r--   0     1001      123     5953 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/sort/sink_multiple.rs
+-rw-r--r--   0     1001      123     3908 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/sort/source.rs
+-rw-r--r--   0     1001      123      635 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/utils.rs
+-rw-r--r--   0     1001      123     5076 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sources/csv.rs
+-rw-r--r--   0     1001      123     1231 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sources/frame.rs
+-rw-r--r--   0     1001      123      987 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sources/ipc_one_shot.rs
+-rw-r--r--   0     1001      123      366 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sources/mod.rs
+-rw-r--r--   0     1001      123     3387 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sources/parquet.rs
+-rw-r--r--   0     1001      123     1146 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sources/reproject.rs
+-rw-r--r--   0     1001      123     1022 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sources/union.rs
+-rw-r--r--   0     1001      123      448 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/expressions.rs
+-rw-r--r--   0     1001      123      272 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/lib.rs
+-rw-r--r--   0     1001      123      719 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/operators/chunks.rs
+-rw-r--r--   0     1001      123      474 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/operators/context.rs
+-rw-r--r--   0     1001      123      223 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/operators/mod.rs
+-rw-r--r--   0     1001      123      514 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/operators/operator.rs
+-rw-r--r--   0     1001      123      626 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/operators/sink.rs
+-rw-r--r--   0     1001      123      241 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/operators/source.rs
+-rw-r--r--   0     1001      123        1 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/pipeline/config.rs
+-rw-r--r--   0     1001      123    21311 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/pipeline/convert.rs
+-rw-r--r--   0     1001      123    18204 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/pipeline/dispatcher.rs
+-rw-r--r--   0     1001      123     1155 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/pipeline/mod.rs
+-rw-r--r--   0        0        0      554 1970-01-01 00:00:00.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-utils/Cargo.toml
+-rw-r--r--   0     1001      123     1055 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-utils/LICENSE
+-rw-r--r--   0     1001      123      141 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-utils/README.md
+-rw-r--r--   0     1001      123      151 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/aliases.rs
+-rw-r--r--   0     1001      123     2862 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/arena.rs
+-rw-r--r--   0     1001      123     1373 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/atomic.rs
+-rw-r--r--   0     1001      123     2659 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/cell.rs
+-rw-r--r--   0     1001      123     1015 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/contention_pool.rs
+-rw-r--r--   0     1001      123      509 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/error.rs
+-rw-r--r--   0     1001      123      271 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/fmt.rs
+-rw-r--r--   0     1001      123      763 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/functions.rs
+-rw-r--r--   0     1001      123      514 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/hash.rs
+-rw-r--r--   0     1001      123     2709 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/iter/enumerate_idx.rs
+-rw-r--r--   0     1001      123       61 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/iter/mod.rs
+-rw-r--r--   0     1001      123      600 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/lib.rs
+-rw-r--r--   0     1001      123      573 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/macros.rs
+-rw-r--r--   0     1001      123      282 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/mem.rs
+-rw-r--r--   0     1001      123     2336 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/slice.rs
+-rw-r--r--   0     1001      123     2467 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/sort.rs
+-rw-r--r--   0     1001      123     1115 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/sync.rs
+-rw-r--r--   0     1001      123      504 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/sys.rs
+-rw-r--r--   0     1001      123      697 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/unwrap.rs
+-rw-r--r--   0     1001      123      616 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/wasm.rs
+-rw-r--r--   0        0        0     6173 1970-01-01 00:00:00.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/Cargo.toml
+-rw-r--r--   0     1001      123     1055 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/LICENSE
+-rw-r--r--   0     1001      123      358 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/README.md
+-rw-r--r--   0     1001      123     1796 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/dot.rs
+-rw-r--r--   0     1001      123     4479 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/dsl/eval.rs
+-rw-r--r--   0     1001      123     4505 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/dsl/functions.rs
+-rw-r--r--   0     1001      123      164 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/dsl/into.rs
+-rw-r--r--   0     1001      123     6754 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/dsl/list.rs
+-rw-r--r--   0     1001      123     2899 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/dsl/mod.rs
+-rw-r--r--   0     1001      123     1182 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/frame/anonymous_scan.rs
+-rw-r--r--   0     1001      123     9278 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/frame/csv.rs
+-rw-r--r--   0     1001      123     4309 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/frame/file_list_reader.rs
+-rw-r--r--   0     1001      123     2261 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/frame/ipc.rs
+-rw-r--r--   0     1001      123    48361 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/frame/mod.rs
+-rw-r--r--   0     1001      123     3414 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/frame/ndjson.rs
+-rw-r--r--   0     1001      123     2734 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/frame/parquet.rs
+-rw-r--r--   0     1001      123     2892 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/frame/pivot.rs
+-rw-r--r--   0     1001      123      416 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/frame/python.rs
+-rw-r--r--   0     1001      123     6376 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/lib.rs
+-rw-r--r--   0     1001      123     1049 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/cache.rs
+-rw-r--r--   0     1001      123      776 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/executor.rs
+-rw-r--r--   0     1001      123      670 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/ext_context.rs
+-rw-r--r--   0     1001      123     1555 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/filter.rs
+-rw-r--r--   0     1001      123     3986 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/groupby.rs
+-rw-r--r--   0     1001      123     4125 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/groupby_dynamic.rs
+-rw-r--r--   0     1001      123    13503 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/groupby_partitioned.rs
+-rw-r--r--   0     1001      123     4883 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/groupby_rolling.rs
+-rw-r--r--   0     1001      123     6058 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/join.rs
+-rw-r--r--   0     1001      123     6753 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/mod.rs
+-rw-r--r--   0     1001      123     2045 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/projection.rs
+-rw-r--r--   0     1001      123     1677 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/python_scan.rs
+-rw-r--r--   0     1001      123     2854 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/scan/csv.rs
+-rw-r--r--   0     1001      123     1963 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/scan/ipc.rs
+-rw-r--r--   0     1001      123     4303 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/scan/mod.rs
+-rw-r--r--   0     1001      123     1209 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/scan/ndjson.rs
+-rw-r--r--   0     1001      123     2421 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/scan/parquet.rs
+-rw-r--r--   0     1001      123      548 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/slice.rs
+-rw-r--r--   0     1001      123     2197 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/sort.rs
+-rw-r--r--   0     1001      123     2015 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/stack.rs
+-rw-r--r--   0     1001      123      663 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/udf.rs
+-rw-r--r--   0     1001      123     3897 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/union.rs
+-rw-r--r--   0     1001      123      838 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/unique.rs
+-rw-r--r--   0     1001      123     1309 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/exotic.rs
+-rw-r--r--   0     1001      123    21959 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/aggregation.rs
+-rw-r--r--   0     1001      123     2689 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/alias.rs
+-rw-r--r--   0     1001      123    18036 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/apply.rs
+-rw-r--r--   0     1001      123    17104 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/binary.rs
+-rw-r--r--   0     1001      123     2583 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/cache.rs
+-rw-r--r--   0     1001      123     3153 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/cast.rs
+-rw-r--r--   0     1001      123     6326 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/column.rs
+-rw-r--r--   0     1001      123     1996 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/count.rs
+-rw-r--r--   0     1001      123     5809 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/filter.rs
+-rw-r--r--   0     1001      123     4131 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/group_iter.rs
+-rw-r--r--   0     1001      123     5304 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/literal.rs
+-rw-r--r--   0     1001      123    23159 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/mod.rs
+-rw-r--r--   0     1001      123    10091 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/slice.rs
+-rw-r--r--   0     1001      123     4332 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/sort.rs
+-rw-r--r--   0     1001      123    13127 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/sortby.rs
+-rw-r--r--   0     1001      123     8331 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/take.rs
+-rw-r--r--   0     1001      123    14360 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/ternary.rs
+-rw-r--r--   0     1001      123    31384 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/window.rs
+-rw-r--r--   0     1001      123     2039 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/file_cache.rs
+-rw-r--r--   0     1001      123      414 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/mod.rs
+-rw-r--r--   0     1001      123     2005 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/node_timer.rs
+-rw-r--r--   0     1001      123    23938 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/planner/expr.rs
+-rw-r--r--   0     1001      123    20614 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/planner/lp.rs
+-rw-r--r--   0     1001      123       87 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/planner/mod.rs
+-rw-r--r--   0     1001      123     9637 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/state.rs
+-rw-r--r--   0     1001      123     2463 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/streaming/checks.rs
+-rw-r--r--   0     1001      123     9100 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/streaming/construct_pipeline.rs
+-rw-r--r--   0     1001      123    15834 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/streaming/convert_alp.rs
+-rw-r--r--   0     1001      123      116 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/streaming/mod.rs
+-rw-r--r--   0     1001      123     5754 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/streaming/tree.rs
+-rw-r--r--   0     1001      123      722 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/prelude.rs
+-rw-r--r--   0     1001      123    14990 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/tests/aggregations.rs
+-rw-r--r--   0     1001      123     2339 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/tests/arity.rs
+-rw-r--r--   0     1001      123     7031 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/tests/cse.rs
+-rw-r--r--   0     1001      123    12759 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/tests/io.rs
+-rw-r--r--   0     1001      123     4166 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/tests/logical.rs
+-rw-r--r--   0     1001      123     4273 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/tests/mod.rs
+-rw-r--r--   0     1001      123    15543 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/tests/optimization_checks.rs
+-rw-r--r--   0     1001      123     6758 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/tests/predicate_queries.rs
+-rw-r--r--   0     1001      123     3158 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/tests/projection_queries.rs
+-rw-r--r--   0     1001      123    47673 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/tests/queries.rs
+-rw-r--r--   0     1001      123     9513 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/tests/streaming.rs
+-rw-r--r--   0     1001      123     2886 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/tests/tpch.rs
+-rw-r--r--   0     1001      123     1028 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/utils.rs
+-rw-r--r--   0        0        0     5546 1970-01-01 00:00:00.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/Cargo.toml
+-rw-r--r--   0     1001      123     1055 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/LICENSE
+-rw-r--r--   0     1001      123      144 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/README.md
+-rw-r--r--   0     1001      123    18769 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/arithmetic.rs
+-rw-r--r--   0     1001      123     2496 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/array/iterator.rs
+-rw-r--r--   0     1001      123     2551 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/array/mod.rs
+-rw-r--r--   0     1001      123     6448 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/bitwise.rs
+-rw-r--r--   0     1001      123     2298 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/builder/binary.rs
+-rw-r--r--   0     1001      123     1207 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/builder/boolean.rs
+-rw-r--r--   0     1001      123     4311 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/builder/fixed_size_list.rs
+-rw-r--r--   0     1001      123     1556 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/builder/from.rs
+-rw-r--r--   0     1001      123    20366 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/builder/list.rs
+-rw-r--r--   0     1001      123     8969 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/builder/mod.rs
+-rw-r--r--   0     1001      123     1410 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/builder/primitive.rs
+-rw-r--r--   0     1001      123     2291 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/builder/utf8.rs
+-rw-r--r--   0     1001      123    16487 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/cast.rs
+-rw-r--r--   0     1001      123    48418 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/comparison/mod.rs
+-rw-r--r--   0     1001      123    10060 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/comparison/scalar.rs
+-rw-r--r--   0     1001      123      551 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/drop.rs
+-rw-r--r--   0     1001      123      963 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/float.rs
+-rw-r--r--   0     1001      123     6859 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/from.rs
+-rw-r--r--   0     1001      123    42339 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/iterator/mod.rs
+-rw-r--r--   0     1001      123     1453 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/iterator/par/list.rs
+-rw-r--r--   0     1001      123       28 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/iterator/par/mod.rs
+-rw-r--r--   0     1001      123     1129 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/iterator/par/utf8.rs
+-rw-r--r--   0     1001      123       21 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/kernels/mod.rs
+-rw-r--r--   0     1001      123     2347 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/kernels/take.rs
+-rw-r--r--   0     1001      123     8111 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/list/iterator.rs
+-rw-r--r--   0     1001      123     3274 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/list/mod.rs
+-rw-r--r--   0     1001      123    19866 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/categorical/builder.rs
+-rw-r--r--   0     1001      123     3688 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/categorical/from.rs
+-rw-r--r--   0     1001      123     4270 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/categorical/merge.rs
+-rw-r--r--   0     1001      123    10219 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/categorical/mod.rs
+-rw-r--r--   0     1001      123     1400 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/categorical/ops/append.rs
+-rw-r--r--   0     1001      123      358 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/categorical/ops/full.rs
+-rw-r--r--   0     1001      123      192 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/categorical/ops/mod.rs
+-rw-r--r--   0     1001      123     2731 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/categorical/ops/take_random.rs
+-rw-r--r--   0     1001      123     2172 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/categorical/ops/unique.rs
+-rw-r--r--   0     1001      123      925 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/categorical/ops/zip.rs
+-rw-r--r--   0     1001      123     6453 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/categorical/stringcache.rs
+-rw-r--r--   0     1001      123     1604 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/date.rs
+-rw-r--r--   0     1001      123     4105 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/datetime.rs
+-rw-r--r--   0     1001      123     4443 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/decimal.rs
+-rw-r--r--   0     1001      123     2434 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/duration.rs
+-rw-r--r--   0     1001      123     2549 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/mod.rs
+-rw-r--r--   0     1001      123      476 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/struct_/from.rs
+-rw-r--r--   0     1001      123    15081 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/struct_/mod.rs
+-rw-r--r--   0     1001      123     1182 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/time.rs
+-rw-r--r--   0     1001      123    23385 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/mod.rs
+-rw-r--r--   0     1001      123     7200 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ndarray.rs
+-rw-r--r--   0     1001      123     4484 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/object/builder.rs
+-rw-r--r--   0     1001      123     1547 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/object/extension/drop.rs
+-rw-r--r--   0     1001      123     3124 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/object/extension/list.rs
+-rw-r--r--   0     1001      123     7054 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/object/extension/mod.rs
+-rw-r--r--   0     1001      123     3410 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/object/extension/polars_extension.rs
+-rw-r--r--   0     1001      123      137 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/object/is_valid.rs
+-rw-r--r--   0     1001      123     4419 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/object/iterator.rs
+-rw-r--r--   0     1001      123     4826 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/object/mod.rs
+-rw-r--r--   0     1001      123     2853 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/object/registry.rs
+-rw-r--r--   0     1001      123      272 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/abs.rs
+-rw-r--r--   0     1001      123    32691 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/aggregate/mod.rs
+-rw-r--r--   0     1001      123    10025 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/aggregate/quantile.rs
+-rw-r--r--   0     1001      123     2880 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/aggregate/var.rs
+-rw-r--r--   0     1001      123    10551 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/any_value.rs
+-rw-r--r--   0     1001      123     2820 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/append.rs
+-rw-r--r--   0     1001      123    28256 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/apply.rs
+-rw-r--r--   0     1001      123    12799 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/bit_repr.rs
+-rw-r--r--   0     1001      123     6214 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/chunkops.rs
+-rw-r--r--   0     1001      123    11537 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/compare_inner.rs
+-rw-r--r--   0     1001      123     1737 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/concat_str.rs
+-rw-r--r--   0     1001      123     4801 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/cum_agg.rs
+-rw-r--r--   0     1001      123     1127 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/decimal.rs
+-rw-r--r--   0     1001      123     7056 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/downcast.rs
+-rw-r--r--   0     1001      123    27356 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/explode.rs
+-rw-r--r--   0     1001      123     8866 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/extend.rs
+-rw-r--r--   0     1001      123    13777 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/fill_null.rs
+-rw-r--r--   0     1001      123     6323 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/filter.rs
+-rw-r--r--   0     1001      123     5876 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/full.rs
+-rw-r--r--   0     1001      123        1 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/interpolate.rs
+-rw-r--r--   0     1001      123    15385 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/is_in.rs
+-rw-r--r--   0     1001      123        1 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/len.rs
+-rw-r--r--   0     1001      123     2658 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/min_max_binary.rs
+-rw-r--r--   0     1001      123    23220 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/mod.rs
+-rw-r--r--   0     1001      123     2403 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/nulls.rs
+-rw-r--r--   0     1001      123      593 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/peaks.rs
+-rw-r--r--   0     1001      123     4375 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/repeat_by.rs
+-rw-r--r--   0     1001      123     2771 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/reverse.rs
+-rw-r--r--   0     1001      123    10234 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/rolling_window.rs
+-rw-r--r--   0     1001      123    12518 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/set.rs
+-rw-r--r--   0     1001      123     7391 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/shift.rs
+-rw-r--r--   0     1001      123     2299 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/sort/arg_sort.rs
+-rw-r--r--   0     1001      123     5528 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/sort/arg_sort_multiple.rs
+-rw-r--r--   0     1001      123     7543 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/sort/categorical.rs
+-rw-r--r--   0     1001      123    31192 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/sort/mod.rs
+-rw-r--r--   0     1001      123      380 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/sort/slice.rs
+-rw-r--r--   0     1001      123    22078 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/take/mod.rs
+-rw-r--r--   0     1001      123     7848 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/take/take_chunked.rs
+-rw-r--r--   0     1001      123      301 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/take/take_every.rs
+-rw-r--r--   0     1001      123    16256 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/take/take_random.rs
+-rw-r--r--   0     1001      123     5810 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/take/take_single.rs
+-rw-r--r--   0     1001      123     6064 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/take/traits.rs
+-rw-r--r--   0     1001      123      459 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/tile.rs
+-rw-r--r--   0     1001      123    11241 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/unique/mod.rs
+-rw-r--r--   0     1001      123    14620 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/unique/rank.rs
+-rw-r--r--   0     1001      123     8427 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/zip.rs
+-rw-r--r--   0     1001      123     9093 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/random.rs
+-rw-r--r--   0     1001      123     1959 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/temporal/conversion.rs
+-rw-r--r--   0     1001      123     2826 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/temporal/date.rs
+-rw-r--r--   0     1001      123    10497 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/temporal/datetime.rs
+-rw-r--r--   0     1001      123     3201 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/temporal/duration.rs
+-rw-r--r--   0     1001      123      595 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/temporal/mod.rs
+-rw-r--r--   0     1001      123     3042 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/temporal/time.rs
+-rw-r--r--   0     1001      123      872 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/to_vec.rs
+-rw-r--r--   0     1001      123     8113 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/trusted_len.rs
+-rw-r--r--   0     1001      123    25931 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/upstream_traits.rs
+-rw-r--r--   0     1001      123     7689 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/cloud.rs
+-rw-r--r--   0     1001      123     1549 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/config.rs
+-rw-r--r--   0     1001      123     3946 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/datatypes/_serde.rs
+-rw-r--r--   0     1001      123     2509 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/datatypes/aliases.rs
+-rw-r--r--   0     1001      123    42690 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/datatypes/any_value.rs
+-rw-r--r--   0     1001      123    13256 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/datatypes/dtype.rs
+-rw-r--r--   0     1001      123     5609 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/datatypes/field.rs
+-rw-r--r--   0     1001      123     8059 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/datatypes/mod.rs
+-rw-r--r--   0     1001      123     2016 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/datatypes/time_unit.rs
+-rw-r--r--   0     1001      123      118 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/doc/changelog/mod.rs
+-rw-r--r--   0     1001      123      898 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/doc/changelog/v0_10_0_11.rs
+-rw-r--r--   0     1001      123      481 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/doc/changelog/v0_3.rs
+-rw-r--r--   0     1001      123      293 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/doc/changelog/v0_4.rs
+-rw-r--r--   0     1001      123      499 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/doc/changelog/v0_5.rs
+-rw-r--r--   0     1001      123      288 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/doc/changelog/v0_6.rs
+-rw-r--r--   0     1001      123     1071 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/doc/changelog/v0_7.rs
+-rw-r--r--   0     1001      123      819 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/doc/changelog/v0_8.rs
+-rw-r--r--   0     1001      123      596 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/doc/changelog/v0_9.rs
+-rw-r--r--   0     1001      123       43 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/doc/mod.rs
+-rw-r--r--   0     1001      123       25 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/error.rs
+-rw-r--r--   0     1001      123      433 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/export.rs
+-rw-r--r--   0     1001      123    36432 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/fmt.rs
+-rw-r--r--   0     1001      123     5177 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/arithmetic.rs
+-rw-r--r--   0     1001      123     9916 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/asof_join/asof.rs
+-rw-r--r--   0     1001      123    35761 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/asof_join/groups.rs
+-rw-r--r--   0     1001      123     6973 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/asof_join/mod.rs
+-rw-r--r--   0     1001      123      559 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/chunks.rs
+-rw-r--r--   0     1001      123     5181 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/cross_join.rs
+-rw-r--r--   0     1001      123    16609 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/explode.rs
+-rw-r--r--   0     1001      123     1019 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/from.rs
+-rw-r--r--   0     1001      123    19219 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/groupby/aggregations/agg_list.rs
+-rw-r--r--   0     1001      123     4113 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/groupby/aggregations/boolean.rs
+-rw-r--r--   0     1001      123     7749 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/groupby/aggregations/dispatch.rs
+-rw-r--r--   0     1001      123    39253 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/groupby/aggregations/mod.rs
+-rw-r--r--   0     1001      123     5634 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/groupby/aggregations/utf8.rs
+-rw-r--r--   0     1001      123      218 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/groupby/expr.rs
+-rw-r--r--   0     1001      123    22943 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/groupby/hashing.rs
+-rw-r--r--   0     1001      123    14419 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/groupby/into_groups.rs
+-rw-r--r--   0     1001      123    39508 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/groupby/mod.rs
+-rw-r--r--   0     1001      123    10608 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/groupby/perfect.rs
+-rw-r--r--   0     1001      123    19876 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/groupby/proxy.rs
+-rw-r--r--   0     1001      123    14800 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/hash_join/mod.rs
+-rw-r--r--   0     1001      123    22392 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/hash_join/multiple_keys.rs
+-rw-r--r--   0     1001      123     2413 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/hash_join/single_keys.rs
+-rw-r--r--   0     1001      123    16352 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/hash_join/single_keys_dispatch.rs
+-rw-r--r--   0     1001      123     4295 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/hash_join/single_keys_inner.rs
+-rw-r--r--   0     1001      123     6076 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/hash_join/single_keys_left.rs
+-rw-r--r--   0     1001      123     4247 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/hash_join/single_keys_outer.rs
+-rw-r--r--   0     1001      123     3913 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/hash_join/single_keys_semi_anti.rs
+-rw-r--r--   0     1001      123    11884 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/hash_join/sort_merge.rs
+-rw-r--r--   0     1001      123     3865 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/hash_join/zip_outer.rs
+-rw-r--r--   0     1001      123   124502 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/mod.rs
+-rw-r--r--   0     1001      123    27652 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/row/av_buffer.rs
+-rw-r--r--   0     1001      123     3732 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/row/dataframe.rs
+-rw-r--r--   0     1001      123     5976 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/row/mod.rs
+-rw-r--r--   0     1001      123     9875 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/row/transpose.rs
+-rw-r--r--   0     1001      123     2811 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/top_k.rs
+-rw-r--r--   0     1001      123     1388 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/upstream_traits.rs
+-rw-r--r--   0     1001      123    10198 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/functions.rs
+-rw-r--r--   0     1001      123     2149 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/hashing/fx.rs
+-rw-r--r--   0     1001      123     1503 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/hashing/identity.rs
+-rw-r--r--   0     1001      123      457 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/hashing/mod.rs
+-rw-r--r--   0     1001      123     2684 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/hashing/partition.rs
+-rw-r--r--   0     1001      123    17653 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/hashing/vector_hasher.rs
+-rw-r--r--   0     1001      123     1896 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/lib.rs
+-rw-r--r--   0     1001      123    15733 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/named_from.rs
+-rw-r--r--   0     1001      123     2411 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/prelude.rs
+-rw-r--r--   0     1001      123    17015 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/schema.rs
+-rw-r--r--   0     1001      123     4218 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/serde/chunked_array.rs
+-rw-r--r--   0     1001      123     1094 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/serde/df.rs
+-rw-r--r--   0     1001      123     6559 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/serde/mod.rs
+-rw-r--r--   0     1001      123     9929 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/serde/series.rs
+-rw-r--r--   0     1001      123    18526 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/any_value.rs
+-rw-r--r--   0     1001      123    28755 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/arithmetic/borrowed.rs
+-rw-r--r--   0     1001      123      222 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/arithmetic/mod.rs
+-rw-r--r--   0     1001      123     3546 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/arithmetic/owned.rs
+-rw-r--r--   0     1001      123    19293 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/comparison.rs
+-rw-r--r--   0     1001      123    26164 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/from.rs
+-rw-r--r--   0     1001      123     6112 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/array.rs
+-rw-r--r--   0     1001      123     9121 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/binary.rs
+-rw-r--r--   0     1001      123    10867 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/boolean.rs
+-rw-r--r--   0     1001      123    12832 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/categorical.rs
+-rw-r--r--   0     1001      123    18254 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/dates_time.rs
+-rw-r--r--   0     1001      123    15066 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/datetime.rs
+-rw-r--r--   0     1001      123     5656 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/decimal.rs
+-rw-r--r--   0     1001      123    14766 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/duration.rs
+-rw-r--r--   0     1001      123    14103 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/floats.rs
+-rw-r--r--   0     1001      123     6110 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/list.rs
+-rw-r--r--   0     1001      123    18436 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/mod.rs
+-rw-r--r--   0     1001      123     5208 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/null.rs
+-rw-r--r--   0     1001      123     7907 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/object.rs
+-rw-r--r--   0     1001      123    11788 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/struct_.rs
+-rw-r--r--   0     1001      123     9639 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/utf8.rs
+-rw-r--r--   0     1001      123     4471 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/into.rs
+-rw-r--r--   0     1001      123     6297 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/iterator.rs
+-rw-r--r--   0     1001      123    38157 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/mod.rs
+-rw-r--r--   0     1001      123      853 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/ops/diff.rs
+-rw-r--r--   0     1001      123     5814 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/ops/downcast.rs
+-rw-r--r--   0     1001      123     3601 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/ops/ewm.rs
+-rw-r--r--   0     1001      123      413 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/ops/extend.rs
+-rw-r--r--   0     1001      123      562 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/ops/mod.rs
+-rw-r--r--   0     1001      123     5974 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/ops/moment.rs
+-rw-r--r--   0     1001      123     2908 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/ops/null.rs
+-rw-r--r--   0     1001      123     1347 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/ops/pct_change.rs
+-rw-r--r--   0     1001      123     4620 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/ops/round.rs
+-rw-r--r--   0     1001      123     5073 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/ops/to_list.rs
+-rw-r--r--   0     1001      123     1476 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/ops/unique.rs
+-rw-r--r--   0     1001      123    18440 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/series_trait.rs
+-rw-r--r--   0     1001      123     2940 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/unstable.rs
+-rw-r--r--   0     1001      123     7077 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/testing.rs
+-rw-r--r--   0     1001      123      508 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/tests.rs
+-rw-r--r--   0     1001      123     2492 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/utils/flatten.rs
+-rw-r--r--   0     1001      123    30764 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/utils/mod.rs
+-rw-r--r--   0     1001      123     1600 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/utils/series.rs
+-rw-r--r--   0     1001      123    13257 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/utils/supertype.rs
+-rw-r--r--   0        0        0      823 1970-01-01 00:00:00.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-algo/Cargo.toml
+-rw-r--r--   0     1001      123     1055 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-algo/LICENSE
+-rw-r--r--   0     1001      123      142 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-algo/README.md
+-rw-r--r--   0     1001      123     7548 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-algo/src/algo.rs
+-rw-r--r--   0     1001      123       88 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-algo/src/lib.rs
+-rw-r--r--   0     1001      123       28 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-algo/src/prelude.rs
+-rw-r--r--   0        0        0     3437 1970-01-01 00:00:00.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/Cargo.toml
+-rw-r--r--   0     1001      123     1055 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/LICENSE
+-rw-r--r--   0     1001      123      132 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/README.md
+-rw-r--r--   0     1001      123     2382 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/array/min_max.rs
+-rw-r--r--   0     1001      123      267 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/array/mod.rs
+-rw-r--r--   0     1001      123     1188 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/array/namespace.rs
+-rw-r--r--   0     1001      123     4108 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/array/sum_mean.rs
+-rw-r--r--   0     1001      123      234 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/binary/mod.rs
+-rw-r--r--   0     1001      123     3549 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/binary/namespace.rs
+-rw-r--r--   0     1001      123    11023 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/interpolate.rs
+-rw-r--r--   0     1001      123     1679 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/list/count.rs
+-rw-r--r--   0     1001      123     2452 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/list/hash.rs
+-rw-r--r--   0     1001      123     7861 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/list/min_max.rs
+-rw-r--r--   0     1001      123      511 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/list/mod.rs
+-rw-r--r--   0     1001      123    19010 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/list/namespace.rs
+-rw-r--r--   0     1001      123     7633 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/list/sum_mean.rs
+-rw-r--r--   0     1001      123     2435 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/list/to_struct.rs
+-rw-r--r--   0     1001      123      545 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/mod.rs
+-rw-r--r--   0     1001      123     9380 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/nan_propagating_aggregate.rs
+-rw-r--r--   0     1001      123     6795 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/set.rs
+-rw-r--r--   0     1001      123     7490 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/strings/case.rs
+-rw-r--r--   0     1001      123     8409 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/strings/json_path.rs
+-rw-r--r--   0     1001      123     2345 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/strings/justify.rs
+-rw-r--r--   0     1001      123      514 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/strings/mod.rs
+-rw-r--r--   0     1001      123    14731 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/strings/namespace.rs
+-rw-r--r--   0     1001      123     4053 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/strings/replace.rs
+-rw-r--r--   0     1001      123      439 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/sum.rs
+-rw-r--r--   0     1001      123     2486 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/top_k.rs
+-rw-r--r--   0     1001      123     7727 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/frame/join/merge_sorted.rs
+-rw-r--r--   0     1001      123    18435 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/frame/join/mod.rs
+-rw-r--r--   0     1001      123     4174 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/frame/mod.rs
+-rw-r--r--   0     1001      123    10257 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/frame/pivot/mod.rs
+-rw-r--r--   0     1001      123    13486 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/frame/pivot/positioning.rs
+-rw-r--r--   0     1001      123      237 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/lib.rs
+-rw-r--r--   0     1001      123      290 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/prelude.rs
+-rw-r--r--   0     1001      123       25 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/mod.rs
+-rw-r--r--   0     1001      123     9623 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/approx_algo/hyperloglogplus.rs
+-rw-r--r--   0     1001      123      118 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/approx_algo/mod.rs
+-rw-r--r--   0     1001      123     2016 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/approx_unique.rs
+-rw-r--r--   0     1001      123    11866 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/arg_min_max.rs
+-rw-r--r--   0     1001      123     3688 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/floor_divide.rs
+-rw-r--r--   0     1001      123     5245 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/fused.rs
+-rw-r--r--   0     1001      123     3423 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/is_first.rs
+-rw-r--r--   0     1001      123     2975 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/is_unique.rs
+-rw-r--r--   0     1001      123     3626 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/log.rs
+-rw-r--r--   0     1001      123     1187 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/mod.rs
+-rw-r--r--   0     1001      123     1769 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/rolling.rs
+-rw-r--r--   0     1001      123     7642 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/search_sorted.rs
+-rw-r--r--   0     1001      123     2500 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/to_dummies.rs
+-rw-r--r--   0     1001      123     2067 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/various.rs
+-rw-r--r--   0        0        0     1342 1970-01-01 00:00:00.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-json/Cargo.toml
+-rw-r--r--   0     1001      123    16716 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-json/src/json/deserialize.rs
+-rw-r--r--   0     1001      123     6468 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-json/src/json/infer_schema.rs
+-rw-r--r--   0     1001      123      189 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-json/src/json/mod.rs
+-rw-r--r--   0     1001      123       30 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-json/src/lib.rs
+-rw-r--r--   0     1001      123     1117 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-json/src/ndjson/deserialize.rs
+-rw-r--r--   0     1001      123     4808 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-json/src/ndjson/file.rs
+-rw-r--r--   0     1001      123      143 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-json/src/ndjson/mod.rs
+-rw-r--r--   0        0        0     2059 1970-01-01 00:00:00.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/Cargo.toml
+-rw-r--r--   0     1001      123     1055 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/LICENSE
+-rw-r--r--   0     1001      123      143 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/README.md
+-rw-r--r--   0     1001      123     3565 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/date.rs
+-rw-r--r--   0     1001      123     6465 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/datetime.rs
+-rw-r--r--   0     1001      123     3305 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/duration.rs
+-rw-r--r--   0     1001      123     5607 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/kernels.rs
+-rw-r--r--   0     1001      123     1062 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/mod.rs
+-rw-r--r--   0     1001      123     7302 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/rolling_window/floats.rs
+-rw-r--r--   0     1001      123     2582 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/rolling_window/ints.rs
+-rw-r--r--   0     1001      123    10495 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/rolling_window/mod.rs
+-rw-r--r--   0     1001      123      428 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/rolling_window/rolling_kernels/mod.rs
+-rw-r--r--   0     1001      123     5987 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/rolling_window/rolling_kernels/no_nulls.rs
+-rw-r--r--   0     1001      123     2372 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/time.rs
+-rw-r--r--   0     1001      123    18180 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/utf8/infer.rs
+-rw-r--r--   0     1001      123    18638 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/utf8/mod.rs
+-rw-r--r--   0     1001      123     4115 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/utf8/patterns.rs
+-rw-r--r--   0     1001      123    10548 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/utf8/strptime.rs
+-rw-r--r--   0     1001      123     3498 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/date_range.rs
+-rw-r--r--   0     1001      123    34303 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/groupby/dynamic.rs
+-rw-r--r--   0     1001      123       88 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/groupby/mod.rs
+-rw-r--r--   0     1001      123      621 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/lib.rs
+-rw-r--r--   0     1001      123     2976 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/month_end.rs
+-rw-r--r--   0     1001      123     3365 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/month_start.rs
+-rw-r--r--   0     1001      123      274 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/prelude.rs
+-rw-r--r--   0     1001      123     1381 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/round.rs
+-rw-r--r--   0     1001      123     4028 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/series/_trait.rs
+-rw-r--r--   0     1001      123      136 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/series/implementations/boolean.rs
+-rw-r--r--   0     1001      123      140 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/series/implementations/categoricals.rs
+-rw-r--r--   0     1001      123      133 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/series/implementations/date.rs
+-rw-r--r--   0     1001      123      137 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/series/implementations/datetime.rs
+-rw-r--r--   0     1001      123      137 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/series/implementations/duration.rs
+-rw-r--r--   0     1001      123     1863 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/series/implementations/floats.rs
+-rw-r--r--   0     1001      123     1792 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/series/implementations/integers.rs
+-rw-r--r--   0     1001      123      133 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/series/implementations/list.rs
+-rw-r--r--   0     1001      123      486 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/series/implementations/mod.rs
+-rw-r--r--   0     1001      123      155 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/series/implementations/object.rs
+-rw-r--r--   0     1001      123      135 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/series/implementations/struct_.rs
+-rw-r--r--   0     1001      123      133 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/series/implementations/time.rs
+-rw-r--r--   0     1001      123      133 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/series/implementations/utf8.rs
+-rw-r--r--   0     1001      123    12787 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/series/mod.rs
+-rw-r--r--   0     1001      123     1443 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/truncate.rs
+-rw-r--r--   0     1001      123     6815 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/upsample.rs
+-rw-r--r--   0     1001      123     2511 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/utils.rs
+-rw-r--r--   0     1001      123     1524 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/windows/bounds.rs
+-rw-r--r--   0     1001      123     2672 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/windows/calendar.rs
+-rw-r--r--   0     1001      123    25094 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/windows/duration.rs
+-rw-r--r--   0     1001      123    21244 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/windows/groupby.rs
+-rw-r--r--   0     1001      123      503 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/windows/mod.rs
+-rw-r--r--   0     1001      123    23652 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/windows/test.rs
+-rw-r--r--   0     1001      123    11666 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/windows/window.rs
+-rw-r--r--   0        0        0      883 1970-01-01 00:00:00.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-error/Cargo.toml
+-rw-r--r--   0     1001      123     1055 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-error/LICENSE
+-rw-r--r--   0     1001      123      145 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-error/README.md
+-rw-r--r--   0     1001      123     6297 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/local_dependencies/polars-error/src/lib.rs
+-rw-r--r--   0        0        0     4408 1970-01-01 00:00:00.000000 polars_lts_cpu-0.18.0/Cargo.toml
+-rw-r--r--   0     1001      123       76 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/.gitignore
+-rw-r--r--   0     1001      123     1055 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/LICENSE
+-rw-r--r--   0     1001      123     2414 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/Makefile
+-rw-r--r--   0     1001      123    11998 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/README.md
+-rw-r--r--   0     1001      123      651 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/build.rs
+-rw-r--r--   0     1001      123       32 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/.gitignore
+-rw-r--r--   0     1001      123      679 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/Makefile
+-rw-r--r--   0     1001      123      318 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/_templates/api_redirect.html
+-rw-r--r--   0     1001      123      151 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/_templates/autosummary/accessor.rst
+-rw-r--r--   0     1001      123      160 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/_templates/autosummary/accessor_attribute.rst
+-rw-r--r--   0     1001      123      168 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/_templates/autosummary/accessor_callable.rst
+-rw-r--r--   0     1001      123      157 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/_templates/autosummary/accessor_method.rst
+-rw-r--r--   0     1001      123      836 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/_templates/autosummary/class.rst
+-rw-r--r--   0     1001      123       94 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/_templates/autosummary/class_without_autosummary.rst
+-rw-r--r--   0     1001      123      406 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/_templates/sidebar-nav-bs.html
+-rw-r--r--   0     1001      123      491 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/requirements-docs.txt
+-rw-r--r--   0     1001      123     1164 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/run_live_docs_server.py
+-rw-r--r--   0     1001      123     1567 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/_static/css/custom.css
+-rw-r--r--   0     1001      123     7297 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/conf.py
+-rw-r--r--   0     1001      123       51 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/index.rst
+-rw-r--r--   0     1001      123     6767 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/api.rst
+-rw-r--r--   0     1001      123     1694 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/config.rst
+-rw-r--r--   0     1001      123      274 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/dataframe/aggregation.rst
+-rw-r--r--   0     1001      123      221 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/dataframe/attributes.rst
+-rw-r--r--   0     1001      123      142 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/dataframe/computation.rst
+-rw-r--r--   0     1001      123      319 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/dataframe/descriptive.rst
+-rw-r--r--   0     1001      123      319 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/dataframe/export.rst
+-rw-r--r--   0     1001      123      464 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/dataframe/groupby.rst
+-rw-r--r--   0     1001      123      379 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/dataframe/index.rst
+-rw-r--r--   0     1001      123      189 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/dataframe/miscellaneous.rst
+-rw-r--r--   0     1001      123     1538 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/dataframe/modify_select.rst
+-rw-r--r--   0     1001      123      673 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/datatypes.rst
+-rw-r--r--   0     1001      123      421 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/exceptions.rst
+-rw-r--r--   0     1001      123      391 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/expressions/aggregation.rst
+-rw-r--r--   0     1001      123      247 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/expressions/array.rst
+-rw-r--r--   0     1001      123      309 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/expressions/binary.rst
+-rw-r--r--   0     1001      123      338 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/expressions/boolean.rst
+-rw-r--r--   0     1001      123      237 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/expressions/categories.rst
+-rw-r--r--   0     1001      123      221 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/expressions/columns.rst
+-rw-r--r--   0     1001      123     1061 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/expressions/computation.rst
+-rw-r--r--   0     1001      123     1118 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/expressions/functions.rst
+-rw-r--r--   0     1001      123      470 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/expressions/index.rst
+-rw-r--r--   0     1001      123      722 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/expressions/list.rst
+-rw-r--r--   0     1001      123      407 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/expressions/meta.rst
+-rw-r--r--   0     1001      123      140 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/expressions/miscellaneous.rst
+-rw-r--r--   0     1001      123      977 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/expressions/modify_select.rst
+-rw-r--r--   0     1001      123      639 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/expressions/operators.rst
+-rw-r--r--   0     1001      123      951 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/expressions/string.rst
+-rw-r--r--   0     1001      123      254 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/expressions/struct.rst
+-rw-r--r--   0     1001      123     1036 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/expressions/temporal.rst
+-rw-r--r--   0     1001      123       98 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/expressions/window.rst
+-rw-r--r--   0     1001      123      683 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/functions.rst
+-rw-r--r--   0     1001      123      392 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/index.rst
+-rw-r--r--   0     1001      123     1294 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/io.rst
+-rw-r--r--   0     1001      123      277 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/lazyframe/aggregation.rst
+-rw-r--r--   0     1001      123      179 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/lazyframe/attributes.rst
+-rw-r--r--   0     1001      123      146 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/lazyframe/descriptive.rst
+-rw-r--r--   0     1001      123      497 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/lazyframe/groupby.rst
+-rw-r--r--   0     1001      123      354 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/lazyframe/index.rst
+-rw-r--r--   0     1001      123      455 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/lazyframe/miscellaneous.rst
+-rw-r--r--   0     1001      123     1013 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/lazyframe/modify_select.rst
+-rw-r--r--   0     1001      123      358 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/series/aggregation.rst
+-rw-r--r--   0     1001      123      255 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/series/array.rst
+-rw-r--r--   0     1001      123      257 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/series/attributes.rst
+-rw-r--r--   0     1001      123      321 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/series/binary.rst
+-rw-r--r--   0     1001      123      117 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/series/boolean.rst
+-rw-r--r--   0     1001      123      241 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/series/categories.rst
+-rw-r--r--   0     1001      123     1103 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/series/computation.rst
+-rw-r--r--   0     1001      123      744 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/series/descriptive.rst
+-rw-r--r--   0     1001      123      240 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/series/export.rst
+-rw-r--r--   0     1001      123      437 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/series/index.rst
+-rw-r--r--   0     1001      123      776 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/series/list.rst
+-rw-r--r--   0     1001      123      236 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/series/miscellaneous.rst
+-rw-r--r--   0     1001      123     1077 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/series/modify_select.rst
+-rw-r--r--   0     1001      123     1021 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/series/string.rst
+-rw-r--r--   0     1001      123      421 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/series/struct.rst
+-rw-r--r--   0     1001      123     1192 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/series/temporal.rst
+-rw-r--r--   0     1001      123      503 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/sql.rst
+-rw-r--r--   0     1001      123     8067 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/testing.rst
+-rw-r--r--   0     1001      123      168 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/docs/source/reference/utils.rst
+-rw-r--r--   0     1001      123     6098 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/__init__.py
+-rw-r--r--   0     1001      123      280 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/_reexport.py
+-rw-r--r--   0     1001      123    13229 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/api.py
+-rw-r--r--   0     1001      123    28746 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/config.py
+-rw-r--r--   0     1001      123    28105 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/convert.py
+-rw-r--r--   0     1001      123       77 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/dataframe/__init__.py
+-rw-r--r--   0     1001      123     5227 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/dataframe/_html.py
+-rw-r--r--   0     1001      123   314170 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/dataframe/frame.py
+-rw-r--r--   0     1001      123    40842 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/dataframe/groupby.py
+-rw-r--r--   0     1001      123     2692 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/datatypes/__init__.py
+-rw-r--r--   0     1001      123    16192 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/datatypes/classes.py
+-rw-r--r--   0     1001      123     1603 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/datatypes/constants.py
+-rw-r--r--   0     1001      123     4701 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/datatypes/constructor.py
+-rw-r--r--   0     1001      123    15739 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/datatypes/convert.py
+-rw-r--r--   0     1001      123     7338 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/dependencies.py
+-rw-r--r--   0     1001      123     3573 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/exceptions.py
+-rw-r--r--   0     1001      123       61 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/expr/__init__.py
+-rw-r--r--   0     1001      123     2139 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/expr/array.py
+-rw-r--r--   0     1001      123     2704 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/expr/binary.py
+-rw-r--r--   0     1001      123     1698 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/expr/categorical.py
+-rw-r--r--   0     1001      123    77496 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/expr/datetime.py
+-rw-r--r--   0     1001      123   256586 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/expr/expr.py
+-rw-r--r--   0     1001      123    23961 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/expr/list.py
+-rw-r--r--   0     1001      123     2403 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/expr/meta.py
+-rw-r--r--   0     1001      123    57885 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/expr/string.py
+-rw-r--r--   0     1001      123     5426 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/expr/struct.py
+-rw-r--r--   0     1001      123     2038 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/functions/__init__.py
+-rw-r--r--   0     1001      123    16399 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/functions/as_datatype.py
+-rw-r--r--   0     1001      123    18348 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/functions/eager.py
+-rw-r--r--   0     1001      123    71716 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/functions/lazy.py
+-rw-r--r--   0     1001      123    16085 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/functions/range.py
+-rw-r--r--   0     1001      123     6027 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/functions/repeat.py
+-rw-r--r--   0     1001      123     6195 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/functions/whenthen.py
+-rw-r--r--   0     1001      123      952 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/io/__init__.py
+-rw-r--r--   0     1001      123     6264 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/io/_utils.py
+-rw-r--r--   0     1001      123      861 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/io/avro.py
+-rw-r--r--   0     1001      123      144 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/io/csv/__init__.py
+-rw-r--r--   0     1001      123     1072 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/io/csv/_utils.py
+-rw-r--r--   0     1001      123     4681 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/io/csv/batched_reader.py
+-rw-r--r--   0     1001      123    35482 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/io/csv/functions.py
+-rw-r--r--   0     1001      123     5627 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/io/database.py
+-rw-r--r--   0     1001      123    11047 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/io/delta.py
+-rw-r--r--   0     1001      123       75 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/io/excel/__init__.py
+-rw-r--r--   0     1001      123    18449 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/io/excel/_write_utils.py
+-rw-r--r--   0     1001      123     6466 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/io/excel/functions.py
+-rw-r--r--   0     1001      123      142 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/io/ipc/__init__.py
+-rw-r--r--   0     1001      123     1227 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/io/ipc/anonymous_scan.py
+-rw-r--r--   0     1001      123     5804 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/io/ipc/functions.py
+-rw-r--r--   0     1001      123      502 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/io/json.py
+-rw-r--r--   0     1001      123     2207 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/io/ndjson.py
+-rw-r--r--   0     1001      123      170 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/io/parquet/__init__.py
+-rw-r--r--   0     1001      123     1259 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/io/parquet/anonymous_scan.py
+-rw-r--r--   0     1001      123     7177 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/io/parquet/functions.py
+-rw-r--r--   0     1001      123      136 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/io/pyarrow_dataset/__init__.py
+-rw-r--r--   0     1001      123     2291 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/io/pyarrow_dataset/anonymous_scan.py
+-rw-r--r--   0     1001      123     3601 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/io/pyarrow_dataset/functions.py
+-rw-r--r--   0     1001      123       77 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/lazyframe/__init__.py
+-rw-r--r--   0     1001      123   166400 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/lazyframe/frame.py
+-rw-r--r--   0     1001      123    23676 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/lazyframe/groupby.py
+-rw-r--r--   0     1001      123        0 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/py.typed
+-rw-r--r--   0     1001      123       69 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/series/__init__.py
+-rw-r--r--   0     1001      123     1572 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/series/_numpy.py
+-rw-r--r--   0     1001      123     1700 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/series/array.py
+-rw-r--r--   0     1001      123     1913 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/series/binary.py
+-rw-r--r--   0     1001      123     1692 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/series/categorical.py
+-rw-r--r--   0     1001      123    51602 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/series/datetime.py
+-rw-r--r--   0     1001      123    13196 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/series/list.py
+-rw-r--r--   0     1001      123   167955 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/series/series.py
+-rw-r--r--   0     1001      123    37216 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/series/string.py
+-rw-r--r--   0     1001      123     2542 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/series/struct.py
+-rw-r--r--   0     1001      123     5361 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/series/utils.py
+-rw-r--r--   0     1001      123     7559 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/slice.py
+-rw-r--r--   0     1001      123       75 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/sql/__init__.py
+-rw-r--r--   0     1001      123    17409 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/sql/context.py
+-rw-r--r--   0     1001      123     4764 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/string_cache.py
+-rw-r--r--   0     1001      123      362 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/testing/__init__.py
+-rw-r--r--   0     1001      123     1060 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/testing/_private.py
+-rw-r--r--   0     1001      123     3689 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/testing/_tempdir.py
+-rw-r--r--   0     1001      123    16425 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/testing/asserts.py
+-rw-r--r--   0     1001      123      898 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/testing/parametric/__init__.py
+-rw-r--r--   0     1001      123    26833 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/testing/parametric/primitives.py
+-rw-r--r--   0     1001      123     3409 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/testing/parametric/profiles.py
+-rw-r--r--   0     1001      123    12132 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/testing/parametric/strategies.py
+-rw-r--r--   0     1001      123     6214 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/type_aliases.py
+-rw-r--r--   0     1001      123     1169 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/utils/__init__.py
+-rw-r--r--   0     1001      123    54272 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/utils/_construction.py
+-rw-r--r--   0     1001      123     3891 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/utils/_parse_expr_input.py
+-rw-r--r--   0     1001      123      711 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/utils/_scan.py
+-rw-r--r--   0     1001      123      579 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/utils/_wrap.py
+-rw-r--r--   0     1001      123      683 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/utils/build_info.py
+-rw-r--r--   0     1001      123     8609 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/utils/convert.py
+-rw-r--r--   0     1001      123     6132 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/utils/decorators.py
+-rw-r--r--   0     1001      123     1660 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/utils/meta.py
+-rw-r--r--   0     1001      123      514 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/utils/polars_version.py
+-rw-r--r--   0     1001      123     2673 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/utils/show_versions.py
+-rw-r--r--   0     1001      123    12905 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/polars/utils/various.py
+-rw-r--r--   0     1001      123     5377 2023-05-29 20:01:51.000000 polars_lts_cpu-0.18.0/pyproject.toml
+-rw-r--r--   0     1001      123      699 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/requirements-dev.txt
+-rw-r--r--   0     1001      123       70 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/requirements-lint.txt
+-rw-r--r--   0     1001      123     1640 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/scripts/check_stacklevels.py
+-rw-r--r--   0     1001      123    11023 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/apply/dataframe.rs
+-rw-r--r--   0     1001      123     7448 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/apply/lazy.rs
+-rw-r--r--   0     1001      123     8402 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/apply/mod.rs
+-rw-r--r--   0     1001      123    90063 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/apply/series.rs
+-rw-r--r--   0     1001      123       32 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/arrow_interop/mod.rs
+-rw-r--r--   0     1001      123     1306 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/arrow_interop/to_py.rs
+-rw-r--r--   0     1001      123     3902 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/arrow_interop/to_rust.rs
+-rw-r--r--   0     1001      123     5250 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/batched_csv.rs
+-rw-r--r--   0     1001      123    47907 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/conversion.rs
+-rw-r--r--   0     1001      123    45844 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/dataframe.rs
+-rw-r--r--   0     1001      123     3950 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/datatypes.rs
+-rw-r--r--   0     1001      123     3288 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/error.rs
+-rw-r--r--   0     1001      123      337 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/expr/array.rs
+-rw-r--r--   0     1001      123     2080 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/expr/binary.rs
+-rw-r--r--   0     1001      123      274 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/expr/categorical.rs
+-rw-r--r--   0     1001      123     5935 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/expr/datetime.rs
+-rw-r--r--   0     1001      123    34212 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/expr/general.rs
+-rw-r--r--   0     1001      123     3937 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/expr/list.rs
+-rw-r--r--   0     1001      123     1096 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/expr/meta.rs
+-rw-r--r--   0     1001      123      870 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/expr/mod.rs
+-rw-r--r--   0     1001      123     8677 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/expr/string.rs
+-rw-r--r--   0     1001      123      467 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/expr/struct.rs
+-rw-r--r--   0     1001      123     9482 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/file.rs
+-rw-r--r--   0     1001      123     3307 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/functions/eager.rs
+-rw-r--r--   0     1001      123     1657 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/functions/io.rs
+-rw-r--r--   0     1001      123    11456 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/functions/lazy.rs
+-rw-r--r--   0     1001      123     1312 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/functions/meta.rs
+-rw-r--r--   0     1001      123      217 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/functions/misc.rs
+-rw-r--r--   0     1001      123       87 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/functions/mod.rs
+-rw-r--r--   0     1001      123     1474 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/functions/whenthen.rs
+-rw-r--r--   0     1001      123    30642 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/lazyframe.rs
+-rw-r--r--   0     1001      123     2670 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/lazygroupby.rs
+-rw-r--r--   0     1001      123     8072 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/lib.rs
+-rw-r--r--   0     1001      123     1029 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/object.rs
+-rw-r--r--   0     1001      123      122 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/prelude.rs
+-rw-r--r--   0     1001      123      435 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/py_modules.rs
+-rw-r--r--   0     1001      123     1964 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/series/aggregation.rs
+-rw-r--r--   0     1001      123     5406 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/series/arithmetic.rs
+-rw-r--r--   0     1001      123     5138 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/series/comparison.rs
+-rw-r--r--   0     1001      123     9077 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/series/construction.rs
+-rw-r--r--   0     1001      123     8971 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/series/export.rs
+-rw-r--r--   0     1001      123    26521 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/series/mod.rs
+-rw-r--r--   0     1001      123     4569 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/series/numpy_ufunc.rs
+-rw-r--r--   0     1001      123     4046 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/series/set_at_idx.rs
+-rw-r--r--   0     1001      123     1036 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/sql.rs
+-rw-r--r--   0     1001      123     2335 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/src/utils.rs
+-rw-r--r--   0     1001      123     6165 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/README.md
+-rw-r--r--   0     1001      123     2189 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/benchmark/groupby-datagen.R
+-rw-r--r--   0     1001      123     7963 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/benchmark/run_h2oai_benchmark.py
+-rw-r--r--   0     1001      123     6530 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/benchmark/test_release.py
+-rw-r--r--   0     1001      123     4589 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/docs/run_doctest.py
+-rw-r--r--   0     1001      123      179 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/parametric/conftest.py
+-rw-r--r--   0     1001      123     3856 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/parametric/test_dataframe.py
+-rw-r--r--   0     1001      123     1692 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/parametric/test_lazyframe.py
+-rw-r--r--   0     1001      123     6897 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/parametric/test_series.py
+-rw-r--r--   0     1001      123     8299 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/parametric/test_testing.py
+-rw-r--r--   0     1001      123        0 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/__init__.py
+-rw-r--r--   0     1001      123     3382 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/conftest.py
+-rw-r--r--   0     1001      123       86 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/datatypes/__init__.py
+-rw-r--r--   0     1001      123      973 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/datatypes/test_array.py
+-rw-r--r--   0     1001      123      351 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/datatypes/test_binary.py
+-rw-r--r--   0     1001      123     1420 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/datatypes/test_bool.py
+-rw-r--r--   0     1001      123    13226 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/datatypes/test_categorical.py
+-rw-r--r--   0     1001      123     3289 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/datatypes/test_decimal.py
+-rw-r--r--   0     1001      123      549 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/datatypes/test_duration.py
+-rw-r--r--   0     1001      123    12837 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/datatypes/test_list.py
+-rw-r--r--   0     1001      123      284 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/datatypes/test_null.py
+-rw-r--r--   0     1001      123     2801 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/datatypes/test_object.py
+-rw-r--r--   0     1001      123    27425 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/datatypes/test_struct.py
+-rw-r--r--   0     1001      123    86309 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/datatypes/test_temporal.py
+-rw-r--r--   0     1001      123      418 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/datatypes/test_time.py
+-rw-r--r--   0     1001      123        0 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/functions/__init__.py
+-rw-r--r--   0     1001      123    13649 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/functions/test_as_datatype.py
+-rw-r--r--   0     1001      123    15610 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/functions/test_functions.py
+-rw-r--r--   0     1001      123    17732 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/functions/test_range.py
+-rw-r--r--   0     1001      123     3002 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/functions/test_repeat.py
+-rw-r--r--   0     1001      123      218 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/conftest.py
+-rw-r--r--   0     1001      123       16 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/files/delta-table/.part-00000-e42312d7-60e5-454d-acbc-db192d220e73-c000.snappy.parquet.crc
+-rw-r--r--   0     1001      123       16 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/files/delta-table/.part-00000-e4a999da-df45-4fb0-bdc4-d999fc0f58aa-c000.snappy.parquet.crc
+-rw-r--r--   0     1001      123       16 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/files/delta-table/_delta_log/.00000000000000000000.json.crc
+-rw-r--r--   0     1001      123       16 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/files/delta-table/_delta_log/.00000000000000000001.json.crc
+-rw-r--r--   0     1001      123      905 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/files/delta-table/_delta_log/00000000000000000000.json
+-rw-r--r--   0     1001      123      936 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/files/delta-table/_delta_log/00000000000000000001.json
+-rw-r--r--   0     1001      123      972 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/files/delta-table/part-00000-e42312d7-60e5-454d-acbc-db192d220e73-c000.snappy.parquet
+-rw-r--r--   0     1001      123      690 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/files/delta-table/part-00000-e4a999da-df45-4fb0-bdc4-d999fc0f58aa-c000.snappy.parquet
+-rw-r--r--   0     1001      123        0 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/files/empty.csv
+-rw-r--r--   0     1001      123     5959 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/files/example.xlsx
+-rw-r--r--   0     1001      123      457 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/files/foods1.csv
+-rw-r--r--   0     1001      123     2351 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/files/foods1.ipc
+-rw-r--r--   0     1001      123     1713 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/files/foods1.ndjson
+-rw-r--r--   0     1001      123     1427 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/files/foods1.parquet
+-rw-r--r--   0     1001      123      455 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/files/foods2.csv
+-rw-r--r--   0     1001      123     2351 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/files/foods2.ipc
+-rw-r--r--   0     1001      123     1711 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/files/foods2.ndjson
+-rw-r--r--   0     1001      123     1916 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/files/foods2.parquet
+-rw-r--r--   0     1001      123      455 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/files/foods3.csv
+-rw-r--r--   0     1001      123      457 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/files/foods4.csv
+-rw-r--r--   0     1001      123      452 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/files/foods5.csv
+-rw-r--r--   0     1001      123       49 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/files/gzipped.csv
+-rw-r--r--   0     1001      123       57 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/files/small.csv
+-rw-r--r--   0     1001      123      756 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/files/small.parquet
+-rw-r--r--   0     1001      123     1937 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/test_avro.py
+-rw-r--r--   0     1001      123    39389 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/test_csv.py
+-rw-r--r--   0     1001      123     6650 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/test_database.py
+-rw-r--r--   0     1001      123     6172 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/test_delta.py
+-rw-r--r--   0     1001      123    11169 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/test_excel.py
+-rw-r--r--   0     1001      123     5919 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/test_ipc.py
+-rw-r--r--   0     1001      123     3995 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/test_json.py
+-rw-r--r--   0     1001      123     7456 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/test_lazy_csv.py
+-rw-r--r--   0     1001      123     2060 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/test_lazy_ipc.py
+-rw-r--r--   0     1001      123     2881 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/test_lazy_json.py
+-rw-r--r--   0     1001      123    11849 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/test_lazy_parquet.py
+-rw-r--r--   0     1001      123     2012 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/test_other.py
+-rw-r--r--   0     1001      123    14077 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/test_parquet.py
+-rw-r--r--   0     1001      123      612 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/test_pickle.py
+-rw-r--r--   0     1001      123     3886 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/io/test_pyarrow_dataset.py
+-rw-r--r--   0     1001      123      509 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/namespaces/__init__.py
+-rw-r--r--   0     1001      123      377 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/namespaces/test_array.py
+-rw-r--r--   0     1001      123     3218 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/namespaces/test_binary.py
+-rw-r--r--   0     1001      123     2489 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/namespaces/test_categorical.py
+-rw-r--r--   0     1001      123    19210 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/namespaces/test_datetime.py
+-rw-r--r--   0     1001      123    13879 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/namespaces/test_list.py
+-rw-r--r--   0     1001      123     1829 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/namespaces/test_meta.py
+-rw-r--r--   0     1001      123    23349 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/namespaces/test_string.py
+-rw-r--r--   0     1001      123    16029 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/namespaces/test_strptime.py
+-rw-r--r--   0     1001      123      982 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/namespaces/test_struct.py
+-rw-r--r--   0     1001      123       85 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/operations/__init__.py
+-rw-r--r--   0     1001      123     7475 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/operations/test_aggregations.py
+-rw-r--r--   0     1001      123     9922 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/operations/test_apply.py
+-rw-r--r--   0     1001      123     6932 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/operations/test_arithmetic.py
+-rw-r--r--   0     1001      123     4631 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/operations/test_comparison.py
+-rw-r--r--   0     1001      123     3275 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/operations/test_drop.py
+-rw-r--r--   0     1001      123     8519 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/operations/test_explode.py
+-rw-r--r--   0     1001      123     3664 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/operations/test_filter.py
+-rw-r--r--   0     1001      123     1801 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/operations/test_folds.py
+-rw-r--r--   0     1001      123    24317 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/operations/test_groupby.py
+-rw-r--r--   0     1001      123     7649 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/operations/test_groupby_rolling.py
+-rw-r--r--   0     1001      123     2983 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/operations/test_is_in.py
+-rw-r--r--   0     1001      123    17498 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/operations/test_join.py
+-rw-r--r--   0     1001      123    14612 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/operations/test_join_asof.py
+-rw-r--r--   0     1001      123      643 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/operations/test_melt.py
+-rw-r--r--   0     1001      123    10253 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/operations/test_pivot.py
+-rw-r--r--   0     1001      123    19405 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/operations/test_rolling.py
+-rw-r--r--   0     1001      123    20578 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/operations/test_sort.py
+-rw-r--r--   0     1001      123     4038 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/operations/test_statistics.py
+-rw-r--r--   0     1001      123     4130 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/operations/test_transpose.py
+-rw-r--r--   0     1001      123      771 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/operations/test_unique.py
+-rw-r--r--   0     1001      123    11694 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/operations/test_window.py
+-rw-r--r--   0     1001      123        0 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/streaming/__init__.py
+-rw-r--r--   0     1001      123      196 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/streaming/conftest.py
+-rw-r--r--   0     1001      123      839 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/streaming/test_ooc.py
+-rw-r--r--   0     1001      123    16053 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/streaming/test_streaming.py
+-rw-r--r--   0     1001      123     4775 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_api.py
+-rw-r--r--   0     1001      123     1077 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_arity.py
+-rw-r--r--   0     1001      123    19865 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_cfg.py
+-rw-r--r--   0     1001      123    41153 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_constructors.py
+-rw-r--r--   0     1001      123      454 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_context.py
+-rw-r--r--   0     1001      123     1628 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_cse.py
+-rw-r--r--   0     1001      123     3817 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_datatypes.py
+-rw-r--r--   0     1001      123   124160 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_df.py
+-rw-r--r--   0     1001      123     1906 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_empty.py
+-rw-r--r--   0     1001      123    18539 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_errors.py
+-rw-r--r--   0     1001      123     2387 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_expr_multi_cols.py
+-rw-r--r--   0     1001      123    34736 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_exprs.py
+-rw-r--r--   0     1001      123     3516 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_fmt.py
+-rw-r--r--   0     1001      123     3763 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_interchange.py
+-rw-r--r--   0     1001      123    37738 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_interop.py
+-rw-r--r--   0     1001      123    49117 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_lazy.py
+-rw-r--r--   0     1001      123     2369 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_polars_import.py
+-rw-r--r--   0     1001      123     4610 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_predicates.py
+-rw-r--r--   0     1001      123     7073 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_projections.py
+-rw-r--r--   0     1001      123    11551 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_queries.py
+-rw-r--r--   0     1001      123     4743 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_rows.py
+-rw-r--r--   0     1001      123    13203 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_schema.py
+-rw-r--r--   0     1001      123     2634 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_serde.py
+-rw-r--r--   0     1001      123    83565 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_series.py
+-rw-r--r--   0     1001      123      657 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_single.py
+-rw-r--r--   0     1001      123     6527 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_sql.py
+-rw-r--r--   0     1001      123    12957 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/test_testing.py
+-rw-r--r--   0     1001      123       41 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/utils/__init__.py
+-rw-r--r--   0     1001      123      306 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/utils/test_build_info.py
+-rw-r--r--   0     1001      123     2631 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/utils/test_parse_expr_input.py
+-rw-r--r--   0     1001      123      247 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/utils/test_show_versions.py
+-rw-r--r--   0     1001      123     5026 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/tests/unit/utils/test_utils.py
+-rw-r--r--   0     1001      123    63477 2023-05-29 20:01:50.000000 polars_lts_cpu-0.18.0/Cargo.lock
+-rw-r--r--   0        0        0    14535 1970-01-01 00:00:00.000000 polars_lts_cpu-0.18.0/PKG-INFO
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/Cargo.toml` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/Cargo.toml`

 * *Files 7% similar despite different names*

```diff
@@ -1,33 +1,33 @@
 [package]
 name = "polars-io"
-version= "0.28.0"
+version= "0.30.0"
 authors = ["ritchie46 <ritchie46@gmail.com>"]
 edition = "2021"
 license = "MIT"
 repository = "https://github.com/pola-rs/polars"
 description = "IO related logic for the Polars DataFrame library"
 
 # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html
 
 [features]
 # support for arrows json parsing
-json = ["arrow/io_json", "simd-json", "memmap", "lexical", "lexical-core", "serde_json"]
+json = ["arrow/io_json_write", "polars-json", "simd-json", "memmap", "lexical", "lexical-core", "serde_json"]
 # support for arrows ipc file parsing
 ipc = ["arrow/io_ipc", "arrow/io_ipc_compression", "memmap"]
 # support for arrows streaming ipc file parsing
 ipc_streaming = ["arrow/io_ipc", "arrow/io_ipc_compression"]
 # support for arrow avro parsing
 avro = ["arrow/io_avro", "arrow/io_avro_compression"]
-# ipc = []
 csv = ["memmap", "lexical", "polars-core/rows", "lexical-core", "fast-float", "simdutf8"]
 decompress = ["flate2/miniz_oxide"]
 decompress-fast = ["flate2/zlib-ng"]
 dtype-categorical = ["polars-core/dtype-categorical"]
 dtype-date = ["polars-core/dtype-date", "polars-time/dtype-date"]
+object = []
 dtype-datetime = [
   "polars-core/dtype-datetime",
   "polars-core/temporal",
   "polars-time/dtype-datetime",
   "chrono",
 ]
 timezones = [
@@ -63,35 +63,36 @@
 lexical = { version = "6", optional = true, default-features = false, features = ["std", "parse-integers"] }
 lexical-core = { version = "0.8", optional = true }
 memchr= "2"
 memmap = { package = "memmap2", version = "0.5.2", optional = true }
 num-traits= "0.2"
 object_store = { version = "0.5.3", default-features = false, optional = true }
 once_cell = "1"
-polars-arrow = { version = "0.28.0", path = "../polars-arrow" }
-polars-core = { version = "0.28.0", path = "../polars-core", features = ["private"], default-features = false }
-polars-error = { version = "0.28.0", path = "../polars-error", default-features = false }
-polars-time = { version = "0.28.0", path = "../polars-time", features = ["private"], default-features = false, optional = true }
-polars-utils = { version = "0.28.0", path = "../polars-utils" }
+polars-arrow = { version = "0.30.0", path = "../polars-arrow" }
+polars-core = { version = "0.30.0", path = "../polars-core", features = ["private"], default-features = false }
+polars-error = { version = "0.30.0", path = "../polars-error", default-features = false }
+polars-json = { version = "0.30.0", optional = true, path = "../polars-json" }
+polars-time = { version = "0.30.0", path = "../polars-time", features = ["private"], default-features = false, optional = true }
+polars-utils = { version = "0.30.0", path = "../polars-utils" }
 rayon= "1.6"
 regex = "1.6"
 serde = { version = "1", features = ["derive"], optional = true }
 serde_json = { version = "1", optional = true, default-features = false, features = ["alloc", "raw_value"] }
-simd-json = { version = "0.7.0", optional = true, features = ["allow-non-simd", "known-key"] }
+simd-json = { version = "0.10", optional = true, features = ["allow-non-simd", "known-key"] }
 simdutf8 = { version = "0.1", optional = true }
 tokio = { version = "1.26.0", features = ["net"], optional = true }
 url = { version = "2.3.1", optional = true }
 
 [dependencies.arrow]
 package = "arrow2"
 # git = "https://github.com/jorgecarleitao/arrow2"
-git = "https://github.com/ritchie46/arrow2"
+# git = "https://github.com/ritchie46/arrow2"
 # rev = "1491c6e8f4fd100f53c358e4f3ef1536d9e75090"
 # path = "../arrow2"
-branch = "polars_2023-04-20"
+# branch = "polars_2023-05-25"
 version = "0.17"
 default-features = false
 features = [
   "compute_aggregate",
   "compute_arithmetics",
   "compute_boolean",
   "compute_boolean_kleene",
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/LICENSE` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/LICENSE`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/avro/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/avro/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/avro/read.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/avro/read.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/avro/write.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/avro/write.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/cloud/adaptors.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/cloud/adaptors.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/cloud/glob.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/cloud/glob.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/cloud/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/cloud/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/csv/buffer.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/csv/buffer.rs`

 * *Files 1% similar despite different names*

```diff
@@ -420,47 +420,43 @@
         polars_bail!(ComputeError: "invalid utf-8 sequence");
     } else {
         buf.builder.append_null();
         return Ok(());
     };
 
     match &buf.compiled {
-        Some(compiled) => {
-            match DatetimeInfer::<T::Native>::try_from(compiled.pattern_with_offset.pattern) {
+        Some(compiled) => match DatetimeInfer::<T::Native>::try_from(compiled.pattern) {
+            Ok(mut infer) => {
+                let parsed = infer.parse(val);
+                buf.compiled = Some(infer);
+                buf.builder.append_option(parsed);
+                Ok(())
+            }
+            Err(_) => {
+                buf.builder.append_null();
+                Ok(())
+            }
+        },
+        None => match infer_pattern_single(val) {
+            None => {
+                buf.builder.append_null();
+                Ok(())
+            }
+            Some(pattern) => match DatetimeInfer::<T::Native>::try_from(pattern) {
                 Ok(mut infer) => {
-                    let parsed = infer.parse(val, compiled.pattern_with_offset.offset);
+                    let parsed = infer.parse(val);
                     buf.compiled = Some(infer);
                     buf.builder.append_option(parsed);
                     Ok(())
                 }
                 Err(_) => {
                     buf.builder.append_null();
                     Ok(())
                 }
-            }
-        }
-        None => match infer_pattern_single(val) {
-            None => {
-                buf.builder.append_null();
-                Ok(())
-            }
-            Some(pattern_with_offset) => {
-                match DatetimeInfer::<T::Native>::try_from(pattern_with_offset.pattern) {
-                    Ok(mut infer) => {
-                        let parsed = infer.parse(val, pattern_with_offset.offset);
-                        buf.compiled = Some(infer);
-                        buf.builder.append_option(parsed);
-                        Ok(())
-                    }
-                    Err(_) => {
-                        buf.builder.append_null();
-                        Ok(())
-                    }
-                }
-            }
+            },
         },
     }
 }
 
 #[cfg(any(feature = "dtype-datetime", feature = "dtype-date"))]
 impl<T> ParsedBuffer for DatetimeField<T>
 where
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/csv/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/csv/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/csv/parser.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/csv/parser.rs`

 * *Files 2% similar despite different names*

```diff
@@ -145,25 +145,16 @@
     &input[read..]
 }
 
 /// Makes sure that the bytes stream starts with
 ///     'field_1,field_2'
 /// and not with
 ///     '\nfield_1,field_1'
-pub(crate) fn skip_header(input: &[u8], eol_char: u8) -> (&[u8], usize) {
-    match next_line_position_naive(input, eol_char) {
-        Some(mut pos) => {
-            if input[pos] == eol_char {
-                pos += 1;
-            }
-            (&input[pos..], pos)
-        }
-        // no lines in the file, so skipping the header is skipping all.
-        None => (&[], input.len()),
-    }
+pub(crate) fn skip_header(input: &[u8], quote: Option<u8>, eol_char: u8) -> &[u8] {
+    skip_this_line(input, quote, eol_char)
 }
 
 /// Remove whitespace from the start of buffer.
 #[inline]
 pub(crate) fn skip_whitespace(input: &[u8]) -> &[u8] {
     skip_condition(input, is_whitespace)
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/csv/read.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/csv/read.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/csv/read_impl/batched_mmap.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/csv/read_impl/batched_mmap.rs`

 * *Files 1% similar despite different names*

```diff
@@ -103,15 +103,16 @@
 }
 
 impl<'a> CoreReader<'a> {
     /// Create a batched csv reader that uses mmap to load data.
     pub fn batched_mmap(mut self, _has_cat: bool) -> PolarsResult<BatchedCsvReaderMmap<'a>> {
         let reader_bytes = self.reader_bytes.take().unwrap();
         let bytes = reader_bytes.as_ref();
-        let (bytes, starting_point_offset) = self.find_starting_point(bytes, self.eol_char)?;
+        let (bytes, starting_point_offset) =
+            self.find_starting_point(bytes, self.quote_char, self.eol_char)?;
 
         // this is arbitrarily chosen.
         // we don't want this to depend on the thread pool size
         // otherwise the chunks are not deterministic
         let offset_batch_size = 16;
         // extend lifetime. It is bound to `readerbytes` and we keep track of that
         // lifetime so this is sound.
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/csv/read_impl/batched_read.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/csv/read_impl/batched_read.rs`

 * *Files 1% similar despite different names*

```diff
@@ -189,15 +189,16 @@
 
 impl<'a> CoreReader<'a> {
     /// Create a batched csv reader that uses read calls to load data.
     pub fn batched_read(mut self, _has_cat: bool) -> PolarsResult<BatchedCsvReaderRead<'a>> {
         let reader_bytes = self.reader_bytes.take().unwrap();
 
         let ReaderBytes::Mapped(bytes, mut file) = &reader_bytes else { unreachable!() };
-        let (_, starting_point_offset) = self.find_starting_point(bytes, self.eol_char)?;
+        let (_, starting_point_offset) =
+            self.find_starting_point(bytes, self.quote_char, self.eol_char)?;
         if let Some(starting_point_offset) = starting_point_offset {
             file.seek(SeekFrom::Current(starting_point_offset as i64))
                 .unwrap();
         }
 
         let chunk_iter = ChunkReader::new(
             file,
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/csv/read_impl/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/csv/read_impl/mod.rs`

 * *Files 2% similar despite different names*

```diff
@@ -13,15 +13,14 @@
 use polars_core::prelude::*;
 use polars_core::utils::accumulate_dataframes_vertical;
 use polars_core::POOL;
 #[cfg(feature = "polars-time")]
 use polars_time::prelude::*;
 use polars_utils::flatten;
 use rayon::prelude::*;
-use rayon::ThreadPoolBuilder;
 
 use crate::csv::buffer::*;
 use crate::csv::parser::*;
 use crate::csv::read::NullValuesCompiled;
 use crate::csv::utils::*;
 use crate::csv::{CsvEncoding, NullValues};
 use crate::mmap::ReaderBytes;
@@ -41,15 +40,15 @@
             .unwrap()
             .as_date(None, false)
             .map(|ca| ca.into_series()),
         #[cfg(feature = "temporal")]
         (DataType::Utf8, DataType::Datetime(tu, _)) => s
             .utf8()
             .unwrap()
-            .as_datetime(None, *tu, false, false, false, None)
+            .as_datetime(None, *tu, false, false, None)
             .map(|ca| ca.into_series()),
         (_, dt) => s.cast(dt),
     };
 
     if parallel {
         let cols = df
             .get_columns()
@@ -292,29 +291,30 @@
             row_count,
         })
     }
 
     fn find_starting_point<'b>(
         &self,
         mut bytes: &'b [u8],
+        quote_char: Option<u8>,
         eol_char: u8,
     ) -> PolarsResult<(&'b [u8], Option<usize>)> {
         let starting_point_offset = bytes.as_ptr() as usize;
 
         // Skip all leading white space and the occasional utf8-bom
         bytes = skip_whitespace_exclude(skip_bom(bytes), self.delimiter);
         // \n\n can be a empty string row of a single column
         // in other cases we skip it.
         if self.schema.len() > 1 {
             bytes = skip_line_ending(bytes, eol_char)
         }
 
         // If there is a header we skip it.
         if self.has_header {
-            bytes = skip_header(bytes, eol_char).0;
+            bytes = skip_header(bytes, quote_char, eol_char);
         }
 
         if self.skip_rows_before_header > 0 {
             for _ in 0..self.skip_rows_before_header {
                 let pos = next_line_position_naive(bytes, eol_char)
                     .ok_or_else(|| polars_err!(NoData: "not enough lines to skip"))?;
                 bytes = &bytes[pos..];
@@ -420,15 +420,16 @@
         usize,
         usize,
         Option<usize>,
         &'a [u8],
         Option<&'a [u8]>,
     )> {
         // Make the variable mutable so that we can reassign the sliced file to this variable.
-        let (bytes, starting_point_offset) = self.find_starting_point(bytes, self.eol_char)?;
+        let (bytes, starting_point_offset) =
+            self.find_starting_point(bytes, self.quote_char, self.eol_char)?;
 
         let (bytes, total_rows, remaining_bytes) =
             self.estimate_rows_and_set_upper_bound(bytes, logging, true);
         if total_rows == 128 {
             *n_threads = 1;
 
             if logging {
@@ -526,32 +527,14 @@
     ) -> PolarsResult<DataFrame> {
         let logging = verbose();
         let (file_chunks, chunk_size, total_rows, starting_point_offset, bytes, remaining_bytes) =
             self.determine_file_chunks_and_statistics(&mut n_threads, bytes, logging)?;
         let projection = self.get_projection();
         let str_columns = self.get_string_columns(&projection)?;
 
-        // If the number of threads given by the user is lower than our global thread pool we create
-        // new one.
-        #[cfg(not(target_family = "wasm"))]
-        let owned_pool;
-        #[cfg(not(target_family = "wasm"))]
-        let pool = if POOL.current_num_threads() != n_threads {
-            owned_pool = Some(
-                ThreadPoolBuilder::new()
-                    .num_threads(n_threads)
-                    .build()
-                    .unwrap(),
-            );
-            owned_pool.as_ref().unwrap()
-        } else {
-            &POOL
-        };
-        #[cfg(target_family = "wasm")] // use a pre-created pool for wasm
-        let pool = &POOL;
         // An empty file with a schema should return an empty DataFrame with that schema
         if bytes.is_empty() {
             // TODO! add DataFrame::new_from_schema
             let buffers = init_buffers(
                 &projection,
                 0,
                 &self.schema,
@@ -570,15 +553,15 @@
         }
 
         // all the buffers returned from the threads
         // Structure:
         //      the inner vec has got buffers from all the columns.
         if let Some(predicate) = predicate {
             let str_capacities = self.init_string_size_stats(&str_columns, chunk_size);
-            let dfs = pool.install(|| {
+            let dfs = POOL.install(|| {
                 file_chunks
                     .into_par_iter()
                     .map(|(bytes_offset_thread, stop_at_nbytes)| {
                         let delimiter = self.delimiter;
                         let schema = self.schema.as_ref();
                         let ignore_errors = self.ignore_errors;
                         let projection = &projection;
@@ -663,15 +646,15 @@
                 chunk_size
             } else {
                 std::cmp::min(rows_per_thread, max_proxy)
             };
 
             let str_capacities = self.init_string_size_stats(&str_columns, capacity);
 
-            let mut dfs = pool.install(|| {
+            let mut dfs = POOL.install(|| {
                 file_chunks
                     .into_par_iter()
                     .map(|(bytes_offset_thread, stop_at_nbytes)| {
                         let mut df = read_chunk(
                             bytes,
                             self.delimiter,
                             self.schema.as_ref(),
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/csv/splitfields.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/csv/splitfields.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/csv/utils.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/csv/utils.rs`

 * *Files 6% similar despite different names*

```diff
@@ -3,15 +3,14 @@
 use std::mem::MaybeUninit;
 
 use once_cell::sync::Lazy;
 use polars_core::datatypes::PlHashSet;
 use polars_core::prelude::*;
 #[cfg(feature = "polars-time")]
 use polars_time::chunkedarray::utf8::infer as date_infer;
-use polars_time::chunkedarray::utf8::PatternWithOffset;
 #[cfg(feature = "polars-time")]
 use polars_time::prelude::utf8::Pattern;
 use regex::{Regex, RegexBuilder};
 
 #[cfg(any(feature = "decompress", feature = "decompress-fast"))]
 use crate::csv::parser::next_line_position_naive;
 use crate::csv::parser::{next_line_position, skip_bom, skip_line_ending, SplitLines};
@@ -108,27 +107,21 @@
     // Utf8 for them
     if string.starts_with('"') {
         if try_parse_dates {
             #[cfg(feature = "polars-time")]
             {
                 match date_infer::infer_pattern_single(&string[1..string.len() - 1]) {
                     Some(pattern_with_offset) => match pattern_with_offset {
-                        PatternWithOffset {
-                            pattern: Pattern::DatetimeYMD | Pattern::DatetimeDMY,
-                            offset: _,
-                        } => DataType::Datetime(TimeUnit::Microseconds, None),
-                        PatternWithOffset {
-                            pattern: Pattern::DateYMD | Pattern::DateDMY,
-                            offset: _,
-                        } => DataType::Date,
-                        PatternWithOffset {
-                            pattern: Pattern::DatetimeYMDZ,
-                            offset: _,
-                        } => DataType::Utf8, // TODO: support tz-aware,
-                                             // need to keep track of offset
+                        Pattern::DatetimeYMD | Pattern::DatetimeDMY => {
+                            DataType::Datetime(TimeUnit::Microseconds, None)
+                        }
+                        Pattern::DateYMD | Pattern::DateDMY => DataType::Date,
+                        Pattern::DatetimeYMDZ => {
+                            DataType::Datetime(TimeUnit::Microseconds, Some("UTC".to_string()))
+                        }
                     },
                     None => DataType::Utf8,
                 }
             }
             #[cfg(not(feature = "polars-time"))]
             {
                 panic!("activate one of {{'dtype-date', 'dtype-datetime', dtype-time'}} features")
@@ -145,27 +138,21 @@
     } else if INTEGER_RE.is_match(string) {
         DataType::Int64
     } else if try_parse_dates {
         #[cfg(feature = "polars-time")]
         {
             match date_infer::infer_pattern_single(string) {
                 Some(pattern_with_offset) => match pattern_with_offset {
-                    PatternWithOffset {
-                        pattern: Pattern::DatetimeYMD | Pattern::DatetimeDMY,
-                        offset: _,
-                    } => DataType::Datetime(TimeUnit::Microseconds, None),
-                    PatternWithOffset {
-                        pattern: Pattern::DateYMD | Pattern::DateDMY,
-                        offset: _,
-                    } => DataType::Date,
-                    PatternWithOffset {
-                        pattern: Pattern::DatetimeYMDZ,
-                        offset: _,
-                    } => DataType::Utf8, // TODO: support tz-aware,
-                                         // need to keep track of offset
+                    Pattern::DatetimeYMD | Pattern::DatetimeDMY => {
+                        DataType::Datetime(TimeUnit::Microseconds, None)
+                    }
+                    Pattern::DateYMD | Pattern::DateDMY => DataType::Date,
+                    Pattern::DatetimeYMDZ => {
+                        DataType::Datetime(TimeUnit::Microseconds, Some("UTC".to_string()))
+                    }
                 },
                 None => DataType::Utf8,
             }
         }
         #[cfg(not(feature = "polars-time"))]
         {
             panic!("activate one of {{'dtype-date', 'dtype-datetime', dtype-time'}} features")
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/csv/write.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/csv/write.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/csv/write_impl.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/csv/write_impl.rs`

 * *Files 6% similar despite different names*

```diff
@@ -4,18 +4,17 @@
     feature = "dtype-date",
     feature = "dtype-time",
     feature = "dtype-datetime"
 ))]
 use arrow::temporal_conversions;
 #[cfg(feature = "timezones")]
 use chrono::TimeZone;
-#[cfg(feature = "timezones")]
-use chrono_tz::Tz;
 use lexical_core::{FormattedSize, ToLexical};
 use memchr::{memchr, memchr2};
+use polars_arrow::time_zone::Tz;
 use polars_core::prelude::*;
 use polars_core::series::SeriesIter;
 use polars_core::POOL;
 use polars_utils::contention_pool::LowContentionPool;
 use rayon::prelude::*;
 
 fn fmt_and_escape_str(f: &mut Vec<u8>, v: &str, options: &SerializeOptions) -> std::io::Result<()> {
@@ -52,19 +51,21 @@
         let buffer = std::slice::from_raw_parts_mut(f.as_mut_ptr().add(len), write_size);
         let written_n = n.to_lexical(buffer).len();
         f.set_len(len + written_n);
     }
     Ok(())
 }
 
-fn write_anyvalue(
+unsafe fn write_anyvalue(
     f: &mut Vec<u8>,
     value: AnyValue,
     options: &SerializeOptions,
-    #[allow(unused_variables)] datetime_format: Option<&str>,
+    datetime_formats: &[&str],
+    time_zones: &[Option<Tz>],
+    i: usize,
 ) -> PolarsResult<()> {
     match value {
         AnyValue::Null => write!(f, "{}", &options.null),
         AnyValue::Int8(v) => write!(f, "{v}"),
         AnyValue::Int16(v) => write!(f, "{v}"),
         AnyValue::Int32(v) => write!(f, "{v}"),
         AnyValue::Int64(v) => write!(f, "{v}"),
@@ -92,31 +93,25 @@
             let date = temporal_conversions::date32_to_date(v);
             match &options.date_format {
                 None => write!(f, "{date}"),
                 Some(fmt) => write!(f, "{}", date.format(fmt)),
             }
         }
         #[cfg(feature = "dtype-datetime")]
-        AnyValue::Datetime(v, tu, tz) => {
-            // If this is a datetime, then datetime_format was either set or inferred.
-            let datetime_format = datetime_format.unwrap();
+        AnyValue::Datetime(v, tu, _) => {
+            let datetime_format = { *datetime_formats.get_unchecked(i) };
+            let time_zone = { time_zones.get_unchecked(i) };
             let ndt = match tu {
                 TimeUnit::Nanoseconds => temporal_conversions::timestamp_ns_to_datetime(v),
                 TimeUnit::Microseconds => temporal_conversions::timestamp_us_to_datetime(v),
                 TimeUnit::Milliseconds => temporal_conversions::timestamp_ms_to_datetime(v),
             };
-            let formatted = match tz {
+            let formatted = match time_zone {
                 #[cfg(feature = "timezones")]
-                Some(tz) => match tz.parse::<Tz>() {
-                    Ok(parsed_tz) => parsed_tz.from_utc_datetime(&ndt).format(datetime_format),
-                    Err(_) => match temporal_conversions::parse_offset(tz) {
-                        Ok(parsed_tz) => parsed_tz.from_utc_datetime(&ndt).format(datetime_format),
-                        Err(_) => unreachable!(),
-                    },
-                },
+                Some(time_zone) => time_zone.from_utc_datetime(&ndt).format(datetime_format),
                 #[cfg(not(feature = "timezones"))]
                 Some(_) => {
                     panic!("activate 'timezones' feature");
                 }
                 _ => ndt.format(datetime_format),
             };
             write!(f, "{formatted}")
@@ -130,16 +125,15 @@
             }
         }
         ref dt => polars_bail!(ComputeError: "datatype {} cannot be written to csv", dt),
     }
     .map_err(|err| match value {
         #[cfg(feature = "dtype-datetime")]
         AnyValue::Datetime(_, _, tz) => {
-            // If this is a datetime, then datetime_format was either set or inferred.
-            let datetime_format = datetime_format.unwrap_or_default();
+            let datetime_format = unsafe { *datetime_formats.get_unchecked(i) };
             let type_name = if tz.is_some() {
                 "DateTime"
             } else {
                 "NaiveDateTime"
             };
             polars_err!(
                 ComputeError: "cannot format {} with format '{}'", type_name, datetime_format,
@@ -200,14 +194,20 @@
     options: &SerializeOptions,
 ) -> PolarsResult<()> {
     for s in df.get_columns() {
         let nested = match s.dtype() {
             DataType::List(_) => true,
             #[cfg(feature = "dtype-struct")]
             DataType::Struct(_) => true,
+            #[cfg(feature = "object")]
+            DataType::Object(_) => {
+                return Err(PolarsError::ComputeError(
+                    "csv writer does not suppert object dtype".into(),
+                ))
+            }
             _ => false,
         };
         polars_ensure!(
             !nested,
             ComputeError: "CSV format does not support nested data",
         );
     }
@@ -215,37 +215,83 @@
     // check that the double quote is valid utf8
     polars_ensure!(
         std::str::from_utf8(&[options.quote, options.quote]).is_ok(),
         ComputeError: "quote char results in invalid utf-8",
     );
     let delimiter = char::from(options.delimiter);
 
-    let formats: Option<Vec<Option<&str>>> = match &options.datetime_format {
-        None => Some(
-            df.get_columns()
-                .iter()
-                .map(|col| match col.dtype() {
-                    DataType::Datetime(TimeUnit::Milliseconds, tz) => match tz {
-                        Some(_) => Some("%FT%H:%M:%S.%3f%z"),
-                        None => Some("%FT%H:%M:%S.%3f"),
-                    },
-                    DataType::Datetime(TimeUnit::Microseconds, tz) => match tz {
-                        Some(_) => Some("%FT%H:%M:%S.%6f%z"),
-                        None => Some("%FT%H:%M:%S.%6f"),
-                    },
-                    DataType::Datetime(TimeUnit::Nanoseconds, tz) => match tz {
-                        Some(_) => Some("%FT%H:%M:%S.%9f%z"),
-                        None => Some("%FT%H:%M:%S.%9f"),
-                    },
-                    _ => None,
-                })
-                .collect::<Vec<_>>(),
-        ),
-        Some(_) => None,
-    };
+    let (datetime_formats, time_zones): (Vec<&str>, Vec<Option<Tz>>) = df
+        .get_columns()
+        .iter()
+        .map(|column| match column.dtype() {
+            DataType::Datetime(TimeUnit::Milliseconds, tz) => {
+                let (format, tz_parsed) = match tz {
+                    #[cfg(feature = "timezones")]
+                    Some(tz) => (
+                        options
+                            .datetime_format
+                            .as_deref()
+                            .unwrap_or("%FT%H:%M:%S.%3f%z"),
+                        tz.parse::<Tz>().ok(),
+                    ),
+                    _ => (
+                        options
+                            .datetime_format
+                            .as_deref()
+                            .unwrap_or("%FT%H:%M:%S.%3f"),
+                        None,
+                    ),
+                };
+                (format, tz_parsed)
+            }
+            DataType::Datetime(TimeUnit::Microseconds, tz) => {
+                let (format, tz_parsed) = match tz {
+                    #[cfg(feature = "timezones")]
+                    Some(tz) => (
+                        options
+                            .datetime_format
+                            .as_deref()
+                            .unwrap_or("%FT%H:%M:%S.%6f%z"),
+                        tz.parse::<Tz>().ok(),
+                    ),
+                    _ => (
+                        options
+                            .datetime_format
+                            .as_deref()
+                            .unwrap_or("%FT%H:%M:%S.%6f"),
+                        None,
+                    ),
+                };
+                (format, tz_parsed)
+            }
+            DataType::Datetime(TimeUnit::Nanoseconds, tz) => {
+                let (format, tz_parsed) = match tz {
+                    #[cfg(feature = "timezones")]
+                    Some(tz) => (
+                        options
+                            .datetime_format
+                            .as_deref()
+                            .unwrap_or("%FT%H:%M:%S.%9f%z"),
+                        tz.parse::<Tz>().ok(),
+                    ),
+                    _ => (
+                        options
+                            .datetime_format
+                            .as_deref()
+                            .unwrap_or("%FT%H:%M:%S.%9f"),
+                        None,
+                    ),
+                };
+                (format, tz_parsed)
+            }
+            _ => ("", None),
+        })
+        .unzip();
+    let datetime_formats = datetime_formats.into_iter().collect::<Vec<_>>();
+    let time_zones = time_zones.into_iter().collect::<Vec<_>>();
 
     let len = df.height();
     let n_threads = POOL.current_num_threads();
     let total_rows_per_pool_iter = n_threads * chunk_size;
     let any_value_iter_pool = LowContentionPool::<Vec<_>>::new(n_threads);
     let write_buffer_pool = LowContentionPool::<Vec<_>>::new(n_threads);
 
@@ -283,22 +329,25 @@
             col_iters.extend(any_value_iters);
 
             let last_ptr = &col_iters[col_iters.len() - 1] as *const SeriesIter;
             let mut finished = false;
             // loop rows
             while !finished {
                 for (i, col) in &mut col_iters.iter_mut().enumerate() {
-                    let datetime_format = match &options.datetime_format {
-                        Some(datetime_format) => Some(datetime_format.as_str()),
-                        None => unsafe { *formats.as_ref().unwrap().get_unchecked(i) },
-                    };
                     match col.next() {
-                        Some(value) => {
-                            write_anyvalue(&mut write_buffer, value, options, datetime_format)?;
-                        }
+                        Some(value) => unsafe {
+                            write_anyvalue(
+                                &mut write_buffer,
+                                value,
+                                options,
+                                &datetime_formats,
+                                &time_zones,
+                                i,
+                            )?;
+                        },
                         None => {
                             finished = true;
                             break;
                         }
                     }
                     let current_ptr = col as *const SeriesIter;
                     if current_ptr != last_ptr {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/ipc/ipc_file.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/ipc/ipc_file.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/ipc/ipc_stream.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/ipc/ipc_stream.rs`

 * *Files 1% similar despite different names*

```diff
@@ -270,15 +270,15 @@
             WriteOptions {
                 compression: self.compression,
             },
         );
 
         ipc_stream_writer.start(&df.schema().to_arrow(), None)?;
 
-        df.rechunk();
+        df.align_chunks();
         let iter = df.iter_chunks();
 
         for batch in iter {
             ipc_stream_writer.write(&batch, None)?
         }
         ipc_stream_writer.finish()?;
         Ok(())
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/ipc/mmap.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/ipc/mmap.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/ipc/write.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/ipc/write.rs`

 * *Files 1% similar despite different names*

```diff
@@ -72,15 +72,15 @@
             &mut self.writer,
             df.schema().to_arrow(),
             None,
             WriteOptions {
                 compression: self.compression.map(|c| c.into()),
             },
         )?;
-        df.rechunk();
+        df.align_chunks();
         let iter = df.iter_chunks();
 
         for batch in iter {
             ipc_writer.write(&batch, None)?
         }
         ipc_writer.finish()?;
         Ok(())
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/ipc/write_async.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/ipc/write_async.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/json.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/json/mod.rs`

 * *Files 2% similar despite different names*

```diff
@@ -69,14 +69,16 @@
 pub use arrow::error::Result as ArrowResult;
 pub use arrow::io::json;
 use polars_arrow::conversion::chunk_to_struct;
 use polars_arrow::utils::CustomIterTools;
 use polars_core::error::to_compute_err;
 use polars_core::prelude::*;
 use polars_core::utils::try_get_supertype;
+use polars_json::json::infer;
+use simd_json::BorrowedValue;
 
 use crate::mmap::{MmapBytesReader, ReaderBytes};
 use crate::prelude::*;
 
 /// The format to use to write the DataFrame to JSON: `Json` (a JSON array) or `JsonLines` (each row output on a
 /// separate line). In either case, each row is serialized as a JSON object whose keys are the column names and whose
 /// values are the row's corresponding values.
@@ -127,15 +129,15 @@
         JsonWriter {
             buffer,
             json_format: JsonFormat::JsonLines,
         }
     }
 
     fn finish(&mut self, df: &mut DataFrame) -> PolarsResult<()> {
-        df.rechunk();
+        df.align_chunks();
         let fields = df.iter().map(|s| s.field().to_arrow()).collect::<Vec<_>>();
         let batches = df
             .iter_chunks()
             .map(|chunk| Ok(Box::new(chunk_to_struct(chunk, fields.clone())) as ArrayRef));
 
         match self.json_format {
             JsonFormat::JsonLines => {
@@ -195,41 +197,41 @@
     /// incompatible types in the input. In the event that a column contains mixed dtypes, is it unspecified whether an
     /// error is returned or whether elements of incompatible dtypes are replaced with `null`.
     fn finish(self) -> PolarsResult<DataFrame> {
         let rb: ReaderBytes = (&self.reader).into();
 
         let out = match self.json_format {
             JsonFormat::Json => {
-                use arrow::io::json::read::json_deserializer::Value;
-                let bytes = rb.deref();
+                let mut bytes = rb.deref().to_vec();
                 let json_value =
-                    json::read::json_deserializer::parse(bytes).map_err(to_compute_err)?;
+                    simd_json::to_borrowed_value(&mut bytes).map_err(to_compute_err)?;
+
                 // likely struct type
-                let dtype = if let Value::Array(values) = &json_value {
+                let dtype = if let BorrowedValue::Array(values) = &json_value {
                     // struct types may have missing fields so find supertype
                     let dtype = values
                         .iter()
                         .take(self.infer_schema_len.unwrap_or(usize::MAX))
                         .map(|value| {
-                            json::read::infer(value)
+                            infer(value)
                                 .map_err(PolarsError::from)
                                 .map(|dt| DataType::from(&dt))
                         })
                         .fold_first_(|l, r| {
                             let l = l?;
                             let r = r?;
                             try_get_supertype(&l, &r)
                         })
                         .unwrap()?;
                     let dtype = DataType::List(Box::new(dtype));
                     dtype.to_arrow()
                 } else {
-                    json::read::infer(&json_value)?
+                    infer(&json_value)?
                 };
-                let arr = json::read::deserialize(&json_value, dtype)?;
+                let arr = polars_json::json::deserialize(&json_value, dtype)?;
                 let arr = arr.as_any().downcast_ref::<StructArray>().ok_or_else(
                     || polars_err!(ComputeError: "can only deserialize json objects"),
                 )?;
                 DataFrame::try_from(arr.clone())
             }
             JsonFormat::JsonLines => {
                 let mut json_reader = CoreJsonReader::new(
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/lib.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/lib.rs`

 * *Files 1% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 #[cfg(feature = "parquet")]
 pub mod export;
 #[cfg(any(feature = "ipc", feature = "ipc_streaming"))]
 pub mod ipc;
 #[cfg(feature = "json")]
 pub mod json;
 #[cfg(feature = "json")]
-pub mod ndjson_core;
+pub mod ndjson;
 #[cfg(feature = "cloud")]
 pub use crate::cloud::glob as async_glob;
 
 #[cfg(any(
     feature = "csv",
     feature = "parquet",
     feature = "ipc",
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/mmap.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/mmap.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/ndjson_core/buffer.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/ndjson/buffer.rs`

 * *Files 3% similar despite different names*

```diff
@@ -150,17 +150,17 @@
     T: PolarsNumericType,
     DatetimeInfer<T::Native>: TryFrom<Pattern>,
 {
     let val = match value {
         Value::String(s) => s,
         _ => return None,
     };
-    infer_pattern_single(val).and_then(|pattern_with_offset| {
-        match DatetimeInfer::<T::Native>::try_from(pattern_with_offset.pattern) {
-            Ok(mut infer) => infer.parse(val, pattern_with_offset.offset),
+    infer_pattern_single(val).and_then(|pattern| {
+        match DatetimeInfer::<T::Native>::try_from(pattern) {
+            Ok(mut infer) => infer.parse(val),
             Err(_) => None,
         }
     })
 }
 
 fn deserialize_all<'a>(json: &Value, dtype: &DataType) -> PolarsResult<AnyValue<'a>> {
     let out = match json {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/ndjson_core/ndjson.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/ndjson/core.rs`

 * *Files 4% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 use polars_core::prelude::*;
 use polars_core::utils::accumulate_dataframes_vertical;
 use polars_core::POOL;
 use rayon::prelude::*;
 
 use crate::csv::utils::*;
 use crate::mmap::{MmapBytesReader, ReaderBytes};
-use crate::ndjson_core::buffer::*;
+use crate::ndjson::buffer::*;
 use crate::prelude::*;
 const NEWLINE: u8 = b'\n';
 const RETURN: u8 = b'\r';
 const CLOSING_BRACKET: u8 = b'}';
 
 #[must_use]
 pub struct JsonLineReader<'a, R>
@@ -154,15 +154,16 @@
 
         let schema = match schema {
             Some(schema) => Cow::Borrowed(schema),
             None => {
                 let bytes: &[u8] = &reader_bytes;
                 let mut cursor = Cursor::new(bytes);
 
-                let data_type = arrow_ndjson::read::infer(&mut cursor, infer_schema_len)?;
+                let data_type = polars_json::ndjson::infer(&mut cursor, infer_schema_len)?;
+                dbg!(&data_type);
                 let schema = StructArray::get_fields(&data_type).iter().collect();
 
                 Cow::Owned(schema)
             }
         };
         Ok(CoreJsonReader {
             reader_bytes: Some(reader_bytes),
@@ -209,15 +210,15 @@
         };
         let file_chunks = get_file_chunks_json(bytes, n_threads);
         let dfs = POOL.install(|| {
             file_chunks
                 .into_par_iter()
                 .map(|(start_pos, stop_at_nbytes)| {
                     let mut buffers = init_buffers(&self.schema, capacity)?;
-                    let _ = parse_lines(&bytes[start_pos..stop_at_nbytes], &mut buffers);
+                    parse_lines(&bytes[start_pos..stop_at_nbytes], &mut buffers)?;
                     DataFrame::new(
                         buffers
                             .into_values()
                             .map(|buf| buf.into_series())
                             .collect::<_>(),
                     )
                 })
@@ -243,34 +244,35 @@
     }
 }
 
 #[inline(always)]
 fn parse_impl(
     bytes: &[u8],
     buffers: &mut PlIndexMap<BufferKey, Buffer>,
-    line: &mut Vec<u8>,
+    scratch: &mut Vec<u8>,
 ) -> PolarsResult<usize> {
-    line.clear();
-    line.extend_from_slice(bytes);
-    let n = line.len();
+    scratch.clear();
+    scratch.extend_from_slice(bytes);
+    let n = scratch.len();
     let all_good = match n {
         0 => true,
-        1 => line[0] == NEWLINE,
-        2 => line[0] == NEWLINE && line[1] == RETURN,
+        1 => scratch[0] == NEWLINE,
+        2 => scratch[0] == NEWLINE && scratch[1] == RETURN,
         _ => {
-            let value: simd_json::BorrowedValue = simd_json::to_borrowed_value(line)
+            let value: simd_json::BorrowedValue = simd_json::to_borrowed_value(scratch)
                 .map_err(|e| polars_err!(ComputeError: "error parsing line: {}", e))?;
             match value {
                 simd_json::BorrowedValue::Object(value) => {
-                    buffers
-                        .iter_mut()
-                        .for_each(|(s, inner)| match s.0.map_lookup(&value) {
-                            Some(v) => inner.add(v).expect("inner.add(v)"),
+                    buffers.iter_mut().try_for_each(|(s, inner)| {
+                        match s.0.map_lookup(&value) {
+                            Some(v) => inner.add(v)?,
                             None => inner.add_null(),
-                        });
+                        }
+                        PolarsResult::Ok(())
+                    })?;
                 }
                 _ => {
                     buffers.iter_mut().for_each(|(_, inner)| inner.add_null());
                 }
             };
             true
         }
@@ -278,29 +280,22 @@
     polars_ensure!(all_good, ComputeError: "invalid JSON: unexpected end of file");
     Ok(n)
 }
 
 fn parse_lines(bytes: &[u8], buffers: &mut PlIndexMap<BufferKey, Buffer>) -> PolarsResult<()> {
     let mut buf = vec![];
 
-    let total_bytes = bytes.len();
-    let mut offset = 0;
     // The `RawValue` is a pointer to the original JSON string and does not perform any deserialization.
     // It is used to properly iterate over the lines without re-implementing the splitlines logic when this does the same thing.
     let mut iter =
         serde_json::Deserializer::from_slice(bytes).into_iter::<Box<serde_json::value::RawValue>>();
     while let Some(Ok(value)) = iter.next() {
         let bytes = value.get().as_bytes();
-        offset += bytes.len();
         parse_impl(bytes, buffers, &mut buf)?;
     }
-    polars_ensure!(
-        offset == total_bytes,
-        ComputeError: "expected {} bytes, but only parsed {}", total_bytes, offset,
-    );
     Ok(())
 }
 
 /// Find the nearest next line position.
 /// Does not check for new line characters embedded in String fields.
 /// This just looks for `}\n`
 pub(crate) fn next_line_position_naive_json(input: &[u8]) -> Option<usize> {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/parquet/async_impl.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/parquet/async_impl.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/parquet/mmap.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/parquet/mmap.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/parquet/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/parquet/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/parquet/predicates.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/parquet/predicates.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/parquet/read.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/parquet/read.rs`

 * *Files 1% similar despite different names*

```diff
@@ -73,15 +73,15 @@
             predicate,
             self.parallel,
             self.row_count,
             self.use_statistics,
         )
         .map(|mut df| {
             if rechunk {
-                df.rechunk();
+                df.align_chunks();
             };
             df
         })
     }
 
     /// Try to reduce memory pressure at the expense of performance. If setting this does not reduce memory
     /// enough, turn off parallelization.
@@ -205,15 +205,15 @@
             None,
             self.parallel,
             self.row_count,
             self.use_statistics,
         )
         .map(|mut df| {
             if self.rechunk {
-                df.rechunk();
+                df.as_single_chunk_par();
             }
             df
         })
     }
 }
 
 /// A Parquet reader on top of the async object_store API. Only the batch reader is implemented since
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/parquet/read_impl.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/parquet/read_impl.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/parquet/write.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/parquet/write.rs`

 * *Files 0% similar despite different names*

```diff
@@ -90,15 +90,15 @@
 #[must_use]
 pub struct ParquetWriter<W> {
     writer: W,
     /// Data page compression
     compression: CompressionOptions,
     /// Compute and write column statistics.
     statistics: bool,
-    /// If `None` will be all written to a single row group.
+    /// if `None` will be 512^2 rows
     row_group_size: Option<usize>,
     /// if `None` will be 1024^2 bytes
     data_pagesize_limit: Option<usize>,
     /// Serialize columns in parallel
     parallel: bool,
 }
 
@@ -181,22 +181,20 @@
             parallel: self.parallel,
         })
     }
 
     /// Write the given DataFrame in the the writer `W`. Returns the total size of the file.
     pub fn finish(self, df: &mut DataFrame) -> PolarsResult<u64> {
         // ensures all chunks are aligned.
-        df.rechunk();
+        df.align_chunks();
 
-        if let Some(n) = self.row_group_size {
-            let n_splits = df.height() / n;
-            if n_splits > 0 {
-                *df = accumulate_dataframes_vertical_unchecked(split_df(df, n_splits)?);
-            }
-        };
+        let n_splits = df.height() / self.row_group_size.unwrap_or(512 * 512);
+        if n_splits > 0 {
+            *df = accumulate_dataframes_vertical_unchecked(split_df(df, n_splits)?);
+        }
         let mut batched = self.batched(&df.schema())?;
         batched.write_batch(df)?;
         batched.finish()
     }
 }
 
 // Note that the df should be rechunked
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/partition.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/partition.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/predicates.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/predicates.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/prelude.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/prelude.rs`

 * *Files 1% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 #[cfg(feature = "csv")]
 pub use crate::csv::*;
 #[cfg(any(feature = "ipc", feature = "ipc_streaming"))]
 pub use crate::ipc::*;
 #[cfg(feature = "json")]
 pub use crate::json::*;
 #[cfg(feature = "json")]
-pub use crate::ndjson_core::ndjson::*;
+pub use crate::ndjson::core::*;
 #[cfg(feature = "parquet")]
 pub use crate::parquet::*;
 pub use crate::utils::*;
 pub use crate::{SerReader, SerWriter};
 #[cfg(test)]
 pub(crate) fn create_df() -> DataFrame {
     let s0 = Series::new("days", [0, 1, 2, 3, 4].as_ref());
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-io/src/utils.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/src/utils.rs`

 * *Files 4% similar despite different names*

```diff
@@ -56,18 +56,17 @@
         let mut column_names = AHashMap::with_capacity(schema.fields.len());
         schema.fields.iter().enumerate().for_each(|(i, c)| {
             column_names.insert(c.name.as_str(), i);
         });
 
         for column in columns.iter() {
             let Some(&i) = column_names.get(column.as_str()) else {
-                let valid_columns: Vec<String> = schema.fields.iter().map(|f| f.name.clone()).collect();
                 polars_bail!(
                     ColumnNotFound:
-                    "unable to find {:?}; valid columns: {:?}", column, valid_columns,
+                    "unable to find column {:?}; valid columns: {:?}", column, schema.get_names(),
                 );
             };
             prj.push(i);
         }
     } else {
         for column in columns.iter() {
             let i = schema.try_index_of(column)?;
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/Cargo.toml` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/Cargo.toml`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 [package]
 name = "polars-core"
-version= "0.28.0"
+version= "0.30.0"
 authors = ["ritchie46 <ritchie46@gmail.com>"]
 edition = "2021"
 license = "MIT"
 repository = "https://github.com/pola-rs/polars"
 description = "Core of the Polars DataFrame library"
 
 # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html
@@ -81,17 +81,18 @@
 dynamic_groupby = ["dtype-datetime", "dtype-date"]
 
 # opt-in datatypes for Series
 dtype-date = ["temporal"]
 dtype-datetime = ["temporal"]
 dtype-duration = ["temporal"]
 dtype-time = ["temporal"]
+dtype-array = ["polars-arrow/dtype-array"]
 dtype-i8 = []
 dtype-i16 = []
-dtype-decimal = ["dep:itoap"]
+dtype-decimal = ["dep:itoap", "polars-arrow/dtype-decimal"]
 dtype-u8 = []
 dtype-u16 = []
 dtype-categorical = []
 dtype-struct = []
 
 parquet = ["arrow/io_parquet"]
 
@@ -152,24 +153,24 @@
 ahash= "0.8"
 bitflags= "1.3"
 chrono = { version = "0.4", default-features = false, features = ["std"], optional = true }
 chrono-tz = { version = "0.8", optional = true }
 comfy-table = { version = "6.1.4", optional = true, default_features = false }
 either= "1.8"
 hashbrown= { version = "0.13.1", features = ["rayon", "ahash"] }
-indexmap = { version = "1", features = ["std"] }
-itoap = { version = "1", optional = true }
+indexmap= { version = "1", features = ["std"] }
+itoap = { version = "1", optional = true, feature = ["simd"] }
 ndarray = { version = "0.15", optional = true, default_features = false }
 num-traits= "0.2"
 object_store = { version = "0.5.3", default-features = false, optional = true }
 once_cell= "1"
-polars-arrow = { version = "0.28.0", path = "../polars-arrow", features = ["compute"] }
-polars-error = { version = "0.28.0", path = "../polars-error" }
-polars-row = { version = "0.28.0", path = "../polars-row" }
-polars-utils = { version = "0.28.0", path = "../polars-utils" }
+polars-arrow = { version = "0.30.0", path = "../polars-arrow", features = ["compute"] }
+polars-error = { version = "0.30.0", path = "../polars-error" }
+polars-row = { version = "0.30.0", path = "../polars-row" }
+polars-utils = { version = "0.30.0", path = "../polars-utils" }
 rand = { version = "0.8", optional = true, features = ["small_rng", "std"] }
 rand_distr = { version = "0.4", optional = true }
 rayon= "1.6"
 regex = { version = "1.6", optional = true }
 # activate if you want serde support for Series and DataFrames
 serde = { version = "1", features = ["derive"], optional = true }
 serde_json = { version = "1", optional = true }
@@ -177,18 +178,18 @@
 thiserror= "^1"
 url = { version = "2.3.1", optional = true }
 xxhash-rust= { version = "0.8.6", features = ["xxh3"] }
 
 [dependencies.arrow]
 package = "arrow2"
 # git = "https://github.com/jorgecarleitao/arrow2"
-git = "https://github.com/ritchie46/arrow2"
+# git = "https://github.com/ritchie46/arrow2"
 # rev = "1491c6e8f4fd100f53c358e4f3ef1536d9e75090"
 # path = "../arrow2"
-branch = "polars_2023-04-20"
+# branch = "polars_2023-05-25"
 version = "0.17"
 default-features = false
 features = [
   "compute_aggregate",
   "compute_arithmetics",
   "compute_boolean",
   "compute_boolean_kleene",
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/LICENSE` & `polars_lts_cpu-0.18.0/local_dependencies/polars-io/LICENSE`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/arithmetic.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/arithmetic.rs`

 * *Files 11% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 //! Implementations of arithmetic operations on ChunkedArray's.
-use std::borrow::Cow;
 use std::ops::{Add, Div, Mul, Rem, Sub};
 
 use arrow::array::PrimitiveArray;
 use arrow::compute::arithmetics::basic;
 #[cfg(feature = "dtype-decimal")]
 use arrow::compute::arithmetics::decimal;
 use arrow::compute::arity_assign;
 use arrow::types::NativeType;
-use num_traits::{Num, NumCast, ToPrimitive};
+use num_traits::{Num, NumCast, ToPrimitive, Zero};
+use polars_arrow::utils::combine_validities_and;
 
 use crate::prelude::*;
 use crate::series::IsSorted;
 use crate::utils::{align_chunks_binary, align_chunks_binary_owned};
 
 pub trait ArrayArithmetics
 where
@@ -89,59 +89,15 @@
     }
 
     fn rem_scalar(_lhs: &PrimitiveArray<Self>, _rhs: &Self) -> PrimitiveArray<Self> {
         unimplemented!("requires support in arrow2 crate")
     }
 }
 
-macro_rules! apply_operand_on_chunkedarray_by_iter {
-
-    ($self:ident, $rhs:ident, $operand:tt) => {
-            {
-                match ($self.has_validity(), $rhs.has_validity()) {
-                    (false, false) => {
-                        let a: NoNull<ChunkedArray<_>> = $self
-                        .into_no_null_iter()
-                        .zip($rhs.into_no_null_iter())
-                        .map(|(left, right)| left $operand right)
-                        .collect_trusted();
-                        a.into_inner()
-                    },
-                    (false, _) => {
-                        $self
-                        .into_no_null_iter()
-                        .zip($rhs.into_iter())
-                        .map(|(left, opt_right)| opt_right.map(|right| left $operand right))
-                        .collect_trusted()
-                    },
-                    (_, false) => {
-                        $self
-                        .into_iter()
-                        .zip($rhs.into_no_null_iter())
-                        .map(|(opt_left, right)| opt_left.map(|left| left $operand right))
-                        .collect_trusted()
-                    },
-                    (_, _) => {
-                    $self.into_iter()
-                        .zip($rhs.into_iter())
-                        .map(|(opt_left, opt_right)| match (opt_left, opt_right) {
-                            (None, None) => None,
-                            (None, Some(_)) => None,
-                            (Some(_), None) => None,
-                            (Some(left), Some(right)) => Some(left $operand right),
-                        })
-                        .collect_trusted()
-
-                    }
-                }
-            }
-    }
-}
-
-fn arithmetic_helper<T, Kernel, F>(
+pub(super) fn arithmetic_helper<T, Kernel, F>(
     lhs: &ChunkedArray<T>,
     rhs: &ChunkedArray<T>,
     kernel: Kernel,
     operation: F,
 ) -> ChunkedArray<T>
 where
     T: PolarsNumericType,
@@ -394,103 +350,106 @@
     T: PolarsNumericType,
     N: Num + ToPrimitive,
 {
     type Output = ChunkedArray<T>;
 
     fn add(self, rhs: N) -> Self::Output {
         let adder: T::Native = NumCast::from(rhs).unwrap();
-        self.apply(|val| val + adder)
+        let mut out = self.apply(|val| val + adder);
+        out.set_sorted_flag(self.is_sorted_flag());
+        out
     }
 }
 
 impl<T, N> Sub<N> for &ChunkedArray<T>
 where
     T: PolarsNumericType,
     N: Num + ToPrimitive,
 {
     type Output = ChunkedArray<T>;
 
     fn sub(self, rhs: N) -> Self::Output {
         let subber: T::Native = NumCast::from(rhs).unwrap();
-        self.apply(|val| val - subber)
+        let mut out = self.apply(|val| val - subber);
+        out.set_sorted_flag(self.is_sorted_flag());
+        out
     }
 }
 
 impl<T, N> Div<N> for &ChunkedArray<T>
 where
     T: PolarsNumericType,
     N: Num + ToPrimitive,
 {
     type Output = ChunkedArray<T>;
 
     fn div(self, rhs: N) -> Self::Output {
         let rhs: T::Native = NumCast::from(rhs).expect("could not cast");
-        self.apply_kernel(&|arr| Box::new(<T::Native as ArrayArithmetics>::div_scalar(arr, &rhs)))
+        let mut out = self
+            .apply_kernel(&|arr| Box::new(<T::Native as ArrayArithmetics>::div_scalar(arr, &rhs)));
+
+        if rhs < T::Native::zero() {
+            out.set_sorted_flag(self.is_sorted_flag().reverse());
+        } else {
+            out.set_sorted_flag(self.is_sorted_flag());
+        }
+        out
     }
 }
 
 impl<T, N> Mul<N> for &ChunkedArray<T>
 where
     T: PolarsNumericType,
     N: Num + ToPrimitive,
 {
     type Output = ChunkedArray<T>;
 
     fn mul(self, rhs: N) -> Self::Output {
+        // don't set sorted flag as probability of overflow is higher
         let multiplier: T::Native = NumCast::from(rhs).unwrap();
-        self.apply(|val| val * multiplier)
+        let rhs = ChunkedArray::from_vec("", vec![multiplier]);
+        self.mul(&rhs)
     }
 }
 
 impl<T, N> Rem<N> for &ChunkedArray<T>
 where
     T: PolarsNumericType,
     N: Num + ToPrimitive,
 {
     type Output = ChunkedArray<T>;
 
     fn rem(self, rhs: N) -> Self::Output {
         let rhs: T::Native = NumCast::from(rhs).expect("could not cast");
-        self.apply_kernel(&|arr| Box::new(<T::Native as ArrayArithmetics>::rem_scalar(arr, &rhs)))
+        let rhs = ChunkedArray::from_vec("", vec![rhs]);
+        self.rem(&rhs)
     }
 }
 
 impl<T, N> Add<N> for ChunkedArray<T>
 where
     T: PolarsNumericType,
     N: Num + ToPrimitive,
 {
     type Output = ChunkedArray<T>;
 
-    fn add(mut self, rhs: N) -> Self::Output {
-        if std::env::var("ASSIGN").is_ok() {
-            let adder: T::Native = NumCast::from(rhs).unwrap();
-            self.apply_mut(|val| val + adder);
-            self
-        } else {
-            (&self).add(rhs)
-        }
+    fn add(self, rhs: N) -> Self::Output {
+        (&self).add(rhs)
     }
 }
 
 impl<T, N> Sub<N> for ChunkedArray<T>
 where
     T: PolarsNumericType,
     N: Num + ToPrimitive,
 {
     type Output = ChunkedArray<T>;
 
-    fn sub(mut self, rhs: N) -> Self::Output {
-        if std::env::var("ASSIGN").is_ok() {
-            let subber: T::Native = NumCast::from(rhs).unwrap();
-            self.apply_mut(|val| val - subber);
-            self
-        } else {
-            (&self).sub(rhs)
-        }
+    fn sub(self, rhs: N) -> Self::Output {
+        (&self).sub(rhs)
     }
 }
 
 impl<T, N> Div<N> for ChunkedArray<T>
 where
     T: PolarsNumericType,
     N: Num + ToPrimitive,
@@ -506,21 +465,17 @@
 where
     T: PolarsNumericType,
     N: Num + ToPrimitive,
 {
     type Output = ChunkedArray<T>;
 
     fn mul(mut self, rhs: N) -> Self::Output {
-        if std::env::var("ASSIGN").is_ok() {
-            let multiplier: T::Native = NumCast::from(rhs).unwrap();
-            self.apply_mut(|val| val * multiplier);
-            self
-        } else {
-            (&self).mul(rhs)
-        }
+        let multiplier: T::Native = NumCast::from(rhs).unwrap();
+        self.apply_mut(|val| val * multiplier);
+        self
     }
 }
 
 impl<T, N> Rem<N> for ChunkedArray<T>
 where
     T: PolarsNumericType,
     N: Num + ToPrimitive,
@@ -528,61 +483,26 @@
     type Output = ChunkedArray<T>;
 
     fn rem(self, rhs: N) -> Self::Output {
         (&self).rem(rhs)
     }
 }
 
-fn concat_strings(l: &str, r: &str) -> String {
-    // fastest way to concat strings according to https://github.com/hoodie/concatenation_benchmarks-rs
-    let mut s = String::with_capacity(l.len() + r.len());
-    s.push_str(l);
-    s.push_str(r);
-    s
-}
+fn concat_binary_arrs(l: &[u8], r: &[u8], buf: &mut Vec<u8>) {
+    buf.clear();
 
-fn concat_binary_arrs(l: &[u8], r: &[u8]) -> Vec<u8> {
-    let mut v = Vec::with_capacity(l.len() + r.len());
-    v.extend_from_slice(l);
-    v.extend_from_slice(r);
-    v
+    buf.extend_from_slice(l);
+    buf.extend_from_slice(r);
 }
 
 impl Add for &Utf8Chunked {
     type Output = Utf8Chunked;
 
     fn add(self, rhs: Self) -> Self::Output {
-        // broadcasting path rhs
-        if rhs.len() == 1 {
-            let rhs = rhs.get(0);
-            return match rhs {
-                Some(rhs) => self.add(rhs),
-                None => Utf8Chunked::full_null(self.name(), self.len()),
-            };
-        }
-        // broadcasting path lhs
-        if self.len() == 1 {
-            let lhs = self.get(0);
-            return match lhs {
-                Some(lhs) => rhs.apply(|s| Cow::Owned(concat_strings(lhs, s))),
-                None => Utf8Chunked::full_null(self.name(), rhs.len()),
-            };
-        }
-
-        // todo! add no_null variants. Need 4 paths.
-        let mut ca: Self::Output = self
-            .into_iter()
-            .zip(rhs.into_iter())
-            .map(|(opt_l, opt_r)| match (opt_l, opt_r) {
-                (Some(l), Some(r)) => Some(concat_strings(l, r)),
-                _ => None,
-            })
-            .collect_trusted();
-        ca.rename(self.name());
-        ca
+        unsafe { (self.as_binary() + rhs.as_binary()).to_utf8() }
     }
 }
 
 impl Add for Utf8Chunked {
     type Output = Utf8Chunked;
 
     fn add(self, rhs: Self) -> Self::Output {
@@ -590,61 +510,80 @@
     }
 }
 
 impl Add<&str> for &Utf8Chunked {
     type Output = Utf8Chunked;
 
     fn add(self, rhs: &str) -> Self::Output {
-        let mut ca: Self::Output = match self.has_validity() {
-            false => self
-                .into_no_null_iter()
-                .map(|l| concat_strings(l, rhs))
-                .collect_trusted(),
-            _ => self
-                .into_iter()
-                .map(|opt_l| opt_l.map(|l| concat_strings(l, rhs)))
-                .collect_trusted(),
-        };
-        ca.rename(self.name());
-        ca
+        unsafe { ((&self.as_binary()) + rhs.as_bytes()).to_utf8() }
     }
 }
 
+fn concat_binary(a: &BinaryArray<i64>, b: &BinaryArray<i64>) -> BinaryArray<i64> {
+    let validity = combine_validities_and(a.validity(), b.validity());
+    let mut values = Vec::with_capacity(a.get_values_size() + b.get_values_size());
+    let mut offsets = Vec::with_capacity(a.len() + 1);
+    let mut offset_so_far = 0i64;
+    offsets.push(offset_so_far);
+
+    for (a, b) in a.values_iter().zip(b.values_iter()) {
+        values.extend_from_slice(a);
+        values.extend_from_slice(b);
+        offset_so_far = values.len() as i64;
+        offsets.push(offset_so_far)
+    }
+    unsafe { BinaryArray::from_data_unchecked_default(offsets.into(), values.into(), validity) }
+}
+
 impl Add for &BinaryChunked {
     type Output = BinaryChunked;
 
     fn add(self, rhs: Self) -> Self::Output {
         // broadcasting path rhs
         if rhs.len() == 1 {
             let rhs = rhs.get(0);
+            let mut buf = vec![];
             return match rhs {
-                Some(rhs) => self.add(rhs),
+                Some(rhs) => {
+                    self.apply_mut(|s| {
+                        concat_binary_arrs(s, rhs, &mut buf);
+                        let out = buf.as_slice();
+                        // safety: lifetime is bound to the outer scope and the
+                        // ref is valid for the lifetime of this closure
+                        unsafe { std::mem::transmute::<_, &'static [u8]>(out) }
+                    })
+                }
                 None => BinaryChunked::full_null(self.name(), self.len()),
             };
         }
         // broadcasting path lhs
         if self.len() == 1 {
             let lhs = self.get(0);
+            let mut buf = vec![];
             return match lhs {
-                Some(lhs) => rhs.apply(|s| Cow::Owned(concat_binary_arrs(lhs, s))),
+                Some(lhs) => rhs.apply_mut(|s| {
+                    concat_binary_arrs(lhs, s, &mut buf);
+
+                    let out = buf.as_slice();
+                    // safety: lifetime is bound to the outer scope and the
+                    // ref is valid for the lifetime of this closure
+                    unsafe { std::mem::transmute::<_, &'static [u8]>(out) }
+                }),
                 None => BinaryChunked::full_null(self.name(), rhs.len()),
             };
         }
 
-        // todo! add no_null variants. Need 4 paths.
-        let mut ca: Self::Output = self
-            .into_iter()
-            .zip(rhs.into_iter())
-            .map(|(opt_l, opt_r)| match (opt_l, opt_r) {
-                (Some(l), Some(r)) => Some(concat_binary_arrs(l, r)),
-                _ => None,
-            })
-            .collect_trusted();
-        ca.rename(self.name());
-        ca
+        let (lhs, rhs) = align_chunks_binary(self, rhs);
+        let chunks = lhs
+            .downcast_iter()
+            .zip(rhs.downcast_iter())
+            .map(|(a, b)| Box::new(concat_binary(a, b)) as ArrayRef)
+            .collect();
+
+        unsafe { BinaryChunked::from_chunks(self.name(), chunks) }
     }
 }
 
 impl Add for BinaryChunked {
     type Output = BinaryChunked;
 
     fn add(self, rhs: Self) -> Self::Output {
@@ -652,26 +591,63 @@
     }
 }
 
 impl Add<&[u8]> for &BinaryChunked {
     type Output = BinaryChunked;
 
     fn add(self, rhs: &[u8]) -> Self::Output {
-        let mut ca: Self::Output = match self.has_validity() {
-            false => self
-                .into_no_null_iter()
-                .map(|l| concat_binary_arrs(l, rhs))
-                .collect_trusted(),
-            _ => self
-                .into_iter()
-                .map(|opt_l| opt_l.map(|l| concat_binary_arrs(l, rhs)))
-                .collect_trusted(),
-        };
-        ca.rename(self.name());
-        ca
+        let arr = BinaryArray::<i64>::from_slice([rhs]);
+        let rhs = unsafe { BinaryChunked::from_chunks("", vec![Box::new(arr) as ArrayRef]) };
+        self.add(&rhs)
+    }
+}
+
+fn add_boolean(a: &BooleanArray, b: &BooleanArray) -> PrimitiveArray<IdxSize> {
+    let validity = combine_validities_and(a.validity(), b.validity());
+
+    let values = a
+        .values_iter()
+        .zip(b.values_iter())
+        .map(|(a, b)| a as IdxSize + b as IdxSize)
+        .collect::<Vec<_>>();
+    PrimitiveArray::from_data_default(values.into(), validity)
+}
+
+impl Add for &BooleanChunked {
+    type Output = IdxCa;
+
+    fn add(self, rhs: Self) -> Self::Output {
+        // broadcasting path rhs
+        if rhs.len() == 1 {
+            let rhs = rhs.get(0);
+            return match rhs {
+                Some(rhs) => self.apply_cast_numeric(|v| v as IdxSize + rhs as IdxSize),
+                None => IdxCa::full_null(self.name(), self.len()),
+            };
+        }
+        // broadcasting path lhs
+        if self.len() == 1 {
+            return rhs.add(self);
+        }
+        let (lhs, rhs) = align_chunks_binary(self, rhs);
+        let chunks = lhs
+            .downcast_iter()
+            .zip(rhs.downcast_iter())
+            .map(|(a, b)| Box::new(add_boolean(a, b)) as ArrayRef)
+            .collect::<Vec<_>>();
+
+        unsafe { IdxCa::from_chunks(self.name(), chunks) }
+    }
+}
+
+impl Add for BooleanChunked {
+    type Output = IdxCa;
+
+    fn add(self, rhs: Self) -> Self::Output {
+        (&self).add(&rhs)
     }
 }
 
 #[cfg(test)]
 pub(crate) mod test {
     use crate::prelude::*;
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/bitwise.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/zip.rs`

 * *Files 24% similar despite different names*

```diff
@@ -1,272 +1,227 @@
-use std::ops::{BitAnd, BitOr, BitXor, Not};
+use arrow::compute::if_then_else::if_then_else;
+use polars_arrow::array::default_arrays::FromData;
 
-use arrow::compute;
+use crate::prelude::*;
+use crate::utils::{align_chunks_ternary, CustomIterTools};
 
-use super::*;
-use crate::utils::{align_chunks_binary, combine_validities_and, CustomIterTools};
-
-impl<T> BitAnd for &ChunkedArray<T>
-where
-    T: PolarsIntegerType,
-    T::Native: BitAnd<Output = T::Native>,
-{
-    type Output = ChunkedArray<T>;
-
-    fn bitand(self, rhs: Self) -> Self::Output {
-        let (l, r) = align_chunks_binary(self, rhs);
-        let chunks = l
-            .downcast_iter()
-            .zip(r.downcast_iter())
-            .map(|(l_arr, r_arr)| {
-                let l_vals = l_arr.values().as_slice();
-                let r_vals = r_arr.values().as_slice();
-                let validity = combine_validities_and(l_arr.validity(), r_arr.validity());
-
-                let av = l_vals
-                    .iter()
-                    .zip(r_vals)
-                    .map(|(l, r)| *l & *r)
-                    .collect_trusted::<Vec<_>>();
-
-                let arr = PrimitiveArray::new(T::get_dtype().to_arrow(), av.into(), validity);
-                Box::new(arr) as ArrayRef
-            })
-            .collect::<Vec<_>>();
-
-        // safety: same type
-        unsafe { ChunkedArray::from_chunks(self.name(), chunks) }
+fn ternary_apply<T>(predicate: bool, truthy: T, falsy: T) -> T {
+    if predicate {
+        truthy
+    } else {
+        falsy
     }
 }
 
-impl<T> BitOr for &ChunkedArray<T>
-where
-    T: PolarsIntegerType,
-    T::Native: BitOr<Output = T::Native>,
-{
-    type Output = ChunkedArray<T>;
-
-    fn bitor(self, rhs: Self) -> Self::Output {
-        let (l, r) = align_chunks_binary(self, rhs);
-        let chunks = l
-            .downcast_iter()
-            .zip(r.downcast_iter())
-            .map(|(l_arr, r_arr)| {
-                let l_vals = l_arr.values().as_slice();
-                let r_vals = r_arr.values().as_slice();
-                let validity = combine_validities_and(l_arr.validity(), r_arr.validity());
-
-                let av = l_vals
-                    .iter()
-                    .zip(r_vals)
-                    .map(|(l, r)| *l | *r)
-                    .collect_trusted::<Vec<_>>();
+fn prepare_mask(mask: &BooleanArray) -> BooleanArray {
+    // make sure that zip works same as main branch
+    // that is that null are ignored from mask and that we take from the right array
 
-                let arr = PrimitiveArray::new(T::get_dtype().to_arrow(), av.into(), validity);
-                Box::new(arr) as ArrayRef
-            })
-            .collect::<Vec<_>>();
-
-        // safety: same type
-        unsafe { ChunkedArray::from_chunks(self.name(), chunks) }
-    }
-}
-
-impl<T> BitXor for &ChunkedArray<T>
-where
-    T: PolarsIntegerType,
-    T::Native: BitXor<Output = T::Native>,
-{
-    type Output = ChunkedArray<T>;
-
-    fn bitxor(self, rhs: Self) -> Self::Output {
-        let (l, r) = align_chunks_binary(self, rhs);
-        let chunks = l
-            .downcast_iter()
-            .zip(r.downcast_iter())
-            .map(|(l_arr, r_arr)| {
-                let l_vals = l_arr.values().as_slice();
-                let r_vals = r_arr.values().as_slice();
-                let validity = combine_validities_and(l_arr.validity(), r_arr.validity());
-
-                let av = l_vals
-                    .iter()
-                    .zip(r_vals)
-                    .map(|(l, r)| l.bitxor(*r))
-                    .collect_trusted::<Vec<_>>();
-
-                let arr = PrimitiveArray::new(T::get_dtype().to_arrow(), av.into(), validity);
-                Box::new(arr) as ArrayRef
-            })
-            .collect::<Vec<_>>();
-
-        // safety: same type
-        unsafe { ChunkedArray::from_chunks(self.name(), chunks) }
+    match mask.validity() {
+        // nulls are set to true meaning we take from the right in the zip/ if_then_else kernel
+        Some(validity) if validity.unset_bits() != 0 => {
+            let mask = mask.values() & validity;
+            BooleanArray::from_data_default(mask, None)
+        }
+        _ => mask.clone(),
     }
 }
 
-impl BitOr for &BooleanChunked {
-    type Output = BooleanChunked;
-
-    fn bitor(self, rhs: Self) -> Self::Output {
-        match (self.len(), rhs.len()) {
-            // make sure that we fall through if both are equal unit lengths
-            // otherwise we stackoverflow
-            (1, 1) => {}
-            (1, _) => {
-                return match self.get(0) {
-                    Some(true) => BooleanChunked::full(self.name(), true, rhs.len()),
-                    Some(false) => {
-                        let mut rhs = rhs.clone();
-                        rhs.rename(self.name());
-                        rhs
-                    }
-                    None => &self.new_from_index(0, rhs.len()) | rhs,
-                };
+macro_rules! impl_ternary_broadcast {
+    ($self:ident, $self_len:expr, $other_len:expr, $other:expr, $mask:expr, $ty:ty) => {{
+        match ($self_len, $other_len) {
+            (1, 1) => {
+                let left = $self.get(0);
+                let right = $other.get(0);
+                let mut val: ChunkedArray<$ty> = $mask
+                    .into_no_null_iter()
+                    .map(|mask_val| ternary_apply(mask_val, left, right))
+                    .collect_trusted();
+                val.rename($self.name());
+                Ok(val)
             }
             (_, 1) => {
-                return match rhs.get(0) {
-                    Some(true) => BooleanChunked::full(self.name(), true, self.len()),
-                    Some(false) => self.clone(),
-                    None => &rhs.new_from_index(0, self.len()) | self,
-                };
+                let right = $other.get(0);
+                let mut val: ChunkedArray<$ty> = $mask
+                    .into_no_null_iter()
+                    .zip($self)
+                    .map(|(mask_val, left)| ternary_apply(mask_val, left, right))
+                    .collect_trusted();
+                val.rename($self.name());
+                Ok(val)
             }
-            _ => {}
+            (1, _) => {
+                let left = $self.get(0);
+                let mut val: ChunkedArray<$ty> = $mask
+                    .into_no_null_iter()
+                    .zip($other)
+                    .map(|(mask_val, right)| ternary_apply(mask_val, left, right))
+                    .collect_trusted();
+                val.rename($self.name());
+                Ok(val)
+            }
+            (_, _) => Err(polars_err!(
+                ShapeMismatch: "shapes of `mask` and `other` are not suitable for `zip_with` operation"
+            )),
         }
-
-        let (lhs, rhs) = align_chunks_binary(self, rhs);
-        let chunks = lhs
-            .downcast_iter()
-            .zip(rhs.downcast_iter())
-            .map(|(lhs, rhs)| Box::new(compute::boolean_kleene::or(lhs, rhs)) as ArrayRef)
-            .collect();
-        // safety: same type
-        unsafe { BooleanChunked::from_chunks(self.name(), chunks) }
-    }
+    }};
 }
 
-impl BitOr for BooleanChunked {
-    type Output = BooleanChunked;
-
-    fn bitor(self, rhs: Self) -> Self::Output {
-        (&self).bitor(&rhs)
+impl<T> ChunkZip<T> for ChunkedArray<T>
+where
+    T: PolarsNumericType,
+{
+    fn zip_with(
+        &self,
+        mask: &BooleanChunked,
+        other: &ChunkedArray<T>,
+    ) -> PolarsResult<ChunkedArray<T>> {
+        // broadcasting path
+        if self.len() != mask.len() || other.len() != mask.len() {
+            impl_ternary_broadcast!(self, self.len(), other.len(), other, mask, T)
+        } else {
+            let (left, right, mask) = align_chunks_ternary(self, other, mask);
+            let chunks = left
+                .downcast_iter()
+                .zip(right.downcast_iter())
+                .zip(mask.downcast_iter())
+                .map(|((left_c, right_c), mask_c)| {
+                    let mask_c = prepare_mask(mask_c);
+                    let arr = if_then_else(&mask_c, left_c, right_c)?;
+                    Ok(arr)
+                })
+                .collect::<PolarsResult<Vec<_>>>()?;
+            unsafe { Ok(ChunkedArray::from_chunks(self.name(), chunks)) }
+        }
     }
 }
 
-impl BitXor for &BooleanChunked {
-    type Output = BooleanChunked;
-
-    fn bitxor(self, rhs: Self) -> Self::Output {
-        match (self.len(), rhs.len()) {
-            // make sure that we fall through if both are equal unit lengths
-            // otherwise we stackoverflow
-            (1, 1) => {}
-            (1, _) => {
-                return match self.get(0) {
-                    Some(true) => {
-                        let mut rhs = rhs.not();
-                        rhs.rename(self.name());
-                        rhs
-                    }
-                    Some(false) => {
-                        let mut rhs = rhs.clone();
-                        rhs.rename(self.name());
-                        rhs
-                    }
-                    None => &self.new_from_index(0, rhs.len()) | rhs,
-                };
-            }
-            (_, 1) => {
-                return match rhs.get(0) {
-                    Some(true) => self.not(),
-                    Some(false) => self.clone(),
-                    None => &rhs.new_from_index(0, self.len()) | self,
-                };
-            }
-            _ => {}
+impl ChunkZip<BooleanType> for BooleanChunked {
+    fn zip_with(
+        &self,
+        mask: &BooleanChunked,
+        other: &BooleanChunked,
+    ) -> PolarsResult<BooleanChunked> {
+        // broadcasting path
+        if self.len() != mask.len() || other.len() != mask.len() {
+            impl_ternary_broadcast!(self, self.len(), other.len(), other, mask, BooleanType)
+        } else {
+            let (left, right, mask) = align_chunks_ternary(self, other, mask);
+            let chunks = left
+                .downcast_iter()
+                .zip(right.downcast_iter())
+                .zip(mask.downcast_iter())
+                .map(|((left_c, right_c), mask_c)| {
+                    let mask_c = prepare_mask(mask_c);
+                    let arr = if_then_else(&mask_c, left_c, right_c)?;
+                    Ok(arr)
+                })
+                .collect::<PolarsResult<Vec<_>>>()?;
+            unsafe { Ok(ChunkedArray::from_chunks(self.name(), chunks)) }
         }
-
-        let (l, r) = align_chunks_binary(self, rhs);
-        let chunks = l
-            .downcast_iter()
-            .zip(r.downcast_iter())
-            .map(|(l_arr, r_arr)| {
-                let validity = combine_validities_and(l_arr.validity(), r_arr.validity());
-                let values = l_arr.values() ^ r_arr.values();
-
-                let arr = BooleanArray::from_data_default(values, validity);
-                Box::new(arr) as ArrayRef
-            })
-            .collect::<Vec<_>>();
-
-        // safety: same type
-        unsafe { ChunkedArray::from_chunks(self.name(), chunks) }
     }
 }
 
-impl BitXor for BooleanChunked {
-    type Output = BooleanChunked;
-
-    fn bitxor(self, rhs: Self) -> Self::Output {
-        (&self).bitxor(&rhs)
+impl ChunkZip<Utf8Type> for Utf8Chunked {
+    fn zip_with(&self, mask: &BooleanChunked, other: &Utf8Chunked) -> PolarsResult<Utf8Chunked> {
+        if self.len() != mask.len() || other.len() != mask.len() {
+            impl_ternary_broadcast!(self, self.len(), other.len(), other, mask, Utf8Type)
+        } else {
+            let (left, right, mask) = align_chunks_ternary(self, other, mask);
+            let chunks = left
+                .downcast_iter()
+                .zip(right.downcast_iter())
+                .zip(mask.downcast_iter())
+                .map(|((left_c, right_c), mask_c)| {
+                    let mask_c = prepare_mask(mask_c);
+                    let arr = if_then_else(&mask_c, left_c, right_c)?;
+                    Ok(arr)
+                })
+                .collect::<PolarsResult<Vec<_>>>()?;
+            unsafe { Ok(ChunkedArray::from_chunks(self.name(), chunks)) }
+        }
     }
 }
 
-impl BitAnd for &BooleanChunked {
-    type Output = BooleanChunked;
-
-    fn bitand(self, rhs: Self) -> Self::Output {
-        match (self.len(), rhs.len()) {
-            // make sure that we fall through if both are equal unit lengths
-            // otherwise we stackoverflow
-            (1, 1) => {}
-            (1, _) => {
-                return match self.get(0) {
-                    Some(true) => rhs.clone(),
-                    Some(false) => BooleanChunked::full(self.name(), false, rhs.len()),
-                    None => &self.new_from_index(0, rhs.len()) & rhs,
-                };
-            }
-            (_, 1) => {
-                return match rhs.get(0) {
-                    Some(true) => self.clone(),
-                    Some(false) => BooleanChunked::full(self.name(), false, self.len()),
-                    None => self & &rhs.new_from_index(0, self.len()),
-                };
-            }
-            _ => {}
+impl ChunkZip<BinaryType> for BinaryChunked {
+    fn zip_with(
+        &self,
+        mask: &BooleanChunked,
+        other: &BinaryChunked,
+    ) -> PolarsResult<BinaryChunked> {
+        if self.len() != mask.len() || other.len() != mask.len() {
+            impl_ternary_broadcast!(self, self.len(), other.len(), other, mask, BinaryType)
+        } else {
+            let (left, right, mask) = align_chunks_ternary(self, other, mask);
+            let chunks = left
+                .downcast_iter()
+                .zip(right.downcast_iter())
+                .zip(mask.downcast_iter())
+                .map(|((left_c, right_c), mask_c)| {
+                    let mask_c = prepare_mask(mask_c);
+                    let arr = if_then_else(&mask_c, left_c, right_c)?;
+                    Ok(arr)
+                })
+                .collect::<PolarsResult<Vec<_>>>()?;
+            unsafe { Ok(ChunkedArray::from_chunks(self.name(), chunks)) }
         }
-
-        let (lhs, rhs) = align_chunks_binary(self, rhs);
-        let chunks = lhs
-            .downcast_iter()
-            .zip(rhs.downcast_iter())
-            .map(|(lhs, rhs)| Box::new(compute::boolean_kleene::and(lhs, rhs)) as ArrayRef)
-            .collect();
-        // safety: same type
-        unsafe { BooleanChunked::from_chunks(self.name(), chunks) }
     }
 }
 
-impl BitAnd for BooleanChunked {
-    type Output = BooleanChunked;
+impl ChunkZip<ListType> for ListChunked {
+    fn zip_with(&self, mask: &BooleanChunked, other: &ListChunked) -> PolarsResult<ListChunked> {
+        let (left, right, mask) = align_chunks_ternary(self, other, mask);
+        let chunks = left
+            .downcast_iter()
+            .zip(right.downcast_iter())
+            .zip(mask.downcast_iter())
+            .map(|((left_c, right_c), mask_c)| {
+                let mask_c = prepare_mask(mask_c);
+                let arr = if_then_else(&mask_c, left_c, right_c)?;
+                Ok(arr)
+            })
+            .collect::<PolarsResult<Vec<_>>>()?;
+        unsafe { Ok(ChunkedArray::from_chunks(self.name(), chunks)) }
+    }
+}
 
-    fn bitand(self, rhs: Self) -> Self::Output {
-        (&self).bitand(&rhs)
+#[cfg(feature = "dtype-array")]
+impl ChunkZip<FixedSizeListType> for ArrayChunked {
+    fn zip_with(&self, mask: &BooleanChunked, other: &ArrayChunked) -> PolarsResult<ArrayChunked> {
+        let (left, right, mask) = align_chunks_ternary(self, other, mask);
+        let chunks = left
+            .downcast_iter()
+            .zip(right.downcast_iter())
+            .zip(mask.downcast_iter())
+            .map(|((left_c, right_c), mask_c)| {
+                let mask_c = prepare_mask(mask_c);
+                let arr = if_then_else(&mask_c, left_c, right_c)?;
+                Ok(arr)
+            })
+            .collect::<PolarsResult<Vec<_>>>()?;
+        unsafe { Ok(ChunkedArray::from_chunks(self.name(), chunks)) }
     }
 }
 
-#[cfg(test)]
-mod test {
-    use super::*;
-
-    #[test]
-    fn guard_so_issue_2494() {
-        // this cause a stack overflow
-        let a = BooleanChunked::new("a", [None]);
-        let b = BooleanChunked::new("b", [None]);
-
-        assert_eq!((&a).bitand(&b).null_count(), 1);
-        assert_eq!((&a).bitor(&b).null_count(), 1);
-        assert_eq!((&a).bitxor(&b).null_count(), 1);
+#[cfg(feature = "object")]
+impl<T: PolarsObject> ChunkZip<ObjectType<T>> for ObjectChunked<T> {
+    fn zip_with(
+        &self,
+        mask: &BooleanChunked,
+        other: &ChunkedArray<ObjectType<T>>,
+    ) -> PolarsResult<ChunkedArray<ObjectType<T>>> {
+        let (left, right, mask) = align_chunks_ternary(self, other, mask);
+        let mut ca: Self = left
+            .as_ref()
+            .into_iter()
+            .zip(right.into_iter())
+            .zip(mask.into_iter())
+            .map(|((left_c, right_c), mask_c)| match mask_c {
+                Some(true) => left_c.cloned(),
+                Some(false) => right_c.cloned(),
+                None => None,
+            })
+            .collect();
+        ca.rename(self.name());
+        Ok(ca)
     }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/builder/binary.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/builder/binary.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/builder/boolean.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/builder/boolean.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/builder/from.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/builder/from.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/builder/list.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/builder/list.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/builder/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/builder/mod.rs`

 * *Files 1% similar despite different names*

```diff
@@ -1,23 +1,27 @@
 mod binary;
 mod boolean;
+#[cfg(feature = "dtype-array")]
+pub mod fixed_size_list;
 mod from;
 pub mod list;
 mod primitive;
 mod utf8;
 
 use std::borrow::Cow;
 use std::iter::FromIterator;
 use std::marker::PhantomData;
 use std::sync::Arc;
 
 use arrow::array::*;
 use arrow::bitmap::Bitmap;
 pub use binary::*;
 pub use boolean::*;
+#[cfg(feature = "dtype-array")]
+pub(crate) use fixed_size_list::*;
 pub use list::*;
 pub use primitive::*;
 pub use utf8::*;
 
 use crate::prelude::*;
 use crate::utils::{get_iter_capacity, NoNull};
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/builder/primitive.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/builder/primitive.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/builder/utf8.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/builder/utf8.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/cast.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/categorical.rs`

 * *Files 22% similar despite different names*

```diff
@@ -1,343 +1,398 @@
-//! Implementations of the ChunkCast Trait.
-use std::convert::TryFrom;
+use std::borrow::Cow;
 
-use arrow::compute::cast::CastOptions;
+use ahash::RandomState;
+use polars_arrow::prelude::QuantileInterpolOptions;
 
-#[cfg(feature = "dtype-categorical")]
-use crate::chunked_array::categorical::CategoricalChunkedBuilder;
+use super::{private, IntoSeries, SeriesTrait, *};
+use crate::chunked_array::comparison::*;
+use crate::chunked_array::ops::compare_inner::{IntoPartialOrdInner, PartialOrdInner};
+use crate::chunked_array::ops::explode::ExplodeByOffsets;
+use crate::chunked_array::AsSinglePtr;
+use crate::frame::groupby::*;
+use crate::frame::hash_join::ZipOuterJoinColumn;
+#[cfg(feature = "is_in")]
+use crate::frame::hash_join::_check_categorical_src;
 use crate::prelude::*;
+use crate::series::implementations::SeriesWrap;
 
-pub(crate) fn cast_chunks(
-    chunks: &[ArrayRef],
-    dtype: &DataType,
-    checked: bool,
-) -> PolarsResult<Vec<ArrayRef>> {
-    let options = if checked {
-        Default::default()
-    } else {
-        CastOptions {
-            wrapped: true,
-            partial: false,
-        }
-    };
-
-    let arrow_dtype = dtype.to_arrow();
-    let chunks = chunks
-        .iter()
-        .map(|arr| arrow::compute::cast::cast(arr.as_ref(), &arrow_dtype, options))
-        .collect::<arrow::error::Result<Vec<_>>>()?;
-    Ok(chunks)
+unsafe impl IntoSeries for CategoricalChunked {
+    fn into_series(self) -> Series {
+        Series(Arc::new(SeriesWrap(self)))
+    }
 }
 
-fn cast_impl_inner(
-    name: &str,
-    chunks: &[ArrayRef],
-    dtype: &DataType,
-    checked: bool,
-) -> PolarsResult<Series> {
-    let chunks = cast_chunks(chunks, &dtype.to_physical(), checked)?;
-    let out = Series::try_from((name, chunks))?;
-    use DataType::*;
-    let out = match dtype {
-        Date => out.into_date(),
-        Datetime(tu, tz) => out.into_datetime(*tu, tz.clone()),
-        Duration(tu) => out.into_duration(*tu),
-        #[cfg(feature = "dtype-time")]
-        Time => out.into_time(),
-        _ => out,
-    };
+impl SeriesWrap<CategoricalChunked> {
+    fn finish_with_state(&self, keep_fast_unique: bool, cats: UInt32Chunked) -> CategoricalChunked {
+        let mut out = unsafe {
+            CategoricalChunked::from_cats_and_rev_map_unchecked(cats, self.0.get_rev_map().clone())
+        };
+        if keep_fast_unique && self.0.can_fast_unique() {
+            out.set_fast_unique(true)
+        }
+        out.set_lexical_sorted(self.0.use_lexical_sort());
+        out
+    }
 
-    Ok(out)
-}
+    fn with_state<F>(&self, keep_fast_unique: bool, apply: F) -> CategoricalChunked
+    where
+        F: Fn(&UInt32Chunked) -> UInt32Chunked,
+    {
+        let cats = apply(self.0.logical());
+        self.finish_with_state(keep_fast_unique, cats)
+    }
 
-fn cast_impl(name: &str, chunks: &[ArrayRef], dtype: &DataType) -> PolarsResult<Series> {
-    cast_impl_inner(name, chunks, dtype, true)
+    fn try_with_state<'a, F>(
+        &'a self,
+        keep_fast_unique: bool,
+        apply: F,
+    ) -> PolarsResult<CategoricalChunked>
+    where
+        F: for<'b> Fn(&'a UInt32Chunked) -> PolarsResult<UInt32Chunked>,
+    {
+        let cats = apply(self.0.logical())?;
+        Ok(self.finish_with_state(keep_fast_unique, cats))
+    }
 }
 
-#[cfg(feature = "dtype-struct")]
-fn cast_single_to_struct(
-    name: &str,
-    chunks: &[ArrayRef],
-    fields: &[Field],
-) -> PolarsResult<Series> {
-    let mut new_fields = Vec::with_capacity(fields.len());
-    // cast to first field dtype
-    let mut fields = fields.iter();
-    let fld = fields.next().unwrap();
-    let s = cast_impl_inner(&fld.name, chunks, &fld.dtype, true)?;
-    let length = s.len();
-    new_fields.push(s);
+impl private::PrivateSeries for SeriesWrap<CategoricalChunked> {
+    fn compute_len(&mut self) {
+        self.0.logical_mut().compute_len()
+    }
+    fn _field(&self) -> Cow<Field> {
+        Cow::Owned(self.0.field())
+    }
+    fn _dtype(&self) -> &DataType {
+        self.0.dtype()
+    }
 
-    for fld in fields {
-        new_fields.push(Series::full_null(&fld.name, length, &fld.dtype));
+    fn explode_by_offsets(&self, offsets: &[i64]) -> Series {
+        // TODO! explode by offset should return concrete type
+        self.with_state(true, |cats| {
+            cats.explode_by_offsets(offsets).u32().unwrap().clone()
+        })
+        .into_series()
     }
 
-    Ok(StructChunked::new_unchecked(name, &new_fields).into_series())
-}
+    fn _set_sorted_flag(&mut self, is_sorted: IsSorted) {
+        self.0.logical_mut().set_sorted_flag(is_sorted)
+    }
 
-impl<T> ChunkedArray<T>
-where
-    T: PolarsNumericType,
-{
-    fn cast_impl(&self, data_type: &DataType, checked: bool) -> PolarsResult<Series> {
-        match data_type {
-            #[cfg(feature = "dtype-categorical")]
-            DataType::Categorical(_) => {
-                polars_ensure!(
-                    self.dtype() == &DataType::UInt32,
-                    ComputeError: "cannot cast numeric types to 'Categorical'"
-                );
-                // SAFETY
-                // we are guarded by the type system
-                let ca = unsafe { &*(self as *const ChunkedArray<T> as *const UInt32Chunked) };
-                CategoricalChunked::from_global_indices(ca.clone()).map(|ca| ca.into_series())
-            }
-            #[cfg(feature = "dtype-struct")]
-            DataType::Struct(fields) => cast_single_to_struct(self.name(), &self.chunks, fields),
-            _ => cast_impl_inner(self.name(), &self.chunks, data_type, checked).map(|mut s| {
-                // maintain sorted if data types remain signed
-                // this may still fail with overflow?
-                if ((self.dtype().is_signed() && data_type.is_signed())
-                    || (self.dtype().is_unsigned() && data_type.is_unsigned()))
-                    && (s.null_count() == self.null_count())
-                    // physical to logicals
-                    || (self.dtype().to_physical() == data_type.to_physical())
-                {
-                    let is_sorted = self.is_sorted_flag2();
-                    s.set_sorted_flag(is_sorted)
-                }
-                s
-            }),
+    unsafe fn equal_element(&self, idx_self: usize, idx_other: usize, other: &Series) -> bool {
+        self.0.logical().equal_element(idx_self, idx_other, other)
+    }
+
+    #[cfg(feature = "zip_with")]
+    fn zip_with_same_type(&self, mask: &BooleanChunked, other: &Series) -> PolarsResult<Series> {
+        self.0
+            .zip_with(mask, other.categorical()?)
+            .map(|ca| ca.into_series())
+    }
+    fn into_partial_ord_inner<'a>(&'a self) -> Box<dyn PartialOrdInner + 'a> {
+        if self.0.use_lexical_sort() {
+            (&self.0).into_partial_ord_inner()
+        } else {
+            self.0.logical().into_partial_ord_inner()
         }
     }
-}
 
-impl<T> ChunkCast for ChunkedArray<T>
-where
-    T: PolarsNumericType,
-{
-    fn cast(&self, data_type: &DataType) -> PolarsResult<Series> {
-        self.cast_impl(data_type, true)
+    fn vec_hash(&self, random_state: RandomState, buf: &mut Vec<u64>) -> PolarsResult<()> {
+        self.0.logical().vec_hash(random_state, buf);
+        Ok(())
+    }
+
+    fn vec_hash_combine(&self, build_hasher: RandomState, hashes: &mut [u64]) -> PolarsResult<()> {
+        self.0.logical().vec_hash_combine(build_hasher, hashes);
+        Ok(())
     }
 
-    unsafe fn cast_unchecked(&self, data_type: &DataType) -> PolarsResult<Series> {
-        match data_type {
-            #[cfg(feature = "dtype-categorical")]
-            DataType::Categorical(Some(rev_map)) => {
-                if self.dtype() == &DataType::UInt32 {
-                    // safety:
-                    // we are guarded by the type system.
-                    let ca = unsafe { &*(self as *const ChunkedArray<T> as *const UInt32Chunked) };
-                    Ok(unsafe {
-                        CategoricalChunked::from_cats_and_rev_map_unchecked(
-                            ca.clone(),
-                            rev_map.clone(),
-                        )
-                    }
-                    .into_series())
-                } else {
-                    polars_bail!(ComputeError: "cannot cast numeric types to 'Categorical'");
-                }
-            }
-            _ => self.cast_impl(data_type, false),
+    unsafe fn agg_list(&self, groups: &GroupsProxy) -> Series {
+        // we cannot cast and dispatch as the inner type of the list would be incorrect
+        let list = self.0.logical().agg_list(groups);
+        let mut list = list.list().unwrap().clone();
+        list.to_physical(self.dtype().clone());
+        list.into_series()
+    }
+
+    fn zip_outer_join_column(
+        &self,
+        right_column: &Series,
+        opt_join_tuples: &[(Option<IdxSize>, Option<IdxSize>)],
+    ) -> Series {
+        let new_rev_map = self
+            .0
+            .merge_categorical_map(right_column.categorical().unwrap())
+            .unwrap();
+        let left = self.0.logical();
+        let right = right_column
+            .categorical()
+            .unwrap()
+            .logical()
+            .clone()
+            .into_series();
+
+        let cats = left.zip_outer_join_column(&right, opt_join_tuples);
+        let cats = cats.u32().unwrap().clone();
+
+        unsafe {
+            CategoricalChunked::from_cats_and_rev_map_unchecked(cats, new_rev_map).into_series()
+        }
+    }
+    fn group_tuples(&self, multithreaded: bool, sorted: bool) -> PolarsResult<GroupsProxy> {
+        #[cfg(feature = "performant")]
+        {
+            Ok(self.0.group_tuples_perfect(multithreaded, sorted))
+        }
+        #[cfg(not(feature = "performant"))]
+        {
+            self.0.logical().group_tuples(multithreaded, sorted)
         }
     }
+
+    fn arg_sort_multiple(&self, options: &SortMultipleOptions) -> PolarsResult<IdxCa> {
+        self.0.arg_sort_multiple(options)
+    }
 }
 
-impl ChunkCast for Utf8Chunked {
-    fn cast(&self, data_type: &DataType) -> PolarsResult<Series> {
-        match data_type {
-            #[cfg(feature = "dtype-categorical")]
-            DataType::Categorical(_) => {
-                let iter = self.into_iter();
-                let mut builder = CategoricalChunkedBuilder::new(self.name(), self.len());
-                builder.drain_iter(iter);
-                let ca = builder.finish();
-                Ok(ca.into_series())
-            }
-            #[cfg(feature = "dtype-struct")]
-            DataType::Struct(fields) => cast_single_to_struct(self.name(), &self.chunks, fields),
-            _ => cast_impl(self.name(), &self.chunks, data_type),
+impl SeriesTrait for SeriesWrap<CategoricalChunked> {
+    fn is_sorted_flag(&self) -> IsSorted {
+        if self.0.logical().is_sorted_ascending_flag() {
+            IsSorted::Ascending
+        } else if self.0.logical().is_sorted_descending_flag() {
+            IsSorted::Descending
+        } else {
+            IsSorted::Not
         }
     }
 
-    unsafe fn cast_unchecked(&self, data_type: &DataType) -> PolarsResult<Series> {
-        self.cast(data_type)
+    fn rename(&mut self, name: &str) {
+        self.0.logical_mut().rename(name);
     }
-}
 
-unsafe fn binary_to_utf8_unchecked(from: &BinaryArray<i64>) -> Utf8Array<i64> {
-    let values = from.values().clone();
-    let offsets = from.offsets().clone();
-    Utf8Array::<i64>::try_new_unchecked(
-        ArrowDataType::LargeUtf8,
-        offsets,
-        values,
-        from.validity().cloned(),
-    )
-    .unwrap()
-}
+    fn chunk_lengths(&self) -> ChunkIdIter {
+        self.0.logical().chunk_id()
+    }
+    fn name(&self) -> &str {
+        self.0.logical().name()
+    }
 
-impl BinaryChunked {
-    /// # Safety
-    /// Utf8 is not validated
-    pub unsafe fn to_utf8(&self) -> Utf8Chunked {
-        let chunks = self
-            .downcast_iter()
-            .map(|arr| Box::new(binary_to_utf8_unchecked(arr)) as ArrayRef)
-            .collect();
-        Utf8Chunked::from_chunks(self.name(), chunks)
+    fn chunks(&self) -> &Vec<ArrayRef> {
+        self.0.logical().chunks()
+    }
+    fn shrink_to_fit(&mut self) {
+        self.0.logical_mut().shrink_to_fit()
     }
-}
 
-impl Utf8Chunked {
-    pub fn as_binary(&self) -> BinaryChunked {
-        let chunks = self
-            .downcast_iter()
-            .map(|arr| {
-                Box::new(arrow::compute::cast::utf8_to_binary(
-                    arr,
-                    ArrowDataType::LargeBinary,
-                )) as ArrayRef
-            })
-            .collect();
-        unsafe { BinaryChunked::from_chunks(self.name(), chunks) }
+    fn slice(&self, offset: i64, length: usize) -> Series {
+        self.with_state(false, |cats| cats.slice(offset, length))
+            .into_series()
     }
-}
 
-impl ChunkCast for BinaryChunked {
-    fn cast(&self, data_type: &DataType) -> PolarsResult<Series> {
-        match data_type {
-            #[cfg(feature = "dtype-struct")]
-            DataType::Struct(fields) => cast_single_to_struct(self.name(), &self.chunks, fields),
-            _ => cast_impl(self.name(), &self.chunks, data_type),
-        }
+    fn append(&mut self, other: &Series) -> PolarsResult<()> {
+        polars_ensure!(self.0.dtype() == other.dtype(), append);
+        self.0.append(other.categorical().unwrap())
     }
 
-    unsafe fn cast_unchecked(&self, data_type: &DataType) -> PolarsResult<Series> {
-        match data_type {
-            DataType::Utf8 => unsafe { Ok(self.to_utf8().into_series()) },
-            _ => self.cast(data_type),
-        }
+    fn extend(&mut self, other: &Series) -> PolarsResult<()> {
+        polars_ensure!(self.0.dtype() == other.dtype(), extend);
+        let other = other.categorical()?;
+        self.0.logical_mut().extend(other.logical());
+        let new_rev_map = self.0.merge_categorical_map(other)?;
+        // SAFETY
+        // rev_maps are merged
+        unsafe { self.0.set_rev_map(new_rev_map, false) };
+        Ok(())
     }
-}
 
-fn boolean_to_utf8(ca: &BooleanChunked) -> Utf8Chunked {
-    ca.into_iter()
-        .map(|opt_b| match opt_b {
-            Some(true) => Some("true"),
-            Some(false) => Some("false"),
-            None => None,
-        })
-        .collect()
-}
+    fn filter(&self, filter: &BooleanChunked) -> PolarsResult<Series> {
+        self.try_with_state(false, |cats| cats.filter(filter))
+            .map(|ca| ca.into_series())
+    }
 
-impl ChunkCast for BooleanChunked {
-    fn cast(&self, data_type: &DataType) -> PolarsResult<Series> {
-        match data_type {
-            DataType::Utf8 => {
-                let mut ca = boolean_to_utf8(self);
-                ca.rename(self.name());
-                Ok(ca.into_series())
-            }
-            #[cfg(feature = "dtype-struct")]
-            DataType::Struct(fields) => cast_single_to_struct(self.name(), &self.chunks, fields),
-            _ => cast_impl(self.name(), &self.chunks, data_type),
-        }
+    #[cfg(feature = "chunked_ids")]
+    unsafe fn _take_chunked_unchecked(&self, by: &[ChunkId], sorted: IsSorted) -> Series {
+        let cats = self.0.logical().take_chunked_unchecked(by, sorted);
+        self.finish_with_state(false, cats).into_series()
     }
 
-    unsafe fn cast_unchecked(&self, data_type: &DataType) -> PolarsResult<Series> {
-        self.cast(data_type)
+    #[cfg(feature = "chunked_ids")]
+    unsafe fn _take_opt_chunked_unchecked(&self, by: &[Option<ChunkId>]) -> Series {
+        let cats = self.0.logical().take_opt_chunked_unchecked(by);
+        self.finish_with_state(false, cats).into_series()
+    }
+
+    fn take(&self, indices: &IdxCa) -> PolarsResult<Series> {
+        let indices = if indices.chunks.len() > 1 {
+            Cow::Owned(indices.rechunk())
+        } else {
+            Cow::Borrowed(indices)
+        };
+        self.try_with_state(false, |cats| cats.take((&*indices).into()))
+            .map(|ca| ca.into_series())
+    }
+
+    fn take_iter(&self, iter: &mut dyn TakeIterator) -> PolarsResult<Series> {
+        let cats = self.0.logical().take(iter.into())?;
+        Ok(self.finish_with_state(false, cats).into_series())
+    }
+
+    unsafe fn take_iter_unchecked(&self, iter: &mut dyn TakeIterator) -> Series {
+        let cats = self.0.logical().take_unchecked(iter.into());
+        self.finish_with_state(false, cats).into_series()
+    }
+
+    unsafe fn take_unchecked(&self, idx: &IdxCa) -> PolarsResult<Series> {
+        let idx = if idx.chunks.len() > 1 {
+            Cow::Owned(idx.rechunk())
+        } else {
+            Cow::Borrowed(idx)
+        };
+        Ok(self
+            .with_state(false, |cats| cats.take_unchecked((&*idx).into()))
+            .into_series())
+    }
+
+    unsafe fn take_opt_iter_unchecked(&self, iter: &mut dyn TakeIteratorNulls) -> Series {
+        let cats = self.0.logical().take_unchecked(iter.into());
+        self.finish_with_state(false, cats).into_series()
+    }
+
+    #[cfg(feature = "take_opt_iter")]
+    fn take_opt_iter(&self, iter: &mut dyn TakeIteratorNulls) -> PolarsResult<Series> {
+        let cats = self.0.logical().take(iter.into())?;
+        Ok(self.finish_with_state(false, cats).into_series())
+    }
+
+    fn len(&self) -> usize {
+        self.0.len()
+    }
+
+    fn rechunk(&self) -> Series {
+        self.with_state(true, |ca| ca.rechunk()).into_series()
+    }
+
+    fn new_from_index(&self, index: usize, length: usize) -> Series {
+        self.with_state(true, |cats| cats.new_from_index(index, length))
+            .into_series()
     }
-}
 
-/// We cannot cast anything to or from List/LargeList
-/// So this implementation casts the inner type
-impl ChunkCast for ListChunked {
     fn cast(&self, data_type: &DataType) -> PolarsResult<Series> {
-        use DataType::*;
-        match data_type {
-            List(child_type) => {
-                match (self.inner_dtype(), &**child_type) {
-                    #[cfg(feature = "dtype-categorical")]
-                    (dt, Categorical(None)) if !matches!(dt, Utf8) => {
-                        polars_bail!(ComputeError: "cannot cast list inner type: '{:?}' to Categorical", dt)
-                    }
-                    _ => {
-                        // ensure the inner logical type bubbles up
-                        let (arr, child_type) = cast_list(self, child_type)?;
-                        // Safety: we just casted so the dtype matches.
-                        // we must take this path to correct for physical types.
-                        unsafe {
-                            Ok(Series::from_chunks_and_dtype_unchecked(
-                                self.name(),
-                                vec![arr],
-                                &List(Box::new(child_type)),
-                            ))
-                        }
-                    }
-                }
-            }
-            _ => polars_bail!(ComputeError: "cannot cast list type"),
-        }
+        self.0.cast(data_type)
     }
 
-    unsafe fn cast_unchecked(&self, data_type: &DataType) -> PolarsResult<Series> {
-        self.cast(data_type)
+    fn get(&self, index: usize) -> PolarsResult<AnyValue> {
+        self.0.get_any_value(index)
     }
-}
 
-// Returns inner data type. This is needed because a cast can instantiate the dtype inner
-// values for instance with categoricals
-fn cast_list(ca: &ListChunked, child_type: &DataType) -> PolarsResult<(ArrayRef, DataType)> {
-    let ca = ca.rechunk();
-    let arr = ca.downcast_iter().next().unwrap();
-    // safety: inner dtype is passed correctly
-    let s = unsafe {
-        Series::from_chunks_and_dtype_unchecked("", vec![arr.values().clone()], &ca.inner_dtype())
-    };
-    let new_inner = s.cast(child_type)?;
-
-    let inner_dtype = new_inner.dtype().clone();
-    debug_assert_eq!(&inner_dtype, child_type);
-
-    let new_values = new_inner.array_ref(0).clone();
-
-    let data_type = ListArray::<i64>::default_datatype(new_values.data_type().clone());
-    let new_arr = ListArray::<i64>::new(
-        data_type,
-        arr.offsets().clone(),
-        new_values,
-        arr.validity().cloned(),
-    );
-    Ok((Box::new(new_arr), inner_dtype))
-}
+    #[inline]
+    #[cfg(feature = "private")]
+    unsafe fn get_unchecked(&self, index: usize) -> AnyValue {
+        self.0.get_any_value_unchecked(index)
+    }
 
-#[cfg(test)]
-mod test {
-    use crate::prelude::*;
-
-    #[test]
-    fn test_cast_list() -> PolarsResult<()> {
-        let mut builder =
-            ListPrimitiveChunkedBuilder::<Int32Type>::new("a", 10, 10, DataType::Int32);
-        builder.append_opt_slice(Some(&[1i32, 2, 3]));
-        builder.append_opt_slice(Some(&[1i32, 2, 3]));
-        let ca = builder.finish();
+    fn sort_with(&self, options: SortOptions) -> Series {
+        self.0.sort_with(options).into_series()
+    }
 
-        let new = ca.cast(&DataType::List(DataType::Float64.into()))?;
+    fn arg_sort(&self, options: SortOptions) -> IdxCa {
+        self.0.arg_sort(options)
+    }
 
-        assert_eq!(new.dtype(), &DataType::List(DataType::Float64.into()));
-        Ok(())
+    fn null_count(&self) -> usize {
+        self.0.logical().null_count()
+    }
+
+    fn has_validity(&self) -> bool {
+        self.0.logical().has_validity()
+    }
+
+    fn unique(&self) -> PolarsResult<Series> {
+        self.0.unique().map(|ca| ca.into_series())
+    }
+
+    fn n_unique(&self) -> PolarsResult<usize> {
+        self.0.n_unique()
+    }
+
+    fn arg_unique(&self) -> PolarsResult<IdxCa> {
+        self.0.logical().arg_unique()
     }
 
-    #[test]
-    #[cfg(feature = "dtype-categorical")]
-    fn test_cast_noop() {
-        // check if we can cast categorical twice without panic
-        let ca = Utf8Chunked::new("foo", &["bar", "ham"]);
-        let out = ca.cast(&DataType::Categorical(None)).unwrap();
-        let out = out.cast(&DataType::Categorical(None)).unwrap();
-        assert!(matches!(out.dtype(), &DataType::Categorical(_)))
+    fn is_null(&self) -> BooleanChunked {
+        self.0.logical().is_null()
+    }
+
+    fn is_not_null(&self) -> BooleanChunked {
+        self.0.logical().is_not_null()
+    }
+
+    fn reverse(&self) -> Series {
+        self.with_state(true, |cats| cats.reverse()).into_series()
+    }
+
+    fn as_single_ptr(&mut self) -> PolarsResult<usize> {
+        self.0.logical_mut().as_single_ptr()
+    }
+
+    fn shift(&self, periods: i64) -> Series {
+        self.with_state(false, |ca| ca.shift(periods)).into_series()
+    }
+
+    fn _sum_as_series(&self) -> Series {
+        CategoricalChunked::full_null(self.0.logical().name(), 1).into_series()
+    }
+    fn max_as_series(&self) -> Series {
+        CategoricalChunked::full_null(self.0.logical().name(), 1).into_series()
+    }
+    fn min_as_series(&self) -> Series {
+        CategoricalChunked::full_null(self.0.logical().name(), 1).into_series()
+    }
+    fn median_as_series(&self) -> Series {
+        CategoricalChunked::full_null(self.0.logical().name(), 1).into_series()
+    }
+    fn var_as_series(&self, _ddof: u8) -> Series {
+        CategoricalChunked::full_null(self.0.logical().name(), 1).into_series()
+    }
+    fn std_as_series(&self, _ddof: u8) -> Series {
+        CategoricalChunked::full_null(self.0.logical().name(), 1).into_series()
+    }
+    fn quantile_as_series(
+        &self,
+        _quantile: f64,
+        _interpol: QuantileInterpolOptions,
+    ) -> PolarsResult<Series> {
+        Ok(CategoricalChunked::full_null(self.0.logical().name(), 1).into_series())
+    }
+
+    fn clone_inner(&self) -> Arc<dyn SeriesTrait> {
+        Arc::new(SeriesWrap(Clone::clone(&self.0)))
+    }
+
+    #[cfg(feature = "is_in")]
+    fn is_in(&self, other: &Series) -> PolarsResult<BooleanChunked> {
+        _check_categorical_src(self.dtype(), other.dtype())?;
+        self.0.logical().is_in(&other.to_physical_repr())
+    }
+    #[cfg(feature = "repeat_by")]
+    fn repeat_by(&self, by: &IdxCa) -> PolarsResult<ListChunked> {
+        let out = self.0.logical().repeat_by(by)?;
+        let casted = out
+            .cast(&DataType::List(Box::new(self.dtype().clone())))
+            .unwrap();
+        Ok(casted.list().unwrap().clone())
+    }
+
+    #[cfg(feature = "mode")]
+    fn mode(&self) -> PolarsResult<Series> {
+        let cats = self.0.logical().mode()?;
+        Ok(self.finish_with_state(false, cats).into_series())
+    }
+}
+
+impl private::PrivateSeriesNumeric for SeriesWrap<CategoricalChunked> {
+    fn bit_repr_is_large(&self) -> bool {
+        false
+    }
+    fn bit_repr_small(&self) -> UInt32Chunked {
+        self.0.logical().clone()
     }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/comparison/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/comparison/mod.rs`

 * *Files 10% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 use arrow::scalar::{BinaryScalar, PrimitiveScalar, Scalar, Utf8Scalar};
 use num_traits::{NumCast, ToPrimitive};
 use polars_arrow::kernels::rolling::compare_fn_nan_max;
 use polars_arrow::prelude::FromData;
 
 use crate::prelude::*;
 use crate::series::IsSorted;
-use crate::utils::{align_chunks_binary, NoNull};
+use crate::utils::align_chunks_binary;
 
 impl<T> ChunkedArray<T>
 where
     T: PolarsNumericType,
 {
     /// First ensure that the chunks of lhs and rhs match and then iterates over the chunks and applies
     /// the comparison operator.
@@ -33,14 +33,20 @@
                 let arr = f(left, right);
                 Box::new(arr) as ArrayRef
             })
             .collect::<Vec<_>>();
 
         unsafe { ChunkedArray::from_chunks("", chunks) }
     }
+
+    // also includes validity  in comparison
+    pub fn not_equal_and_validity(&self, rhs: &ChunkedArray<T>) -> BooleanChunked {
+        let (lhs, rhs) = align_chunks_binary(self, rhs);
+        lhs.comparison(&rhs, |x, y| comparison::neq_and_validity(x, y))
+    }
 }
 
 impl<T> ChunkCompare<&ChunkedArray<T>> for ChunkedArray<T>
 where
     T: PolarsNumericType,
 {
     type Item = BooleanChunked;
@@ -48,21 +54,46 @@
     fn equal(&self, rhs: &ChunkedArray<T>) -> BooleanChunked {
         // broadcast
         match (self.len(), rhs.len()) {
             (_, 1) => {
                 if let Some(value) = rhs.get(0) {
                     self.equal(value)
                 } else {
-                    self.is_null()
+                    BooleanChunked::full_null("", self.len())
                 }
             }
             (1, _) => {
                 if let Some(value) = self.get(0) {
                     rhs.equal(value)
                 } else {
+                    BooleanChunked::full_null("", rhs.len())
+                }
+            }
+            _ => {
+                // same length
+                let (lhs, rhs) = align_chunks_binary(self, rhs);
+                lhs.comparison(&rhs, |x, y| comparison::eq(x, y))
+            }
+        }
+    }
+
+    fn equal_missing(&self, rhs: &ChunkedArray<T>) -> BooleanChunked {
+        // broadcast
+        match (self.len(), rhs.len()) {
+            (_, 1) => {
+                if let Some(value) = rhs.get(0) {
+                    self.equal_missing(value)
+                } else {
+                    self.is_null()
+                }
+            }
+            (1, _) => {
+                if let Some(value) = self.get(0) {
+                    rhs.equal_missing(value)
+                } else {
                     rhs.is_null()
                 }
             }
             _ => {
                 // same length
                 let (lhs, rhs) = align_chunks_binary(self, rhs);
                 lhs.comparison(&rhs, |x, y| comparison::eq_and_validity(x, y))
@@ -73,21 +104,46 @@
     fn not_equal(&self, rhs: &ChunkedArray<T>) -> BooleanChunked {
         // broadcast
         match (self.len(), rhs.len()) {
             (_, 1) => {
                 if let Some(value) = rhs.get(0) {
                     self.not_equal(value)
                 } else {
-                    self.is_not_null()
+                    BooleanChunked::full_null("", self.len())
                 }
             }
             (1, _) => {
                 if let Some(value) = self.get(0) {
                     rhs.not_equal(value)
                 } else {
+                    BooleanChunked::full_null("", rhs.len())
+                }
+            }
+            _ => {
+                // same length
+                let (lhs, rhs) = align_chunks_binary(self, rhs);
+                lhs.comparison(&rhs, |x, y| comparison::neq(x, y))
+            }
+        }
+    }
+
+    fn not_equal_missing(&self, rhs: &ChunkedArray<T>) -> BooleanChunked {
+        // broadcast
+        match (self.len(), rhs.len()) {
+            (_, 1) => {
+                if let Some(value) = rhs.get(0) {
+                    self.not_equal_missing(value)
+                } else {
+                    self.is_not_null()
+                }
+            }
+            (1, _) => {
+                if let Some(value) = self.get(0) {
+                    rhs.not_equal_missing(value)
+                } else {
                     rhs.is_not_null()
                 }
             }
             _ => {
                 // same length
                 let (lhs, rhs) = align_chunks_binary(self, rhs);
                 lhs.comparison(&rhs, |x, y| comparison::neq_and_validity(x, y))
@@ -98,22 +154,22 @@
     fn gt(&self, rhs: &ChunkedArray<T>) -> BooleanChunked {
         // broadcast
         match (self.len(), rhs.len()) {
             (_, 1) => {
                 if let Some(value) = rhs.get(0) {
                     self.gt(value)
                 } else {
-                    BooleanChunked::full("", false, self.len())
+                    BooleanChunked::full_null("", self.len())
                 }
             }
             (1, _) => {
                 if let Some(value) = self.get(0) {
                     rhs.lt(value)
                 } else {
-                    BooleanChunked::full("", false, rhs.len())
+                    BooleanChunked::full_null("", rhs.len())
                 }
             }
             _ => {
                 // same length
                 let (lhs, rhs) = align_chunks_binary(self, rhs);
                 lhs.comparison(&rhs, |x, y| comparison::gt(x, y))
             }
@@ -123,22 +179,22 @@
     fn gt_eq(&self, rhs: &ChunkedArray<T>) -> BooleanChunked {
         // broadcast
         match (self.len(), rhs.len()) {
             (_, 1) => {
                 if let Some(value) = rhs.get(0) {
                     self.gt_eq(value)
                 } else {
-                    BooleanChunked::full("", false, self.len())
+                    BooleanChunked::full_null("", self.len())
                 }
             }
             (1, _) => {
                 if let Some(value) = self.get(0) {
                     rhs.lt_eq(value)
                 } else {
-                    BooleanChunked::full("", false, rhs.len())
+                    BooleanChunked::full_null("", rhs.len())
                 }
             }
             _ => {
                 // same length
                 let (lhs, rhs) = align_chunks_binary(self, rhs);
                 lhs.comparison(&rhs, |x, y| comparison::gt_eq(x, y))
             }
@@ -148,22 +204,22 @@
     fn lt(&self, rhs: &ChunkedArray<T>) -> BooleanChunked {
         // broadcast
         match (self.len(), rhs.len()) {
             (_, 1) => {
                 if let Some(value) = rhs.get(0) {
                     self.lt(value)
                 } else {
-                    BooleanChunked::full("", false, self.len())
+                    BooleanChunked::full_null("", self.len())
                 }
             }
             (1, _) => {
                 if let Some(value) = self.get(0) {
                     rhs.gt(value)
                 } else {
-                    BooleanChunked::full("", false, rhs.len())
+                    BooleanChunked::full_null("", rhs.len())
                 }
             }
             _ => {
                 // same length
                 let (lhs, rhs) = align_chunks_binary(self, rhs);
                 lhs.comparison(&rhs, |x, y| comparison::lt(x, y))
             }
@@ -173,22 +229,22 @@
     fn lt_eq(&self, rhs: &ChunkedArray<T>) -> BooleanChunked {
         // broadcast
         match (self.len(), rhs.len()) {
             (_, 1) => {
                 if let Some(value) = rhs.get(0) {
                     self.lt_eq(value)
                 } else {
-                    BooleanChunked::full("", false, self.len())
+                    BooleanChunked::full_null("", self.len())
                 }
             }
             (1, _) => {
                 if let Some(value) = self.get(0) {
                     rhs.gt_eq(value)
                 } else {
-                    BooleanChunked::full("", false, rhs.len())
+                    BooleanChunked::full_null("", rhs.len())
                 }
             }
             _ => {
                 // same length
                 let (lhs, rhs) = align_chunks_binary(self, rhs);
                 lhs.comparison(&rhs, |x, y| comparison::lt_eq(x, y))
             }
@@ -214,122 +270,96 @@
     type Item = BooleanChunked;
 
     fn equal(&self, rhs: &BooleanChunked) -> BooleanChunked {
         // broadcast
         match (self.len(), rhs.len()) {
             (_, 1) => {
                 if let Some(value) = rhs.get(0) {
-                    match value {
-                        true => {
-                            if self.null_count() == 0 {
-                                self.clone()
-                            } else {
-                                let chunks = self
-                                    .downcast_iter()
-                                    .map(|arr| {
-                                        if let Some(validity) = arr.validity() {
-                                            Box::new(BooleanArray::from_data_default(
-                                                arr.values() & validity,
-                                                None,
-                                            ))
-                                                as ArrayRef
-                                        } else {
-                                            Box::new(arr.clone())
-                                        }
-                                    })
-                                    .collect();
-                                unsafe { BooleanChunked::from_chunks("", chunks) }
-                            }
-                        }
-                        false => {
-                            if self.null_count() == 0 {
-                                self.not()
-                            } else {
-                                let chunks = self
-                                    .downcast_iter()
-                                    .map(|arr| {
-                                        let bitmap = if let Some(validity) = arr.validity() {
-                                            arr.values() ^ validity
-                                        } else {
-                                            arr.values().not()
-                                        };
-                                        Box::new(BooleanArray::from_data_default(bitmap, None))
-                                            as ArrayRef
-                                    })
-                                    .collect();
-                                unsafe { BooleanChunked::from_chunks("", chunks) }
-                            }
-                        }
+                    if value {
+                        self.clone()
+                    } else {
+                        self.not()
                     }
                 } else {
-                    self.is_null()
+                    BooleanChunked::full_null("", self.len())
                 }
             }
             (1, _) => rhs.equal(self),
             _ => {
                 // same length
                 let (lhs, rhs) = align_chunks_binary(self, rhs);
+                compare_bools(&lhs, &rhs, |lhs, rhs| comparison::eq(lhs, rhs))
+            }
+        }
+    }
+
+    fn equal_missing(&self, rhs: &BooleanChunked) -> BooleanChunked {
+        // broadcast
+        match (self.len(), rhs.len()) {
+            (_, 1) => {
+                if let Some(value) = rhs.get(0) {
+                    if value {
+                        self.clone()
+                    } else {
+                        self.not()
+                    }
+                } else {
+                    self.is_null()
+                }
+            }
+            (1, _) => rhs.equal_missing(self),
+            _ => {
+                // same length
+                let (lhs, rhs) = align_chunks_binary(self, rhs);
                 compare_bools(&lhs, &rhs, |lhs, rhs| comparison::eq_and_validity(lhs, rhs))
             }
         }
     }
 
     fn not_equal(&self, rhs: &BooleanChunked) -> BooleanChunked {
         // broadcast
         match (self.len(), rhs.len()) {
             (_, 1) => {
                 if let Some(value) = rhs.get(0) {
-                    match value {
-                        true => {
-                            if self.null_count() == 0 {
-                                self.not()
-                            } else {
-                                let chunks = self
-                                    .downcast_iter()
-                                    .map(|arr| {
-                                        let bitmap = if let Some(validity) = arr.validity() {
-                                            (arr.values() & validity).not()
-                                        } else {
-                                            arr.values().not()
-                                        };
-                                        Box::new(BooleanArray::from_data_default(bitmap, None))
-                                            as ArrayRef
-                                    })
-                                    .collect();
-                                unsafe { BooleanChunked::from_chunks("", chunks) }
-                            }
-                        }
-                        false => {
-                            if self.null_count() == 0 {
-                                self.clone()
-                            } else {
-                                let chunks = self
-                                    .downcast_iter()
-                                    .map(|arr| {
-                                        let bitmap = if let Some(validity) = arr.validity() {
-                                            (arr.values() ^ validity).not()
-                                        } else {
-                                            arr.values().clone()
-                                        };
-                                        Box::new(BooleanArray::from_data_default(bitmap, None))
-                                            as ArrayRef
-                                    })
-                                    .collect();
-                                unsafe { BooleanChunked::from_chunks("", chunks) }
-                            }
-                        }
+                    if value {
+                        self.not()
+                    } else {
+                        self.clone()
                     }
                 } else {
-                    self.is_not_null()
+                    BooleanChunked::full_null("", self.len())
                 }
             }
             (1, _) => rhs.not_equal(self),
             _ => {
                 // same length
                 let (lhs, rhs) = align_chunks_binary(self, rhs);
+                compare_bools(&lhs, &rhs, |lhs, rhs| comparison::neq(lhs, rhs))
+            }
+        }
+    }
+
+    fn not_equal_missing(&self, rhs: &BooleanChunked) -> BooleanChunked {
+        // broadcast
+        match (self.len(), rhs.len()) {
+            (_, 1) => {
+                if let Some(value) = rhs.get(0) {
+                    if value {
+                        self.not()
+                    } else {
+                        self.clone()
+                    }
+                } else {
+                    self.is_not_null()
+                }
+            }
+            (1, _) => rhs.not_equal_missing(self),
+            _ => {
+                // same length
+                let (lhs, rhs) = align_chunks_binary(self, rhs);
                 compare_bools(&lhs, &rhs, |lhs, rhs| {
                     comparison::neq_and_validity(lhs, rhs)
                 })
             }
         }
     }
 
@@ -339,25 +369,25 @@
             (_, 1) => {
                 if let Some(value) = rhs.get(0) {
                     match value {
                         true => BooleanChunked::full("", false, self.len()),
                         false => self.clone(),
                     }
                 } else {
-                    BooleanChunked::full("", false, self.len())
+                    BooleanChunked::full_null("", self.len())
                 }
             }
             (1, _) => {
                 if let Some(value) = self.get(0) {
                     match value {
                         true => rhs.not(),
                         false => BooleanChunked::full("", false, rhs.len()),
                     }
                 } else {
-                    BooleanChunked::full("", false, rhs.len())
+                    BooleanChunked::full_null("", rhs.len())
                 }
             }
             _ => {
                 // same length
                 let (lhs, rhs) = align_chunks_binary(self, rhs);
                 compare_bools(&lhs, &rhs, |lhs, rhs| comparison::gt(lhs, rhs))
             }
@@ -370,25 +400,25 @@
             (_, 1) => {
                 if let Some(value) = rhs.get(0) {
                     match value {
                         true => self.clone(),
                         false => BooleanChunked::full("", true, self.len()),
                     }
                 } else {
-                    BooleanChunked::full("", false, self.len())
+                    BooleanChunked::full_null("", self.len())
                 }
             }
             (1, _) => {
                 if let Some(value) = self.get(0) {
                     match value {
                         true => BooleanChunked::full("", true, rhs.len()),
                         false => rhs.not(),
                     }
                 } else {
-                    BooleanChunked::full("", false, rhs.len())
+                    BooleanChunked::full_null("", rhs.len())
                 }
             }
             _ => {
                 // same length
                 let (lhs, rhs) = align_chunks_binary(self, rhs);
                 compare_bools(&lhs, &rhs, |lhs, rhs| comparison::gt_eq(lhs, rhs))
             }
@@ -401,25 +431,25 @@
             (_, 1) => {
                 if let Some(value) = rhs.get(0) {
                     match value {
                         true => self.not(),
                         false => BooleanChunked::full("", false, self.len()),
                     }
                 } else {
-                    BooleanChunked::full("", false, self.len())
+                    BooleanChunked::full_null("", self.len())
                 }
             }
             (1, _) => {
                 if let Some(value) = self.get(0) {
                     match value {
                         true => BooleanChunked::full("", false, rhs.len()),
                         false => rhs.clone(),
                     }
                 } else {
-                    BooleanChunked::full("", false, rhs.len())
+                    BooleanChunked::full_null("", rhs.len())
                 }
             }
             _ => {
                 // same length
                 let (lhs, rhs) = align_chunks_binary(self, rhs);
                 compare_bools(&lhs, &rhs, |lhs, rhs| comparison::lt(lhs, rhs))
             }
@@ -432,187 +462,68 @@
             (_, 1) => {
                 if let Some(value) = rhs.get(0) {
                     match value {
                         true => BooleanChunked::full("", true, self.len()),
                         false => BooleanChunked::full("", false, self.len()),
                     }
                 } else {
-                    BooleanChunked::full("", false, self.len())
+                    BooleanChunked::full_null("", self.len())
                 }
             }
             (1, _) => {
                 if let Some(value) = self.get(0) {
                     match value {
                         true => rhs.clone(),
                         false => BooleanChunked::full("", true, rhs.len()),
                     }
                 } else {
-                    BooleanChunked::full("", false, rhs.len())
+                    BooleanChunked::full_null("", rhs.len())
                 }
             }
             _ => {
                 // same length
                 let (lhs, rhs) = align_chunks_binary(self, rhs);
                 compare_bools(&lhs, &rhs, |lhs, rhs| comparison::lt_eq(lhs, rhs))
             }
         }
     }
 }
 
-impl Utf8Chunked {
-    fn comparison(
-        &self,
-        rhs: &Utf8Chunked,
-        f: impl Fn(&Utf8Array<i64>, &Utf8Array<i64>) -> BooleanArray,
-    ) -> BooleanChunked {
-        let chunks = self
-            .downcast_iter()
-            .zip(rhs.downcast_iter())
-            .map(|(left, right)| {
-                let arr = f(left, right);
-                Box::new(arr) as ArrayRef
-            })
-            .collect();
-        unsafe { BooleanChunked::from_chunks("", chunks) }
-    }
-}
-
 impl ChunkCompare<&Utf8Chunked> for Utf8Chunked {
     type Item = BooleanChunked;
 
     fn equal(&self, rhs: &Utf8Chunked) -> BooleanChunked {
-        // broadcast
-        if rhs.len() == 1 {
-            if let Some(value) = rhs.get(0) {
-                self.equal(value)
-            } else {
-                self.is_null()
-            }
-        } else if self.len() == 1 {
-            if let Some(value) = self.get(0) {
-                rhs.equal(value)
-            } else {
-                rhs.is_null()
-            }
-        } else {
-            let (lhs, rhs) = align_chunks_binary(self, rhs);
-            lhs.comparison(&rhs, comparison::utf8::eq_and_validity)
-        }
+        self.as_binary().equal(&rhs.as_binary())
+    }
+
+    fn equal_missing(&self, rhs: &Utf8Chunked) -> BooleanChunked {
+        self.as_binary().equal_missing(&rhs.as_binary())
     }
 
     fn not_equal(&self, rhs: &Utf8Chunked) -> BooleanChunked {
-        // broadcast
-        if rhs.len() == 1 {
-            if let Some(value) = rhs.get(0) {
-                self.not_equal(value)
-            } else {
-                self.is_not_null()
-            }
-        } else if self.len() == 1 {
-            if let Some(value) = self.get(0) {
-                rhs.not_equal(value)
-            } else {
-                rhs.is_not_null()
-            }
-        } else {
-            let (lhs, rhs) = align_chunks_binary(self, rhs);
-            lhs.comparison(&rhs, comparison::utf8::neq_and_validity)
-        }
+        self.as_binary().not_equal(&rhs.as_binary())
+    }
+    fn not_equal_missing(&self, rhs: &Utf8Chunked) -> BooleanChunked {
+        self.as_binary().not_equal_missing(&rhs.as_binary())
     }
 
     fn gt(&self, rhs: &Utf8Chunked) -> BooleanChunked {
-        // broadcast
-        if rhs.len() == 1 {
-            if let Some(value) = rhs.get(0) {
-                self.gt(value)
-            } else {
-                BooleanChunked::full("", false, self.len())
-            }
-        } else if self.len() == 1 {
-            if let Some(value) = self.get(0) {
-                rhs.lt(value)
-            } else {
-                BooleanChunked::full("", false, self.len())
-            }
-        }
-        // same length
-        else if self.chunk_id().zip(rhs.chunk_id()).all(|(l, r)| l == r) {
-            self.comparison(rhs, |l, r| comparison::gt(l, r))
-        } else {
-            apply_operand_on_chunkedarray_by_iter!(self, rhs, >)
-        }
+        self.as_binary().gt(&rhs.as_binary())
     }
 
     fn gt_eq(&self, rhs: &Utf8Chunked) -> BooleanChunked {
-        // broadcast
-        if rhs.len() == 1 {
-            if let Some(value) = rhs.get(0) {
-                self.gt_eq(value)
-            } else {
-                BooleanChunked::full("", false, self.len())
-            }
-        } else if self.len() == 1 {
-            if let Some(value) = self.get(0) {
-                rhs.lt_eq(value)
-            } else {
-                BooleanChunked::full("", false, self.len())
-            }
-        }
-        // same length
-        else if self.chunk_id().zip(rhs.chunk_id()).all(|(l, r)| l == r) {
-            self.comparison(rhs, |l, r| comparison::gt_eq(l, r))
-        } else {
-            apply_operand_on_chunkedarray_by_iter!(self, rhs, >=)
-        }
+        self.as_binary().gt_eq(&rhs.as_binary())
     }
 
     fn lt(&self, rhs: &Utf8Chunked) -> BooleanChunked {
-        // broadcast
-        if rhs.len() == 1 {
-            if let Some(value) = rhs.get(0) {
-                self.lt(value)
-            } else {
-                BooleanChunked::full("", false, self.len())
-            }
-        } else if self.len() == 1 {
-            if let Some(value) = self.get(0) {
-                rhs.gt(value)
-            } else {
-                BooleanChunked::full("", false, self.len())
-            }
-        }
-        // same length
-        else if self.chunk_id().zip(rhs.chunk_id()).all(|(l, r)| l == r) {
-            self.comparison(rhs, |l, r| comparison::lt(l, r))
-        } else {
-            apply_operand_on_chunkedarray_by_iter!(self, rhs, <)
-        }
+        self.as_binary().lt(&rhs.as_binary())
     }
 
     fn lt_eq(&self, rhs: &Utf8Chunked) -> BooleanChunked {
-        // broadcast
-        if rhs.len() == 1 {
-            if let Some(value) = rhs.get(0) {
-                self.lt_eq(value)
-            } else {
-                BooleanChunked::full("", false, self.len())
-            }
-        } else if self.len() == 1 {
-            if let Some(value) = self.get(0) {
-                rhs.gt_eq(value)
-            } else {
-                BooleanChunked::full("", false, self.len())
-            }
-        }
-        // same length
-        else if self.chunk_id().zip(rhs.chunk_id()).all(|(l, r)| l == r) {
-            self.comparison(rhs, |l, r| comparison::lt_eq(l, r))
-        } else {
-            apply_operand_on_chunkedarray_by_iter!(self, rhs, <=)
-        }
+        self.as_binary().lt_eq(&rhs.as_binary())
     }
 }
 
 impl BinaryChunked {
     fn comparison(
         &self,
         rhs: &BinaryChunked,
@@ -635,147 +546,184 @@
 
     fn equal(&self, rhs: &BinaryChunked) -> BooleanChunked {
         // broadcast
         if rhs.len() == 1 {
             if let Some(value) = rhs.get(0) {
                 self.equal(value)
             } else {
-                BooleanChunked::full("", false, self.len())
+                BooleanChunked::full_null("", self.len())
             }
         } else if self.len() == 1 {
             if let Some(value) = self.get(0) {
                 rhs.equal(value)
             } else {
-                BooleanChunked::full("", false, self.len())
+                BooleanChunked::full_null("", rhs.len())
+            }
+        } else {
+            let (lhs, rhs) = align_chunks_binary(self, rhs);
+            lhs.comparison(&rhs, comparison::binary::eq)
+        }
+    }
+
+    fn equal_missing(&self, rhs: &BinaryChunked) -> BooleanChunked {
+        // broadcast
+        if rhs.len() == 1 {
+            if let Some(value) = rhs.get(0) {
+                self.equal_missing(value)
+            } else {
+                self.is_null()
+            }
+        } else if self.len() == 1 {
+            if let Some(value) = self.get(0) {
+                rhs.equal_missing(value)
+            } else {
+                rhs.is_null()
             }
         } else {
             let (lhs, rhs) = align_chunks_binary(self, rhs);
             lhs.comparison(&rhs, comparison::binary::eq_and_validity)
         }
     }
 
     fn not_equal(&self, rhs: &BinaryChunked) -> BooleanChunked {
         // broadcast
         if rhs.len() == 1 {
             if let Some(value) = rhs.get(0) {
                 self.not_equal(value)
             } else {
-                BooleanChunked::full("", false, self.len())
+                BooleanChunked::full_null("", self.len())
             }
         } else if self.len() == 1 {
             if let Some(value) = self.get(0) {
                 rhs.not_equal(value)
             } else {
-                BooleanChunked::full("", false, self.len())
+                BooleanChunked::full_null("", rhs.len())
+            }
+        } else {
+            let (lhs, rhs) = align_chunks_binary(self, rhs);
+            lhs.comparison(&rhs, comparison::binary::neq)
+        }
+    }
+
+    fn not_equal_missing(&self, rhs: &BinaryChunked) -> BooleanChunked {
+        // broadcast
+        if rhs.len() == 1 {
+            if let Some(value) = rhs.get(0) {
+                self.not_equal_missing(value)
+            } else {
+                self.is_not_null()
+            }
+        } else if self.len() == 1 {
+            if let Some(value) = self.get(0) {
+                rhs.not_equal_missing(value)
+            } else {
+                rhs.is_not_null()
             }
         } else {
             let (lhs, rhs) = align_chunks_binary(self, rhs);
             lhs.comparison(&rhs, comparison::binary::neq_and_validity)
         }
     }
 
     fn gt(&self, rhs: &BinaryChunked) -> BooleanChunked {
         // broadcast
         if rhs.len() == 1 {
             if let Some(value) = rhs.get(0) {
                 self.gt(value)
             } else {
-                BooleanChunked::full("", false, self.len())
+                BooleanChunked::full_null("", self.len())
             }
         } else if self.len() == 1 {
             if let Some(value) = self.get(0) {
                 rhs.lt(value)
             } else {
-                BooleanChunked::full("", false, self.len())
+                BooleanChunked::full_null("", self.len())
             }
-        }
-        // same length
-        else if self.chunk_id().zip(rhs.chunk_id()).all(|(l, r)| l == r) {
-            self.comparison(rhs, |l, r| comparison::gt(l, r))
         } else {
-            apply_operand_on_chunkedarray_by_iter!(self, rhs, >)
+            let (lhs, rhs) = align_chunks_binary(self, rhs);
+            lhs.comparison(&rhs, |l, r| comparison::gt(l, r))
         }
     }
 
     fn gt_eq(&self, rhs: &BinaryChunked) -> BooleanChunked {
         // broadcast
         if rhs.len() == 1 {
             if let Some(value) = rhs.get(0) {
                 self.gt_eq(value)
             } else {
-                BooleanChunked::full("", false, self.len())
+                BooleanChunked::full_null("", self.len())
             }
         } else if self.len() == 1 {
             if let Some(value) = self.get(0) {
                 rhs.lt_eq(value)
             } else {
-                BooleanChunked::full("", false, self.len())
+                BooleanChunked::full_null("", self.len())
             }
-        }
-        // same length
-        else if self.chunk_id().zip(rhs.chunk_id()).all(|(l, r)| l == r) {
-            self.comparison(rhs, |l, r| comparison::gt_eq(l, r))
         } else {
-            apply_operand_on_chunkedarray_by_iter!(self, rhs, >=)
+            let (lhs, rhs) = align_chunks_binary(self, rhs);
+            lhs.comparison(&rhs, |l, r| comparison::gt_eq(l, r))
         }
     }
 
     fn lt(&self, rhs: &BinaryChunked) -> BooleanChunked {
         // broadcast
         if rhs.len() == 1 {
             if let Some(value) = rhs.get(0) {
                 self.lt(value)
             } else {
-                BooleanChunked::full("", false, self.len())
+                BooleanChunked::full_null("", self.len())
             }
         } else if self.len() == 1 {
             if let Some(value) = self.get(0) {
                 rhs.gt(value)
             } else {
-                BooleanChunked::full("", false, self.len())
+                BooleanChunked::full_null("", self.len())
             }
-        }
-        // same length
-        else if self.chunk_id().zip(rhs.chunk_id()).all(|(l, r)| l == r) {
-            self.comparison(rhs, |l, r| comparison::lt(l, r))
         } else {
-            apply_operand_on_chunkedarray_by_iter!(self, rhs, <)
+            let (lhs, rhs) = align_chunks_binary(self, rhs);
+            lhs.comparison(&rhs, |l, r| comparison::lt(l, r))
         }
     }
 
     fn lt_eq(&self, rhs: &BinaryChunked) -> BooleanChunked {
         // broadcast
         if rhs.len() == 1 {
             if let Some(value) = rhs.get(0) {
                 self.lt_eq(value)
             } else {
-                BooleanChunked::full("", false, self.len())
+                BooleanChunked::full_null("", self.len())
             }
         } else if self.len() == 1 {
             if let Some(value) = self.get(0) {
                 rhs.gt_eq(value)
             } else {
-                BooleanChunked::full("", false, self.len())
+                BooleanChunked::full_null("", self.len())
             }
-        }
-        // same length
-        else if self.chunk_id().zip(rhs.chunk_id()).all(|(l, r)| l == r) {
-            self.comparison(rhs, |l, r| comparison::lt_eq(l, r))
         } else {
-            apply_operand_on_chunkedarray_by_iter!(self, rhs, <=)
+            let (lhs, rhs) = align_chunks_binary(self, rhs);
+            lhs.comparison(&rhs, |l, r| comparison::lt_eq(l, r))
         }
     }
 }
 
 impl ChunkCompare<&str> for Utf8Chunked {
     type Item = BooleanChunked;
     fn equal(&self, rhs: &str) -> BooleanChunked {
+        self.utf8_compare_scalar(rhs, |l, rhs| comparison::eq_scalar(l, rhs))
+    }
+
+    fn equal_missing(&self, rhs: &str) -> BooleanChunked {
         self.utf8_compare_scalar(rhs, |l, rhs| comparison::eq_scalar_and_validity(l, rhs))
     }
+
     fn not_equal(&self, rhs: &str) -> BooleanChunked {
+        self.utf8_compare_scalar(rhs, |l, rhs| comparison::neq_scalar(l, rhs))
+    }
+
+    fn not_equal_missing(&self, rhs: &str) -> BooleanChunked {
         self.utf8_compare_scalar(rhs, |l, rhs| comparison::neq_scalar_and_validity(l, rhs))
     }
 
     fn gt(&self, rhs: &str) -> BooleanChunked {
         self.utf8_compare_scalar(rhs, |l, rhs| comparison::gt_scalar(l, rhs))
     }
 
@@ -794,31 +742,48 @@
 
 impl ChunkCompare<&ListChunked> for ListChunked {
     type Item = BooleanChunked;
     fn equal(&self, rhs: &ListChunked) -> BooleanChunked {
         self.amortized_iter()
             .zip(rhs.amortized_iter())
             .map(|(left, right)| match (left, right) {
-                (None, None) => true,
+                (Some(l), Some(r)) => Some(l.as_ref().series_equal_missing(r.as_ref())),
+                _ => None,
+            })
+            .collect_trusted()
+    }
+
+    fn equal_missing(&self, rhs: &ListChunked) -> BooleanChunked {
+        self.amortized_iter()
+            .zip(rhs.amortized_iter())
+            .map(|(left, right)| match (left, right) {
                 (Some(l), Some(r)) => l.as_ref().series_equal_missing(r.as_ref()),
+                (None, None) => true,
                 _ => false,
             })
             .collect_trusted()
     }
 
     fn not_equal(&self, rhs: &ListChunked) -> BooleanChunked {
         self.amortized_iter()
             .zip(rhs.amortized_iter())
-            .map(|(left, right)| {
-                let out = match (left, right) {
-                    (None, None) => true,
-                    (Some(l), Some(r)) => l.as_ref().series_equal_missing(r.as_ref()),
-                    _ => false,
-                };
-                !out
+            .map(|(left, right)| match (left, right) {
+                (Some(l), Some(r)) => Some(!l.as_ref().series_equal_missing(r.as_ref())),
+                _ => None,
+            })
+            .collect_trusted()
+    }
+
+    fn not_equal_missing(&self, rhs: &ListChunked) -> BooleanChunked {
+        self.amortized_iter()
+            .zip(rhs.amortized_iter())
+            .map(|(left, right)| match (left, right) {
+                (Some(l), Some(r)) => !l.as_ref().series_equal_missing(r.as_ref()),
+                (None, None) => false,
+                _ => true,
             })
             .collect_trusted()
     }
 
     // following are not implemented because gt, lt comparison of series don't make sense
     fn gt(&self, _rhs: &ListChunked) -> BooleanChunked {
         unimplemented!()
@@ -833,14 +798,146 @@
     }
 
     fn lt_eq(&self, _rhs: &ListChunked) -> BooleanChunked {
         unimplemented!()
     }
 }
 
+#[cfg(feature = "dtype-struct")]
+impl ChunkCompare<&StructChunked> for StructChunked {
+    type Item = BooleanChunked;
+    fn equal(&self, rhs: &StructChunked) -> BooleanChunked {
+        use std::ops::BitAnd;
+        if self.len() != rhs.len() || self.fields().len() != rhs.fields().len() {
+            BooleanChunked::full("", false, self.len())
+        } else {
+            self.fields()
+                .iter()
+                .zip(rhs.fields().iter())
+                .map(|(l, r)| l.equal(r).unwrap())
+                .reduce(|lhs, rhs| lhs.bitand(rhs))
+                .unwrap()
+        }
+    }
+
+    fn equal_missing(&self, rhs: &StructChunked) -> BooleanChunked {
+        use std::ops::BitAnd;
+        if self.len() != rhs.len() || self.fields().len() != rhs.fields().len() {
+            BooleanChunked::full("", false, self.len())
+        } else {
+            self.fields()
+                .iter()
+                .zip(rhs.fields().iter())
+                .map(|(l, r)| l.equal_missing(r).unwrap())
+                .reduce(|lhs, rhs| lhs.bitand(rhs))
+                .unwrap()
+        }
+    }
+
+    fn not_equal(&self, rhs: &StructChunked) -> BooleanChunked {
+        use std::ops::BitOr;
+        if self.len() != rhs.len() || self.fields().len() != rhs.fields().len() {
+            BooleanChunked::full("", true, self.len())
+        } else {
+            self.fields()
+                .iter()
+                .zip(rhs.fields().iter())
+                .map(|(l, r)| l.not_equal(r).unwrap())
+                .reduce(|lhs, rhs| lhs.bitor(rhs))
+                .unwrap()
+        }
+    }
+
+    fn not_equal_missing(&self, rhs: &StructChunked) -> BooleanChunked {
+        use std::ops::BitOr;
+        if self.len() != rhs.len() || self.fields().len() != rhs.fields().len() {
+            BooleanChunked::full("", true, self.len())
+        } else {
+            self.fields()
+                .iter()
+                .zip(rhs.fields().iter())
+                .map(|(l, r)| l.not_equal_missing(r).unwrap())
+                .reduce(|lhs, rhs| lhs.bitor(rhs))
+                .unwrap()
+        }
+    }
+
+    // following are not implemented because gt, lt comparison of series don't make sense
+    fn gt(&self, _rhs: &StructChunked) -> BooleanChunked {
+        unimplemented!()
+    }
+
+    fn gt_eq(&self, _rhs: &StructChunked) -> BooleanChunked {
+        unimplemented!()
+    }
+
+    fn lt(&self, _rhs: &StructChunked) -> BooleanChunked {
+        unimplemented!()
+    }
+
+    fn lt_eq(&self, _rhs: &StructChunked) -> BooleanChunked {
+        unimplemented!()
+    }
+}
+
+#[cfg(feature = "dtype-array")]
+impl ChunkCompare<&ArrayChunked> for ArrayChunked {
+    type Item = BooleanChunked;
+    fn equal(&self, rhs: &ArrayChunked) -> BooleanChunked {
+        let (a, b) = align_chunks_binary(self, rhs);
+        let chunks = a
+            .downcast_iter()
+            .zip(b.downcast_iter())
+            .map(|(a, b)| {
+                Box::new(polars_arrow::kernels::comparison::fixed_size_list_eq(a, b)) as ArrayRef
+            })
+            .collect::<Vec<_>>();
+        unsafe { BooleanChunked::from_chunks(self.name(), chunks) }
+    }
+
+    fn equal_missing(&self, rhs: &ArrayChunked) -> BooleanChunked {
+        // TODO!: maybe do something else here
+        self.equal(rhs)
+    }
+
+    fn not_equal(&self, rhs: &ArrayChunked) -> BooleanChunked {
+        let (a, b) = align_chunks_binary(self, rhs);
+        let chunks = a
+            .downcast_iter()
+            .zip(b.downcast_iter())
+            .map(|(a, b)| {
+                Box::new(polars_arrow::kernels::comparison::fixed_size_list_neq(a, b)) as ArrayRef
+            })
+            .collect::<Vec<_>>();
+        unsafe { BooleanChunked::from_chunks(self.name(), chunks) }
+    }
+
+    fn not_equal_missing(&self, rhs: &ArrayChunked) -> Self::Item {
+        // TODO!: maybe do something else here
+        self.not_equal(rhs)
+    }
+
+    // following are not implemented because gt, lt comparison of series don't make sense
+    fn gt(&self, _rhs: &ArrayChunked) -> BooleanChunked {
+        unimplemented!()
+    }
+
+    fn gt_eq(&self, _rhs: &ArrayChunked) -> BooleanChunked {
+        unimplemented!()
+    }
+
+    fn lt(&self, _rhs: &ArrayChunked) -> BooleanChunked {
+        unimplemented!()
+    }
+
+    fn lt_eq(&self, _rhs: &ArrayChunked) -> BooleanChunked {
+        unimplemented!()
+    }
+}
+
 impl Not for &BooleanChunked {
     type Output = BooleanChunked;
 
     fn not(self) -> Self::Output {
         let chunks = self
             .downcast_iter()
             .map(|a| {
@@ -922,63 +1019,16 @@
         debug_assert!(self.dtype() == other.dtype());
         let ca_other = &*(ca_other as *const BinaryChunked);
         self.get(idx_self) == ca_other.get(idx_other)
     }
 }
 
 impl ChunkEqualElement for ListChunked {}
-
-#[cfg(feature = "dtype-struct")]
-impl ChunkCompare<&StructChunked> for StructChunked {
-    type Item = BooleanChunked;
-    fn equal(&self, rhs: &StructChunked) -> BooleanChunked {
-        use std::ops::BitAnd;
-        if self.len() != rhs.len() || self.fields().len() != rhs.fields().len() {
-            BooleanChunked::full("", false, self.len())
-        } else {
-            self.fields()
-                .iter()
-                .zip(rhs.fields().iter())
-                .map(|(l, r)| l.equal(r).unwrap())
-                .reduce(|lhs, rhs| lhs.bitand(rhs))
-                .unwrap()
-        }
-    }
-
-    fn not_equal(&self, rhs: &StructChunked) -> BooleanChunked {
-        use std::ops::BitOr;
-        if self.len() != rhs.len() || self.fields().len() != rhs.fields().len() {
-            BooleanChunked::full("", false, self.len())
-        } else {
-            self.fields()
-                .iter()
-                .zip(rhs.fields().iter())
-                .map(|(l, r)| l.not_equal(r).unwrap())
-                .reduce(|lhs, rhs| lhs.bitor(rhs))
-                .unwrap()
-        }
-    }
-
-    // following are not implemented because gt, lt comparison of series don't make sense
-    fn gt(&self, _rhs: &StructChunked) -> BooleanChunked {
-        unimplemented!()
-    }
-
-    fn gt_eq(&self, _rhs: &StructChunked) -> BooleanChunked {
-        unimplemented!()
-    }
-
-    fn lt(&self, _rhs: &StructChunked) -> BooleanChunked {
-        unimplemented!()
-    }
-
-    fn lt_eq(&self, _rhs: &StructChunked) -> BooleanChunked {
-        unimplemented!()
-    }
-}
+#[cfg(feature = "dtype-array")]
+impl ChunkEqualElement for ArrayChunked {}
 
 #[cfg(test)]
 mod test {
     use std::iter::repeat;
 
     use super::super::arithmetic::test::create_two_chunked;
     use super::super::test::get_chunked_array;
@@ -1264,30 +1314,30 @@
         let out = false_.lt_eq(&a);
         assert_eq!(Vec::from(&out), &[Some(true), Some(true), Some(true)]);
 
         let a = BooleanChunked::from_slice_options("", &[Some(true), Some(false), None]);
         let all_true = BooleanChunked::from_slice("", &[true, true, true]);
         let all_false = BooleanChunked::from_slice("", &[false, false, false]);
         let out = a.equal(&true_);
-        assert_eq!(Vec::from(&out), &[Some(true), Some(false), Some(false)]);
+        assert_eq!(Vec::from(&out), &[Some(true), Some(false), None]);
         let out = a.not_equal(&true_);
-        assert_eq!(Vec::from(&out), &[Some(false), Some(true), Some(true)]);
+        assert_eq!(Vec::from(&out), &[Some(false), Some(true), None]);
 
         let out = a.equal(&all_true);
-        assert_eq!(Vec::from(&out), &[Some(true), Some(false), Some(false)]);
+        assert_eq!(Vec::from(&out), &[Some(true), Some(false), None]);
         let out = a.not_equal(&all_true);
-        assert_eq!(Vec::from(&out), &[Some(false), Some(true), Some(true)]);
+        assert_eq!(Vec::from(&out), &[Some(false), Some(true), None]);
         let out = a.equal(&false_);
-        assert_eq!(Vec::from(&out), &[Some(false), Some(true), Some(false)]);
+        assert_eq!(Vec::from(&out), &[Some(false), Some(true), None]);
         let out = a.not_equal(&false_);
-        assert_eq!(Vec::from(&out), &[Some(true), Some(false), Some(true)]);
+        assert_eq!(Vec::from(&out), &[Some(true), Some(false), None]);
         let out = a.equal(&all_false);
-        assert_eq!(Vec::from(&out), &[Some(false), Some(true), Some(false)]);
+        assert_eq!(Vec::from(&out), &[Some(false), Some(true), None]);
         let out = a.not_equal(&all_false);
-        assert_eq!(Vec::from(&out), &[Some(true), Some(false), Some(true)]);
+        assert_eq!(Vec::from(&out), &[Some(true), Some(false), None]);
     }
 
     #[test]
     fn test_broadcasting_numeric() {
         let a = Int32Chunked::from_slice("", &[1, 2, 3]);
         let one = Int32Chunked::from_slice("", &[1]);
         let three = Int32Chunked::from_slice("", &[3]);
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/comparison/scalar.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/comparison/scalar.rs`

 * *Files 4% similar despite different names*

```diff
@@ -72,23 +72,31 @@
 impl<T, Rhs> ChunkCompare<Rhs> for ChunkedArray<T>
 where
     T: PolarsNumericType,
     Rhs: ToPrimitive,
 {
     type Item = BooleanChunked;
     fn equal(&self, rhs: Rhs) -> BooleanChunked {
+        self.primitive_compare_scalar(rhs, |l, rhs| comparison::eq_scalar(l, rhs))
+    }
+
+    fn equal_missing(&self, rhs: Rhs) -> BooleanChunked {
         self.primitive_compare_scalar(rhs, |l, rhs| comparison::eq_scalar_and_validity(l, rhs))
     }
 
     fn not_equal(&self, rhs: Rhs) -> BooleanChunked {
+        self.primitive_compare_scalar(rhs, |l, rhs| comparison::neq_scalar(l, rhs))
+    }
+
+    fn not_equal_missing(&self, rhs: Rhs) -> BooleanChunked {
         self.primitive_compare_scalar(rhs, |l, rhs| comparison::neq_scalar_and_validity(l, rhs))
     }
 
     fn gt(&self, rhs: Rhs) -> BooleanChunked {
-        match (self.is_sorted_flag2(), self.null_count()) {
+        match (self.is_sorted_flag(), self.null_count()) {
             (IsSorted::Ascending, 0) => {
                 let rhs: T::Native = NumCast::from(rhs).unwrap();
 
                 let cmp_fn = |a: &T::Native| match compare_fn_nan_max(a, &rhs) {
                     Ordering::Equal | Ordering::Less => Ordering::Less,
                     _ => Ordering::Greater,
                 };
@@ -97,15 +105,15 @@
                 ca
             }
             _ => self.primitive_compare_scalar(rhs, |l, rhs| comparison::gt_scalar(l, rhs)),
         }
     }
 
     fn gt_eq(&self, rhs: Rhs) -> BooleanChunked {
-        match (self.is_sorted_flag2(), self.null_count()) {
+        match (self.is_sorted_flag(), self.null_count()) {
             (IsSorted::Ascending, 0) => {
                 let rhs: T::Native = NumCast::from(rhs).unwrap();
 
                 let cmp_fn = |a: &T::Native| match compare_fn_nan_max(a, &rhs) {
                     Ordering::Equal | Ordering::Greater => Ordering::Greater,
                     Ordering::Less => Ordering::Less,
                 };
@@ -114,15 +122,15 @@
                 ca
             }
             _ => self.primitive_compare_scalar(rhs, |l, rhs| comparison::gt_eq_scalar(l, rhs)),
         }
     }
 
     fn lt(&self, rhs: Rhs) -> BooleanChunked {
-        match (self.is_sorted_flag2(), self.null_count()) {
+        match (self.is_sorted_flag(), self.null_count()) {
             (IsSorted::Ascending, 0) => {
                 let rhs: T::Native = NumCast::from(rhs).unwrap();
 
                 let cmp_fn = |a: &T::Native| match compare_fn_nan_max(a, &rhs) {
                     Ordering::Equal | Ordering::Greater => Ordering::Greater,
                     Ordering::Less => Ordering::Less,
                 };
@@ -131,15 +139,15 @@
                 ca
             }
             _ => self.primitive_compare_scalar(rhs, |l, rhs| comparison::lt_scalar(l, rhs)),
         }
     }
 
     fn lt_eq(&self, rhs: Rhs) -> BooleanChunked {
-        match (self.is_sorted_flag2(), self.null_count()) {
+        match (self.is_sorted_flag(), self.null_count()) {
             (IsSorted::Ascending, 0) => {
                 let rhs: T::Native = NumCast::from(rhs).unwrap();
 
                 let cmp_fn = |a: &T::Native| match compare_fn_nan_max(a, &rhs) {
                     Ordering::Greater => Ordering::Greater,
                     Ordering::Equal | Ordering::Less => Ordering::Less,
                 };
@@ -173,17 +181,26 @@
         self.apply_kernel_cast(&|arr| Box::new(f(arr, &scalar)))
     }
 }
 
 impl ChunkCompare<&[u8]> for BinaryChunked {
     type Item = BooleanChunked;
     fn equal(&self, rhs: &[u8]) -> BooleanChunked {
+        self.binary_compare_scalar(rhs, |l, rhs| comparison::eq_scalar(l, rhs))
+    }
+
+    fn equal_missing(&self, rhs: &[u8]) -> BooleanChunked {
         self.binary_compare_scalar(rhs, |l, rhs| comparison::eq_scalar_and_validity(l, rhs))
     }
+
     fn not_equal(&self, rhs: &[u8]) -> BooleanChunked {
+        self.binary_compare_scalar(rhs, |l, rhs| comparison::neq_scalar(l, rhs))
+    }
+
+    fn not_equal_missing(&self, rhs: &[u8]) -> BooleanChunked {
         self.binary_compare_scalar(rhs, |l, rhs| comparison::neq_scalar_and_validity(l, rhs))
     }
 
     fn gt(&self, rhs: &[u8]) -> BooleanChunked {
         self.binary_compare_scalar(rhs, |l, rhs| comparison::gt_scalar(l, rhs))
     }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/drop.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/drop.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/float.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/float.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/from.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/from.rs`

 * *Files 16% similar despite different names*

```diff
@@ -36,14 +36,40 @@
                 cat.array_ref(0).clone(),
                 list_arr.validity().cloned(),
             );
             chunks.clear();
             chunks.push(Box::new(new_array));
             DataType::List(Box::new(cat.dtype().clone()))
         }
+        #[cfg(all(feature = "dtype-array", feature = "dtype-categorical"))]
+        DataType::Array(inner, width) if *inner == DataType::Categorical(None) => {
+            let array = concatenate_owned_unchecked(chunks).unwrap();
+            let list_arr = array.as_any().downcast_ref::<FixedSizeListArray>().unwrap();
+            let values_arr = list_arr.values();
+            let cat = unsafe {
+                Series::try_from_arrow_unchecked(
+                    "",
+                    vec![values_arr.clone()],
+                    values_arr.data_type(),
+                )
+                .unwrap()
+            };
+
+            // we nest only the physical representation
+            // the mapping is still in our rev-map
+            let arrow_dtype = FixedSizeListArray::default_datatype(ArrowDataType::UInt32, width);
+            let new_array = FixedSizeListArray::new(
+                arrow_dtype,
+                cat.array_ref(0).clone(),
+                list_arr.validity().cloned(),
+            );
+            chunks.clear();
+            chunks.push(Box::new(new_array));
+            DataType::Array(Box::new(cat.dtype().clone()), width)
+        }
         _ => dtype,
     }
 }
 
 impl<T> ChunkedArray<T>
 where
     T: PolarsDataType,
@@ -51,14 +77,16 @@
     /// Create a new ChunkedArray from existing chunks.
     ///
     /// # Safety
     /// The Arrow datatype of all chunks must match the [`PolarsDataType`] `T`.
     pub unsafe fn from_chunks(name: &str, mut chunks: Vec<ArrayRef>) -> Self {
         let dtype = match T::get_dtype() {
             dtype @ DataType::List(_) => from_chunks_list_dtype(&mut chunks, dtype),
+            #[cfg(feature = "dtype-array")]
+            dtype @ DataType::Array(_, _) => from_chunks_list_dtype(&mut chunks, dtype),
             dt => dt,
         };
         let field = Arc::new(Field::new(name, dtype));
         let mut out = ChunkedArray {
             field,
             chunks,
             phantom: PhantomData,
@@ -89,14 +117,34 @@
     pub(crate) unsafe fn from_chunks_and_dtype_unchecked(
         name: &str,
         chunks: Vec<ArrayRef>,
         dtype: DataType,
     ) -> Self {
         let field = Arc::new(Field::new(name, dtype));
         let mut out = ChunkedArray {
+            field,
+            chunks,
+            phantom: PhantomData,
+            bit_settings: Default::default(),
+            length: 0,
+        };
+        out.compute_len();
+        out
+    }
+}
+
+#[cfg(feature = "dtype-array")]
+impl ArrayChunked {
+    pub(crate) unsafe fn from_chunks_and_dtype_unchecked(
+        name: &str,
+        chunks: Vec<ArrayRef>,
+        dtype: DataType,
+    ) -> Self {
+        let field = Arc::new(Field::new(name, dtype));
+        let mut out = ChunkedArray {
             field,
             chunks,
             phantom: PhantomData,
             bit_settings: Default::default(),
             length: 0,
         };
         out.compute_len();
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/iterator/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/iterator/mod.rs`

 * *Files 8% similar despite different names*

```diff
@@ -412,14 +412,145 @@
             self.downcast_iter()
                 .flat_map(move |arr| ListIterNoNull::new(arr, inner_type.clone()))
                 .trust_my_length(self.len())
         }
     }
 }
 
+#[cfg(feature = "dtype-array")]
+impl<'a> IntoIterator for &'a ArrayChunked {
+    type Item = Option<Series>;
+    type IntoIter = Box<dyn PolarsIterator<Item = Self::Item> + 'a>;
+    fn into_iter(self) -> Self::IntoIter {
+        let dtype = self.inner_dtype();
+
+        if self.null_count() == 0 {
+            // we know that we only iterate over length == self.len()
+            unsafe {
+                Box::new(
+                    self.downcast_iter()
+                        .flat_map(|arr| arr.iter().unwrap_required())
+                        .trust_my_length(self.len())
+                        .map(move |arr| {
+                            Some(Series::from_chunks_and_dtype_unchecked(
+                                "",
+                                vec![arr],
+                                &dtype,
+                            ))
+                        }),
+                )
+            }
+        } else {
+            // we know that we only iterate over length == self.len()
+            unsafe {
+                Box::new(
+                    self.downcast_iter()
+                        .flat_map(|arr| arr.iter())
+                        .trust_my_length(self.len())
+                        .map(move |arr| {
+                            arr.map(|arr| {
+                                Series::from_chunks_and_dtype_unchecked("", vec![arr], &dtype)
+                            })
+                        }),
+                )
+            }
+        }
+    }
+}
+
+#[cfg(feature = "dtype-array")]
+pub struct FixedSizeListIterNoNull<'a> {
+    array: &'a FixedSizeListArray,
+    inner_type: DataType,
+    current: usize,
+    current_end: usize,
+}
+
+#[cfg(feature = "dtype-array")]
+impl<'a> FixedSizeListIterNoNull<'a> {
+    /// create a new iterator
+    pub fn new(array: &'a FixedSizeListArray, inner_type: DataType) -> Self {
+        FixedSizeListIterNoNull {
+            array,
+            inner_type,
+            current: 0,
+            current_end: array.len(),
+        }
+    }
+}
+
+#[cfg(feature = "dtype-array")]
+impl<'a> Iterator for FixedSizeListIterNoNull<'a> {
+    type Item = Series;
+
+    fn next(&mut self) -> Option<Self::Item> {
+        if self.current == self.current_end {
+            None
+        } else {
+            let old = self.current;
+            self.current += 1;
+            unsafe {
+                Some(Series::from_chunks_and_dtype_unchecked(
+                    "",
+                    vec![self.array.value_unchecked(old)],
+                    &self.inner_type,
+                ))
+            }
+        }
+    }
+
+    fn size_hint(&self) -> (usize, Option<usize>) {
+        (
+            self.array.len() - self.current,
+            Some(self.array.len() - self.current),
+        )
+    }
+}
+
+#[cfg(feature = "dtype-array")]
+impl<'a> DoubleEndedIterator for FixedSizeListIterNoNull<'a> {
+    fn next_back(&mut self) -> Option<Self::Item> {
+        if self.current_end == self.current {
+            None
+        } else {
+            self.current_end -= 1;
+            unsafe {
+                Some(Series::try_from(("", self.array.value_unchecked(self.current_end))).unwrap())
+            }
+        }
+    }
+}
+
+/// all arrays have known size.
+#[cfg(feature = "dtype-array")]
+impl<'a> ExactSizeIterator for FixedSizeListIterNoNull<'a> {}
+
+#[cfg(feature = "dtype-array")]
+impl ArrayChunked {
+    #[allow(clippy::wrong_self_convention)]
+    #[doc(hidden)]
+    pub fn into_no_null_iter(
+        &self,
+    ) -> impl Iterator<Item = Series>
+           + '_
+           + Send
+           + Sync
+           + ExactSizeIterator
+           + DoubleEndedIterator
+           + TrustedLen {
+        // we know that we only iterate over length == self.len()
+        let inner_type = self.inner_dtype();
+        unsafe {
+            self.downcast_iter()
+                .flat_map(move |arr| FixedSizeListIterNoNull::new(arr, inner_type.clone()))
+                .trust_my_length(self.len())
+        }
+    }
+}
+
 #[cfg(feature = "object")]
 impl<'a, T> IntoIterator for &'a ObjectChunked<T>
 where
     T: PolarsObject,
 {
     type Item = Option<&'a T>;
     type IntoIter = Box<dyn PolarsIterator<Item = Self::Item> + 'a>;
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/iterator/par/list.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/iterator/par/list.rs`

 * *Files 6% similar despite different names*

```diff
@@ -23,14 +23,15 @@
             (0..arr.len())
                 .into_par_iter()
                 .map(move |idx| unsafe { idx_to_array(idx, arr, &dtype) })
         })
     }
 
     // Get an indexed parallel iterator over the [`Series`] in this [`ListChunked`].
+    // Also might be faster as it doesn't use `flat_map`.
     pub fn par_iter_indexed(&mut self) -> impl IndexedParallelIterator<Item = Option<Series>> + '_ {
         *self = self.rechunk();
         let arr = self.downcast_iter().next().unwrap();
 
         let dtype = self.inner_dtype();
         (0..arr.len())
             .into_par_iter()
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/iterator/par/utf8.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/iterator/par/utf8.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/kernels/take.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/kernels/take.rs`

 * *Files 22% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 use std::convert::TryFrom;
 
-use arrow::bitmap::MutableBitmap;
-use polars_arrow::array::PolarsArray;
-use polars_arrow::bit_util::unset_bit_raw;
+use polars_arrow::compute::take::bitmap::take_bitmap_unchecked;
 use polars_arrow::compute::take::take_value_indices_from_list;
+use polars_arrow::utils::combine_validities_and;
 
 use crate::prelude::*;
 
 /// Take kernel for multiple chunks. We directly return a ChunkedArray because that path chooses the fastest collection path.
 pub(crate) fn take_primitive_iter_n_chunks<T: PolarsNumericType, I: IntoIterator<Item = usize>>(
     ca: &ChunkedArray<T>,
     indices: I,
@@ -28,15 +27,14 @@
     let taker = ca.take_rand();
     indices
         .into_iter()
         .map(|opt_idx| opt_idx.and_then(|idx| taker.get(idx)))
         .collect()
 }
 
-/// Forked and adapted from arrow-rs
 /// This is faster because it does no bounds checks and allocates directly into aligned memory
 ///
 /// # Safety
 /// No bounds checks
 pub(crate) unsafe fn take_list_unchecked(
     values: &ListArray<i64>,
     indices: &IdxArr,
@@ -51,34 +49,19 @@
             "",
             vec![Box::new(list_indices) as ArrayRef],
         ))
         .unwrap();
 
     let taken = taken.array_ref(0).clone();
 
-    let validity =
-        // if null count > 0
-        if values.has_validity() || indices.has_validity() {
-            // determine null buffer, which are a function of `values` and `indices`
-            let mut validity = MutableBitmap::with_capacity(indices.len());
-            let validity_ptr = validity.as_slice().as_ptr() as *mut u8;
-            validity.extend_constant(indices.len(), true);
-
-            {
-                offsets.as_slice().windows(2).enumerate().for_each(
-                    |(i, window): (usize, &[i64])| {
-                        if window[0] == window[1] {
-                            // offsets are equal, slot is null
-                            unset_bit_raw(validity_ptr, i);
-                        }
-                    },
-                );
-            }
-            Some(validity.into())
-        } else {
-            None
-        };
+    let validity = if let Some(validity) = values.validity() {
+        let validity = take_bitmap_unchecked(validity, indices.values().as_slice());
+        combine_validities_and(Some(&validity), indices.validity())
+    } else {
+        indices.validity().cloned()
+    };
+
     let dtype = ListArray::<i64>::default_datatype(taken.data_type().clone());
     // Safety:
     // offsets are monotonically increasing
     ListArray::new(dtype, offsets.into(), taken, validity)
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/list/iterator.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/list/iterator.rs`

 * *Files 3% similar despite different names*

```diff
@@ -13,14 +13,33 @@
     lifetime: PhantomData<&'a ArrayRef>,
     iter: I,
     // used only if feature="dtype-struct"
     #[allow(dead_code)]
     inner_dtype: DataType,
 }
 
+impl<'a, I: Iterator<Item = Option<ArrayBox>>> AmortizedListIter<'a, I> {
+    pub(crate) fn new(
+        len: usize,
+        series_container: Box<Series>,
+        inner: NonNull<ArrayRef>,
+        iter: I,
+        inner_dtype: DataType,
+    ) -> Self {
+        Self {
+            len,
+            series_container,
+            inner,
+            lifetime: PhantomData,
+            iter,
+            inner_dtype,
+        }
+    }
+}
+
 impl<'a, I: Iterator<Item = Option<ArrayBox>>> Iterator for AmortizedListIter<'a, I> {
     type Item = Option<UnstableSeries<'a>>;
 
     fn next(&mut self) -> Option<Self::Item> {
         self.iter.next().map(|opt_val| {
             opt_val.map(|array_ref| {
                 #[cfg(feature = "dtype-struct")]
@@ -125,22 +144,21 @@
                 vec![inner_values.clone()],
                 &iter_dtype,
             ))
         };
 
         let ptr = series_container.array_ref(0) as *const ArrayRef as *mut ArrayRef;
 
-        AmortizedListIter {
-            len: self.len(),
+        AmortizedListIter::new(
+            self.len(),
             series_container,
-            inner: NonNull::new(ptr).unwrap(),
-            lifetime: PhantomData,
-            iter: self.downcast_iter().flat_map(|arr| arr.iter()),
+            NonNull::new(ptr).unwrap(),
+            self.downcast_iter().flat_map(|arr| arr.iter()),
             inner_dtype,
-        }
+        )
     }
 
     /// Apply a closure `F` elementwise.
     #[cfg(feature = "private")]
     #[must_use]
     pub fn apply_amortized<'a, F>(&'a self, mut f: F) -> Self
     where
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/list/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/list/mod.rs`

 * *Files 12% similar despite different names*

```diff
@@ -1,14 +1,27 @@
 //! Special list utility methods
-mod iterator;
+pub(super) mod iterator;
 
 use crate::chunked_array::Settings;
 use crate::prelude::*;
 
 impl ListChunked {
+    /// Get the inner data type of the list.
+    pub fn inner_dtype(&self) -> DataType {
+        match self.dtype() {
+            DataType::List(dt) => *dt.clone(),
+            _ => unreachable!(),
+        }
+    }
+
+    pub fn set_inner_dtype(&mut self, dtype: DataType) {
+        assert_eq!(dtype.to_physical(), self.inner_dtype().to_physical());
+        let field = Arc::make_mut(&mut self.field);
+        field.coerce(DataType::List(Box::new(dtype)));
+    }
     #[cfg(feature = "private")]
     pub fn set_fast_explode(&mut self) {
         self.bit_settings.insert(Settings::FAST_EXPLODE_LIST)
     }
     pub(crate) fn unset_fast_explode(&mut self) {
         self.bit_settings.remove(Settings::FAST_EXPLODE_LIST)
     }
@@ -20,15 +33,15 @@
     pub(crate) fn is_nested(&self) -> bool {
         match self.dtype() {
             DataType::List(inner) => matches!(&**inner, DataType::List(_)),
             _ => unreachable!(),
         }
     }
 
-    pub fn to_logical(&mut self, inner_dtype: DataType) {
+    pub fn to_physical(&mut self, inner_dtype: DataType) {
         debug_assert_eq!(inner_dtype.to_physical(), self.inner_dtype());
         let fld = Arc::make_mut(&mut self.field);
         fld.coerce(DataType::List(Box::new(inner_dtype)))
     }
 
     /// Get the inner values as `Series`, ignoring the list offsets.
     pub fn get_inner(&self) -> Series {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/categorical/builder.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/categorical/builder.rs`

 * *Files 0% similar despite different names*

```diff
@@ -85,14 +85,18 @@
 
 #[allow(clippy::len_without_is_empty)]
 impl RevMapping {
     pub fn is_global(&self) -> bool {
         matches!(self, Self::Global(_, _, _))
     }
 
+    pub fn is_local(&self) -> bool {
+        !self.is_global()
+    }
+
     /// Get the length of the [`RevMapping`]
     pub fn len(&self) -> usize {
         match self {
             Self::Global(_, a, _) => a.len(),
             Self::Local(a) => a.len(),
         }
     }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/categorical/from.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/categorical/from.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/categorical/merge.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/categorical/merge.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/categorical/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/categorical/mod.rs`

 * *Files 0% similar despite different names*

```diff
@@ -291,15 +291,15 @@
 
     #[test]
     fn test_categorical_flow() -> PolarsResult<()> {
         let _lock = SINGLE_LOCK.lock();
         reset_string_cache();
         enable_string_cache(false);
 
-        // tests several things that may loose the dtype information
+        // tests several things that may lose the dtype information
         let s = Series::new("a", vec!["a", "b", "c"]).cast(&DataType::Categorical(None))?;
 
         assert_eq!(
             s.field().into_owned(),
             Field::new("a", DataType::Categorical(None))
         );
         assert!(matches!(
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/categorical/ops/append.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/categorical/ops/append.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/categorical/ops/take_random.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/categorical/ops/take_random.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/categorical/ops/unique.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/categorical/ops/unique.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/categorical/ops/zip.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/categorical/ops/zip.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/categorical/stringcache.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/categorical/stringcache.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/date.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/date.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/datetime.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/datetime.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/decimal.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/decimal.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/duration.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/duration.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/struct_/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/struct_/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/logical/time.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/logical/time.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/mod.rs`

 * *Files 2% similar despite different names*

```diff
@@ -17,14 +17,16 @@
 pub mod comparison;
 pub mod float;
 pub mod iterator;
 pub mod kernels;
 #[cfg(feature = "ndarray")]
 mod ndarray;
 
+#[cfg(feature = "dtype-array")]
+pub(crate) mod array;
 mod bitwise;
 #[cfg(feature = "object")]
 mod drop;
 mod from;
 pub(crate) mod list;
 pub(crate) mod logical;
 #[cfg(feature = "object")]
@@ -168,15 +170,15 @@
         self.bit_settings.contains(Settings::SORTED_DSC)
     }
 
     pub fn unset_fast_explode_list(&mut self) {
         self.bit_settings.remove(Settings::FAST_EXPLODE_LIST)
     }
 
-    pub fn is_sorted_flag2(&self) -> IsSorted {
+    pub fn is_sorted_flag(&self) -> IsSorted {
         if self.is_sorted_ascending_flag() {
             IsSorted::Ascending
         } else if self.is_sorted_descending_flag() {
             IsSorted::Descending
         } else {
             IsSorted::Not
         }
@@ -426,14 +428,16 @@
         let ptr = a.as_ptr();
         Ok(ptr as usize)
     }
 }
 
 impl AsSinglePtr for BooleanChunked {}
 impl AsSinglePtr for ListChunked {}
+#[cfg(feature = "dtype-array")]
+impl AsSinglePtr for ArrayChunked {}
 impl AsSinglePtr for Utf8Chunked {}
 impl AsSinglePtr for BinaryChunked {}
 #[cfg(feature = "object")]
 impl<T: PolarsObject> AsSinglePtr for ObjectChunked<T> {}
 
 impl<T> ChunkedArray<T>
 where
@@ -510,43 +514,35 @@
     fn get_values_size(&self) -> usize {
         self.chunks
             .iter()
             .fold(0usize, |acc, arr| acc + arr.get_values_size())
     }
 }
 
-impl ValueSize for Utf8Chunked {
+#[cfg(feature = "dtype-array")]
+impl ValueSize for ArrayChunked {
     fn get_values_size(&self) -> usize {
         self.chunks
             .iter()
             .fold(0usize, |acc, arr| acc + arr.get_values_size())
     }
 }
-
-impl ValueSize for BinaryChunked {
+impl ValueSize for Utf8Chunked {
     fn get_values_size(&self) -> usize {
         self.chunks
             .iter()
             .fold(0usize, |acc, arr| acc + arr.get_values_size())
     }
 }
 
-impl ListChunked {
-    /// Get the inner data type of the list.
-    pub fn inner_dtype(&self) -> DataType {
-        match self.dtype() {
-            DataType::List(dt) => *dt.clone(),
-            _ => unreachable!(),
-        }
-    }
-
-    pub fn set_inner_dtype(&mut self, dtype: DataType) {
-        assert_eq!(dtype.to_physical(), self.inner_dtype().to_physical());
-        let field = Arc::make_mut(&mut self.field);
-        field.coerce(DataType::List(Box::new(dtype)));
+impl ValueSize for BinaryChunked {
+    fn get_values_size(&self) -> usize {
+        self.chunks
+            .iter()
+            .fold(0usize, |acc, arr| acc + arr.get_values_size())
     }
 }
 
 pub(crate) fn to_primitive<T: PolarsNumericType>(
     values: Vec<T::Native>,
     validity: Option<Bitmap>,
 ) -> PrimitiveArray<T::Native> {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ndarray.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ndarray.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/object/builder.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/object/builder.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/object/extension/drop.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/object/extension/drop.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/object/extension/list.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/object/extension/list.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/object/extension/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/object/extension/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/object/extension/polars_extension.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/object/extension/polars_extension.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/object/iterator.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/object/iterator.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/object/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/object/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/object/registry.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/object/registry.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/aggregate/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/aggregate/mod.rs`

 * *Files 1% similar despite different names*

```diff
@@ -102,15 +102,15 @@
                     Some(acc) => Some(acc + v),
                 },
                 None => acc,
             })
     }
 
     fn min(&self) -> Option<T::Native> {
-        match self.is_sorted_flag2() {
+        match self.is_sorted_flag() {
             IsSorted::Ascending => {
                 self.first_non_null().and_then(|idx| {
                     // Safety:
                     // first_non_null returns in bound index
                     unsafe { self.get_unchecked(idx) }
                 })
             }
@@ -131,15 +131,15 @@
                         v
                     }
                 }),
         }
     }
 
     fn max(&self) -> Option<T::Native> {
-        match self.is_sorted_flag2() {
+        match self.is_sorted_flag() {
             IsSorted::Ascending => {
                 self.last_non_null().and_then(|idx| {
                     // Safety:
                     // first_non_null returns in bound index
                     unsafe { self.get_unchecked(idx) }
                 })
             }
@@ -440,25 +440,25 @@
         let v = self.min();
         Series::new(self.name(), [v])
     }
 }
 
 impl Utf8Chunked {
     pub(crate) fn max_str(&self) -> Option<&str> {
-        match self.is_sorted_flag2() {
+        match self.is_sorted_flag() {
             IsSorted::Ascending => self.get(self.len() - 1),
             IsSorted::Descending => self.get(0),
             IsSorted::Not => self
                 .downcast_iter()
                 .filter_map(compute::aggregate::max_string)
                 .fold_first_(|acc, v| if acc > v { acc } else { v }),
         }
     }
     pub(crate) fn min_str(&self) -> Option<&str> {
-        match self.is_sorted_flag2() {
+        match self.is_sorted_flag() {
             IsSorted::Ascending => self.get(0),
             IsSorted::Descending => self.get(self.len() - 1),
             IsSorted::Not => self
                 .downcast_iter()
                 .filter_map(compute::aggregate::min_string)
                 .fold_first_(|acc, v| if acc < v { acc } else { v }),
         }
@@ -509,14 +509,30 @@
         ListChunked::full_null_with_dtype(self.name(), 1, &self.inner_dtype()).into_series()
     }
     fn min_as_series(&self) -> Series {
         ListChunked::full_null_with_dtype(self.name(), 1, &self.inner_dtype()).into_series()
     }
 }
 
+#[cfg(feature = "dtype-array")]
+impl ChunkAggSeries for ArrayChunked {
+    fn sum_as_series(&self) -> Series {
+        ArrayChunked::full_null_with_dtype(self.name(), 1, &self.inner_dtype(), self.width())
+            .into_series()
+    }
+    fn max_as_series(&self) -> Series {
+        ArrayChunked::full_null_with_dtype(self.name(), 1, &self.inner_dtype(), self.width())
+            .into_series()
+    }
+    fn min_as_series(&self) -> Series {
+        ArrayChunked::full_null_with_dtype(self.name(), 1, &self.inner_dtype(), self.width())
+            .into_series()
+    }
+}
+
 #[cfg(feature = "object")]
 impl<T: PolarsObject> ChunkAggSeries for ObjectChunked<T> {}
 
 #[cfg(test)]
 mod test {
     use polars_arrow::prelude::QuantileInterpolOptions;
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/aggregate/quantile.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/aggregate/quantile.rs`

 * *Files 2% similar despite different names*

```diff
@@ -313,10 +313,12 @@
         self.quantile_faster(0.5, QuantileInterpolOptions::Linear)
             .unwrap()
     }
 }
 
 impl ChunkQuantile<String> for Utf8Chunked {}
 impl ChunkQuantile<Series> for ListChunked {}
+#[cfg(feature = "dtype-array")]
+impl ChunkQuantile<Series> for ArrayChunked {}
 #[cfg(feature = "object")]
 impl<T: PolarsObject> ChunkQuantile<Series> for ObjectChunked<T> {}
 impl ChunkQuantile<bool> for BooleanChunked {}
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/aggregate/var.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/aggregate/var.rs`

 * *Files 2% similar despite different names*

```diff
@@ -11,17 +11,14 @@
 where
     T: PolarsIntegerType,
     <T::Native as Simd>::Simd: Add<Output = <T::Native as Simd>::Simd>
         + compute::aggregate::Sum<T::Native>
         + compute::aggregate::SimdOrd<T::Native>,
 {
     fn var(&self, ddof: u8) -> Option<f64> {
-        if self.len() == 1 {
-            return Some(0.0);
-        }
         let n_values = self.len() - self.null_count();
 
         if ddof as usize > n_values {
             return None;
         }
         let n_values = n_values as f64;
 
@@ -86,10 +83,12 @@
     fn std(&self, ddof: u8) -> Option<f64> {
         self.var(ddof).map(|var| var.sqrt())
     }
 }
 
 impl ChunkVar<String> for Utf8Chunked {}
 impl ChunkVar<Series> for ListChunked {}
+#[cfg(feature = "dtype-array")]
+impl ChunkVar<Series> for ArrayChunked {}
 #[cfg(feature = "object")]
 impl<T: PolarsObject> ChunkVar<Series> for ObjectChunked<T> {}
 impl ChunkVar<bool> for BooleanChunked {}
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/any_value.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/any_value.rs`

 * *Files 6% similar despite different names*

```diff
@@ -52,14 +52,27 @@
             } else {
                 let s = Series::from_chunks_and_dtype_unchecked("", vec![v], &dt.to_physical())
                     .cast_unchecked(dt)
                     .unwrap();
                 AnyValue::List(s)
             }
         }
+        #[cfg(feature = "dtype-array")]
+        DataType::Array(dt, width) => {
+            let v: ArrayRef = downcast!(FixedSizeListArray);
+            if dt.is_primitive() {
+                let s = Series::from_chunks_and_dtype_unchecked("", vec![v], dt);
+                AnyValue::Array(s, *width)
+            } else {
+                let s = Series::from_chunks_and_dtype_unchecked("", vec![v], &dt.to_physical())
+                    .cast_unchecked(dt)
+                    .unwrap();
+                AnyValue::Array(s, *width)
+            }
+        }
         #[cfg(feature = "dtype-categorical")]
         DataType::Categorical(rev_map) => {
             let arr = &*(arr as *const dyn Array as *const UInt32Array);
             let v = arr.value_unchecked(idx);
             AnyValue::Categorical(v, rev_map.as_ref().unwrap().as_ref(), SyncPtr::new_null())
         }
         #[cfg(feature = "dtype-struct")]
@@ -233,14 +246,26 @@
     #[inline]
     unsafe fn get_any_value_unchecked(&self, index: usize) -> AnyValue {
         get_any_value_unchecked!(self, index)
     }
 
     fn get_any_value(&self, index: usize) -> PolarsResult<AnyValue> {
         get_any_value!(self, index)
+    }
+}
+
+#[cfg(feature = "dtype-array")]
+impl ChunkAnyValue for ArrayChunked {
+    #[inline]
+    unsafe fn get_any_value_unchecked(&self, index: usize) -> AnyValue {
+        get_any_value_unchecked!(self, index)
+    }
+
+    fn get_any_value(&self, index: usize) -> PolarsResult<AnyValue> {
+        get_any_value!(self, index)
     }
 }
 
 #[cfg(feature = "object")]
 impl<T: PolarsObject> ChunkAnyValue for ObjectChunked<T> {
     #[inline]
     unsafe fn get_any_value_unchecked(&self, index: usize) -> AnyValue {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/append.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/append.rs`

 * *Files 2% similar despite different names*

```diff
@@ -66,14 +66,30 @@
         self.set_sorted_flag(IsSorted::Not);
         if !other._can_fast_explode() {
             self.unset_fast_explode()
         }
         Ok(())
     }
 }
+
+#[cfg(feature = "dtype-array")]
+#[doc(hidden)]
+impl ArrayChunked {
+    pub fn append(&mut self, other: &Self) -> PolarsResult<()> {
+        let dtype = merge_dtypes(self.dtype(), other.dtype())?;
+        self.field = Arc::new(Field::new(self.name(), dtype));
+
+        let len = self.len();
+        self.length += other.length;
+        new_chunks(&mut self.chunks, &other.chunks, len);
+        self.set_sorted_flag(IsSorted::Not);
+        Ok(())
+    }
+}
+
 #[cfg(feature = "object")]
 #[doc(hidden)]
 impl<T: PolarsObject> ObjectChunked<T> {
     pub fn append(&mut self, other: &Self) {
         let len = self.len();
         self.length += other.length;
         self.set_sorted_flag(IsSorted::Not);
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/apply.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/apply.rs`

 * *Files 2% similar despite different names*

```diff
@@ -67,25 +67,33 @@
                 .as_any()
                 .downcast_ref::<PrimitiveArray<S::Native>>()
                 .unwrap()
                 .clone();
             // make sure we have a single ref count coming in.
             drop(arr);
 
-            match owned_arr.into_mut() {
-                Left(immutable) => Box::new(arrow::compute::arity::unary(
-                    &immutable,
+            let compute_immutable = |arr: &PrimitiveArray<S::Native>| {
+                Box::new(arrow::compute::arity::unary(
+                    arr,
                     f,
                     S::get_dtype().to_arrow(),
-                )),
-                Right(mut mutable) => {
-                    let vals = mutable.values_mut_slice();
-                    vals.iter_mut().for_each(|v| *v = f(*v));
-                    let a: PrimitiveArray<_> = mutable.into();
-                    Box::new(a) as ArrayRef
+                ))
+            };
+
+            if owned_arr.values().is_sliced() {
+                compute_immutable(&owned_arr)
+            } else {
+                match owned_arr.into_mut() {
+                    Left(immutable) => compute_immutable(&immutable),
+                    Right(mut mutable) => {
+                        let vals = mutable.values_mut_slice();
+                        vals.iter_mut().for_each(|v| *v = f(*v));
+                        let a: PrimitiveArray<_> = mutable.into();
+                        Box::new(a) as ArrayRef
+                    }
                 }
             }
         })
         .collect();
     unsafe { ChunkedArray::<S>::from_chunks(name, chunks) }
 }
 
@@ -376,14 +384,33 @@
                 Box::new(new.with_validity(arr.validity().cloned())) as ArrayRef
             })
             .collect();
         unsafe { Utf8Chunked::from_chunks(self.name(), chunks) }
     }
 }
 
+impl BinaryChunked {
+    pub fn apply_mut<'a, F>(&'a self, mut f: F) -> Self
+    where
+        F: FnMut(&'a [u8]) -> &'a [u8],
+    {
+        use polars_arrow::array::utf8::BinaryFromIter;
+        let chunks = self
+            .downcast_iter()
+            .map(|arr| {
+                let iter = arr.values_iter().map(&mut f);
+                let value_size = (arr.get_values_size() as f64 * 1.3) as usize;
+                let new = BinaryArray::<i64>::from_values_iter(iter, arr.len(), value_size);
+                Box::new(new.with_validity(arr.validity().cloned())) as ArrayRef
+            })
+            .collect();
+        unsafe { BinaryChunked::from_chunks(self.name(), chunks) }
+    }
+}
+
 impl<'a> ChunkApply<'a, &'a str, Cow<'a, str>> for Utf8Chunked {
     fn apply_cast_numeric<F, S>(&'a self, f: F) -> ChunkedArray<S>
     where
         F: Fn(&'a str) -> S::Native + Copy,
         S: PolarsNumericType,
     {
         let chunks = self
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/bit_repr.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/bit_repr.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/chunkops.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/chunkops.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/compare_inner.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/compare_inner.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/concat_str.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/concat_str.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/cum_agg.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/cum_agg.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/downcast.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/downcast.rs`

 * *Files 2% similar despite different names*

```diff
@@ -175,14 +175,38 @@
         if self.chunks.len() == 1 {
             return (0, index);
         }
         index_to_chunked_index(self.downcast_iter().map(|arr| arr.len()), index)
     }
 }
 
+#[cfg(feature = "dtype-array")]
+#[doc(hidden)]
+impl ArrayChunked {
+    pub fn downcast_iter(&self) -> impl Iterator<Item = &FixedSizeListArray> + DoubleEndedIterator {
+        // Safety:
+        // This is the array type that must be in a ArrayChunked
+        self.chunks.iter().map(|arr| {
+            let arr = &**arr;
+            unsafe { &*(arr as *const dyn Array as *const FixedSizeListArray) }
+        })
+    }
+    pub fn downcast_chunks(&self) -> Chunks<'_, FixedSizeListArray> {
+        Chunks::new(&self.chunks)
+    }
+
+    #[inline]
+    pub(crate) fn index_to_chunked_index(&self, index: usize) -> (usize, usize) {
+        if self.chunks.len() == 1 {
+            return (0, index);
+        }
+        index_to_chunked_index(self.downcast_iter().map(|arr| arr.len()), index)
+    }
+}
+
 #[cfg(feature = "object")]
 #[doc(hidden)]
 impl<T> ObjectChunked<T>
 where
     T: PolarsObject,
 {
     pub fn downcast_iter(&self) -> impl Iterator<Item = &ObjectArray<T>> + DoubleEndedIterator {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/explode.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/explode.rs`

 * *Files 6% similar despite different names*

```diff
@@ -1,17 +1,22 @@
 use std::convert::TryFrom;
 
 use arrow::array::*;
 use arrow::bitmap::{Bitmap, MutableBitmap};
 use arrow::offset::OffsetsBuffer;
+use polars_arrow::array::list::AnonymousBuilder;
 use polars_arrow::array::PolarsArray;
 use polars_arrow::bit_util::unset_bit_raw;
+#[cfg(feature = "dtype-array")]
+use polars_arrow::is_valid::IsValid;
 use polars_arrow::prelude::*;
+use polars_arrow::trusted_len::PushUnchecked;
 
-use crate::chunked_array::builder::AnonymousOwnedListBuilder;
+#[cfg(feature = "dtype-array")]
+use crate::chunked_array::builder::get_fixed_size_list_builder;
 use crate::prelude::*;
 use crate::series::implementations::null::NullChunked;
 
 pub(crate) trait ExplodeByOffsets {
     fn explode_by_offsets(&self, offsets: &[i64]) -> Series;
 }
 
@@ -168,15 +173,15 @@
     fn explode_by_offsets(&self, offsets: &[i64]) -> Series {
         self.apply_as_ints(|s| s.explode_by_offsets(offsets))
     }
 }
 
 impl ExplodeByOffsets for NullChunked {
     fn explode_by_offsets(&self, offsets: &[i64]) -> Series {
-        NullChunked::new(self.name.clone(), get_capacity(offsets)).into_series()
+        NullChunked::new(self.name.clone(), offsets.len() - 1).into_series()
     }
 }
 
 impl ExplodeByOffsets for BooleanChunked {
     fn explode_by_offsets(&self, offsets: &[i64]) -> Series {
         debug_assert_eq!(self.chunks.len(), 1);
         let arr = self.downcast_iter().next().unwrap();
@@ -212,48 +217,118 @@
                 .extend_trusted_len_values(vals.values_iter())
         } else {
             builder.array_builder.extend_trusted_len(vals.into_iter());
         }
         builder.finish().into()
     }
 }
+
 impl ExplodeByOffsets for ListChunked {
     fn explode_by_offsets(&self, offsets: &[i64]) -> Series {
         debug_assert_eq!(self.chunks.len(), 1);
         let arr = self.downcast_iter().next().unwrap();
 
         let cap = get_capacity(offsets);
         let inner_type = self.inner_dtype();
-        let mut builder = AnonymousOwnedListBuilder::new(self.name(), cap, Some(inner_type));
 
+        let mut builder = polars_arrow::array::list::AnonymousBuilder::new(cap);
+        let mut owned = Vec::with_capacity(cap);
         let mut start = offsets[0] as usize;
         let mut last = start;
+
+        let mut process_range = |start: usize, last: usize, builder: &mut AnonymousBuilder<'_>| {
+            let vals = arr.slice_typed(start, last - start);
+            for opt_arr in vals.into_iter() {
+                match opt_arr {
+                    None => builder.push_null(),
+                    Some(arr) => {
+                        unsafe {
+                            // we create a pointer to evade the bck
+                            let ptr = arr.as_ref() as *const dyn Array;
+                            // safety: we preallocated
+                            owned.push_unchecked(arr);
+                            // safety: the pointer is still valid as `owned` will not reallocate
+                            builder.push(&*ptr as &dyn Array);
+                        }
+                    }
+                }
+            }
+        };
+
         for &o in &offsets[1..] {
             let o = o as usize;
             if o == last {
                 if start != last {
-                    let vals = arr.slice_typed(start, last - start);
-                    let ca = unsafe { ListChunked::from_chunks("", vec![Box::new(vals)]) };
-                    for s in &ca {
-                        builder.append_opt_series(s.as_ref())
+                    process_range(start, last, &mut builder);
+                }
+                builder.push_null();
+                start = o;
+            }
+            last = o;
+        }
+        process_range(start, last, &mut builder);
+        let arr = builder.finish(Some(&inner_type.to_arrow())).unwrap();
+        self.copy_with_chunks(vec![Box::new(arr)], true, true)
+            .into_series()
+    }
+}
+
+#[cfg(feature = "dtype-array")]
+impl ExplodeByOffsets for ArrayChunked {
+    fn explode_by_offsets(&self, offsets: &[i64]) -> Series {
+        debug_assert_eq!(self.chunks.len(), 1);
+        let arr = self.downcast_iter().next().unwrap();
+
+        let cap = get_capacity(offsets);
+        let inner_type = self.inner_dtype();
+        let mut builder =
+            get_fixed_size_list_builder(&inner_type, cap, self.width(), self.name()).unwrap();
+
+        let mut start = offsets[0] as usize;
+        let mut last = start;
+        for &o in &offsets[1..] {
+            let o = o as usize;
+            if o == last {
+                if start != last {
+                    let array = arr.slice_typed(start, last - start);
+                    let values = array.values().as_ref();
+
+                    for i in 0..array.len() {
+                        unsafe {
+                            if array.is_valid_unchecked(i) {
+                                builder.push_unchecked(values, i)
+                            } else {
+                                builder.push_null()
+                            }
+                        }
                     }
                 }
-                builder.append_null();
+                unsafe {
+                    builder.push_null();
+                }
                 start = o;
             }
             last = o;
         }
-        let vals = arr.slice_typed(start, last - start);
-        let ca = unsafe { ListChunked::from_chunks("", vec![Box::new(vals)]) };
-        for s in &ca {
-            builder.append_opt_series(s.as_ref())
+        let array = arr.slice_typed(start, last - start);
+        let values = array.values().as_ref();
+        for i in 0..array.len() {
+            unsafe {
+                if array.is_valid_unchecked(i) {
+                    builder.push_unchecked(values, i)
+                } else {
+                    builder.push_null()
+                }
+            }
         }
+
         builder.finish().into()
     }
 }
+
 impl ExplodeByOffsets for Utf8Chunked {
     fn explode_by_offsets(&self, offsets: &[i64]) -> Series {
         unsafe {
             self.as_binary()
                 .explode_by_offsets(offsets)
                 .cast_unchecked(&DataType::Utf8)
                 .unwrap()
@@ -303,52 +378,44 @@
 }
 
 /// Convert Arrow array offsets to indexes of the original list
 pub(crate) fn offsets_to_indexes(offsets: &[i64], capacity: usize) -> Vec<IdxSize> {
     if offsets.is_empty() {
         return vec![];
     }
+
     let mut idx = Vec::with_capacity(capacity);
 
-    // `value_count` counts the taken values from the list values
-    // and are the same unit as `offsets`
-    // we also add the start offset as a list can be sliced
-    let mut value_count = offsets[0];
-    // `empty_count` counts the duplicates taken because of empty list
-    let mut empty_count = 0usize;
     let mut last_idx = 0;
+    for (offset_start, offset_end) in offsets.iter().zip(offsets[1..].iter()) {
+        if idx.len() >= capacity {
+            // significant speed-up in edge cases with many offsets,
+            // no measurable overhead in typical case due to branch prediction
+            break;
+        }
 
-    for offset in &offsets[1..] {
-        // this get all the elements up till offsets
-        while value_count < *offset {
-            value_count += 1;
-            idx.push(last_idx)
-        }
-
-        // then we compute the previous offsets
-        // Safety:
-        // we started iterating from 1, so there is always a previous offset
-        // we take the pointer to the previous element and deref that to get
-        // the previous offset
-        let previous_offset = unsafe { *(offset as *const i64).offset(-1) };
-
-        // if the previous offset is equal to the current offset we have an empty
-        // list and we duplicate previous index
-        if previous_offset == *offset {
-            empty_count += 1;
+        if offset_start == offset_end {
+            // if the previous offset is equal to the current offset, we have an empty
+            // list and we duplicate the previous index
             idx.push(last_idx);
+        } else {
+            let width = (offset_end - offset_start) as usize;
+            for _ in 0..width {
+                idx.push(last_idx);
+            }
         }
 
         last_idx += 1;
     }
 
     // take the remaining values
-    for _ in 0..(capacity - (value_count - offsets[0]) as usize - empty_count) {
+    for _ in 0..capacity.saturating_sub(idx.len()) {
         idx.push(last_idx);
     }
+    idx.truncate(capacity);
     idx
 }
 
 impl ChunkExplode for ListChunked {
     fn explode_and_offsets(&self) -> PolarsResult<(Series, OffsetsBuffer<i64>)> {
         // A list array's memory layout is actually already 'exploded', so we can just take the values array
         // of the list. And we also return a slice of the offsets. This slice can be used to find the old
@@ -412,36 +479,14 @@
             values.explode_by_offsets(offsets)
         };
         debug_assert_eq!(s.name(), self.name());
         // restore logical type
         unsafe {
             s = s.cast_unchecked(&self.inner_dtype()).unwrap();
         }
-        // // make sure we restore the logical type
-        // match self.inner_dtype() {
-        //     #[cfg(feature = "dtype-categorical")]
-        //     DataType::Categorical(rev_map) => {
-        //         let cats = s.u32().unwrap().clone();
-        //         // safety:
-        //         // rev_map is from same array, so we are still in bounds
-        //         s = unsafe {
-        //             CategoricalChunked::from_cats_and_rev_map_unchecked(cats, rev_map.unwrap())
-        //                 .into_series()
-        //         };
-        //     }
-        //     #[cfg(feature = "dtype-date")]
-        //     DataType::Date => s = s.into_date(),
-        //     #[cfg(feature = "dtype-datetime")]
-        //     DataType::Datetime(tu, tz) => s = s.into_datetime(tu, tz),
-        //     #[cfg(feature = "dtype-duration")]
-        //     DataType::Duration(tu) => s = s.into_duration(tu),
-        //     #[cfg(feature = "dtype-time")]
-        //     DataType::Time => s = s.into_time(),
-        //     _ => {}
-        // }
 
         Ok((s, offsets_buf))
     }
 }
 
 impl ChunkExplode for Utf8Chunked {
     fn explode_and_offsets(&self) -> PolarsResult<(Series, OffsetsBuffer<i64>)> {
@@ -684,8 +729,30 @@
 
     #[test]
     fn test_row_offsets() {
         let offsets = &[0, 1, 2, 2, 3, 4, 4];
         let out = offsets_to_indexes(offsets, 6);
         assert_eq!(out, &[0, 1, 2, 3, 4, 5]);
     }
+
+    #[test]
+    fn test_empty_row_offsets() {
+        let offsets = &[0, 0];
+        let out = offsets_to_indexes(offsets, 0);
+        let expected: Vec<IdxSize> = Vec::new();
+        assert_eq!(out, expected);
+    }
+
+    #[test]
+    fn test_row_offsets_over_capacity() {
+        let offsets = &[0, 1, 1, 2, 2];
+        let out = offsets_to_indexes(offsets, 2);
+        assert_eq!(out, &[0, 1]);
+    }
+
+    #[test]
+    fn test_row_offsets_nonzero_first_offset() {
+        let offsets = &[3, 6, 8];
+        let out = offsets_to_indexes(offsets, 10);
+        assert_eq!(out, &[0, 0, 0, 1, 1, 2, 2, 2, 2, 2]);
+    }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/extend.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/extend.rs`

 * *Files 5% similar despite different names*

```diff
@@ -58,27 +58,31 @@
         // decrements 1
         {
             self.chunks.clear();
         }
 
         use Either::*;
 
-        match arr.into_mut() {
-            Left(immutable) => {
-                extend_immutable(&immutable, &mut self.chunks, &other.chunks);
-            }
-            Right(mut mutable) => {
-                for arr in other.downcast_iter() {
-                    match arr.null_count() {
-                        0 => mutable.extend_from_slice(arr.values()),
-                        _ => mutable.extend_trusted_len(arr.into_iter()),
+        if arr.values().is_sliced() {
+            extend_immutable(&arr, &mut self.chunks, &other.chunks);
+        } else {
+            match arr.into_mut() {
+                Left(immutable) => {
+                    extend_immutable(&immutable, &mut self.chunks, &other.chunks);
+                }
+                Right(mut mutable) => {
+                    for arr in other.downcast_iter() {
+                        match arr.null_count() {
+                            0 => mutable.extend_from_slice(arr.values()),
+                            _ => mutable.extend_trusted_len(arr.into_iter()),
+                        }
                     }
+                    let arr: PrimitiveArray<T::Native> = mutable.into();
+                    self.chunks.push(Box::new(arr) as ArrayRef)
                 }
-                let arr: PrimitiveArray<T::Native> = mutable.into();
-                self.chunks.push(Box::new(arr) as ArrayRef)
             }
         }
         self.compute_len();
         self.set_sorted_flag(IsSorted::Not);
     }
 }
 
@@ -201,14 +205,25 @@
     pub fn extend(&mut self, other: &Self) -> PolarsResult<()> {
         // TODO! properly implement mutation
         // this is harder because we don't know the inner type of the list
         self.set_sorted_flag(IsSorted::Not);
         self.append(other)
     }
 }
+
+#[cfg(feature = "dtype-array")]
+#[doc(hidden)]
+impl ArrayChunked {
+    pub fn extend(&mut self, other: &Self) -> PolarsResult<()> {
+        // TODO! properly implement mutation
+        // this is harder because we don't know the inner type of the list
+        self.set_sorted_flag(IsSorted::Not);
+        self.append(other)
+    }
+}
 
 #[cfg(test)]
 mod test {
     use super::*;
 
     #[test]
     #[allow(clippy::redundant_clone)]
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/fill_null.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/fill_null.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/filter.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/filter.rs`

 * *Files 12% similar despite different names*

```diff
@@ -117,14 +117,45 @@
         let mut ca = self.clone();
         ca.chunks = chunks;
         ca.compute_len();
         Ok(ca)
     }
 }
 
+#[cfg(feature = "dtype-array")]
+impl ChunkFilter<FixedSizeListType> for ArrayChunked {
+    fn filter(&self, filter: &BooleanChunked) -> PolarsResult<ArrayChunked> {
+        // broadcast
+        if filter.len() == 1 {
+            return match filter.get(0) {
+                Some(true) => Ok(self.clone()),
+                _ => unsafe {
+                    Ok(ChunkedArray::from_chunks(
+                        self.name(),
+                        vec![new_empty_array(self.dtype().to_arrow())],
+                    ))
+                },
+            };
+        }
+        let (left, filter) = align_chunks_binary(self, filter);
+
+        let chunks = left
+            .downcast_iter()
+            .zip(filter.downcast_iter())
+            .map(|(left, mask)| filter_fn(left, mask).unwrap())
+            .collect::<Vec<_>>();
+
+        // inner type may be categorical or logical type so we clone the state.
+        let mut ca = self.clone();
+        ca.chunks = chunks;
+        ca.compute_len();
+        Ok(ca)
+    }
+}
+
 #[cfg(feature = "object")]
 impl<T> ChunkFilter<ObjectType<T>> for ObjectChunked<T>
 where
     T: PolarsObject,
 {
     fn filter(&self, filter: &BooleanChunked) -> PolarsResult<ChunkedArray<ObjectType<T>>>
     where
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/full.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/full.rs`

 * *Files 17% similar despite different names*

```diff
@@ -97,14 +97,59 @@
 
 impl ChunkFullNull for ListChunked {
     fn full_null(name: &str, length: usize) -> ListChunked {
         ListChunked::full_null_with_dtype(name, length, &DataType::Null)
     }
 }
 
+#[cfg(feature = "dtype-array")]
+impl ArrayChunked {
+    pub fn full_null_with_dtype(
+        name: &str,
+        length: usize,
+        inner_dtype: &DataType,
+        width: usize,
+    ) -> ArrayChunked {
+        let arr = new_null_array(
+            ArrowDataType::FixedSizeList(
+                Box::new(ArrowField::new("item", inner_dtype.to_arrow(), true)),
+                width,
+            ),
+            length,
+        );
+        unsafe { ArrayChunked::from_chunks(name, vec![arr]) }
+    }
+}
+
+#[cfg(feature = "dtype-array")]
+impl ChunkFull<&Series> for ArrayChunked {
+    fn full(name: &str, value: &Series, length: usize) -> ArrayChunked {
+        if !value.dtype().is_numeric() {
+            todo!("FixedSizeList only supports numeric data types");
+        };
+        let width = value.len();
+        let values = value.tile(length);
+        let values = values.chunks()[0].clone();
+        let data_type = ArrowDataType::FixedSizeList(
+            Box::new(ArrowField::new("item", values.data_type().clone(), true)),
+            width,
+        );
+
+        let arr = Box::new(FixedSizeListArray::new(data_type, values, None)) as ArrayRef;
+        unsafe { ArrayChunked::from_chunks(name, vec![arr]) }
+    }
+}
+
+#[cfg(feature = "dtype-array")]
+impl ChunkFullNull for ArrayChunked {
+    fn full_null(name: &str, length: usize) -> ArrayChunked {
+        ArrayChunked::full_null_with_dtype(name, length, &DataType::Null, 0)
+    }
+}
+
 impl ListChunked {
     pub fn full_null_with_dtype(name: &str, length: usize, inner_dtype: &DataType) -> ListChunked {
         let arr = new_null_array(
             ArrowDataType::LargeList(Box::new(ArrowField::new(
                 "item",
                 inner_dtype.to_arrow(),
                 true,
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/is_in.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/is_in.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/min_max_binary.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/min_max_binary.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/mod.rs`

 * *Files 2% similar despite different names*

```diff
@@ -16,36 +16,40 @@
 mod bit_repr;
 pub(crate) mod chunkops;
 pub(crate) mod compare_inner;
 #[cfg(feature = "concat_str")]
 mod concat_str;
 #[cfg(feature = "cum_agg")]
 mod cum_agg;
+#[cfg(feature = "dtype-decimal")]
+mod decimal;
 pub(crate) mod downcast;
 pub(crate) mod explode;
 mod extend;
 mod fill_null;
 mod filter;
 pub mod full;
 #[cfg(feature = "interpolate")]
 mod interpolate;
 #[cfg(feature = "is_in")]
 mod is_in;
 mod len;
+#[cfg(feature = "zip_with")]
 pub(crate) mod min_max_binary;
 mod nulls;
 mod peaks;
 #[cfg(feature = "repeat_by")]
 mod repeat_by;
 mod reverse;
 pub(crate) mod rolling_window;
 mod set;
 mod shift;
 pub mod sort;
 pub(crate) mod take;
+mod tile;
 pub(crate) mod unique;
 #[cfg(feature = "zip_with")]
 pub mod zip;
 
 #[cfg(feature = "serde-lazy")]
 use serde::{Deserialize, Serialize};
 
@@ -107,20 +111,14 @@
     }
     /// Get an array with the cumulative product computed at every element
     fn cumprod(&self, _reverse: bool) -> ChunkedArray<T> {
         panic!("operation cumprod not supported for this dtype")
     }
 }
 
-/// Traverse and collect every nth element
-pub trait ChunkTakeEvery<T: PolarsDataType> {
-    /// Traverse and collect every nth element in a new array.
-    fn take_every(&self, n: usize) -> ChunkedArray<T>;
-}
-
 /// Explode/ flatten a List or Utf8 Series
 pub trait ChunkExplode {
     fn explode(&self) -> PolarsResult<Series> {
         self.explode_and_offsets().map(|t| t.0)
     }
     fn explode_and_offsets(&self) -> PolarsResult<(Series, OffsetsBuffer<i64>)>;
 }
@@ -425,17 +423,23 @@
 /// ```
 pub trait ChunkCompare<Rhs> {
     type Item;
 
     /// Check for equality.
     fn equal(&self, rhs: Rhs) -> Self::Item;
 
+    /// Check for equality where `None == None`.
+    fn equal_missing(&self, rhs: Rhs) -> Self::Item;
+
     /// Check for inequality.
     fn not_equal(&self, rhs: Rhs) -> Self::Item;
 
+    /// Check for inequality where `None == None`.
+    fn not_equal_missing(&self, rhs: Rhs) -> Self::Item;
+
     /// Greater than comparison.
     fn gt(&self, rhs: Rhs) -> Self::Item;
 
     /// Greater than or equal comparison.
     fn gt_eq(&self, rhs: Rhs) -> Self::Item;
 
     /// Less than comparison.
@@ -471,14 +475,22 @@
 #[cfg_attr(feature = "serde-lazy", derive(Serialize, Deserialize))]
 pub struct SortOptions {
     pub descending: bool,
     pub nulls_last: bool,
     pub multithreaded: bool,
 }
 
+#[derive(Clone)]
+#[cfg_attr(feature = "serde-lazy", derive(Serialize, Deserialize))]
+pub struct SortMultipleOptions {
+    pub other: Vec<Series>,
+    pub descending: Vec<bool>,
+    pub multithreaded: bool,
+}
+
 impl Default for SortOptions {
     fn default() -> Self {
         Self {
             descending: false,
             nulls_last: false,
             multithreaded: true,
         }
@@ -493,15 +505,15 @@
     /// Returned a sorted `ChunkedArray`.
     fn sort(&self, descending: bool) -> ChunkedArray<T>;
 
     /// Retrieve the indexes needed to sort this array.
     fn arg_sort(&self, options: SortOptions) -> IdxCa;
 
     /// Retrieve the indexes need to sort this and the other arrays.
-    fn arg_sort_multiple(&self, _other: &[Series], _descending: &[bool]) -> PolarsResult<IdxCa> {
+    fn arg_sort_multiple(&self, _options: &SortMultipleOptions) -> PolarsResult<IdxCa> {
         polars_bail!(opq = arg_sort_multiple, T::get_dtype());
     }
 }
 
 pub type FillNullLimit = Option<IdxSize>;
 
 #[derive(Copy, Clone, Debug)]
@@ -544,17 +556,17 @@
 pub trait ChunkFullNull {
     fn full_null(_name: &str, _length: usize) -> Self
     where
         Self: Sized;
 }
 
 /// Reverse a ChunkedArray<T>
-pub trait ChunkReverse<T: PolarsDataType> {
+pub trait ChunkReverse {
     /// Return a reversed version of this array.
-    fn reverse(&self) -> ChunkedArray<T>;
+    fn reverse(&self) -> Self;
 }
 
 /// Filter values by a boolean mask.
 pub trait ChunkFilter<T: PolarsDataType> {
     /// Filter values in the ChunkedArray with a boolean mask.
     ///
     /// ```rust
@@ -627,22 +639,37 @@
 
 impl ChunkExpandAtIndex<ListType> for ListChunked {
     fn new_from_index(&self, index: usize, length: usize) -> ListChunked {
         let opt_val = self.get(index);
         match opt_val {
             Some(val) => {
                 let mut ca = ListChunked::full(self.name(), &val, length);
-                ca.to_logical(self.inner_dtype());
+                ca.to_physical(self.inner_dtype());
                 ca
             }
             None => ListChunked::full_null_with_dtype(self.name(), length, &self.inner_dtype()),
         }
     }
 }
 
+#[cfg(feature = "dtype-array")]
+impl ChunkExpandAtIndex<FixedSizeListType> for ArrayChunked {
+    fn new_from_index(&self, index: usize, length: usize) -> ArrayChunked {
+        let opt_val = self.get(index);
+        match opt_val {
+            Some(val) => {
+                let mut ca = ArrayChunked::full(self.name(), &val, length);
+                ca.to_physical(self.inner_dtype());
+                ca
+            }
+            None => ArrayChunked::full_null_with_dtype(self.name(), length, &self.inner_dtype(), 0),
+        }
+    }
+}
+
 #[cfg(feature = "object")]
 impl<T: PolarsObject> ChunkExpandAtIndex<ObjectType<T>> for ObjectChunked<T> {
     fn new_from_index(&self, index: usize, length: usize) -> ObjectChunked<T> {
         let opt_val = self.get(index);
         match opt_val {
             Some(val) => ObjectChunked::<T>::full(self.name(), val.clone(), length),
             None => ObjectChunked::<T>::full_null(self.name(), length),
@@ -706,28 +733,28 @@
     }
 }
 
 /// Repeat the values `n` times.
 #[cfg(feature = "repeat_by")]
 pub trait RepeatBy {
     /// Repeat the values `n` times, where `n` is determined by the values in `by`.
-    fn repeat_by(&self, _by: &IdxCa) -> ListChunked {
+    fn repeat_by(&self, _by: &IdxCa) -> PolarsResult<ListChunked> {
         unimplemented!()
     }
 }
 
 #[cfg(feature = "is_first")]
 /// Mask the first unique values as `true`
 pub trait IsFirst<T: PolarsDataType> {
     fn is_first(&self) -> PolarsResult<BooleanChunked> {
         polars_bail!(opq = is_first, T::get_dtype());
     }
 }
 
-#[cfg(feature = "is_first")]
+#[cfg(feature = "is_last")]
 /// Mask the last unique values as `true`
 pub trait IsLast<T: PolarsDataType> {
     fn is_last(&self) -> PolarsResult<BooleanChunked> {
         polars_bail!(opq = is_last, T::get_dtype());
     }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/nulls.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/nulls.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/peaks.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/peaks.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/reverse.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/various.rs`

 * *Files 27% similar despite different names*

```diff
@@ -1,52 +1,62 @@
-use crate::prelude::*;
-use crate::series::IsSorted;
-use crate::utils::{CustomIterTools, NoNull};
-
-impl<T> ChunkReverse<T> for ChunkedArray<T>
-where
-    T: PolarsNumericType,
-{
-    fn reverse(&self) -> ChunkedArray<T> {
-        let mut out = if let Ok(slice) = self.cont_slice() {
-            let ca: NoNull<ChunkedArray<T>> = slice.iter().rev().copied().collect_trusted();
-            ca.into_inner()
+#[cfg(feature = "hash")]
+use polars_core::export::ahash;
+use polars_core::prelude::*;
+use polars_core::series::IsSorted;
+
+use crate::series::ops::SeriesSealed;
+
+pub trait SeriesMethods: SeriesSealed {
+    /// Create a [`DataFrame`] with the unique `values` of this [`Series`] and a column `"counts"`
+    /// with dtype [`IdxType`]
+    fn value_counts(&self, multithreaded: bool, sorted: bool) -> PolarsResult<DataFrame> {
+        let s = self.as_series();
+        // we need to sort here as well in case of `maintain_order` because duplicates behavior is undefined
+        let groups = s.group_tuples(multithreaded, sorted)?;
+        let values = unsafe { s.agg_first(&groups) };
+        let counts = groups.group_lengths("counts");
+        let cols = vec![values, counts.into_series()];
+        let df = DataFrame::new_no_checks(cols);
+        if sorted {
+            df.sort(["counts"], true)
         } else {
-            self.into_iter().rev().collect_trusted()
-        };
-        out.rename(self.name());
-
-        match self.is_sorted_flag2() {
-            IsSorted::Ascending => out.set_sorted_flag(IsSorted::Descending),
-            IsSorted::Descending => out.set_sorted_flag(IsSorted::Ascending),
-            _ => {}
+            Ok(df)
         }
-
-        out
     }
-}
 
-macro_rules! impl_reverse {
-    ($arrow_type:ident, $ca_type:ident) => {
-        impl ChunkReverse<$arrow_type> for $ca_type {
-            fn reverse(&self) -> Self {
-                let mut ca: Self = self.into_iter().rev().collect_trusted();
-                ca.rename(self.name());
-                ca
+    #[cfg(feature = "hash")]
+    fn hash(&self, build_hasher: ahash::RandomState) -> UInt64Chunked {
+        let s = self.as_series().to_physical_repr();
+        match s.dtype() {
+            DataType::List(_) => {
+                let mut ca = s.list().unwrap().clone();
+                crate::chunked_array::hash::hash(&mut ca, build_hasher)
+            }
+            _ => {
+                let mut h = vec![];
+                s.0.vec_hash(build_hasher, &mut h).unwrap();
+                UInt64Chunked::from_vec(s.name(), h)
             }
         }
-    };
-}
+    }
 
-impl_reverse!(BooleanType, BooleanChunked);
-impl_reverse!(Utf8Type, Utf8Chunked);
-impl_reverse!(BinaryType, BinaryChunked);
-impl_reverse!(ListType, ListChunked);
-
-#[cfg(feature = "object")]
-impl<T: PolarsObject> ChunkReverse<ObjectType<T>> for ObjectChunked<T> {
-    fn reverse(&self) -> Self {
-        // Safety
-        // we we know we don't get out of bounds
-        unsafe { self.take_unchecked((0..self.len()).rev().into()) }
+    fn is_sorted(&self, options: SortOptions) -> bool {
+        let s = self.as_series();
+
+        // fast paths
+        if (options.descending
+            && options.nulls_last
+            && matches!(s.is_sorted_flag(), IsSorted::Descending))
+            || (!options.descending
+                && !options.nulls_last
+                && matches!(s.is_sorted_flag(), IsSorted::Ascending))
+        {
+            return true;
+        }
+
+        // TODO! optimize
+        let out = s.sort_with(options);
+        out.eq(s)
     }
 }
+
+impl SeriesMethods for Series {}
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/rolling_window.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/rolling_window.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/set.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/set.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/shift.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/shift.rs`

 * *Files 10% similar despite different names*

```diff
@@ -117,14 +117,49 @@
 
 impl ChunkShift<ListType> for ListChunked {
     fn shift(&self, periods: i64) -> Self {
         self.shift_and_fill(periods, None)
     }
 }
 
+#[cfg(feature = "dtype-array")]
+impl ChunkShiftFill<FixedSizeListType, Option<&Series>> for ArrayChunked {
+    fn shift_and_fill(&self, periods: i64, fill_value: Option<&Series>) -> ArrayChunked {
+        // This has its own implementation because a ArrayChunked cannot have a full-null without
+        // knowing the inner type
+        let periods = clamp(periods, -(self.len() as i64), self.len() as i64);
+        let slice_offset = (-periods).max(0);
+        let length = self.len() - abs(periods) as usize;
+        let mut slice = self.slice(slice_offset, length);
+
+        let fill_length = abs(periods) as usize;
+        let mut fill = match fill_value {
+            Some(val) => Self::full(self.name(), val, fill_length),
+            None => {
+                ArrayChunked::full_null_with_dtype(self.name(), fill_length, &self.inner_dtype(), 0)
+            }
+        };
+
+        if periods < 0 {
+            slice.append(&fill).unwrap();
+            slice
+        } else {
+            fill.append(&slice).unwrap();
+            fill
+        }
+    }
+}
+
+#[cfg(feature = "dtype-array")]
+impl ChunkShift<FixedSizeListType> for ArrayChunked {
+    fn shift(&self, periods: i64) -> Self {
+        self.shift_and_fill(periods, None)
+    }
+}
+
 #[cfg(feature = "object")]
 impl<T: PolarsObject> ChunkShiftFill<ObjectType<T>, Option<ObjectType<T>>> for ObjectChunked<T> {
     fn shift_and_fill(
         &self,
         _periods: i64,
         _fill_value: Option<ObjectType<T>>,
     ) -> ChunkedArray<ObjectType<T>> {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/sort/arg_sort.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/sort/arg_sort.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/sort/arg_sort_multiple.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/sort/arg_sort_multiple.rs`

 * *Files 2% similar despite different names*

```diff
@@ -21,19 +21,20 @@
         descending.len(), other.len() + 1,
     );
     Ok(())
 }
 
 pub(crate) fn arg_sort_multiple_impl<T: PartialOrd + Send + IsFloat + Copy>(
     mut vals: Vec<(IdxSize, T)>,
-    other: &[Series],
-    descending: &[bool],
+    options: &SortMultipleOptions,
 ) -> PolarsResult<IdxCa> {
-    assert_eq!(descending.len() - 1, other.len());
-    let compare_inner: Vec<_> = other
+    let descending = &options.descending;
+    debug_assert_eq!(descending.len() - 1, options.other.len());
+    let compare_inner: Vec<_> = options
+        .other
         .iter()
         .map(|s| s.into_partial_ord_inner())
         .collect_trusted();
 
     let first_descending = descending[0];
     POOL.install(|| {
         vals.par_sort_by(|tpl_a, tpl_b| {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/sort/categorical.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/sort/categorical.rs`

 * *Files 8% similar despite different names*

```diff
@@ -116,34 +116,33 @@
         } else {
             self.logical().arg_sort(options)
         }
     }
 
     /// Retrieve the indexes need to sort this and the other arrays.
 
-    pub(crate) fn arg_sort_multiple(
-        &self,
-        other: &[Series],
-        descending: &[bool],
-    ) -> PolarsResult<IdxCa> {
+    pub(crate) fn arg_sort_multiple(&self, options: &SortMultipleOptions) -> PolarsResult<IdxCa> {
         if self.use_lexical_sort() {
-            args_validate(self.logical(), other, descending)?;
+            args_validate(self.logical(), &options.other, &options.descending)?;
             let mut count: IdxSize = 0;
+
+            // we use bytes to save a monomorphisized str impl
+            // as bytes already is used for binary and utf8 sorting
             let vals: Vec<_> = self
                 .iter_str()
                 .map(|v| {
                     let i = count;
                     count += 1;
-                    (i, v)
+                    (i, v.map(|v| v.as_bytes()))
                 })
                 .collect_trusted();
 
-            arg_sort_multiple_impl(vals, other, descending)
+            arg_sort_multiple_impl(vals, options)
         } else {
-            self.logical().arg_sort_multiple(other, descending)
+            self.logical().arg_sort_multiple(options)
         }
     }
 }
 
 #[cfg(test)]
 mod test {
     use crate::prelude::*;
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/sort/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/sort/mod.rs`

 * *Files 6% similar despite different names*

```diff
@@ -285,29 +285,42 @@
             .map(|arr| arr.iter().map(|opt| opt.copied()));
         arg_sort::arg_sort(ca.name(), iter, options, ca.null_count(), ca.len())
     }
 }
 
 fn arg_sort_multiple_numeric<T: PolarsNumericType>(
     ca: &ChunkedArray<T>,
-    other: &[Series],
-    descending: &[bool],
+    options: &SortMultipleOptions,
 ) -> PolarsResult<IdxCa> {
-    args_validate(ca, other, descending)?;
+    args_validate(ca, &options.other, &options.descending)?;
     let mut count: IdxSize = 0;
-    let vals: Vec<_> = ca
-        .into_iter()
-        .map(|v| {
-            let i = count;
-            count += 1;
-            (i, v)
-        })
-        .collect_trusted();
 
-    arg_sort_multiple_impl(vals, other, descending)
+    let no_nulls = ca.null_count() == 0;
+
+    if no_nulls {
+        let mut vals = Vec::with_capacity(ca.len());
+        for arr in ca.downcast_iter() {
+            vals.extend_trusted_len(arr.values().as_slice().iter().map(|v| {
+                let i = count;
+                count += 1;
+                (i, *v)
+            }))
+        }
+        arg_sort_multiple_impl(vals, options)
+    } else {
+        let mut vals = Vec::with_capacity(ca.len());
+        for arr in ca.downcast_iter() {
+            vals.extend_trusted_len(arr.into_iter().map(|v| {
+                let i = count;
+                count += 1;
+                (i, v.copied())
+            }));
+        }
+        arg_sort_multiple_impl(vals, options)
+    }
 }
 
 impl<T> ChunkSort<T> for ChunkedArray<T>
 where
     T: PolarsIntegerType,
     T::Native: Default + Ord,
 {
@@ -326,16 +339,16 @@
         arg_sort_numeric(self, options)
     }
 
     /// # Panics
     ///
     /// This function is very opinionated.
     /// We assume that all numeric `Series` are of the same type, if not it will panic
-    fn arg_sort_multiple(&self, other: &[Series], descending: &[bool]) -> PolarsResult<IdxCa> {
-        arg_sort_multiple_numeric(self, other, descending)
+    fn arg_sort_multiple(&self, options: &SortMultipleOptions) -> PolarsResult<IdxCa> {
+        arg_sort_multiple_numeric(self, options)
     }
 }
 
 impl ChunkSort<Float32Type> for Float32Chunked {
     fn sort_with(&self, options: SortOptions) -> Float32Chunked {
         sort_with_numeric(self, options, order_ascending_flt, order_descending_flt)
     }
@@ -351,16 +364,16 @@
         arg_sort_numeric(self, options)
     }
 
     /// # Panics
     ///
     /// This function is very opinionated.
     /// We assume that all numeric `Series` are of the same type, if not it will panic
-    fn arg_sort_multiple(&self, other: &[Series], descending: &[bool]) -> PolarsResult<IdxCa> {
-        arg_sort_multiple_numeric(self, other, descending)
+    fn arg_sort_multiple(&self, options: &SortMultipleOptions) -> PolarsResult<IdxCa> {
+        arg_sort_multiple_numeric(self, options)
     }
 }
 
 impl ChunkSort<Float64Type> for Float64Chunked {
     fn sort_with(&self, options: SortOptions) -> Float64Chunked {
         sort_with_numeric(self, options, order_ascending_flt, order_descending_flt)
     }
@@ -376,16 +389,16 @@
         arg_sort_numeric(self, options)
     }
 
     /// # Panics
     ///
     /// This function is very opinionated.
     /// We assume that all numeric `Series` are of the same type, if not it will panic
-    fn arg_sort_multiple(&self, other: &[Series], descending: &[bool]) -> PolarsResult<IdxCa> {
-        arg_sort_multiple_numeric(self, other, descending)
+    fn arg_sort_multiple(&self, options: &SortMultipleOptions) -> PolarsResult<IdxCa> {
+        arg_sort_multiple_numeric(self, options)
     }
 }
 
 fn ordering_other_columns<'a>(
     compare_inner: &'a [Box<dyn PartialOrdInner + 'a>],
     descending: &[bool],
     idx_a: usize,
@@ -515,16 +528,16 @@
     ///
     /// This function is very opinionated. On the implementation of `ChunkedArray<T>` for numeric types,
     /// we assume that all numeric `Series` are of the same type.
     ///
     /// In this case we assume that all numeric `Series` are `f64` types. The caller needs to
     /// uphold this contract. If not, it will panic.
     ///
-    fn arg_sort_multiple(&self, other: &[Series], descending: &[bool]) -> PolarsResult<IdxCa> {
-        self.as_binary().arg_sort_multiple(other, descending)
+    fn arg_sort_multiple(&self, options: &SortMultipleOptions) -> PolarsResult<IdxCa> {
+        self.as_binary().arg_sort_multiple(options)
     }
 }
 
 impl ChunkSort<BinaryType> for BinaryChunked {
     fn sort_with(&self, options: SortOptions) -> ChunkedArray<BinaryType> {
         sort_with_fast_path!(self, options);
         let mut v: Vec<&[u8]> = if self.null_count() > 0 {
@@ -640,27 +653,27 @@
     ///
     /// This function is very opinionated. On the implementation of `ChunkedArray<T>` for numeric types,
     /// we assume that all numeric `Series` are of the same type.
     ///
     /// In this case we assume that all numeric `Series` are `f64` types. The caller needs to
     /// uphold this contract. If not, it will panic.
     ///
-    fn arg_sort_multiple(&self, other: &[Series], descending: &[bool]) -> PolarsResult<IdxCa> {
-        args_validate(self, other, descending)?;
+    fn arg_sort_multiple(&self, options: &SortMultipleOptions) -> PolarsResult<IdxCa> {
+        args_validate(self, &options.other, &options.descending)?;
 
         let mut count: IdxSize = 0;
         let vals: Vec<_> = self
             .into_iter()
             .map(|v| {
                 let i = count;
                 count += 1;
                 (i, v)
             })
             .collect_trusted();
-        arg_sort_multiple_impl(vals, other, descending)
+        arg_sort_multiple_impl(vals, options)
     }
 }
 
 #[cfg(feature = "dtype-struct")]
 impl StructChunked {
     pub(crate) fn arg_sort(&self, options: SortOptions) -> IdxCa {
         let bin = _get_rows_encoded_ca(
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/take/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/take/mod.rs`

 * *Files 3% similar despite different names*

```diff
@@ -429,14 +429,61 @@
         indices.check_bounds(self.len())?;
         // Safety:
         // just checked bounds
         Ok(unsafe { self.take_unchecked(indices) })
     }
 }
 
+#[cfg(feature = "dtype-array")]
+impl ChunkTake for ArrayChunked {
+    unsafe fn take_unchecked<I, INulls>(&self, indices: TakeIdx<I, INulls>) -> Self
+    where
+        Self: std::marker::Sized,
+        I: TakeIterator,
+        INulls: TakeIteratorNulls,
+    {
+        let ca_self = self.rechunk();
+        match indices {
+            TakeIdx::Array(idx_array) => {
+                if idx_array.null_count() == idx_array.len() {
+                    return Self::full_null_with_dtype(
+                        self.name(),
+                        idx_array.len(),
+                        &self.inner_dtype(),
+                        ca_self.width(),
+                    );
+                }
+                let arr = self.chunks[0].as_ref();
+                let arr = take_unchecked(arr, idx_array);
+                self.finish_from_array(arr)
+            }
+            TakeIdx::Iter(iter) => {
+                let idx: NoNull<IdxCa> = iter.map(|v| v as IdxSize).collect();
+                ca_self.take_unchecked((&idx.into_inner()).into())
+            }
+            TakeIdx::IterNulls(iter) => {
+                let idx: IdxCa = iter.map(|v| v.map(|v| v as IdxSize)).collect();
+                ca_self.take_unchecked((&idx).into())
+            }
+        }
+    }
+
+    fn take<I, INulls>(&self, indices: TakeIdx<I, INulls>) -> PolarsResult<Self>
+    where
+        Self: std::marker::Sized,
+        I: TakeIterator,
+        INulls: TakeIteratorNulls,
+    {
+        indices.check_bounds(self.len())?;
+        // Safety:
+        // just checked bounds
+        Ok(unsafe { self.take_unchecked(indices) })
+    }
+}
+
 #[cfg(feature = "object")]
 impl<T: PolarsObject> ChunkTake for ObjectChunked<T> {
     unsafe fn take_unchecked<I, INulls>(&self, indices: TakeIdx<I, INulls>) -> Self
     where
         Self: std::marker::Sized,
         I: TakeIterator,
         INulls: TakeIteratorNulls,
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/take/take_chunked.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/take/take_chunked.rs`

 * *Files 12% similar despite different names*

```diff
@@ -163,14 +163,52 @@
             })
             .collect();
 
         ca.rename(self.name());
         ca
     }
 }
+
+#[cfg(feature = "dtype-array")]
+impl TakeChunked for ArrayChunked {
+    unsafe fn take_chunked_unchecked(&self, by: &[ChunkId], sorted: IsSorted) -> Self {
+        let arrs = self.downcast_iter().collect::<Vec<_>>();
+        let iter = by.iter().map(|[chunk_idx, array_idx]| {
+            let arr = arrs.get_unchecked(*chunk_idx as usize);
+            arr.get_unchecked(*array_idx as usize)
+        });
+        let mut ca = Self::from_iter_and_args(
+            iter,
+            self.width(),
+            by.len(),
+            Some(self.inner_dtype()),
+            self.name(),
+        );
+        ca.set_sorted_flag(sorted);
+        ca
+    }
+
+    unsafe fn take_opt_chunked_unchecked(&self, by: &[Option<ChunkId>]) -> Self {
+        let arrs = self.downcast_iter().collect::<Vec<_>>();
+        let iter = by.iter().map(|opt_idx| {
+            opt_idx.and_then(|[chunk_idx, array_idx]| {
+                let arr = arrs.get_unchecked(chunk_idx as usize);
+                arr.get_unchecked(array_idx as usize)
+            })
+        });
+
+        Self::from_iter_and_args(
+            iter,
+            self.width(),
+            by.len(),
+            Some(self.inner_dtype()),
+            self.name(),
+        )
+    }
+}
 #[cfg(feature = "object")]
 impl<T: PolarsObject> TakeChunked for ObjectChunked<T> {
     unsafe fn take_chunked_unchecked(&self, by: &[ChunkId], sorted: IsSorted) -> Self {
         let arrs = self.downcast_iter().collect::<Vec<_>>();
 
         let mut ca: Self = by
             .iter()
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/take/take_random.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/take/take_random.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/take/take_single.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/take/take_single.rs`

 * *Files 12% similar despite different names*

```diff
@@ -173,7 +173,32 @@
         let opt_arr = impl_take_random_get_unchecked!(self, index, LargeListArray);
         opt_arr.map(|arr| {
             let s = Series::try_from((self.name(), arr));
             s.unwrap()
         })
     }
 }
+
+#[cfg(feature = "dtype-array")]
+impl TakeRandom for ArrayChunked {
+    type Item = Series;
+
+    #[inline]
+    fn get(&self, index: usize) -> Option<Self::Item> {
+        // Safety:
+        // Out of bounds is checked and downcast is of correct type
+        let opt_arr = unsafe { impl_take_random_get!(self, index, FixedSizeListArray) };
+        opt_arr.map(|arr| {
+            let s = Series::try_from((self.name(), arr));
+            s.unwrap()
+        })
+    }
+
+    #[inline]
+    unsafe fn get_unchecked(&self, index: usize) -> Option<Self::Item> {
+        let opt_arr = impl_take_random_get_unchecked!(self, index, FixedSizeListArray);
+        opt_arr.map(|arr| {
+            let s = Series::try_from((self.name(), arr));
+            s.unwrap()
+        })
+    }
+}
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/take/traits.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/take/traits.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/unique/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/unique/mod.rs`

 * *Files 1% similar despite different names*

```diff
@@ -133,15 +133,15 @@
     ChunkedArray<T>: IntoSeries,
 {
     fn unique(&self) -> PolarsResult<Self> {
         // prevent stackoverflow repeated sorted.unique call
         if self.is_empty() {
             return Ok(self.clone());
         }
-        match self.is_sorted_flag2() {
+        match self.is_sorted_flag() {
             IsSorted::Ascending | IsSorted::Descending => {
                 // TODO! optimize this branch
                 if self.null_count() > 0 {
                     let mut arr = MutablePrimitiveArray::with_capacity(self.len());
                     let mut iter = self.into_iter();
                     let mut last = None;
 
@@ -166,15 +166,15 @@
                     unsafe {
                         Ok(ChunkedArray::from_chunks(
                             self.name(),
                             vec![Box::new(arr) as ArrayRef],
                         ))
                     }
                 } else {
-                    let mask = self.not_equal(&self.shift(1));
+                    let mask = self.not_equal_and_validity(&self.shift(1));
                     self.filter(&mask)
                 }
             }
             IsSorted::Not => {
                 let sorted = self.sort(false);
                 sorted.unique()
             }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/ops/unique/rank.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/ops/unique/rank.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/random.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/random.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/temporal/conversion.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/temporal/conversion.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/temporal/date.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/temporal/date.rs`

 * *Files 14% similar despite different names*

```diff
@@ -26,16 +26,17 @@
 
     /// Construct a new [`DateChunked`] from an iterator over [`NaiveDate`].
     pub fn from_naive_date<I: IntoIterator<Item = NaiveDate>>(name: &str, v: I) -> Self {
         let unit = v.into_iter().map(naive_date_to_date).collect::<Vec<_>>();
         Int32Chunked::from_vec(name, unit).into()
     }
 
-    /// Format Date with a `format` rule. See [chrono strftime/strptime](https://docs.rs/chrono/0.4.19/chrono/format/strftime/index.html).
-    pub fn strftime(&self, format: &str) -> Utf8Chunked {
+    /// Convert from Date into Utf8 with the given format.
+    /// See [chrono strftime/strptime](https://docs.rs/chrono/0.4.19/chrono/format/strftime/index.html).
+    pub fn to_string(&self, format: &str) -> Utf8Chunked {
         let date = NaiveDate::from_ymd_opt(2001, 1, 1).unwrap();
         let fmted = format!("{}", date.format(format));
 
         let mut ca: Utf8Chunked = self.apply_kernel_cast(&|arr| {
             let mut buf = String::new();
             let mut mutarr =
                 MutableUtf8Array::with_capacities(arr.len(), arr.len() * fmted.len() + 1);
@@ -55,14 +56,22 @@
             let arr: Utf8Array<i64> = mutarr.into();
             Box::new(arr)
         });
         ca.rename(self.name());
         ca
     }
 
+    /// Convert from Date into Utf8 with the given format.
+    /// See [chrono strftime/strptime](https://docs.rs/chrono/0.4.19/chrono/format/strftime/index.html).
+    ///
+    /// Alias for `to_string`.
+    pub fn strftime(&self, format: &str) -> Utf8Chunked {
+        self.to_string(format)
+    }
+
     /// Construct a new [`DateChunked`] from an iterator over optional [`NaiveDate`].
     pub fn from_naive_date_options<I: IntoIterator<Item = Option<NaiveDate>>>(
         name: &str,
         v: I,
     ) -> Self {
         let unit = v.into_iter().map(|opt| opt.map(naive_date_to_date));
         Int32Chunked::from_iter_options(name, unit).into()
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/temporal/datetime.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/temporal/datetime.rs`

 * *Files 14% similar despite different names*

```diff
@@ -1,37 +1,30 @@
 use std::fmt::Write;
 
-#[cfg(feature = "timezones")]
-use arrow::temporal_conversions::parse_offset;
 use arrow::temporal_conversions::{
     timestamp_ms_to_datetime, timestamp_ns_to_datetime, timestamp_us_to_datetime,
 };
 use chrono::format::{DelayedFormat, StrftimeItems};
 #[cfg(feature = "timezones")]
-use chrono::FixedOffset;
-#[cfg(feature = "timezones")]
 use chrono::TimeZone as TimeZoneTrait;
 #[cfg(feature = "timezones")]
 use chrono_tz::Tz;
 #[cfg(feature = "timezones")]
 use polars_arrow::kernels::replace_timezone;
 
 use super::conversion::{datetime_to_timestamp_ms, datetime_to_timestamp_ns};
 use super::*;
 use crate::prelude::DataType::Datetime;
 use crate::prelude::*;
 
 #[cfg(feature = "timezones")]
 fn validate_time_zone(tz: TimeZone) -> PolarsResult<()> {
-    match parse_offset(&tz) {
+    match tz.parse::<Tz>() {
         Ok(_) => Ok(()),
-        Err(_) => match tz.parse::<Tz>() {
-            Ok(_) => Ok(()),
-            Err(_) => polars_bail!(ComputeError: "unable to parse time zone: '{}'", tz),
-        },
+        Err(_) => polars_bail!(ComputeError: "unable to parse time zone: '{}'", tz),
     }
 }
 
 fn apply_datefmt_f<'a>(
     arr: &PrimitiveArray<i64>,
     fmted: &'a str,
     conversion_f: fn(i64) -> NaiveDateTime,
@@ -52,25 +45,14 @@
         }
     }
     let arr: Utf8Array<i64> = mutarr.into();
     Box::new(arr)
 }
 
 #[cfg(feature = "timezones")]
-fn format_fixed_offset(
-    tz: FixedOffset,
-    arr: &PrimitiveArray<i64>,
-    fmt: &str,
-    fmted: &str,
-    conversion_f: fn(i64) -> NaiveDateTime,
-) -> ArrayRef {
-    let datefmt_f = |ndt| tz.from_utc_datetime(&ndt).format(fmt);
-    apply_datefmt_f(arr, fmted, conversion_f, datefmt_f)
-}
-#[cfg(feature = "timezones")]
 fn format_tz(
     tz: Tz,
     arr: &PrimitiveArray<i64>,
     fmt: &str,
     fmted: &str,
     conversion_f: fn(i64) -> NaiveDateTime,
 ) -> ArrayRef {
@@ -120,60 +102,34 @@
 
     #[cfg(feature = "timezones")]
     pub fn replace_time_zone(
         &self,
         time_zone: Option<&str>,
         use_earliest: Option<bool>,
     ) -> PolarsResult<DatetimeChunked> {
-        let out: PolarsResult<_> = match (self.time_zone(), time_zone) {
-            (Some(from), Some(to)) => {
-                let chunks = self
-                    .downcast_iter()
-                    .map(|arr| {
-                        replace_timezone(arr, self.time_unit().to_arrow(), to, from, use_earliest)
-                    })
-                    .collect::<PolarsResult<_>>()?;
-                let out = unsafe { ChunkedArray::from_chunks(self.name(), chunks) };
-                Ok(out.into_datetime(self.time_unit(), Some(to.to_string())))
-            }
-            (Some(from), None) => {
-                let chunks = self
-                    .downcast_iter()
-                    .map(|arr| {
-                        replace_timezone(
-                            arr,
-                            self.time_unit().to_arrow(),
-                            "UTC",
-                            from,
-                            use_earliest,
-                        )
-                    })
-                    .collect::<PolarsResult<_>>()?;
-                let out = unsafe { ChunkedArray::from_chunks(self.name(), chunks) };
-                Ok(out.into_datetime(self.time_unit(), None))
-            }
-            (None, Some(to)) => {
-                let chunks = self
-                    .downcast_iter()
-                    .map(|arr| {
-                        replace_timezone(arr, self.time_unit().to_arrow(), to, "UTC", use_earliest)
-                    })
-                    .collect::<PolarsResult<_>>()?;
-                let out = unsafe { ChunkedArray::from_chunks(self.name(), chunks) };
-                Ok(out.into_datetime(self.time_unit(), Some(to.to_string())))
-            }
-            (None, None) => Ok(self.clone()),
+        let out: PolarsResult<_> = {
+            let from = self.time_zone().as_deref().unwrap_or("UTC");
+            let to = time_zone.unwrap_or("UTC");
+            let chunks = self
+                .downcast_iter()
+                .map(|arr| {
+                    replace_timezone(arr, self.time_unit().to_arrow(), to, from, use_earliest)
+                })
+                .collect::<PolarsResult<_>>()?;
+            let out = unsafe { ChunkedArray::from_chunks(self.name(), chunks) };
+            Ok(out.into_datetime(self.time_unit(), time_zone.map(|x| x.to_string())))
         };
         let mut out = out?;
-        out.set_sorted_flag(self.is_sorted_flag2());
+        out.set_sorted_flag(self.is_sorted_flag());
         Ok(out)
     }
 
-    /// Format Datetime with a `format` rule. See [chrono strftime/strptime](https://docs.rs/chrono/0.4.19/chrono/format/strftime/index.html).
-    pub fn strftime(&self, format: &str) -> PolarsResult<Utf8Chunked> {
+    /// Convert from Datetime into Utf8 with the given format.
+    /// See [chrono strftime/strptime](https://docs.rs/chrono/0.4.19/chrono/format/strftime/index.html).
+    pub fn to_string(&self, format: &str) -> PolarsResult<Utf8Chunked> {
         #[cfg(feature = "timezones")]
         use chrono::Utc;
         let conversion_f = match self.time_unit() {
             TimeUnit::Nanoseconds => timestamp_ns_to_datetime,
             TimeUnit::Microseconds => timestamp_us_to_datetime,
             TimeUnit::Milliseconds => timestamp_ms_to_datetime,
         };
@@ -197,31 +153,37 @@
                 |_| polars_err!(ComputeError: "cannot format NaiveDateTime with format '{}'", format),
             )?,
         };
         let fmted = fmted; // discard mut
 
         let mut ca: Utf8Chunked = match self.time_zone() {
             #[cfg(feature = "timezones")]
-            Some(time_zone) => match parse_offset(time_zone) {
-                Ok(time_zone) => self.apply_kernel_cast(&|arr| {
-                    format_fixed_offset(time_zone, arr, format, &fmted, conversion_f)
-                }),
-                Err(_) => match time_zone.parse::<Tz>() {
-                    Ok(time_zone) => self.apply_kernel_cast(&|arr| {
-                        format_tz(time_zone, arr, format, &fmted, conversion_f)
-                    }),
-                    Err(_) => unreachable!(),
-                },
-            },
+            Some(time_zone) => self.apply_kernel_cast(&|arr| {
+                format_tz(
+                    time_zone.parse::<Tz>().unwrap(),
+                    arr,
+                    format,
+                    &fmted,
+                    conversion_f,
+                )
+            }),
             _ => self.apply_kernel_cast(&|arr| format_naive(arr, format, &fmted, conversion_f)),
         };
         ca.rename(self.name());
         Ok(ca)
     }
 
+    /// Convert from Datetime into Utf8 with the given format.
+    /// See [chrono strftime/strptime](https://docs.rs/chrono/0.4.19/chrono/format/strftime/index.html).
+    ///
+    /// Alias for `to_string`.
+    pub fn strftime(&self, format: &str) -> PolarsResult<Utf8Chunked> {
+        self.to_string(format)
+    }
+
     /// Construct a new [`DatetimeChunked`] from an iterator over [`NaiveDateTime`].
     pub fn from_naive_datetime<I: IntoIterator<Item = NaiveDateTime>>(
         name: &str,
         v: I,
         tu: TimeUnit,
     ) -> Self {
         let func = match tu {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/temporal/duration.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/temporal/duration.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/temporal/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/temporal/mod.rs`

 * *Files 10% similar despite different names*

```diff
@@ -4,19 +4,20 @@
 mod date;
 #[cfg(feature = "dtype-datetime")]
 mod datetime;
 #[cfg(feature = "dtype-duration")]
 mod duration;
 #[cfg(feature = "dtype-time")]
 mod time;
-
 #[cfg(feature = "dtype-date")]
 use chrono::NaiveDate;
 use chrono::NaiveDateTime;
 #[cfg(any(feature = "dtype-time", feature = "dtype-date"))]
 use chrono::NaiveTime;
+#[cfg(feature = "dtype-time")]
+pub use time::time_to_time64ns;
 
 pub use self::conversion::*;
 
 pub fn unix_time() -> NaiveDateTime {
     NaiveDateTime::from_timestamp_opt(0, 0).unwrap()
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/temporal/time.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/temporal/time.rs`

 * *Files 10% similar despite different names*

```diff
@@ -5,25 +5,26 @@
 
 use super::*;
 use crate::prelude::*;
 
 const SECONDS_IN_MINUTE: i64 = 60;
 const SECONDS_IN_HOUR: i64 = 3_600;
 
-pub(crate) fn time_to_time64ns(time: &NaiveTime) -> i64 {
+pub fn time_to_time64ns(time: &NaiveTime) -> i64 {
     (time.hour() as i64 * SECONDS_IN_HOUR
         + time.minute() as i64 * SECONDS_IN_MINUTE
         + time.second() as i64)
         * NANOSECONDS
         + time.nanosecond() as i64
 }
 
 impl TimeChunked {
-    /// Format Time with a `format` rule. See [chrono strftime/strptime](https://docs.rs/chrono/0.4.19/chrono/format/strftime/index.html).
-    pub fn strftime(&self, format: &str) -> Utf8Chunked {
+    /// Convert from Time into Utf8 with the given format.
+    /// See [chrono strftime/strptime](https://docs.rs/chrono/0.4.19/chrono/format/strftime/index.html).
+    pub fn to_string(&self, format: &str) -> Utf8Chunked {
         let time = NaiveTime::from_hms_opt(0, 0, 0).unwrap();
         let fmted = format!("{}", time.format(format));
 
         let mut ca: Utf8Chunked = self.apply_kernel_cast(&|arr| {
             let mut buf = String::new();
             let mut mutarr =
                 MutableUtf8Array::with_capacities(arr.len(), arr.len() * fmted.len() + 1);
@@ -44,14 +45,22 @@
             Box::new(arr)
         });
 
         ca.rename(self.name());
         ca
     }
 
+    /// Convert from Time into Utf8 with the given format.
+    /// See [chrono strftime/strptime](https://docs.rs/chrono/0.4.19/chrono/format/strftime/index.html).
+    ///
+    /// Alias for `to_string`.
+    pub fn strftime(&self, format: &str) -> Utf8Chunked {
+        self.to_string(format)
+    }
+
     pub fn as_time_iter(&self) -> impl Iterator<Item = Option<NaiveTime>> + TrustedLen + '_ {
         // we know the iterators len
         unsafe {
             self.downcast_iter()
                 .flat_map(|iter| {
                     iter.into_iter()
                         .map(|opt_v| opt_v.copied().map(time64ns_to_time))
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/to_vec.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/to_vec.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/trusted_len.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/trusted_len.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/chunked_array/upstream_traits.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/chunked_array/upstream_traits.rs`

 * *Files 6% similar despite different names*

```diff
@@ -10,14 +10,16 @@
 use polars_utils::sync::SyncPtr;
 use rayon::iter::{FromParallelIterator, IntoParallelIterator};
 use rayon::prelude::*;
 
 use crate::chunked_array::builder::{
     get_list_builder, AnonymousListBuilder, AnonymousOwnedListBuilder,
 };
+#[cfg(feature = "dtype-array")]
+use crate::chunked_array::builder::{AnonymousOwnedFixedSizeListBuilder, FixedSizeListBuilder};
 #[cfg(feature = "object")]
 use crate::chunked_array::object::ObjectArray;
 use crate::prelude::*;
 use crate::utils::flatten::flatten_par;
 use crate::utils::{get_iter_capacity, CustomIterTools, NoNull};
 
 impl<T: PolarsDataType> Default for ChunkedArray<T> {
@@ -296,14 +298,35 @@
         for val in &vals {
             builder.append_opt_array(val.as_deref());
         }
         builder.finish()
     }
 }
 
+#[cfg(feature = "dtype-array")]
+impl ArrayChunked {
+    pub(crate) unsafe fn from_iter_and_args<I: IntoIterator<Item = Option<Box<dyn Array>>>>(
+        iter: I,
+        width: usize,
+        capacity: usize,
+        inner_dtype: Option<DataType>,
+        name: &str,
+    ) -> Self {
+        let mut builder =
+            AnonymousOwnedFixedSizeListBuilder::new(name, width, capacity, inner_dtype);
+        for val in iter {
+            match val {
+                None => builder.push_null(),
+                Some(arr) => builder.push_unchecked(arr.as_ref(), 0),
+            }
+        }
+        builder.finish()
+    }
+}
+
 #[cfg(feature = "object")]
 impl<T: PolarsObject> FromIterator<Option<T>> for ObjectChunked<T> {
     fn from_iter<I: IntoIterator<Item = Option<T>>>(iter: I) -> Self {
         let iter = iter.into_iter();
         let size = iter.size_hint().0;
         let mut null_mask_builder = MutableBitmap::with_capacity(size);
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/cloud.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/cloud.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/config.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/config.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/datatypes/_serde.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/datatypes/_serde.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/datatypes/aliases.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/datatypes/aliases.rs`

 * *Files 13% similar despite different names*

```diff
@@ -14,25 +14,19 @@
 pub const IDX_DTYPE: DataType = DataType::UInt64;
 
 #[cfg(not(feature = "bigidx"))]
 pub type IdxType = UInt32Type;
 #[cfg(feature = "bigidx")]
 pub type IdxType = UInt64Type;
 
-#[cfg(feature = "private")]
 pub type PlHashMap<K, V> = hashbrown::HashMap<K, V, RandomState>;
-#[cfg(feature = "private")]
-
 /// This hashmap has the uses an IdHasher
 pub type PlIdHashMap<K, V> = hashbrown::HashMap<K, V, IdBuildHasher>;
-#[cfg(feature = "private")]
 pub type PlHashSet<V> = hashbrown::HashSet<V, RandomState>;
-#[cfg(feature = "private")]
 pub type PlIndexMap<K, V> = indexmap::IndexMap<K, V, RandomState>;
-#[cfg(feature = "private")]
 pub type PlIndexSet<K> = indexmap::IndexSet<K, RandomState>;
 
 pub trait InitHashMaps {
     type HashMap;
 
     fn new() -> Self::HashMap;
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/datatypes/any_value.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/datatypes/any_value.rs`

 * *Files 2% similar despite different names*

```diff
@@ -67,16 +67,18 @@
     /// A 64-bit time representing the elapsed time since midnight in nanoseconds
     #[cfg(feature = "dtype-time")]
     Time(i64),
     #[cfg(feature = "dtype-categorical")]
     // If syncptr is_null the data is in the rev-map
     // otherwise it is in the array pointer
     Categorical(u32, &'a RevMapping, SyncPtr<Utf8Array<i64>>),
-    /// Nested type, contains arrays that are filled with one of the datetypes.
+    /// Nested type, contains arrays that are filled with one of the datatypes.
     List(Series),
+    #[cfg(feature = "dtype-array")]
+    Array(Series, usize),
     #[cfg(feature = "object")]
     /// Can be used to fmt and implements Any, so can be downcasted to the proper value type.
     #[cfg(feature = "object")]
     Object(&'a dyn PolarsObjectSafe),
     #[cfg(feature = "object")]
     ObjectOwned(OwnedObject),
     #[cfg(feature = "dtype-struct")]
@@ -722,39 +724,45 @@
     }
 }
 
 impl PartialEq for AnyValue<'_> {
     #[inline]
     fn eq(&self, other: &Self) -> bool {
         use AnyValue::*;
-        match (self.as_borrowed(), other.as_borrowed()) {
-            (UInt8(l), UInt8(r)) => l == r,
-            (UInt16(l), UInt16(r)) => l == r,
-            (UInt32(l), UInt32(r)) => l == r,
-            (UInt64(l), UInt64(r)) => l == r,
-            (Int8(l), Int8(r)) => l == r,
-            (Int16(l), Int16(r)) => l == r,
-            (Int32(l), Int32(r)) => l == r,
-            (Int64(l), Int64(r)) => l == r,
-            (Float32(l), Float32(r)) => l == r,
-            (Float64(l), Float64(r)) => l == r,
+        match (self, other) {
+            (UInt8(l), UInt8(r)) => *l == *r,
+            (UInt16(l), UInt16(r)) => *l == *r,
+            (UInt32(l), UInt32(r)) => *l == *r,
+            (UInt64(l), UInt64(r)) => *l == *r,
+            (Int8(l), Int8(r)) => *l == *r,
+            (Int16(l), Int16(r)) => *l == *r,
+            (Int32(l), Int32(r)) => *l == *r,
+            (Int64(l), Int64(r)) => *l == *r,
+            (Float32(l), Float32(r)) => *l == *r,
+            (Float64(l), Float64(r)) => *l == *r,
+            (Utf8(l), Utf8(r)) => l == r,
+            (Utf8(l), Utf8Owned(r)) => l == r,
+            (Utf8Owned(l), Utf8(r)) => l == r,
+            (Utf8Owned(l), Utf8Owned(r)) => l == r,
+            (Boolean(l), Boolean(r)) => *l == *r,
+            (Binary(l), Binary(r)) => l == r,
+            (BinaryOwned(l), BinaryOwned(r)) => l == r,
+            (Binary(l), BinaryOwned(r)) => l == r,
+            (BinaryOwned(l), Binary(r)) => l == r,
+            // should it?
+            (Null, Null) => true,
             #[cfg(feature = "dtype-time")]
-            (Time(l), Time(r)) => l == r,
+            (Time(l), Time(r)) => *l == *r,
             #[cfg(all(feature = "dtype-datetime", feature = "dtype-date"))]
-            (Date(l), Date(r)) => l == r,
+            (Date(l), Date(r)) => *l == *r,
             #[cfg(all(feature = "dtype-datetime", feature = "dtype-date"))]
-            (Datetime(l, tul, tzl), Datetime(r, tur, tzr)) => l == r && tul == tur && tzl == tzr,
-            (Boolean(l), Boolean(r)) => l == r,
+            (Datetime(l, tul, tzl), Datetime(r, tur, tzr)) => {
+                *l == *r && *tul == *tur && tzl == tzr
+            }
             (List(l), List(r)) => l == r,
-            (Binary(l), Binary(r)) => l == r,
-            (Utf8(l), Utf8(r)) => l == r,
-            #[cfg(feature = "object")]
-            (Object(_), Object(_)) => panic!("eq between object not supported"),
-            // should it?
-            (Null, Null) => true,
             #[cfg(feature = "dtype-categorical")]
             (Categorical(idx_l, rev_l, _), Categorical(idx_r, rev_r, _)) => match (rev_l, rev_r) {
                 (RevMapping::Global(_, _, id_l), RevMapping::Global(_, _, id_r)) => {
                     id_l == id_r && idx_l == idx_r
                 }
                 (RevMapping::Local(arr_l), RevMapping::Local(arr_r)) => {
                     std::ptr::eq(arr_l, arr_r) && idx_l == idx_r
@@ -769,24 +777,23 @@
                 let r = &*r.0;
                 l == r
             }
             // TODO! add structowned with idx and arced structarray
             #[cfg(feature = "dtype-struct")]
             (StructOwned(l), Struct(idx, arr, fields)) => {
                 let fields_left = &*l.0;
-                let avs = struct_to_avs_static(idx, arr, fields);
+                let avs = struct_to_avs_static(*idx, arr, fields);
                 fields_left == avs
             }
             #[cfg(feature = "dtype-struct")]
             (Struct(idx, arr, fields), StructOwned(r)) => {
                 let fields_right = &*r.0;
-                let avs = struct_to_avs_static(idx, arr, fields);
+                let avs = struct_to_avs_static(*idx, arr, fields);
                 fields_right == avs
             }
-
             _ => false,
         }
     }
 }
 
 impl PartialOrd for AnyValue<'_> {
     /// Only implemented for the same types and physical types!
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/datatypes/dtype.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/datatypes/dtype.rs`

 * *Files 7% similar despite different names*

```diff
@@ -28,14 +28,18 @@
     /// A 64-bit date representing the elapsed time since UNIX epoch (1970-01-01)
     /// in the given timeunit (64 bits).
     Datetime(TimeUnit, Option<TimeZone>),
     // 64-bit integer representing difference between times in milliseconds or nanoseconds
     Duration(TimeUnit),
     /// A 64-bit time representing the elapsed time since midnight in nanoseconds
     Time,
+    /// A nested list with a fixed size in each row
+    #[cfg(feature = "dtype-array")]
+    Array(Box<DataType>, usize),
+    /// A nested list with a variable size in each row
     List(Box<DataType>),
     #[cfg(feature = "object")]
     /// A generic type that can be used in a `Series`
     /// &'static str can be used to determine/set inner type
     Object(&'static str),
     Null,
     #[cfg(feature = "dtype-categorical")]
@@ -71,14 +75,18 @@
                 (List(left_inner), List(right_inner)) => left_inner == right_inner,
                 #[cfg(feature = "dtype-duration")]
                 (Duration(tu_l), Duration(tu_r)) => tu_l == tu_r,
                 #[cfg(feature = "object")]
                 (Object(lhs), Object(rhs)) => lhs == rhs,
                 #[cfg(feature = "dtype-struct")]
                 (Struct(lhs), Struct(rhs)) => lhs == rhs,
+                #[cfg(feature = "dtype-array")]
+                (Array(left_inner, left_width), Array(right_inner, right_width)) => {
+                    left_width == right_width && left_inner == right_inner
+                }
                 _ => std::mem::discriminant(self) == std::mem::discriminant(other),
             }
         }
     }
 }
 
 impl Eq for DataType {}
@@ -99,18 +107,19 @@
             Int32 => other.extract::<i32>().is_some(),
             Int64 => other.extract::<i64>().is_some(),
             _ => false,
         }
     }
 
     pub fn inner_dtype(&self) -> Option<&DataType> {
-        if let DataType::List(inner) = self {
-            Some(inner)
-        } else {
-            None
+        match self {
+            DataType::List(inner) => Some(inner),
+            #[cfg(feature = "dtype-array")]
+            DataType::Array(inner, _) => Some(inner),
+            _ => None,
         }
     }
 
     /// Convert to the physical data type
     #[must_use]
     pub fn to_physical(&self) -> DataType {
         use DataType::*;
@@ -224,14 +233,19 @@
             ),
             Utf8 => ArrowDataType::LargeUtf8,
             Binary => ArrowDataType::LargeBinary,
             Date => ArrowDataType::Date32,
             Datetime(unit, tz) => ArrowDataType::Timestamp(unit.to_arrow(), tz.clone()),
             Duration(unit) => ArrowDataType::Duration(unit.to_arrow()),
             Time => ArrowDataType::Time64(ArrowTimeUnit::Nanosecond),
+            #[cfg(feature = "dtype-array")]
+            Array(dt, size) => ArrowDataType::FixedSizeList(
+                Box::new(arrow::datatypes::Field::new("item", dt.to_arrow(), true)),
+                *size,
+            ),
             List(dt) => ArrowDataType::LargeList(Box::new(arrow::datatypes::Field::new(
                 "item",
                 dt.to_arrow(),
                 true,
             ))),
             Null => ArrowDataType::Null,
             #[cfg(feature = "object")]
@@ -287,15 +301,15 @@
             DataType::Float64 => "f64",
             #[cfg(feature = "dtype-decimal")]
             DataType::Decimal(precision, scale) => {
                 return match (precision, scale) {
                     (_, None) => f.write_str("decimal[?]"), // shouldn't happen
                     (None, Some(scale)) => f.write_str(&format!("decimal[{scale}]")),
                     (Some(precision), Some(scale)) => {
-                        f.write_str(&format!("decimal[.{precision},{scale}]"))
+                        f.write_str(&format!("decimal[{precision},{scale}]"))
                     }
                 };
             }
             DataType::Utf8 => "str",
             DataType::Binary => "binary",
             DataType::Date => "date",
             DataType::Datetime(tu, tz) => {
@@ -303,14 +317,16 @@
                     None => format!("datetime[{tu}]"),
                     Some(tz) => format!("datetime[{tu}, {tz}]"),
                 };
                 return f.write_str(&s);
             }
             DataType::Duration(tu) => return write!(f, "duration[{tu}]"),
             DataType::Time => "time",
+            #[cfg(feature = "dtype-array")]
+            DataType::Array(tp, size) => return write!(f, "array[{tp}, {size}]"),
             DataType::List(tp) => return write!(f, "list[{tp}]"),
             #[cfg(feature = "object")]
             DataType::Object(s) => s,
             #[cfg(feature = "dtype-categorical")]
             DataType::Categorical(_) => "cat",
             #[cfg(feature = "dtype-struct")]
             DataType::Struct(fields) => return write!(f, "struct[{}]", fields.len()),
@@ -329,11 +345,17 @@
             let rev_map = merge_categorical_map(rev_map_l, rev_map_r)?;
             Categorical(Some(rev_map))
         }
         (List(inner_l), List(inner_r)) => {
             let merged = merge_dtypes(inner_l, inner_r)?;
             List(Box::new(merged))
         }
+        #[cfg(feature = "dtype-array")]
+        (Array(inner_l, width_l), Array(inner_r, width_r)) => {
+            polars_ensure!(width_l == width_r, ComputeError: "widths of FixedSizeWidth Series are not equal");
+            let merged = merge_dtypes(inner_l, inner_r)?;
+            Array(Box::new(merged), *width_l)
+        }
         (left, right) if left == right => left.clone(),
         _ => polars_bail!(ComputeError: "unable to merge datatypes"),
     })
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/datatypes/field.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/datatypes/field.rs`

 * *Files 2% similar despite different names*

```diff
@@ -123,14 +123,16 @@
             ArrowDataType::Int8 => DataType::Int8,
             ArrowDataType::Int16 => DataType::Int16,
             ArrowDataType::Int32 => DataType::Int32,
             ArrowDataType::Int64 => DataType::Int64,
             ArrowDataType::Boolean => DataType::Boolean,
             ArrowDataType::Float32 => DataType::Float32,
             ArrowDataType::Float64 => DataType::Float64,
+            #[cfg(feature = "dtype-array")]
+            ArrowDataType::FixedSizeList(f, size) => DataType::Array(Box::new(f.data_type().into()), *size),
             ArrowDataType::LargeList(f) | ArrowDataType::List(f) => DataType::List(Box::new(f.data_type().into())),
             ArrowDataType::Date32 => DataType::Date,
             ArrowDataType::Timestamp(tu, tz) => DataType::Datetime(tu.into(), tz.clone()),
             ArrowDataType::Duration(tu) => DataType::Duration(tu.into()),
             ArrowDataType::Date64 => DataType::Datetime(TimeUnit::Milliseconds, None),
             ArrowDataType::LargeUtf8 | ArrowDataType::Utf8 => DataType::Utf8,
             ArrowDataType::LargeBinary | ArrowDataType::Binary => DataType::Binary,
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/datatypes/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/datatypes/mod.rs`

 * *Files 5% similar despite different names*

```diff
@@ -47,14 +47,17 @@
 use crate::prelude::*;
 use crate::utils::Wrap;
 
 pub struct Utf8Type {}
 
 pub struct BinaryType {}
 
+#[cfg(feature = "dtype-array")]
+pub struct FixedSizeListType {}
+
 #[cfg_attr(feature = "serde", derive(Serialize, Deserialize))]
 pub struct ListType {}
 
 pub trait PolarsDataType: Send + Sync {
     fn get_dtype() -> DataType
     where
         Self: Sized;
@@ -110,19 +113,27 @@
     fn get_dtype() -> DataType {
         DataType::Boolean
     }
 }
 
 impl PolarsDataType for ListType {
     fn get_dtype() -> DataType {
-        // null as we cannot no anything without self.
+        // null as we cannot know anything without self.
         DataType::List(Box::new(DataType::Null))
     }
 }
 
+#[cfg(feature = "dtype-array")]
+impl PolarsDataType for FixedSizeListType {
+    fn get_dtype() -> DataType {
+        // null as we cannot know anything without self.
+        DataType::Array(Box::new(DataType::Null), 0)
+    }
+}
+
 #[cfg(feature = "dtype-decimal")]
 pub struct Int128Type {}
 
 #[cfg(feature = "dtype-decimal")]
 impl PolarsDataType for Int128Type {
     fn get_dtype() -> DataType {
         DataType::Decimal(None, Some(0)) // scale is not None to allow for get_any_value() to work
@@ -146,14 +157,16 @@
 
 impl<T> PolarsSingleType for T where T: NativeType + PolarsDataType {}
 
 impl PolarsSingleType for Utf8Type {}
 
 impl PolarsSingleType for BinaryType {}
 
+#[cfg(feature = "dtype-array")]
+pub type ArrayChunked = ChunkedArray<FixedSizeListType>;
 pub type ListChunked = ChunkedArray<ListType>;
 pub type BooleanChunked = ChunkedArray<BooleanType>;
 pub type UInt8Chunked = ChunkedArray<UInt8Type>;
 pub type UInt16Chunked = ChunkedArray<UInt16Type>;
 pub type UInt32Chunked = ChunkedArray<UInt32Type>;
 pub type UInt64Chunked = ChunkedArray<UInt64Type>;
 pub type Int8Chunked = ChunkedArray<Int8Type>;
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/datatypes/time_unit.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/datatypes/time_unit.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/doc/changelog/v0_10_0_11.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/doc/changelog/v0_10_0_11.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/doc/changelog/v0_7.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/doc/changelog/v0_7.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/doc/changelog/v0_8.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/doc/changelog/v0_8.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/doc/changelog/v0_9.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/doc/changelog/v0_9.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/fmt.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/fmt.rs`

 * *Files 1% similar despite different names*

```diff
@@ -180,14 +180,21 @@
 
 impl Debug for ListChunked {
     fn fmt(&self, f: &mut Formatter<'_>) -> fmt::Result {
         format_array!(f, self, "list", self.name(), "ChunkedArray")
     }
 }
 
+#[cfg(feature = "dtype-array")]
+impl Debug for ArrayChunked {
+    fn fmt(&self, f: &mut Formatter<'_>) -> fmt::Result {
+        format_array!(f, self, "fixed size list", self.name(), "ChunkedArray")
+    }
+}
+
 #[cfg(feature = "object")]
 impl<T> Debug for ObjectChunked<T>
 where
     T: PolarsObject,
 {
     fn fmt(&self, f: &mut Formatter<'_>) -> fmt::Result {
         let limit = std::cmp::min(LIMIT, self.len());
@@ -280,14 +287,19 @@
                 format_array!(f, self.duration().unwrap(), &dt, self.name(), "Series")
             }
             #[cfg(feature = "dtype-decimal")]
             DataType::Decimal(_, _) => {
                 let dt = format!("{}", self.dtype());
                 format_array!(f, self.decimal().unwrap(), &dt, self.name(), "Series")
             }
+            #[cfg(feature = "dtype-array")]
+            DataType::Array(_, _) => {
+                let dt = format!("{}", self.dtype());
+                format_array!(f, self.array().unwrap(), &dt, self.name(), "Series")
+            }
             DataType::List(_) => {
                 let dt = format!("{}", self.dtype());
                 format_array!(f, self.list().unwrap(), &dt, self.name(), "Series")
             }
             #[cfg(feature = "object")]
             DataType::Object(_) => format_object_array(f, self, self.name(), "Series"),
             #[cfg(feature = "dtype-categorical")]
@@ -784,14 +796,16 @@
                 write!(f, "{nt}")
             }
             #[cfg(feature = "dtype-categorical")]
             AnyValue::Categorical(_, _, _) => {
                 let s = self.get_str().unwrap();
                 write!(f, "\"{s}\"")
             }
+            #[cfg(feature = "dtype-array")]
+            AnyValue::Array(s, _size) => write!(f, "{}", s.fmt_list()),
             AnyValue::List(s) => write!(f, "{}", s.fmt_list()),
             #[cfg(feature = "object")]
             AnyValue::Object(v) => write!(f, "{v}"),
             #[cfg(feature = "object")]
             AnyValue::ObjectOwned(v) => write!(f, "{}", v.0.as_ref()),
             #[cfg(feature = "dtype-struct")]
             av @ AnyValue::Struct(_, _, _) => {
@@ -828,21 +842,15 @@
         #[cfg(feature = "timezones")]
         match self.tz.parse::<chrono_tz::Tz>() {
             Ok(tz) => {
                 let dt_utc = chrono::Utc.from_local_datetime(&self.ndt).unwrap();
                 let dt_tz_aware = dt_utc.with_timezone(&tz);
                 write!(f, "{dt_tz_aware}")
             }
-            Err(_) => match parse_offset(self.tz) {
-                Ok(offset) => {
-                    let dt_tz_aware = offset.from_utc_datetime(&self.ndt);
-                    write!(f, "{dt_tz_aware}")
-                }
-                Err(_) => write!(f, "invalid timezone"),
-            },
+            Err(_) => write!(f, "invalid timezone"),
         }
         #[cfg(not(feature = "timezones"))]
         {
             panic!("activate 'timezones' feature")
         }
     }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/arithmetic.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/arithmetic.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/asof_join/asof.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/asof_join/asof.rs`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 use std::fmt::Debug;
 use std::ops::Sub;
 
+use num_traits::Bounded;
 use polars_arrow::index::IdxSize;
 
 pub(super) fn join_asof_forward_with_tolerance<T: PartialOrd + Copy + Debug + Sub<Output = T>>(
     left: &[T],
     right: &[T],
     tolerance: T,
 ) -> Vec<Option<IdxSize>> {
@@ -177,14 +178,69 @@
                 }
             }
         }
     }
     out
 }
 
+pub(super) fn join_asof_nearest<T: PartialOrd + Copy + Debug + Sub<Output = T> + Bounded>(
+    left: &[T],
+    right: &[T],
+) -> Vec<Option<IdxSize>> {
+    let mut out = Vec::with_capacity(left.len());
+    let mut offset = 0 as IdxSize;
+    let max_value = <T as num_traits::Bounded>::max_value();
+    let mut dist: T = max_value;
+
+    for &val_l in left {
+        loop {
+            match right.get(offset as usize) {
+                Some(&val_r) => {
+                    // This is (val_r - val_l).abs(), but works on strings/dates
+                    let dist_curr = if val_r > val_l {
+                        val_r - val_l
+                    } else {
+                        val_l - val_r
+                    };
+                    if dist_curr <= dist {
+                        // candidate for match
+                        dist = dist_curr;
+                        offset += 1;
+                    } else {
+                        // distance has increased, we're now farther away, so previous element was closest
+                        out.push(Some(offset - 1));
+
+                        // reset distance
+                        dist = max_value;
+
+                        // The next left-item may match on the same item, so we need to rewind the offset
+                        offset -= 1;
+                        break;
+                    }
+                }
+
+                None => {
+                    if offset > 1 {
+                        // we've reached the end with no matches, so the last item is the nearest for all remaining
+                        out.extend(
+                            std::iter::repeat(Some(offset - 1)).take(left.len() - out.len()),
+                        );
+                    } else {
+                        // this is only hit when the right frame is empty
+                        out.extend(std::iter::repeat(None).take(left.len() - out.len()));
+                    }
+                    return out;
+                }
+            }
+        }
+    }
+
+    out
+}
+
 pub(super) fn join_asof_forward<T: PartialOrd + Copy + Debug>(
     left: &[T],
     right: &[T],
 ) -> Vec<Option<IdxSize>> {
     let mut out = Vec::with_capacity(left.len());
     let mut offset = 0 as IdxSize;
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/asof_join/groups.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/asof_join/groups.rs`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 use std::fmt::Debug;
 use std::hash::Hash;
 use std::ops::Sub;
 
 use ahash::RandomState;
 use arrow::types::NativeType;
-use num_traits::Zero;
+use num_traits::{Bounded, Zero};
 use rayon::prelude::*;
 use smartstring::alias::String as SmartString;
 
 use super::*;
 use crate::frame::groupby::hashing::HASHMAP_INIT_SIZE;
 #[cfg(feature = "dtype-categorical")]
 use crate::frame::hash_join::_check_categorical_src;
@@ -136,14 +136,54 @@
                 return (Some(offset), idx);
             }
         }
     }
     (None, offsets.len())
 }
 
+pub(super) unsafe fn join_asof_nearest_with_indirection<
+    T: PartialOrd + Copy + Debug + Sub<Output = T> + Bounded,
+>(
+    val_l: T,
+    right: &[T],
+    offsets: &[IdxSize],
+    // only there to have the same function signature
+    _: T,
+) -> (Option<IdxSize>, usize) {
+    if offsets.is_empty() {
+        return (None, 0);
+    }
+    let max_value = <T as Bounded>::max_value();
+    let mut dist: T = max_value;
+    for (idx, &offset) in offsets.iter().enumerate() {
+        let val_r = *right.get_unchecked(offset as usize);
+        if val_r >= val_l {
+            // This is (val_r - val_l).abs(), but works on strings/dates
+            let dist_curr = if val_r > val_l {
+                val_r - val_l
+            } else {
+                val_l - val_r
+            };
+            if dist_curr <= dist {
+                // candidate for match
+                dist = dist_curr;
+            } else {
+                // note for a nearest-match, we can re-match on the same val_r next time,
+                // so we need to rewind the idx by 1
+                return (Some(offset - 1), idx - 1);
+            }
+        }
+    }
+
+    // if we've reached the end with nearest and haven't returned, it means that the last item was the closest
+    // note for a nearest-match, we can re-match on the same val_r next time,
+    // so we need to rewind the idx by 1
+    (Some(offsets[offsets.len() - 1]), offsets.len() - 1)
+}
+
 // process the group taken by the `by` operation and keep track of the offset.
 // we don't process a group at once but per `index_left` we find the `right_index` and keep track
 // of the offsets we have already processed in a separate hashmap. Then on a next iteration we can
 // continue from that offsets location.
 #[allow(clippy::too_many_arguments)]
 #[allow(clippy::type_complexity)]
 fn process_group<K, T>(
@@ -230,14 +270,17 @@
         (Some(tolerance), AsofStrategy::Forward) => {
             let tol = tolerance.extract::<T::Native>().unwrap();
             (join_asof_forward_with_indirection_and_tolerance, tol, true)
         }
         (None, AsofStrategy::Forward) => {
             (join_asof_forward_with_indirection, T::Native::zero(), true)
         }
+        (_, AsofStrategy::Nearest) => {
+            (join_asof_nearest_with_indirection, T::Native::zero(), false)
+        }
     };
 
     let left_asof = left_asof.rechunk();
     let err = |_: PolarsError| {
         polars_err!(
             ComputeError: "keys are not allowed to have null values in asof join"
         )
@@ -361,14 +404,17 @@
         (Some(tolerance), AsofStrategy::Forward) => {
             let tol = tolerance.extract::<T::Native>().unwrap();
             (join_asof_forward_with_indirection_and_tolerance, tol, true)
         }
         (None, AsofStrategy::Forward) => {
             (join_asof_forward_with_indirection, T::Native::zero(), true)
         }
+        (_, AsofStrategy::Nearest) => {
+            (join_asof_nearest_with_indirection, T::Native::zero(), false)
+        }
     };
 
     let left_asof = left_asof.rechunk();
     let left_asof = left_asof.cont_slice().unwrap();
 
     let right_asof = right_asof.rechunk();
     let right_asof = right_asof.cont_slice().unwrap();
@@ -484,14 +530,17 @@
         (Some(tolerance), AsofStrategy::Forward) => {
             let tol = tolerance.extract::<T::Native>().unwrap();
             (join_asof_forward_with_indirection_and_tolerance, tol, true)
         }
         (None, AsofStrategy::Forward) => {
             (join_asof_forward_with_indirection, T::Native::zero(), true)
         }
+        (_, AsofStrategy::Nearest) => {
+            (join_asof_nearest_with_indirection, T::Native::zero(), false)
+        }
     };
     let left_asof = left_asof.rechunk();
     let left_asof = left_asof.cont_slice().unwrap();
 
     let right_asof = right_asof.rechunk();
     let right_asof = right_asof.cont_slice().unwrap();
 
@@ -648,15 +697,19 @@
         slice: Option<(i64, usize)>,
     ) -> PolarsResult<DataFrame> {
         let left_asof = self.column(left_on)?.to_physical_repr();
         let right_asof = other.column(right_on)?.to_physical_repr();
         let right_asof_name = right_asof.name();
         let left_asof_name = left_asof.name();
 
-        check_asof_columns(&left_asof, &right_asof)?;
+        check_asof_columns(
+            &left_asof,
+            &right_asof,
+            left_by.is_empty() && right_by.is_empty(),
+        )?;
 
         let mut left_by = self.select(left_by)?;
         let mut right_by = other.select(right_by)?;
 
         unsafe {
             for (l, r) in left_by
                 .get_columns_mut()
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/asof_join/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/asof_join/mod.rs`

 * *Files 6% similar despite different names*

```diff
@@ -23,39 +23,43 @@
     /// - "1d6h"
     /// etc
     pub tolerance_str: Option<SmartString>,
     pub left_by: Option<Vec<SmartString>>,
     pub right_by: Option<Vec<SmartString>>,
 }
 
-fn check_asof_columns(a: &Series, b: &Series) -> PolarsResult<()> {
+fn check_asof_columns(a: &Series, b: &Series, check_sorted: bool) -> PolarsResult<()> {
     let dtype_a = a.dtype();
     let dtype_b = b.dtype();
     polars_ensure!(
         dtype_a == dtype_b,
         ComputeError: "mismatching key dtypes in asof-join: `{}` and `{}`",
         a.dtype(), b.dtype()
     );
     polars_ensure!(
         a.null_count() == 0 && b.null_count() == 0,
         ComputeError: "asof join must not have null values in 'on' arguments"
     );
-    ensure_sorted_arg(a, "asof_join");
-    ensure_sorted_arg(b, "asof_join");
+    if check_sorted {
+        ensure_sorted_arg(a, "asof_join")?;
+        ensure_sorted_arg(b, "asof_join")?;
+    }
     Ok(())
 }
 
 #[derive(Clone, Copy, Debug, PartialEq, Eq, Default)]
 #[cfg_attr(feature = "serde", derive(Serialize, Deserialize))]
 pub enum AsofStrategy {
     /// selects the last row in the right DataFrame whose on key is less than or equal to the lefts key
     #[default]
     Backward,
     /// selects the first row in the right DataFrame whose on key is greater than or equal to the lefts key.
     Forward,
+    /// selects the right in the right DataFrame whose 'on' key is nearest to the left's key.
+    Nearest,
 }
 
 impl<T> ChunkedArray<T>
 where
     T: PolarsNumericType,
     T::Native: Bounded + PartialOrd,
 {
@@ -90,14 +94,17 @@
                     join_asof_backward_with_tolerance(
                         self.cont_slice().unwrap(),
                         other.cont_slice().unwrap(),
                         tolerance,
                     )
                 }
             },
+            AsofStrategy::Nearest => {
+                join_asof_nearest(ca.cont_slice().unwrap(), other.cont_slice().unwrap())
+            }
         };
         Ok(out)
     }
 }
 
 impl DataFrame {
     #[doc(hidden)]
@@ -111,15 +118,15 @@
         tolerance: Option<AnyValue<'static>>,
         suffix: Option<String>,
         slice: Option<(i64, usize)>,
     ) -> PolarsResult<DataFrame> {
         let left_key = self.column(left_on)?;
         let right_key = other.column(right_on)?;
 
-        check_asof_columns(left_key, right_key)?;
+        check_asof_columns(left_key, right_key, true)?;
         let left_key = left_key.to_physical_repr();
         let right_key = right_key.to_physical_repr();
 
         let take_idx = match left_key.dtype() {
             DataType::Int64 => left_key
                 .i64()
                 .unwrap()
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/chunks.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/chunks.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/cross_join.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/cross_join.rs`

 * *Files 0% similar despite different names*

```diff
@@ -49,15 +49,15 @@
         parallel: bool,
     ) -> PolarsResult<(DataFrame, DataFrame)> {
         let n_rows_left = self.height() as IdxSize;
         let n_rows_right = other.height() as IdxSize;
         let Some(total_rows) = n_rows_left.checked_mul(n_rows_right) else {
             polars_bail!(
                 ComputeError: "cross joins would produce more rows than fits into 2^32; \
-                consider comping with polars-big-idx feature, or set 'streaming'"
+                consider compiling with polars-big-idx feature, or set 'streaming'"
             );
         };
         if n_rows_left == 0 || n_rows_right == 0 {
             return Ok((self.clear(), other.clear()));
         }
 
         // the left side has the Nth row combined with every row from right.
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/explode.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/explode.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/from.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/from.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/groupby/aggregations/agg_list.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/groupby/aggregations/agg_list.rs`

 * *Files 7% similar despite different names*

```diff
@@ -1,8 +1,10 @@
 use arrow::offset::Offsets;
+use polars_arrow::kernels::concatenate::concatenate_owned_unchecked;
+use polars_utils::unwrap::UnwrapUncheckedRelease;
 
 use super::*;
 #[cfg(feature = "dtype-struct")]
 use crate::chunked_array::builder::AnonymousOwnedListBuilder;
 
 pub trait AggList {
     /// # Safety
@@ -244,16 +246,15 @@
         &mut offsets,
         &mut length_so_far,
         &mut list_values,
     );
     if groups_len == 0 {
         list_values.push(ca.chunks[0].sliced(0, 0))
     }
-    let arrays = list_values.iter().map(|arr| &**arr).collect::<Vec<_>>();
-    let list_values: ArrayRef = arrow::compute::concatenate::concatenate(&arrays).unwrap();
+    let list_values = concatenate_owned_unchecked(&list_values).unwrap();
     let data_type = ListArray::<i64>::default_datatype(list_values.data_type().clone());
     // Safety:
     // offsets are monotonically increasing
     let arr = unsafe {
         Box::new(ListArray::<i64>::new(
             data_type,
             Offsets::new_unchecked(offsets).into(),
@@ -262,41 +263,42 @@
         )) as ArrayRef
     };
     let mut listarr = unsafe { ListChunked::from_chunks(ca.name(), vec![arr]) };
     if can_fast_explode {
         listarr.set_fast_explode()
     }
     if inner_dtype_physical != inner_dtype {
-        listarr.to_logical(DataType::List(Box::new(inner_dtype)));
+        listarr.to_physical(DataType::List(Box::new(inner_dtype)));
     }
     listarr.into_series()
 }
 
 impl AggList for ListChunked {
     unsafe fn agg_list(&self, groups: &GroupsProxy) -> Series {
         match groups {
             GroupsProxy::Idx(groups) => {
                 let func = |ca: &ListChunked,
                             mut can_fast_explode: bool,
                             offsets: &mut Vec<i64>,
                             length_so_far: &mut i64,
                             list_values: &mut Vec<ArrayRef>| {
+                    assert!(list_values.capacity() >= groups.len());
                     groups.iter().for_each(|(_, idx)| {
                         let idx_len = idx.len();
                         if idx_len == 0 {
                             can_fast_explode = false;
                         }
 
                         *length_so_far += idx_len as i64;
                         // Safety:
                         // group tuples are in bounds
                         {
                             let mut s = ca.take_unchecked(idx.into());
-                            let arr = s.chunks.pop().unwrap();
-                            list_values.push(arr);
+                            let arr = s.chunks.pop().unwrap_unchecked_release();
+                            list_values.push_unchecked(arr);
 
                             // Safety:
                             // we know that offsets has allocated enough slots
                             offsets.push_unchecked(*length_so_far);
                         }
                     });
                     can_fast_explode
@@ -306,23 +308,24 @@
             }
             GroupsProxy::Slice { groups, .. } => {
                 let func = |ca: &ListChunked,
                             mut can_fast_explode: bool,
                             offsets: &mut Vec<i64>,
                             length_so_far: &mut i64,
                             list_values: &mut Vec<ArrayRef>| {
+                    assert!(list_values.capacity() >= groups.len());
                     groups.iter().for_each(|&[first, len]| {
                         if len == 0 {
                             can_fast_explode = false;
                         }
 
                         *length_so_far += len as i64;
                         let mut s = ca.slice(first as i64, len as usize);
-                        let arr = s.chunks.pop().unwrap();
-                        list_values.push(arr);
+                        let arr = s.chunks.pop().unwrap_unchecked_release();
+                        list_values.push_unchecked(arr);
 
                         {
                             // Safety:
                             // we know that offsets has allocated enough slots
                             offsets.push_unchecked(*length_so_far);
                         }
                     });
@@ -331,14 +334,75 @@
 
                 agg_list_list(self, groups.len(), func)
             }
         }
     }
 }
 
+#[cfg(feature = "dtype-array")]
+fn agg_list_fixed_size_list<F: Fn(&ArrayChunked, &mut Vec<ArrayRef>)>(
+    ca: &ArrayChunked,
+    groups_len: usize,
+    func: F,
+    width: usize,
+) -> Series {
+    let inner_dtype = ca.inner_dtype();
+    let inner_dtype_physical = inner_dtype.to_physical();
+    let mut list_values = Vec::with_capacity(groups_len);
+
+    func(ca, &mut list_values);
+    if groups_len == 0 {
+        list_values.push(ca.chunks[0].sliced(0, 0))
+    }
+    let list_values = concatenate_owned_unchecked(&list_values).unwrap();
+    let data_type = FixedSizeListArray::default_datatype(list_values.data_type().clone(), width);
+    let arr = Box::new(FixedSizeListArray::new(data_type, list_values, None)) as ArrayRef;
+    let mut listarr = unsafe { ListChunked::from_chunks(ca.name(), vec![arr]) };
+    if inner_dtype_physical != inner_dtype {
+        listarr.to_physical(DataType::List(Box::new(inner_dtype)));
+    }
+    listarr.into_series()
+}
+
+#[cfg(feature = "dtype-array")]
+impl AggList for ArrayChunked {
+    unsafe fn agg_list(&self, groups: &GroupsProxy) -> Series {
+        match groups {
+            GroupsProxy::Idx(groups) => {
+                let func = |ca: &ArrayChunked, list_values: &mut Vec<ArrayRef>| {
+                    assert!(list_values.capacity() >= groups.len());
+                    groups.iter().for_each(|(_, idx)| {
+                        // Safety:
+                        // group tuples are in bounds
+                        {
+                            let mut s = ca.take_unchecked(idx.into());
+                            let arr = s.chunks.pop().unwrap_unchecked_release();
+                            list_values.push_unchecked(arr);
+                        }
+                    });
+                };
+
+                agg_list_fixed_size_list(self, groups.len(), func, self.width())
+            }
+            GroupsProxy::Slice { groups, .. } => {
+                let func = |ca: &ArrayChunked, list_values: &mut Vec<ArrayRef>| {
+                    assert!(list_values.capacity() >= groups.len());
+                    groups.iter().for_each(|&[first, len]| {
+                        let mut s = ca.slice(first as i64, len as usize);
+                        let arr = s.chunks.pop().unwrap_unchecked_release();
+                        list_values.push_unchecked(arr);
+                    });
+                };
+
+                agg_list_fixed_size_list(self, groups.len(), func, self.width())
+            }
+        }
+    }
+}
+
 #[cfg(feature = "object")]
 impl<T: PolarsObject> AggList for ObjectChunked<T> {
     unsafe fn agg_list(&self, groups: &GroupsProxy) -> Series {
         let mut can_fast_explode = true;
         let mut offsets = Vec::<i64>::with_capacity(groups.len() + 1);
         let mut length_so_far = 0i64;
         offsets.push(length_so_far);
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/groupby/aggregations/boolean.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/groupby/aggregations/boolean.rs`

 * *Files 1% similar despite different names*

```diff
@@ -15,15 +15,15 @@
     let ca: BooleanChunked = POOL.install(|| groups.par_iter().copied().map(f).collect());
     ca.into_series()
 }
 
 impl BooleanChunked {
     pub(crate) unsafe fn agg_min(&self, groups: &GroupsProxy) -> Series {
         // faster paths
-        match (self.is_sorted_flag2(), self.null_count()) {
+        match (self.is_sorted_flag(), self.null_count()) {
             (IsSorted::Ascending, 0) => {
                 return self.clone().into_series().agg_first(groups);
             }
             (IsSorted::Descending, 0) => {
                 return self.clone().into_series().agg_last(groups);
             }
             _ => {}
@@ -58,15 +58,15 @@
                     }
                 }
             }),
         }
     }
     pub(crate) unsafe fn agg_max(&self, groups: &GroupsProxy) -> Series {
         // faster paths
-        match (self.is_sorted_flag2(), self.null_count()) {
+        match (self.is_sorted_flag(), self.null_count()) {
             (IsSorted::Ascending, 0) => {
                 return self.clone().into_series().agg_last(groups);
             }
             (IsSorted::Descending, 0) => {
                 return self.clone().into_series().agg_first(groups);
             }
             _ => {}
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/groupby/aggregations/dispatch.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/groupby/aggregations/dispatch.rs`

 * *Files 5% similar despite different names*

```diff
@@ -43,15 +43,15 @@
                 })
             }
         }
     }
 
     #[doc(hidden)]
     pub unsafe fn agg_first(&self, groups: &GroupsProxy) -> Series {
-        let out = match groups {
+        let mut out = match groups {
             GroupsProxy::Idx(groups) => {
                 let mut iter = groups.iter().map(|(first, idx)| {
                     if idx.is_empty() {
                         None
                     } else {
                         Some(first as usize)
                     }
@@ -72,14 +72,17 @@
                         },
                     );
                 // Safety:
                 // groups are always in bounds
                 self.take_opt_iter_unchecked(&mut iter)
             }
         };
+        if groups.is_sorted_flag() {
+            out.set_sorted_flag(self.is_sorted_flag())
+        }
         self.restore_logical(out)
     }
 
     #[doc(hidden)]
     pub unsafe fn agg_n_unique(&self, groups: &GroupsProxy) -> Series {
         match groups {
             GroupsProxy::Idx(groups) => agg_helper_idx_on_all::<IdxType, _>(groups, |idx| {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/groupby/aggregations/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/groupby/aggregations/mod.rs`

 * *Files 1% similar despite different names*

```diff
@@ -363,15 +363,15 @@
     <T::Native as Simd>::Simd: std::ops::Add<Output = <T::Native as Simd>::Simd>
         + arrow::compute::aggregate::Sum<T::Native>
         + arrow::compute::aggregate::SimdOrd<T::Native>,
     ChunkedArray<T>: IntoSeries,
 {
     pub(crate) unsafe fn agg_min(&self, groups: &GroupsProxy) -> Series {
         // faster paths
-        match (self.is_sorted_flag2(), self.null_count()) {
+        match (self.is_sorted_flag(), self.null_count()) {
             (IsSorted::Ascending, 0) => {
                 return self.clone().into_series().agg_first(groups);
             }
             (IsSorted::Descending, 0) => {
                 return self.clone().into_series().agg_last(groups);
             }
             _ => {}
@@ -440,15 +440,15 @@
                 }
             }
         }
     }
 
     pub(crate) unsafe fn agg_max(&self, groups: &GroupsProxy) -> Series {
         // faster paths
-        match (self.is_sorted_flag2(), self.null_count()) {
+        match (self.is_sorted_flag(), self.null_count()) {
             (IsSorted::Ascending, 0) => {
                 return self.clone().into_series().agg_last(groups);
             }
             (IsSorted::Descending, 0) => {
                 return self.clone().into_series().agg_first(groups);
             }
             _ => {}
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/groupby/aggregations/utf8.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/groupby/aggregations/utf8.rs`

 * *Files 0% similar despite different names*

```diff
@@ -16,15 +16,15 @@
     ca.into_series()
 }
 
 impl Utf8Chunked {
     #[allow(clippy::needless_lifetimes)]
     pub(crate) unsafe fn agg_min<'a>(&'a self, groups: &GroupsProxy) -> Series {
         // faster paths
-        match (&self.is_sorted_flag2(), &self.null_count()) {
+        match (&self.is_sorted_flag(), &self.null_count()) {
             (IsSorted::Ascending, 0) => {
                 return self.clone().into_series().agg_first(groups);
             }
             (IsSorted::Descending, 0) => {
                 return self.clone().into_series().agg_last(groups);
             }
             _ => {}
@@ -78,15 +78,15 @@
             }),
         }
     }
 
     #[allow(clippy::needless_lifetimes)]
     pub(crate) unsafe fn agg_max<'a>(&'a self, groups: &GroupsProxy) -> Series {
         // faster paths
-        match (self.is_sorted_flag2(), self.null_count()) {
+        match (self.is_sorted_flag(), self.null_count()) {
             (IsSorted::Ascending, 0) => {
                 return self.clone().into_series().agg_last(groups);
             }
             (IsSorted::Descending, 0) => {
                 return self.clone().into_series().agg_first(groups);
             }
             _ => {}
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/groupby/hashing.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/groupby/hashing.rs`

 * *Files 8% similar despite different names*

```diff
@@ -1,26 +1,43 @@
 use std::hash::{BuildHasher, Hash};
 
 use hashbrown::hash_map::{Entry, RawEntryMut};
 use hashbrown::HashMap;
+use polars_utils::iter::EnumerateIdxTrait;
 use polars_utils::sync::SyncPtr;
 use polars_utils::HashSingle;
 use rayon::prelude::*;
 
 use super::GroupsProxy;
 use crate::datatypes::PlHashMap;
 use crate::frame::groupby::{GroupsIdx, IdxItem};
 use crate::hashing::{
-    df_rows_to_hashes_threaded_vertical, this_partition, AsU64, IdBuildHasher, IdxHash,
+    df_rows_to_hashes_threaded_vertical, series_to_hashes, this_partition, AsU64, IdBuildHasher,
+    IdxHash,
 };
 use crate::prelude::compare_inner::PartialEqInner;
 use crate::prelude::*;
 use crate::utils::{flatten, split_df, CustomIterTools};
 use crate::POOL;
 
+// We must strike a balance between cache coherence and resizing costs.
+// Overallocation seems a lot more expensive than resizing so we start reasonable small.
+pub(crate) const HASHMAP_INIT_SIZE: usize = 512;
+
+fn get_init_size() -> usize {
+    // we check if this is executed from the main thread
+    // we don't want to pre-allocate this much if executed
+    // group_tuples in a parallel iterator as that explodes allocation
+    if POOL.current_thread_index().is_none() {
+        HASHMAP_INIT_SIZE
+    } else {
+        0
+    }
+}
+
 fn finish_group_order(mut out: Vec<Vec<IdxItem>>, sorted: bool) -> GroupsProxy {
     if sorted {
         // we can just take the first value, no need to flatten
         let mut out = if out.len() == 1 {
             out.pop().unwrap()
         } else {
             let (cap, offsets) = flatten::cap_and_offsets(&out);
@@ -62,14 +79,17 @@
         } else {
             // flattens
             GroupsProxy::Idx(GroupsIdx::from(out))
         }
     }
 }
 
+// The inner vecs should be sorted by IdxSize
+// the groupby multiple keys variants suffice
+// this requirements as they use an IdxMap strategy
 fn finish_group_order_vecs(
     mut vecs: Vec<(Vec<IdxSize>, Vec<Vec<IdxSize>>)>,
     sorted: bool,
 ) -> GroupsProxy {
     if sorted {
         if vecs.len() == 1 {
             let (first, all) = vecs.pop().unwrap();
@@ -122,24 +142,20 @@
         GroupsProxy::Idx(idx)
     } else {
         // this materialization is parallel in the from impl.
         GroupsProxy::Idx(GroupsIdx::from(vecs))
     }
 }
 
-// We must strike a balance between cache coherence and resizing costs.
-// Overallocation seems a lot more expensive than resizing so we start reasonable small.
-pub(crate) const HASHMAP_INIT_SIZE: usize = 512;
-
 pub(crate) fn groupby<T>(a: impl Iterator<Item = T>, sorted: bool) -> GroupsProxy
 where
     T: Hash + Eq,
 {
-    let mut hash_tbl: PlHashMap<T, (IdxSize, Vec<IdxSize>)> =
-        PlHashMap::with_capacity(HASHMAP_INIT_SIZE);
+    let init_size = get_init_size();
+    let mut hash_tbl: PlHashMap<T, (IdxSize, Vec<IdxSize>)> = PlHashMap::with_capacity(init_size);
     let mut cnt = 0;
     a.for_each(|k| {
         let idx = cnt;
         cnt += 1;
         let entry = hash_tbl.entry(k);
 
         match entry {
@@ -175,24 +191,25 @@
     sorted: bool,
 ) -> GroupsProxy
 where
     T: Send + Hash + Eq + Sync + Copy + AsU64,
     IntoSlice: AsRef<[T]> + Send + Sync,
 {
     assert!(n_partitions.is_power_of_two());
+    let init_size = get_init_size();
 
     // We will create a hashtable in every thread.
     // We use the hash to partition the keys to the matching hashtable.
     // Every thread traverses all keys/hashes and ignores the ones that doesn't fall in that partition.
     let out = POOL.install(|| {
         (0..n_partitions)
             .into_par_iter()
             .map(|thread_no| {
                 let mut hash_tbl: PlHashMap<T, (IdxSize, Vec<IdxSize>)> =
-                    PlHashMap::with_capacity(HASHMAP_INIT_SIZE);
+                    PlHashMap::with_capacity(init_size);
 
                 let mut offset = 0;
                 for keys in &keys {
                     let keys = keys.as_ref();
                     let len = keys.len() as IdxSize;
                     let hasher = hash_tbl.hasher().clone();
 
@@ -238,24 +255,25 @@
 ) -> GroupsProxy
 where
     I: IntoIterator<Item = T> + Send + Sync + Copy,
     I::IntoIter: ExactSizeIterator,
     T: Send + Hash + Eq + Sync + Copy + AsU64,
 {
     assert!(n_partitions.is_power_of_two());
+    let init_size = get_init_size();
 
     // We will create a hashtable in every thread.
     // We use the hash to partition the keys to the matching hashtable.
     // Every thread traverses all keys/hashes and ignores the ones that doesn't fall in that partition.
     let out = POOL.install(|| {
         (0..n_partitions)
             .into_par_iter()
             .map(|thread_no| {
                 let mut hash_tbl: PlHashMap<T, (IdxSize, Vec<IdxSize>)> =
-                    PlHashMap::with_capacity(HASHMAP_INIT_SIZE);
+                    PlHashMap::with_capacity(init_size);
 
                 let mut offset = 0;
                 for keys in keys {
                     let keys = keys.into_iter();
                     let len = keys.len() as IdxSize;
                     let hasher = hash_tbl.hasher().clone();
 
@@ -439,14 +457,16 @@
     n_partitions: usize,
     sorted: bool,
 ) -> PolarsResult<GroupsProxy> {
     let dfs = split_df(&mut keys, n_partitions).unwrap();
     let (hashes, _random_state) = df_rows_to_hashes_threaded_vertical(&dfs, None)?;
     let n_partitions = n_partitions as u64;
 
+    let init_size = get_init_size();
+
     // trait object to compare inner types.
     let keys_cmp = keys
         .iter()
         .map(|s| s.into_partial_eq_inner())
         .collect::<Vec<_>>();
 
     // We will create a hashtable in every thread.
@@ -454,18 +474,20 @@
     // Every thread traverses all keys/hashes and ignores the ones that doesn't fall in that partition.
     let v = POOL.install(|| {
         (0..n_partitions)
             .into_par_iter()
             .map(|thread_no| {
                 let hashes = &hashes;
 
+                // IndexMap, the indexes are stored in flat vectors
+                // this ensures that order remains and iteration is fast
                 let mut hash_tbl: HashMap<IdxHash, IdxSize, IdBuildHasher> =
-                    HashMap::with_capacity_and_hasher(HASHMAP_INIT_SIZE, Default::default());
-                let mut first_vals = Vec::with_capacity(HASHMAP_INIT_SIZE);
-                let mut all_vals = Vec::with_capacity(HASHMAP_INIT_SIZE);
+                    HashMap::with_capacity_and_hasher(init_size, Default::default());
+                let mut first_vals = Vec::with_capacity(init_size);
+                let mut all_vals = Vec::with_capacity(init_size);
 
                 // put the buffers behind a pointer so we can access them from as the bchk doesn't allow
                 // 2 mutable borrows (this is safe as we don't alias)
                 // even if the vecs reallocate, we have a pointer to the stack vec, and thus always
                 // access the proper data.
                 let all_buf_ptr =
                     &mut all_vals as *mut Vec<Vec<IdxSize>> as *const Vec<Vec<IdxSize>>;
@@ -515,7 +537,62 @@
                 }
                 (first_vals, all_vals)
             })
             .collect::<Vec<_>>()
     });
     Ok(finish_group_order_vecs(v, sorted))
 }
+
+pub(crate) fn groupby_multiple_keys(keys: DataFrame, sorted: bool) -> PolarsResult<GroupsProxy> {
+    let mut hashes = Vec::with_capacity(keys.height());
+    let _ = series_to_hashes(keys.get_columns(), None, &mut hashes)?;
+
+    let init_size = get_init_size();
+
+    // trait object to compare inner types.
+    let keys_cmp = keys
+        .iter()
+        .map(|s| s.into_partial_eq_inner())
+        .collect::<Vec<_>>();
+
+    // IndexMap, the indexes are stored in flat vectors
+    // this ensures that order remains and iteration is fast
+    let mut hash_tbl: HashMap<IdxHash, IdxSize, IdBuildHasher> =
+        HashMap::with_capacity_and_hasher(init_size, Default::default());
+    let mut first_vals = Vec::with_capacity(init_size);
+    let mut all_vals = Vec::with_capacity(init_size);
+
+    // put the buffers behind a pointer so we can access them from as the bchk doesn't allow
+    // 2 mutable borrows (this is safe as we don't alias)
+    // even if the vecs reallocate, we have a pointer to the stack vec, and thus always
+    // access the proper data.
+    let all_buf_ptr = &mut all_vals as *mut Vec<Vec<IdxSize>> as *const Vec<Vec<IdxSize>>;
+    let first_buf_ptr = &mut first_vals as *mut Vec<IdxSize> as *const Vec<IdxSize>;
+
+    for (row_idx, h) in hashes.into_iter().enumerate_idx() {
+        populate_multiple_key_hashmap2(
+            &mut hash_tbl,
+            row_idx,
+            h,
+            &keys_cmp,
+            || unsafe {
+                let first_vals = &mut *(first_buf_ptr as *mut Vec<IdxSize>);
+                let all_vals = &mut *(all_buf_ptr as *mut Vec<Vec<IdxSize>>);
+                let offset_idx = first_vals.len() as IdxSize;
+
+                let tuples = vec![row_idx];
+                all_vals.push(tuples);
+                first_vals.push(row_idx);
+                offset_idx
+            },
+            |v| unsafe {
+                let all_vals = &mut *(all_buf_ptr as *mut Vec<Vec<IdxSize>>);
+                let offset_idx = *v;
+                let buf = all_vals.get_unchecked_mut(offset_idx as usize);
+                buf.push(row_idx)
+            },
+        );
+    }
+
+    let v = vec![(first_vals, all_vals)];
+    Ok(finish_group_order_vecs(v, sorted))
+}
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/groupby/into_groups.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/groupby/into_groups.rs`

 * *Files 3% similar despite different names*

```diff
@@ -354,14 +354,27 @@
         #[cfg(not(feature = "groupby_list"))]
         {
             panic!("activate 'groupby_list' feature")
         }
     }
 }
 
+#[cfg(feature = "dtype-array")]
+impl IntoGroupsProxy for ArrayChunked {
+    #[allow(clippy::needless_lifetimes)]
+    #[allow(unused_variables)]
+    fn group_tuples<'a>(
+        &'a self,
+        _multithreaded: bool,
+        _sorted: bool,
+    ) -> PolarsResult<GroupsProxy> {
+        todo!("grouping FixedSizeList not yet supported")
+    }
+}
+
 #[cfg(feature = "object")]
 impl<T> IntoGroupsProxy for ObjectChunked<T>
 where
     T: PolarsObject,
 {
     fn group_tuples(&self, _multithreaded: bool, sorted: bool) -> PolarsResult<GroupsProxy> {
         Ok(groupby(self.into_iter(), sorted))
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/groupby/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/groupby/mod.rs`

 * *Files 0% similar despite different names*

```diff
@@ -83,15 +83,19 @@
                 if by.iter().any(|s| matches!(s.dtype(), DataType::Struct(_))) {
                     let rows = encode_rows_vertical(&by)?;
                     let groups = rows.group_tuples(multithreaded, sorted)?;
                     return Ok(GroupBy::new(self, by, groups, None));
                 }
             }
             let keys_df = prepare_dataframe_unsorted(&by);
-            groupby_threaded_multiple_keys_flat(keys_df, n_partitions, sorted)
+            if multithreaded {
+                groupby_threaded_multiple_keys_flat(keys_df, n_partitions, sorted)
+            } else {
+                groupby_multiple_keys(keys_df, sorted)
+            }
         };
         Ok(GroupBy::new(self, by, groups?, None))
     }
 
     /// Group DataFrame using a Series column.
     ///
     /// # Example
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/groupby/perfect.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/groupby/perfect.rs`

 * *Files 2% similar despite different names*

```diff
@@ -118,15 +118,15 @@
                                                     *first.get_unchecked_release_mut(cat) =
                                                         *first_value
                                                 }
                                             }
                                         }
                                     }
                                     // last thread handles null values
-                                    else if thread_no == n_threads - 1 {
+                                    else if thread_no == cache_line_offsets.len() - 2 {
                                         let buf =
                                             unsafe { groups.get_unchecked_release_mut(null_idx) };
                                         buf.push(row_nr);
                                         unsafe {
                                             if buf.len() == 1 {
                                                 let first_value = buf.get_unchecked(0);
                                                 *first.get_unchecked_release_mut(null_idx) =
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/groupby/proxy.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/groupby/proxy.rs`

 * *Files 1% similar despite different names*

```diff
@@ -60,15 +60,15 @@
         let mut global_first = Vec::with_capacity(cap);
         let global_first_ptr = unsafe { SyncPtr::new(global_first.as_mut_ptr()) };
         let mut global_all = Vec::with_capacity(cap);
         let global_all_ptr = unsafe { SyncPtr::new(global_all.as_mut_ptr()) };
 
         POOL.install(|| {
             v.into_par_iter().zip(offsets).for_each(
-                |((local_first_vals, local_all_vals), offset)| unsafe {
+                |((local_first_vals, mut local_all_vals), offset)| unsafe {
                     let global_first: *mut IdxSize = global_first_ptr.get();
                     let global_all: *mut Vec<IdxSize> = global_all_ptr.get();
                     let global_first = global_first.add(offset);
                     let global_all = global_all.add(offset);
 
                     std::ptr::copy_nonoverlapping(
                         local_first_vals.as_ptr(),
@@ -76,16 +76,19 @@
                         local_first_vals.len(),
                     );
                     std::ptr::copy_nonoverlapping(
                         local_all_vals.as_ptr(),
                         global_all,
                         local_all_vals.len(),
                     );
-                    // ensure the vecs don't get dropped
-                    std::mem::forget(local_all_vals);
+                    // local_all_vals: Vec<Vec<IdxSize>>
+                    // we just copied the contents: Vec<IdxSize> to a new buffer
+                    // now, we want to free the outer vec, without freeing
+                    // the inner vecs as they are moved, so we set the len to 0
+                    local_all_vals.set_len(0);
                 },
             );
         });
         unsafe {
             global_all.set_len(cap);
             global_first.set_len(cap);
         }
@@ -278,19 +281,22 @@
     }
 }
 
 /// Every group is indicated by an array where the
 ///  - first value is an index to the start of the group
 ///  - second value is the length of the group
 /// Only used when group values are stored together
+///
+/// This type should have the invariant that it is always sorted in ascending order.
 pub type GroupsSlice = Vec<[IdxSize; 2]>;
 
 #[derive(Debug, Clone, PartialEq, Eq)]
 pub enum GroupsProxy {
     Idx(GroupsIdx),
+    /// Slice is always sorted in ascending order.
     Slice {
         // the groups slices
         groups: GroupsSlice,
         // indicates if we do a rolling groupby
         rolling: bool,
     },
 }
@@ -324,22 +330,27 @@
     pub fn sort(&mut self) {
         match self {
             GroupsProxy::Idx(groups) => {
                 if !groups.is_sorted_flag() {
                     groups.sort()
                 }
             }
-            GroupsProxy::Slice { groups, rolling } => {
-                if !*rolling {
-                    groups.sort_unstable_by_key(|[first, _]| *first);
-                }
+            GroupsProxy::Slice { .. } => {
+                // invariant of the type
             }
         }
     }
 
+    pub(crate) fn is_sorted_flag(&self) -> bool {
+        match self {
+            GroupsProxy::Idx(groups) => groups.is_sorted_flag(),
+            GroupsProxy::Slice { .. } => true,
+        }
+    }
+
     pub fn group_lengths(&self, name: &str) -> IdxCa {
         let ca: NoNull<IdxCa> = match self {
             GroupsProxy::Idx(groups) => groups
                 .iter()
                 .map(|(_, groups)| groups.len() as IdxSize)
                 .collect_trusted(),
             GroupsProxy::Slice { groups, .. } => groups.iter().map(|g| g[1]).collect_trusted(),
@@ -451,14 +462,32 @@
                     let ca: NoNull<IdxCa> = (first..first + len).collect_trusted();
                     ca.into_inner().into_series()
                 })
                 .collect_trusted(),
         }
     }
 
+    pub fn unroll(self) -> GroupsProxy {
+        match self {
+            GroupsProxy::Idx(_) => self,
+            GroupsProxy::Slice { rolling: false, .. } => self,
+            GroupsProxy::Slice { mut groups, .. } => {
+                let mut offset = 0 as IdxSize;
+                for g in groups.iter_mut() {
+                    g[0] = offset;
+                    offset += g[1];
+                }
+                GroupsProxy::Slice {
+                    groups,
+                    rolling: false,
+                }
+            }
+        }
+    }
+
     pub fn slice(&self, offset: i64, len: usize) -> SlicedGroups {
         // Safety:
         // we create new `Vec`s from the sliced groups. But we wrap them in ManuallyDrop
         // so that we never call drop on them.
         // These groups lifetimes are bounded to the `self`. This must remain valid
         // for the scope of the aggregation.
         let sliced = match self {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/hash_join/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/hash_join/mod.rs`

 * *Files 14% similar despite different names*

```diff
@@ -3,14 +3,15 @@
 mod single_keys_dispatch;
 mod single_keys_inner;
 mod single_keys_left;
 mod single_keys_outer;
 #[cfg(feature = "semi_anti_join")]
 mod single_keys_semi_anti;
 pub(super) mod sort_merge;
+mod zip_outer;
 
 use std::fmt::{Debug, Display, Formatter};
 use std::hash::{BuildHasher, Hash, Hasher};
 
 use ahash::RandomState;
 #[cfg(feature = "chunked_ids")]
 use arrow::Either;
@@ -25,14 +26,15 @@
 #[cfg(feature = "asof_join")]
 pub(crate) use single_keys_dispatch::prepare_bytes;
 use single_keys_left::*;
 use single_keys_outer::*;
 #[cfg(feature = "semi_anti_join")]
 use single_keys_semi_anti::*;
 pub use sort_merge::*;
+pub(crate) use zip_outer::*;
 
 #[cfg(feature = "private")]
 pub use self::multiple_keys::private_left_join_multiple_keys;
 use crate::datatypes::PlHashMap;
 use crate::frame::groupby::hashing::HASHMAP_INIT_SIZE;
 pub use crate::frame::hash_join::multiple_keys::{
     _inner_join_multiple_keys, _left_join_multiple_keys, _outer_join_multiple_keys,
@@ -93,14 +95,15 @@
         (a, b, !left_first)
     }};
 }
 
 pub(super) use det_hash_prone_order;
 #[cfg(feature = "performant")]
 use polars_arrow::conversion::primitive_to_vec;
+use polars_utils::hash_to_partition;
 
 use crate::series::IsSorted;
 
 /// If Categorical types are created without a global string cache or under
 /// a different global string cache the mapping will be incorrect.
 #[cfg(feature = "dtype-categorical")]
 pub fn _check_categorical_src(l: &DataType, r: &DataType) -> PolarsResult<()> {
@@ -155,141 +158,26 @@
 }
 
 pub(crate) unsafe fn get_hash_tbl_threaded_join_partitioned<Item>(
     h: u64,
     hash_tables: &[Item],
     len: u64,
 ) -> &Item {
-    for i in 0..len {
-        if this_partition(h, i, len) {
-            return hash_tables.get_unchecked(i as usize);
-        }
-    }
-    unreachable!()
+    let i = hash_to_partition(h, len as usize);
+    hash_tables.get_unchecked(i)
 }
 
 #[allow(clippy::type_complexity)]
 unsafe fn get_hash_tbl_threaded_join_mut_partitioned<T, H>(
     h: u64,
     hash_tables: &mut [HashMap<T, (bool, Vec<IdxSize>), H>],
     len: u64,
 ) -> &mut HashMap<T, (bool, Vec<IdxSize>), H> {
-    for i in 0..len {
-        if this_partition(h, i, len) {
-            return hash_tables.get_unchecked_mut(i as usize);
-        }
-    }
-    unreachable!()
-}
-
-pub trait ZipOuterJoinColumn {
-    fn zip_outer_join_column(
-        &self,
-        _right_column: &Series,
-        _opt_join_tuples: &[(Option<IdxSize>, Option<IdxSize>)],
-    ) -> Series {
-        unimplemented!()
-    }
-}
-
-impl<T> ZipOuterJoinColumn for ChunkedArray<T>
-where
-    T: PolarsIntegerType,
-    ChunkedArray<T>: IntoSeries,
-{
-    fn zip_outer_join_column(
-        &self,
-        right_column: &Series,
-        opt_join_tuples: &[(Option<IdxSize>, Option<IdxSize>)],
-    ) -> Series {
-        let right_ca = self.unpack_series_matching_type(right_column).unwrap();
-
-        let left_rand_access = self.take_rand();
-        let right_rand_access = right_ca.take_rand();
-
-        opt_join_tuples
-            .iter()
-            .map(|(opt_left_idx, opt_right_idx)| {
-                if let Some(left_idx) = opt_left_idx {
-                    unsafe { left_rand_access.get_unchecked(*left_idx as usize) }
-                } else {
-                    unsafe {
-                        let right_idx = opt_right_idx.unwrap_unchecked();
-                        right_rand_access.get_unchecked(right_idx as usize)
-                    }
-                }
-            })
-            .collect_trusted::<ChunkedArray<T>>()
-            .into_series()
-    }
-}
-
-macro_rules! impl_zip_outer_join {
-    ($chunkedtype:ident) => {
-        impl ZipOuterJoinColumn for $chunkedtype {
-            fn zip_outer_join_column(
-                &self,
-                right_column: &Series,
-                opt_join_tuples: &[(Option<IdxSize>, Option<IdxSize>)],
-            ) -> Series {
-                let right_ca = self.unpack_series_matching_type(right_column).unwrap();
-
-                let left_rand_access = self.take_rand();
-                let right_rand_access = right_ca.take_rand();
-
-                opt_join_tuples
-                    .iter()
-                    .map(|(opt_left_idx, opt_right_idx)| {
-                        if let Some(left_idx) = opt_left_idx {
-                            unsafe { left_rand_access.get_unchecked(*left_idx as usize) }
-                        } else {
-                            unsafe {
-                                let right_idx = opt_right_idx.unwrap_unchecked();
-                                right_rand_access.get_unchecked(right_idx as usize)
-                            }
-                        }
-                    })
-                    .collect::<$chunkedtype>()
-                    .into_series()
-            }
-        }
-    };
-}
-impl_zip_outer_join!(BooleanChunked);
-impl_zip_outer_join!(Utf8Chunked);
-impl_zip_outer_join!(BinaryChunked);
-
-impl ZipOuterJoinColumn for Float32Chunked {
-    fn zip_outer_join_column(
-        &self,
-        right_column: &Series,
-        opt_join_tuples: &[(Option<IdxSize>, Option<IdxSize>)],
-    ) -> Series {
-        self.apply_as_ints(|s| {
-            s.zip_outer_join_column(
-                &right_column.bit_repr_small().into_series(),
-                opt_join_tuples,
-            )
-        })
-    }
-}
-
-impl ZipOuterJoinColumn for Float64Chunked {
-    fn zip_outer_join_column(
-        &self,
-        right_column: &Series,
-        opt_join_tuples: &[(Option<IdxSize>, Option<IdxSize>)],
-    ) -> Series {
-        self.apply_as_ints(|s| {
-            s.zip_outer_join_column(
-                &right_column.bit_repr_large().into_series(),
-                opt_join_tuples,
-            )
-        })
-    }
+    let i = hash_to_partition(h, len as usize);
+    hash_tables.get_unchecked_mut(i)
 }
 
 pub fn _join_suffix_name(name: &str, suffix: &str) -> String {
     format!("{name}{suffix}")
 }
 
 /// Utility method to finish a join.
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/hash_join/multiple_keys.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/hash_join/multiple_keys.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/hash_join/single_keys.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/hash_join/single_keys.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/hash_join/single_keys_dispatch.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/hash_join/single_keys_dispatch.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/hash_join/single_keys_inner.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/hash_join/single_keys_inner.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/hash_join/single_keys_left.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/hash_join/single_keys_left.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/hash_join/single_keys_outer.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/hash_join/single_keys_outer.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/hash_join/single_keys_semi_anti.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/hash_join/single_keys_semi_anti.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/hash_join/sort_merge.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/hash_join/sort_merge.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/mod.rs`

 * *Files 0% similar despite different names*

```diff
@@ -33,14 +33,15 @@
 #[cfg(feature = "serde")]
 use serde::{Deserialize, Serialize};
 use smartstring::alias::String as SmartString;
 
 use crate::frame::groupby::GroupsIndicator;
 #[cfg(feature = "row_hash")]
 use crate::hashing::df_rows_to_hashes_threaded_vertical;
+#[cfg(feature = "zip_with")]
 use crate::prelude::min_max_binary::min_max_binary_series;
 use crate::prelude::sort::{argsort_multiple_row_fmt, prepare_arg_sort};
 use crate::series::IsSorted;
 use crate::POOL;
 
 #[derive(Copy, Clone, Debug)]
 pub enum NullStrategy {
@@ -48,22 +49,22 @@
     Propagate,
 }
 
 #[derive(Copy, Clone, Debug, PartialEq, Eq, Default)]
 #[cfg_attr(feature = "serde", derive(Serialize, Deserialize))]
 pub enum UniqueKeepStrategy {
     /// Keep the first unique row.
-    #[default]
     First,
     /// Keep the last unique row.
     Last,
     /// Keep None of the unique rows.
     None,
     /// Keep any of the unique rows
     /// This allows more optimizations
+    #[default]
     Any,
 }
 
 /// A contiguous growable collection of `Series` that have the same length.
 ///
 /// ## Use declarations
 ///
@@ -134,15 +135,14 @@
 ///              "Color" => &["Red", "Yellow", "Green"])?;
 ///
 /// assert_eq!(df["Fruit"], Series::new("Fruit", &["Apple", "Apple", "Pear"]));
 /// assert_eq!(df["Color"], Series::new("Color", &["Red", "Yellow", "Green"]));
 /// # Ok::<(), PolarsError>(())
 /// ```
 #[derive(Clone)]
-#[cfg_attr(feature = "serde", derive(Serialize, Deserialize))]
 pub struct DataFrame {
     pub(crate) columns: Vec<Series>,
 }
 
 impl DataFrame {
     /// Returns an estimation of the total (heap) allocated size of the `DataFrame` in bytes.
     ///
@@ -474,15 +474,15 @@
                 }
                 false
             }
         }
     }
 
     /// Ensure all the chunks in the DataFrame are aligned.
-    pub fn rechunk(&mut self) -> &mut Self {
+    pub fn align_chunks(&mut self) -> &mut Self {
         if self.should_rechunk() {
             self.as_single_chunk_par()
         } else {
             self
         }
     }
 
@@ -815,15 +815,15 @@
         let mut new_cols = self.columns.clone();
         new_cols.extend_from_slice(columns);
         DataFrame::new(new_cols)
     }
 
     /// Concatenate a `DataFrame` to this `DataFrame` and return as newly allocated `DataFrame`.
     ///
-    /// If many `vstack` operations are done, it is recommended to call [`DataFrame::rechunk`].
+    /// If many `vstack` operations are done, it is recommended to call [`DataFrame::align_chunks`].
     ///
     /// # Example
     ///
     /// ```rust
     /// # use polars_core::prelude::*;
     /// let df1: DataFrame = df!("Element" => &["Copper", "Silver", "Gold"],
     ///                          "Melting Point (K)" => &[1357.77, 1234.93, 1337.33])?;
@@ -861,15 +861,15 @@
         let mut df = self.clone();
         df.vstack_mut(other)?;
         Ok(df)
     }
 
     /// Concatenate a DataFrame to this DataFrame
     ///
-    /// If many `vstack` operations are done, it is recommended to call [`DataFrame::rechunk`].
+    /// If many `vstack` operations are done, it is recommended to call [`DataFrame::align_chunks`].
     ///
     /// # Example
     ///
     /// ```rust
     /// # use polars_core::prelude::*;
     /// let mut df1: DataFrame = df!("Element" => &["Copper", "Silver", "Gold"],
     ///                          "Melting Point (K)" => &[1357.77, 1234.93, 1337.33])?;
@@ -945,15 +945,15 @@
     /// and thus will yield faster queries.
     ///
     /// Prefer `extend` over `vstack` when you want to do a query after a single append. For instance during
     /// online operations where you add `n` rows and rerun a query.
     ///
     /// Prefer `vstack` over `extend` when you want to append many times before doing a query. For instance
     /// when you read in multiple files and when to store them in a single `DataFrame`. In the latter case, finish the sequence
-    /// of `append` operations with a [`rechunk`](Self::rechunk).
+    /// of `append` operations with a [`rechunk`](Self::align_chunks).
     pub fn extend(&mut self, other: &DataFrame) -> PolarsResult<()> {
         polars_ensure!(
             self.width() == other.width(),
             ShapeMismatch:
             "unable to extend a dataframe of width {} with a dataframe of width {}",
             self.width(), other.width(),
         );
@@ -1791,16 +1791,42 @@
         &self,
         by_column: Vec<Series>,
         descending: Vec<bool>,
         nulls_last: bool,
         slice: Option<(i64, usize)>,
         parallel: bool,
     ) -> PolarsResult<Self> {
+        // note that the by_column argument also contains evaluated expression from polars-lazy
+        // that may not even be present in this dataframe.
+
+        // therefore when we try to set the first columns as sorted, we ignore the error
+        // as expressions are not present (they are renamed to _POLARS_SORT_COLUMN_i.
+        let first_descending = descending[0];
+        let first_by_column = by_column[0].name().to_string();
+
+        let set_sorted = |df: &mut DataFrame| {
+            // Mark the first sort column as sorted
+            // if the column did not exists it is ok, because we sorted by an expression
+            // not present in the dataframe
+            let _ = df.apply(&first_by_column, |s| {
+                let mut s = s.clone();
+                if first_descending {
+                    s.set_sorted_flag(IsSorted::Descending)
+                } else {
+                    s.set_sorted_flag(IsSorted::Ascending)
+                }
+                s
+            });
+        };
+
         if self.height() == 0 {
-            return Ok(self.clone());
+            let mut out = self.clone();
+            set_sorted(&mut out);
+
+            return Ok(out);
         }
 
         if let Some((0, k)) = slice {
             return self.top_k_impl(k, descending, by_column, nulls_last);
         }
 
         #[cfg(feature = "dtype-struct")]
@@ -1811,21 +1837,14 @@
         #[cfg(not(feature = "dtype-struct"))]
         #[allow(non_upper_case_globals)]
         const has_struct: bool = false;
 
         // a lot of indirection in both sorting and take
         let mut df = self.clone();
         let df = df.as_single_chunk_par();
-        // note that the by_column argument also contains evaluated expression from polars-lazy
-        // that may not even be present in this dataframe.
-
-        // therefore when we try to set the first columns as sorted, we ignore the error
-        // as expressions are not present (they are renamed to _POLARS_SORT_COLUMN_i.
-        let first_descending = descending[0];
-        let first_by_column = by_column[0].name().to_string();
         let mut take = match (by_column.len(), has_struct) {
             (1, false) => {
                 let s = &by_column[0];
                 let options = SortOptions {
                     descending: descending[0],
                     nulls_last,
                     multithreaded: parallel,
@@ -1843,39 +1862,33 @@
                 }
                 s.arg_sort(options)
             }
             _ => {
                 if nulls_last || has_struct || std::env::var("POLARS_ROW_FMT_SORT").is_ok() {
                     argsort_multiple_row_fmt(&by_column, descending, nulls_last, parallel)?
                 } else {
-                    let (first, by_column, descending) = prepare_arg_sort(by_column, descending)?;
-                    first.arg_sort_multiple(&by_column, &descending)?
+                    let (first, other, descending) = prepare_arg_sort(by_column, descending)?;
+                    let options = SortMultipleOptions {
+                        other,
+                        descending,
+                        multithreaded: parallel,
+                    };
+                    first.arg_sort_multiple(&options)?
                 }
             }
         };
 
         if let Some((offset, len)) = slice {
             take = take.slice(offset, len);
         }
 
         // Safety:
         // the created indices are in bounds
         let mut df = unsafe { df.take_unchecked_impl(&take, parallel) };
-        // Mark the first sort column as sorted
-        // if the column did not exists it is ok, because we sorted by an expression
-        // not present in the dataframe
-        let _ = df.apply(&first_by_column, |s| {
-            let mut s = s.clone();
-            if first_descending {
-                s.set_sorted_flag(IsSorted::Descending)
-            } else {
-                s.set_sorted_flag(IsSorted::Ascending)
-            }
-            s
-        });
+        set_sorted(&mut df);
         Ok(df)
     }
 
     /// Return a sorted clone of this `DataFrame`.
     ///
     /// # Example
     ///
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/row/av_buffer.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/row/av_buffer.rs`

 * *Files 1% similar despite different names*

```diff
@@ -122,16 +122,18 @@
         };
         Some(())
     }
 
     pub(crate) fn add_fallible(&mut self, val: &AnyValue<'a>) -> PolarsResult<()> {
         self.add(val.clone()).ok_or_else(|| {
             polars_err!(
-                ComputeError: "could not append {:?} to the builder; make sure that all rows \
-                have the same schema or consider increasing `schema_inference_length`"
+                ComputeError: "could not append value: {} of type: {} to the builder; make sure that all rows \
+                have the same schema or consider increasing `schema_inference_length`\n\
+                \n\
+                it might also be that a value overflows the data-type's capacity", val, val.dtype()
             )
         })
     }
 
     pub fn reset(&mut self, capacity: usize) -> Series {
         use AnyValueBuffer::*;
         match self {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/row/dataframe.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/row/dataframe.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/row/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/row/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/row/transpose.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/row/transpose.rs`

 * *Files 4% similar despite different names*

```diff
@@ -64,14 +64,38 @@
     /// Transpose a DataFrame. This is a very expensive operation.
     pub fn transpose(&self) -> PolarsResult<DataFrame> {
         polars_ensure!(
             self.height() != 0 && self.width() != 0,
             NoData: "unable to transpose an empty dataframe"
         );
         let dtype = self.get_supertype().unwrap()?;
+        match dtype {
+            #[cfg(feature = "dtype-categorical")]
+            DataType::Categorical(_) => {
+                let mut valid = true;
+                let mut cache_id = None;
+                for s in self.columns.iter() {
+                    if let DataType::Categorical(Some(rev_map)) = &s.dtype() {
+                        match &**rev_map {
+                            RevMapping::Local(_) => valid = false,
+                            RevMapping::Global(_, _, id) => {
+                                if let Some(cache_id) = cache_id {
+                                    if cache_id != *id {
+                                        valid = false;
+                                    }
+                                }
+                                cache_id = Some(*id);
+                            }
+                        }
+                    }
+                }
+                polars_ensure!(valid, ComputeError: "'transpose' of categorical can only be done if all are from the same global string cache")
+            }
+            _ => {}
+        }
         self.transpose_from_dtype(&dtype)
     }
 }
 
 #[inline]
 unsafe fn add_value<T: NumericNative>(
     values_buf_ptr: usize,
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/top_k.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/top_k.rs`

 * *Files 16% similar despite different names*

```diff
@@ -6,14 +6,15 @@
 use smartstring::alias::String as SmartString;
 
 use crate::datatypes::IdxCa;
 use crate::frame::DataFrame;
 use crate::prelude::sort::_broadcast_descending;
 use crate::prelude::sort::arg_sort_multiple::_get_rows_encoded;
 use crate::prelude::*;
+use crate::series::IsSorted;
 use crate::utils::NoNull;
 
 #[derive(Eq)]
 struct CompareRow<'a> {
     idx: IdxSize,
     bytes: &'a [u8],
 }
@@ -71,10 +72,27 @@
             let (lower, _el, _upper) = rows.select_nth_unstable(k);
             lower.sort_unstable();
             &*lower
         };
 
         let idx: NoNull<IdxCa> = sorted.iter().map(|cmp_row| cmp_row.idx).collect();
 
-        unsafe { Ok(self.take_unchecked(&idx.into_inner())) }
+        let mut df = unsafe { self.take_unchecked(&idx.into_inner()) };
+
+        let first_descending = descending[0];
+        let first_by_column = by_column[0].name().to_string();
+
+        // Mark the first sort column as sorted
+        // if the column did not exists it is ok, because we sorted by an expression
+        // not present in the dataframe
+        let _ = df.apply(&first_by_column, |s| {
+            let mut s = s.clone();
+            if first_descending {
+                s.set_sorted_flag(IsSorted::Descending)
+            } else {
+                s.set_sorted_flag(IsSorted::Ascending)
+            }
+            s
+        });
+        Ok(df)
     }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/frame/upstream_traits.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/frame/upstream_traits.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/functions.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/functions.rs`

 * *Files 8% similar despite different names*

```diff
@@ -8,15 +8,14 @@
 use ahash::AHashSet;
 use arrow::compute;
 use arrow::types::simd::Simd;
 use num_traits::{Float, NumCast, ToPrimitive};
 #[cfg(feature = "concat_str")]
 use polars_arrow::prelude::ValueSize;
 
-use crate::chunked_array::ops::sort::prepare_arg_sort;
 use crate::prelude::*;
 use crate::utils::coalesce_nulls;
 #[cfg(feature = "diagonal_concat")]
 use crate::utils::concat_df;
 
 /// Compute the covariance between two columns.
 pub fn cov_f<T>(a: &ChunkedArray<T>, b: &ChunkedArray<T>) -> Option<T::Native>
@@ -89,28 +88,14 @@
     let (a, b) = coalesce_nulls(a, b);
     let a = a.as_ref();
     let b = b.as_ref();
 
     Some(cov_f(a, b)? / (a.std(ddof)? * b.std(ddof)?))
 }
 
-/// Find the indexes that would sort these series in order of appearance.
-/// That means that the first `Series` will be used to determine the ordering
-/// until duplicates are found. Once duplicates are found, the next `Series` will
-/// be used and so on.
-pub fn arg_sort_by(by: &[Series], descending: &[bool]) -> PolarsResult<IdxCa> {
-    polars_ensure!(
-        by.len() == descending.len(),
-        ComputeError: "the number of ordering booleans: {} does not match the number of series: {}",
-        descending.len(), by.len()
-    );
-    let (first, by, descending) = prepare_arg_sort(by.to_vec(), descending.to_vec()).unwrap();
-    first.arg_sort_multiple(&by, &descending)
-}
-
 // utility to be able to also add literals to concat_str function
 #[cfg(feature = "concat_str")]
 enum IterBroadCast<'a> {
     Column(Box<dyn PolarsIterator<Item = Option<&'a str>> + 'a>),
     Value(Option<&'a str>),
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/hashing/fx.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/hashing/fx.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/hashing/identity.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/hashing/identity.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/hashing/partition.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/hashing/partition.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/hashing/vector_hasher.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/hashing/vector_hasher.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/lib.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/lib.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/named_from.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/named_from.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/prelude.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/prelude.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/schema.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/schema.rs`

 * *Files 2% similar despite different names*

```diff
@@ -379,28 +379,42 @@
         self.inner.into_iter()
     }
 }
 
 /// This trait exists to be unify the API of polars Schema and arrows Schema
 #[cfg(feature = "private")]
 pub trait IndexOfSchema: Debug {
-    /// Get the index of column by name.
+    /// Get the index of a column by name.
     fn index_of(&self, name: &str) -> Option<usize>;
 
+    /// Get a vector of all column names.
+    fn get_names(&self) -> Vec<&str>;
+
     fn try_index_of(&self, name: &str) -> PolarsResult<usize> {
         self.index_of(name).ok_or_else(|| {
-            polars_err!(SchemaMismatch: "unable to get field '{}' from schema: {:?}", name, self)
+            polars_err!(
+                ColumnNotFound:
+                "unable to find column {:?}; valid columns: {:?}", name, self.get_names(),
+            )
         })
     }
 }
 
 impl IndexOfSchema for Schema {
     fn index_of(&self, name: &str) -> Option<usize> {
         self.inner.get_index_of(name)
     }
+
+    fn get_names(&self) -> Vec<&str> {
+        self.iter_names().map(|name| name.as_str()).collect()
+    }
 }
 
 impl IndexOfSchema for ArrowSchema {
     fn index_of(&self, name: &str) -> Option<usize> {
         self.fields.iter().position(|f| f.name == name)
     }
+
+    fn get_names(&self) -> Vec<&str> {
+        self.fields.iter().map(|f| f.name.as_str()).collect()
+    }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/serde/chunked_array.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/serde/chunked_array.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/serde/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/serde/mod.rs`

 * *Files 0% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 use serde::{Deserialize, Serialize};
 
 use crate::prelude::*;
 
 pub mod chunked_array;
+mod df;
 pub mod series;
 
 /// Intermediate enum. Needed because [crate::datatypes::DataType] has
 /// a &static str and thus requires Deserialize<&static>
 #[derive(Serialize, Deserialize, Debug)]
 enum DeDataType<'a> {
     Boolean,
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/serde/series.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/serde/series.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/any_value.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/any_value.rs`

 * *Files 1% similar despite different names*

```diff
@@ -305,17 +305,15 @@
                             &field_avs,
                             &field.dtype,
                             strict,
                         )?
                     };
                     series_fields.push(s)
                 }
-                return Ok(StructChunked::new(name, &series_fields)
-                    .unwrap()
-                    .into_series());
+                return StructChunked::new(name, &series_fields).map(|ca| ca.into_series());
             }
             #[cfg(feature = "object")]
             DataType::Object(_) => {
                 use crate::chunked_array::object::registry;
                 let converter = registry::get_object_converter();
                 let mut builder = registry::get_object_builder(name, av.len());
                 for av in av {
@@ -408,14 +406,16 @@
             Float64(_) => DataType::Float64,
             #[cfg(feature = "dtype-date")]
             Date(_) => DataType::Date,
             #[cfg(feature = "dtype-datetime")]
             Datetime(_, tu, tz) => DataType::Datetime(*tu, (*tz).clone()),
             #[cfg(feature = "dtype-time")]
             Time(_) => DataType::Time,
+            #[cfg(feature = "dtype-array")]
+            Array(s, size) => DataType::Array(Box::new(s.dtype().clone()), *size),
             List(s) => DataType::List(Box::new(s.dtype().clone())),
             #[cfg(feature = "dtype-struct")]
             StructOwned(payload) => DataType::Struct(payload.1.to_vec()),
             #[cfg(feature = "dtype-struct")]
             Struct(_, _, flds) => DataType::Struct(flds.to_vec()),
             #[cfg(feature = "dtype-duration")]
             Duration(_, tu) => DataType::Duration(*tu),
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/arithmetic/borrowed.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/arithmetic/borrowed.rs`

 * *Files 1% similar despite different names*

```diff
@@ -101,14 +101,22 @@
     fn add_to(lhs: &BinaryChunked, rhs: &Series) -> PolarsResult<Series> {
         let rhs = lhs.unpack_series_matching_type(rhs)?;
         let out = lhs + rhs;
         Ok(out.into_series())
     }
 }
 
+impl NumOpsDispatchInner for BooleanType {
+    fn add_to(lhs: &BooleanChunked, rhs: &Series) -> PolarsResult<Series> {
+        let rhs = lhs.unpack_series_matching_type(rhs)?;
+        let out = lhs + rhs;
+        Ok(out.into_series())
+    }
+}
+
 #[cfg(feature = "checked_arithmetic")]
 pub mod checked {
     use num_traits::{CheckedDiv, One, ToPrimitive, Zero};
 
     use super::*;
     use crate::utils::align_chunks_binary;
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/arithmetic/owned.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/arithmetic/owned.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/comparison.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/comparison.rs`

 * *Files 25% similar despite different names*

```diff
@@ -152,23 +152,79 @@
                     polars_bail!(
                         ComputeError:
                         "cannot compare categoricals originating from different sources; \
                         consider setting a global string cache"
                     );
                 }
             }
-            (Null, Null, _, _) => BooleanChunked::full(self.name(), true, self.len()),
+            (Null, Null, _, _) => BooleanChunked::full_null(self.name(), self.len()),
             _ => {
                 impl_compare!(self, rhs, equal)
             }
         };
         out.rename(self.name());
         Ok(out)
     }
 
+    /// Create a boolean mask by checking for equality.
+    fn equal_missing(&self, rhs: &Series) -> PolarsResult<BooleanChunked> {
+        validate_types(self.dtype(), rhs.dtype())?;
+        use DataType::*;
+        let mut out = match (self.dtype(), rhs.dtype(), self.len(), rhs.len()) {
+            #[cfg(feature = "dtype-categorical")]
+            (Categorical(_), Utf8, _, 1) => {
+                return compare_cat_to_str_series(
+                    self,
+                    rhs,
+                    self.name(),
+                    |s, idx| s.equal_missing(idx),
+                    false,
+                );
+            }
+            #[cfg(feature = "dtype-categorical")]
+            (Utf8, Categorical(_), 1, _) => {
+                return compare_cat_to_str_series(
+                    rhs,
+                    self,
+                    self.name(),
+                    |s, idx| s.equal_missing(idx),
+                    false,
+                );
+            }
+            #[cfg(feature = "dtype-categorical")]
+            (Categorical(Some(rev_map_l)), Categorical(Some(rev_map_r)), _, _) => {
+                if rev_map_l.same_src(rev_map_r) {
+                    let rhs = rhs.categorical().unwrap().logical();
+
+                    // first check the rev-map
+                    if rhs.len() == 1 && rhs.null_count() == 0 {
+                        let rhs = rhs.get(0).unwrap();
+                        if rev_map_l.get_optional(rhs).is_none() {
+                            return Ok(BooleanChunked::full(self.name(), false, self.len()));
+                        }
+                    }
+
+                    self.categorical().unwrap().logical().equal_missing(rhs)
+                } else {
+                    polars_bail!(
+                        ComputeError:
+                        "cannot compare categoricals originating from different sources; \
+                        consider setting a global string cache"
+                    );
+                }
+            }
+            (Null, Null, _, _) => BooleanChunked::full(self.name(), true, self.len()),
+            _ => {
+                impl_compare!(self, rhs, equal_missing)
+            }
+        };
+        out.rename(self.name());
+        Ok(out)
+    }
+
     /// Create a boolean mask by checking for inequality.
     fn not_equal(&self, rhs: &Series) -> PolarsResult<BooleanChunked> {
         validate_types(self.dtype(), rhs.dtype())?;
         use DataType::*;
         let mut out = match (self.dtype(), rhs.dtype(), self.len(), rhs.len()) {
             #[cfg(feature = "dtype-categorical")]
             (Categorical(_), Utf8, _, 1) => {
@@ -208,23 +264,79 @@
                     polars_bail!(
                         ComputeError:
                         "cannot compare categoricals originating from different sources; \
                         consider setting a global string cache"
                     );
                 }
             }
-            (Null, Null, _, _) => BooleanChunked::full(self.name(), false, self.len()),
+            (Null, Null, _, _) => BooleanChunked::full_null(self.name(), self.len()),
             _ => {
                 impl_compare!(self, rhs, not_equal)
             }
         };
         out.rename(self.name());
         Ok(out)
     }
 
+    /// Create a boolean mask by checking for inequality.
+    fn not_equal_missing(&self, rhs: &Series) -> PolarsResult<BooleanChunked> {
+        validate_types(self.dtype(), rhs.dtype())?;
+        use DataType::*;
+        let mut out = match (self.dtype(), rhs.dtype(), self.len(), rhs.len()) {
+            #[cfg(feature = "dtype-categorical")]
+            (Categorical(_), Utf8, _, 1) => {
+                return compare_cat_to_str_series(
+                    self,
+                    rhs,
+                    self.name(),
+                    |s, idx| s.not_equal_missing(idx),
+                    true,
+                );
+            }
+            #[cfg(feature = "dtype-categorical")]
+            (Utf8, Categorical(_), 1, _) => {
+                return compare_cat_to_str_series(
+                    rhs,
+                    self,
+                    self.name(),
+                    |s, idx| s.not_equal_missing(idx),
+                    true,
+                );
+            }
+            #[cfg(feature = "dtype-categorical")]
+            (Categorical(Some(rev_map_l)), Categorical(Some(rev_map_r)), _, _) => {
+                if rev_map_l.same_src(rev_map_r) {
+                    let rhs = rhs.categorical().unwrap().logical();
+
+                    // first check the rev-map
+                    if rhs.len() == 1 && rhs.null_count() == 0 {
+                        let rhs = rhs.get(0).unwrap();
+                        if rev_map_l.get_optional(rhs).is_none() {
+                            return Ok(BooleanChunked::full(self.name(), true, self.len()));
+                        }
+                    }
+
+                    self.categorical().unwrap().logical().not_equal_missing(rhs)
+                } else {
+                    polars_bail!(
+                        ComputeError:
+                        "cannot compare categoricals originating from different sources; \
+                        consider setting a global string cache"
+                    );
+                }
+            }
+            (Null, Null, _, _) => BooleanChunked::full(self.name(), false, self.len()),
+            _ => {
+                impl_compare!(self, rhs, not_equal_missing)
+            }
+        };
+        out.rename(self.name());
+        Ok(out)
+    }
+
     /// Create a boolean mask by checking if self > rhs.
     fn gt(&self, rhs: &Series) -> PolarsResult<BooleanChunked> {
         validate_types(self.dtype(), rhs.dtype())?;
         let mut out = impl_compare!(self, rhs, gt);
         out.rename(self.name());
         Ok(out)
     }
@@ -262,20 +374,32 @@
 
     fn equal(&self, rhs: Rhs) -> PolarsResult<BooleanChunked> {
         validate_types(self.dtype(), &DataType::Int8)?;
         let s = self.to_physical_repr();
         Ok(apply_method_physical_numeric!(&s, equal, rhs))
     }
 
+    fn equal_missing(&self, rhs: Rhs) -> Self::Item {
+        validate_types(self.dtype(), &DataType::Int8)?;
+        let s = self.to_physical_repr();
+        Ok(apply_method_physical_numeric!(&s, equal_missing, rhs))
+    }
+
     fn not_equal(&self, rhs: Rhs) -> PolarsResult<BooleanChunked> {
         validate_types(self.dtype(), &DataType::Int8)?;
         let s = self.to_physical_repr();
         Ok(apply_method_physical_numeric!(&s, not_equal, rhs))
     }
 
+    fn not_equal_missing(&self, rhs: Rhs) -> Self::Item {
+        validate_types(self.dtype(), &DataType::Int8)?;
+        let s = self.to_physical_repr();
+        Ok(apply_method_physical_numeric!(&s, not_equal_missing, rhs))
+    }
+
     fn gt(&self, rhs: Rhs) -> PolarsResult<BooleanChunked> {
         validate_types(self.dtype(), &DataType::Int8)?;
         let s = self.to_physical_repr();
         Ok(apply_method_physical_numeric!(&s, gt, rhs))
     }
 
     fn gt_eq(&self, rhs: Rhs) -> PolarsResult<BooleanChunked> {
@@ -322,14 +446,31 @@
             Categorical(_) => {
                 compare_cat_to_str_value(self, rhs, self.name(), |lhs, idx| lhs.equal(idx), false)
             }
             _ => Ok(BooleanChunked::full(self.name(), false, self.len())),
         }
     }
 
+    fn equal_missing(&self, rhs: &str) -> Self::Item {
+        validate_types(self.dtype(), &DataType::Utf8)?;
+        use DataType::*;
+        match self.dtype() {
+            Utf8 => Ok(self.utf8().unwrap().equal(rhs)),
+            #[cfg(feature = "dtype-categorical")]
+            Categorical(_) => compare_cat_to_str_value(
+                self,
+                rhs,
+                self.name(),
+                |lhs, idx| lhs.equal_missing(idx),
+                false,
+            ),
+            _ => Ok(BooleanChunked::full(self.name(), false, self.len())),
+        }
+    }
+
     fn not_equal(&self, rhs: &str) -> PolarsResult<BooleanChunked> {
         validate_types(self.dtype(), &DataType::Utf8)?;
         use DataType::*;
         match self.dtype() {
             Utf8 => Ok(self.utf8().unwrap().not_equal(rhs)),
             #[cfg(feature = "dtype-categorical")]
             Categorical(_) => compare_cat_to_str_value(
@@ -339,14 +480,31 @@
                 |lhs, idx| lhs.not_equal(idx),
                 true,
             ),
             _ => Ok(BooleanChunked::full(self.name(), false, self.len())),
         }
     }
 
+    fn not_equal_missing(&self, rhs: &str) -> Self::Item {
+        validate_types(self.dtype(), &DataType::Utf8)?;
+        use DataType::*;
+        match self.dtype() {
+            Utf8 => Ok(self.utf8().unwrap().not_equal(rhs)),
+            #[cfg(feature = "dtype-categorical")]
+            Categorical(_) => compare_cat_to_str_value(
+                self,
+                rhs,
+                self.name(),
+                |lhs, idx| lhs.not_equal_missing(idx),
+                true,
+            ),
+            _ => Ok(BooleanChunked::full(self.name(), false, self.len())),
+        }
+    }
+
     fn gt(&self, rhs: &str) -> PolarsResult<BooleanChunked> {
         compare_series_str(self, rhs, |lhs, rhs| lhs.gt(rhs))
     }
 
     fn gt_eq(&self, rhs: &str) -> PolarsResult<BooleanChunked> {
         compare_series_str(self, rhs, |lhs, rhs| lhs.gt_eq(rhs))
     }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/from.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/from.rs`

 * *Files 2% similar despite different names*

```diff
@@ -67,23 +67,32 @@
             #[cfg(feature = "dtype-decimal")]
             Decimal(precision, scale) => Int128Chunked::from_chunks(name, chunks)
                 .into_decimal_unchecked(
                     *precision,
                     scale.unwrap_or_else(|| unreachable!("scale should be set")),
                 )
                 .into_series(),
+            #[cfg(feature = "dtype-array")]
+            Array(_, _) => {
+                ArrayChunked::from_chunks_and_dtype_unchecked(name, chunks, dtype.clone())
+                    .into_series()
+            }
             List(_) => ListChunked::from_chunks_and_dtype_unchecked(name, chunks, dtype.clone())
                 .into_series(),
             Utf8 => Utf8Chunked::from_chunks(name, chunks).into_series(),
             Binary => BinaryChunked::from_chunks(name, chunks).into_series(),
             #[cfg(feature = "dtype-categorical")]
             Categorical(rev_map) => {
                 let cats = UInt32Chunked::from_chunks(name, chunks);
-                CategoricalChunked::from_cats_and_rev_map_unchecked(cats, rev_map.clone().unwrap())
-                    .into_series()
+                let mut ca = CategoricalChunked::from_cats_and_rev_map_unchecked(
+                    cats,
+                    rev_map.clone().unwrap(),
+                );
+                ca.set_fast_unique(false);
+                ca.into_series()
             }
             Boolean => BooleanChunked::from_chunks(name, chunks).into_series(),
             Float32 => Float32Chunked::from_chunks(name, chunks).into_series(),
             Float64 => Float64Chunked::from_chunks(name, chunks).into_series(),
             #[cfg(feature = "dtype-struct")]
             Struct(_) => Series::try_from_arrow_unchecked(name, chunks, &dtype.to_arrow()).unwrap(),
             #[cfg(feature = "object")]
@@ -132,14 +141,19 @@
                 let chunks = cast_chunks(&chunks, &DataType::Binary, false).unwrap();
                 Ok(BinaryChunked::from_chunks(name, chunks).into_series())
             }
             ArrowDataType::List(_) | ArrowDataType::LargeList(_) => {
                 let chunks = chunks.iter().map(convert_inner_types).collect();
                 Ok(ListChunked::from_chunks(name, chunks).into_series())
             }
+            #[cfg(feature = "dtype-array")]
+            ArrowDataType::FixedSizeList(_, _) => {
+                let chunks = chunks.iter().map(convert_inner_types).collect();
+                Ok(ArrayChunked::from_chunks(name, chunks).into_series())
+            }
             ArrowDataType::Boolean => Ok(BooleanChunked::from_chunks(name, chunks).into_series()),
             #[cfg(feature = "dtype-u8")]
             ArrowDataType::UInt8 => Ok(UInt8Chunked::from_chunks(name, chunks).into_series()),
             #[cfg(feature = "dtype-u16")]
             ArrowDataType::UInt16 => Ok(UInt16Chunked::from_chunks(name, chunks).into_series()),
             ArrowDataType::UInt32 => Ok(UInt32Chunked::from_chunks(name, chunks).into_series()),
             ArrowDataType::UInt64 => Ok(UInt64Chunked::from_chunks(name, chunks).into_series()),
@@ -278,15 +292,17 @@
                     ),
                 };
                 let keys = keys.as_any().downcast_ref::<PrimitiveArray<u32>>().unwrap();
                 let values = values.as_any().downcast_ref::<Utf8Array<i64>>().unwrap();
 
                 // Safety
                 // the invariants of an Arrow Dictionary guarantee the keys are in bounds
-                Ok(CategoricalChunked::from_keys_and_values(name, keys, values).into_series())
+                let mut ca = CategoricalChunked::from_keys_and_values(name, keys, values);
+                ca.set_fast_unique(false);
+                Ok(ca.into_series())
             }
             #[cfg(feature = "object")]
             ArrowDataType::Extension(s, _, Some(_)) if s == EXTENSION_NAME => {
                 assert_eq!(chunks.len(), 1);
                 let arr = chunks[0]
                     .as_any()
                     .downcast_ref::<FixedSizeBinaryArray>()
@@ -452,14 +468,25 @@
             let arr = arr.as_any().downcast_ref::<Utf8Array<i32>>().unwrap();
             Box::from(utf8_to_large_utf8(arr))
         }
         ArrowDataType::List(field) => {
             let out = cast(&**arr, &ArrowDataType::LargeList(field.clone())).unwrap();
             convert_inner_types(&out)
         }
+        #[cfg(feature = "dtype-array")]
+        ArrowDataType::FixedSizeList(_, size) => {
+            let arr = arr.as_any().downcast_ref::<FixedSizeListArray>().unwrap();
+            let values = convert_inner_types(arr.values());
+            let dtype = FixedSizeListArray::default_datatype(values.data_type().clone(), *size);
+            Box::from(FixedSizeListArray::new(
+                dtype,
+                values,
+                arr.validity().cloned(),
+            ))
+        }
         ArrowDataType::FixedSizeBinary(_) | ArrowDataType::Binary => {
             let out = cast(&**arr, &ArrowDataType::LargeBinary).unwrap();
             convert_inner_types(&out)
         }
         ArrowDataType::LargeList(_) => {
             let arr = arr.as_any().downcast_ref::<ListArray<i64>>().unwrap();
             let values = convert_inner_types(arr.values());
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/binary.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/binary.rs`

 * *Files 8% similar despite different names*

```diff
@@ -83,16 +83,16 @@
     fn remainder(&self, rhs: &Series) -> PolarsResult<Series> {
         NumOpsDispatch::remainder(&self.0, rhs)
     }
     fn group_tuples(&self, multithreaded: bool, sorted: bool) -> PolarsResult<GroupsProxy> {
         IntoGroupsProxy::group_tuples(&self.0, multithreaded, sorted)
     }
 
-    fn arg_sort_multiple(&self, by: &[Series], descending: &[bool]) -> PolarsResult<IdxCa> {
-        self.0.arg_sort_multiple(by, descending)
+    fn arg_sort_multiple(&self, options: &SortMultipleOptions) -> PolarsResult<IdxCa> {
+        self.0.arg_sort_multiple(options)
     }
 }
 
 impl SeriesTrait for SeriesWrap<BinaryChunked> {
     fn is_sorted_flag(&self) -> IsSorted {
         if self.0.is_sorted_ascending_flag() {
             IsSorted::Ascending
@@ -161,18 +161,14 @@
         Ok(ChunkTake::take(&self.0, (&*indices).into())?.into_series())
     }
 
     fn take_iter(&self, iter: &mut dyn TakeIterator) -> PolarsResult<Series> {
         Ok(ChunkTake::take(&self.0, iter.into())?.into_series())
     }
 
-    fn take_every(&self, n: usize) -> Series {
-        self.0.take_every(n).into_series()
-    }
-
     unsafe fn take_iter_unchecked(&self, iter: &mut dyn TakeIterator) -> Series {
         ChunkTake::take_unchecked(&self.0, iter.into()).into_series()
     }
 
     unsafe fn take_unchecked(&self, idx: &IdxCa) -> PolarsResult<Series> {
         let idx = if idx.chunks.len() > 1 {
             Cow::Owned(idx.rechunk())
@@ -181,15 +177,15 @@
         };
 
         let mut out = ChunkTake::take_unchecked(&self.0, (&*idx).into());
 
         if self.0.is_sorted_ascending_flag()
             && (idx.is_sorted_ascending_flag() || idx.is_sorted_descending_flag())
         {
-            out.set_sorted_flag(idx.is_sorted_flag2())
+            out.set_sorted_flag(idx.is_sorted_flag())
         }
 
         Ok(out.into_series())
     }
 
     unsafe fn take_opt_iter_unchecked(&self, iter: &mut dyn TakeIteratorNulls) -> Series {
         ChunkTake::take_unchecked(&self.0, iter.into()).into_series()
@@ -288,15 +284,15 @@
     }
 
     #[cfg(feature = "is_in")]
     fn is_in(&self, other: &Series) -> PolarsResult<BooleanChunked> {
         IsIn::is_in(&self.0, other)
     }
     #[cfg(feature = "repeat_by")]
-    fn repeat_by(&self, by: &IdxCa) -> ListChunked {
+    fn repeat_by(&self, by: &IdxCa) -> PolarsResult<ListChunked> {
         RepeatBy::repeat_by(&self.0, by)
     }
 
     #[cfg(feature = "mode")]
     fn mode(&self) -> PolarsResult<Series> {
         Ok(self.0.mode()?.into_series())
     }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/boolean.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/boolean.rs`

 * *Files 2% similar despite different names*

```diff
@@ -94,16 +94,19 @@
     ) -> Series {
         ZipOuterJoinColumn::zip_outer_join_column(&self.0, right_column, opt_join_tuples)
     }
     fn group_tuples(&self, multithreaded: bool, sorted: bool) -> PolarsResult<GroupsProxy> {
         IntoGroupsProxy::group_tuples(&self.0, multithreaded, sorted)
     }
 
-    fn arg_sort_multiple(&self, by: &[Series], descending: &[bool]) -> PolarsResult<IdxCa> {
-        self.0.arg_sort_multiple(by, descending)
+    fn arg_sort_multiple(&self, options: &SortMultipleOptions) -> PolarsResult<IdxCa> {
+        self.0.arg_sort_multiple(options)
+    }
+    fn add_to(&self, rhs: &Series) -> PolarsResult<Series> {
+        NumOpsDispatch::add_to(&self.0, rhs)
     }
 }
 
 impl SeriesTrait for SeriesWrap<BooleanChunked> {
     fn is_sorted_flag(&self) -> IsSorted {
         if self.0.is_sorted_ascending_flag() {
             IsSorted::Ascending
@@ -190,18 +193,14 @@
         Ok(ChunkTake::take(&self.0, (&*indices).into())?.into_series())
     }
 
     fn take_iter(&self, iter: &mut dyn TakeIterator) -> PolarsResult<Series> {
         Ok(ChunkTake::take(&self.0, iter.into())?.into_series())
     }
 
-    fn take_every(&self, n: usize) -> Series {
-        self.0.take_every(n).into_series()
-    }
-
     unsafe fn take_iter_unchecked(&self, iter: &mut dyn TakeIterator) -> Series {
         ChunkTake::take_unchecked(&self.0, iter.into()).into_series()
     }
 
     unsafe fn take_unchecked(&self, idx: &IdxCa) -> PolarsResult<Series> {
         let idx = if idx.chunks.len() > 1 {
             Cow::Owned(idx.rechunk())
@@ -340,15 +339,15 @@
     }
 
     #[cfg(feature = "is_in")]
     fn is_in(&self, other: &Series) -> PolarsResult<BooleanChunked> {
         IsIn::is_in(&self.0, other)
     }
     #[cfg(feature = "repeat_by")]
-    fn repeat_by(&self, by: &IdxCa) -> ListChunked {
+    fn repeat_by(&self, by: &IdxCa) -> PolarsResult<ListChunked> {
         RepeatBy::repeat_by(&self.0, by)
     }
 
     #[cfg(feature = "mode")]
     fn mode(&self) -> PolarsResult<Series> {
         Ok(self.0.mode()?.into_series())
     }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/categorical.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/dates_time.rs`

 * *Files 25% similar despite different names*

```diff
@@ -1,403 +1,499 @@
+//! This module exists to reduce compilation times.
+//!
+//! All the data types are backed by a physical type in memory e.g. Date -> i32, Datetime-> i64.
+//!
+//! Series lead to code implementations of all traits. Whereas there are a lot of duplicates due to
+//! data types being backed by the same physical type. In this module we reduce compile times by
+//! opting for a little more run time cost. We cast to the physical type -> apply the operation and
+//! (depending on the result) cast back to the original type
+//!
 use std::borrow::Cow;
+use std::ops::{Deref, DerefMut};
 
 use ahash::RandomState;
 use polars_arrow::prelude::QuantileInterpolOptions;
 
-use super::{private, IntoSeries, SeriesTrait, *};
-use crate::chunked_array::comparison::*;
-use crate::chunked_array::ops::compare_inner::{IntoPartialOrdInner, PartialOrdInner};
+use super::{private, IntoSeries, SeriesTrait, SeriesWrap, *};
 use crate::chunked_array::ops::explode::ExplodeByOffsets;
+use crate::chunked_array::ops::ToBitRepr;
 use crate::chunked_array::AsSinglePtr;
 use crate::frame::groupby::*;
-use crate::frame::hash_join::ZipOuterJoinColumn;
-#[cfg(feature = "is_in")]
-use crate::frame::hash_join::_check_categorical_src;
+use crate::frame::hash_join::*;
 use crate::prelude::*;
-use crate::series::implementations::SeriesWrap;
 
-unsafe impl IntoSeries for CategoricalChunked {
-    fn into_series(self) -> Series {
-        Series(Arc::new(SeriesWrap(self)))
-    }
-}
-
-impl SeriesWrap<CategoricalChunked> {
-    fn finish_with_state(&self, keep_fast_unique: bool, cats: UInt32Chunked) -> CategoricalChunked {
-        let mut out = unsafe {
-            CategoricalChunked::from_cats_and_rev_map_unchecked(cats, self.0.get_rev_map().clone())
-        };
-        if keep_fast_unique && self.0.can_fast_unique() {
-            out.set_fast_unique(true)
+macro_rules! impl_dyn_series {
+    ($ca: ident, $into_logical: ident) => {
+        unsafe impl IntoSeries for $ca {
+            fn into_series(self) -> Series {
+                Series(Arc::new(SeriesWrap(self)))
+            }
         }
-        out.set_lexical_sorted(self.0.use_lexical_sort());
-        out
-    }
-
-    fn with_state<F>(&self, keep_fast_unique: bool, apply: F) -> CategoricalChunked
-    where
-        F: Fn(&UInt32Chunked) -> UInt32Chunked,
-    {
-        let cats = apply(self.0.logical());
-        self.finish_with_state(keep_fast_unique, cats)
-    }
-
-    fn try_with_state<'a, F>(
-        &'a self,
-        keep_fast_unique: bool,
-        apply: F,
-    ) -> PolarsResult<CategoricalChunked>
-    where
-        F: for<'b> Fn(&'a UInt32Chunked) -> PolarsResult<UInt32Chunked>,
-    {
-        let cats = apply(self.0.logical())?;
-        Ok(self.finish_with_state(keep_fast_unique, cats))
-    }
-}
 
-impl private::PrivateSeries for SeriesWrap<CategoricalChunked> {
-    fn compute_len(&mut self) {
-        self.0.logical_mut().compute_len()
-    }
-    fn _field(&self) -> Cow<Field> {
-        Cow::Owned(self.0.field())
-    }
-    fn _dtype(&self) -> &DataType {
-        self.0.dtype()
-    }
-
-    fn explode_by_offsets(&self, offsets: &[i64]) -> Series {
-        // TODO! explode by offset should return concrete type
-        self.with_state(true, |cats| {
-            cats.explode_by_offsets(offsets).u32().unwrap().clone()
-        })
-        .into_series()
-    }
-
-    fn _set_sorted_flag(&mut self, is_sorted: IsSorted) {
-        self.0.logical_mut().set_sorted_flag(is_sorted)
-    }
-
-    unsafe fn equal_element(&self, idx_self: usize, idx_other: usize, other: &Series) -> bool {
-        self.0.logical().equal_element(idx_self, idx_other, other)
-    }
-
-    #[cfg(feature = "zip_with")]
-    fn zip_with_same_type(&self, mask: &BooleanChunked, other: &Series) -> PolarsResult<Series> {
-        self.0
-            .zip_with(mask, other.categorical()?)
-            .map(|ca| ca.into_series())
-    }
-    fn into_partial_ord_inner<'a>(&'a self) -> Box<dyn PartialOrdInner + 'a> {
-        if self.0.use_lexical_sort() {
-            (&self.0).into_partial_ord_inner()
-        } else {
-            self.0.logical().into_partial_ord_inner()
+        impl private::PrivateSeries for SeriesWrap<$ca> {
+            fn compute_len(&mut self) {
+                self.0.compute_len()
+            }
+            fn _field(&self) -> Cow<Field> {
+                Cow::Owned(self.0.field())
+            }
+            fn _dtype(&self) -> &DataType {
+                self.0.dtype()
+            }
+
+            fn explode_by_offsets(&self, offsets: &[i64]) -> Series {
+                self.0
+                    .explode_by_offsets(offsets)
+                    .$into_logical()
+                    .into_series()
+            }
+
+            #[cfg(feature = "cum_agg")]
+            fn _cummax(&self, reverse: bool) -> Series {
+                self.0.cummax(reverse).$into_logical().into_series()
+            }
+
+            #[cfg(feature = "cum_agg")]
+            fn _cummin(&self, reverse: bool) -> Series {
+                self.0.cummin(reverse).$into_logical().into_series()
+            }
+
+            fn _set_sorted_flag(&mut self, is_sorted: IsSorted) {
+                self.0.deref_mut().set_sorted_flag(is_sorted)
+            }
+
+            #[cfg(feature = "zip_with")]
+            fn zip_with_same_type(
+                &self,
+                mask: &BooleanChunked,
+                other: &Series,
+            ) -> PolarsResult<Series> {
+                let other = other.to_physical_repr().into_owned();
+                self.0
+                    .zip_with(mask, &other.as_ref().as_ref())
+                    .map(|ca| ca.$into_logical().into_series())
+            }
+
+            fn vec_hash(&self, random_state: RandomState, buf: &mut Vec<u64>) -> PolarsResult<()> {
+                self.0.vec_hash(random_state, buf);
+                Ok(())
+            }
+
+            fn vec_hash_combine(
+                &self,
+                build_hasher: RandomState,
+                hashes: &mut [u64],
+            ) -> PolarsResult<()> {
+                self.0.vec_hash_combine(build_hasher, hashes);
+                Ok(())
+            }
+
+            unsafe fn agg_min(&self, groups: &GroupsProxy) -> Series {
+                self.0.agg_min(groups).$into_logical().into_series()
+            }
+
+            unsafe fn agg_max(&self, groups: &GroupsProxy) -> Series {
+                self.0.agg_max(groups).$into_logical().into_series()
+            }
+
+            unsafe fn agg_list(&self, groups: &GroupsProxy) -> Series {
+                // we cannot cast and dispatch as the inner type of the list would be incorrect
+                self.0
+                    .agg_list(groups)
+                    .cast(&DataType::List(Box::new(self.dtype().clone())))
+                    .unwrap()
+            }
+
+            fn zip_outer_join_column(
+                &self,
+                right_column: &Series,
+                opt_join_tuples: &[(Option<IdxSize>, Option<IdxSize>)],
+            ) -> Series {
+                let right_column = right_column.to_physical_repr().into_owned();
+                self.0
+                    .zip_outer_join_column(&right_column, opt_join_tuples)
+                    .$into_logical()
+                    .into_series()
+            }
+
+            fn subtract(&self, rhs: &Series) -> PolarsResult<Series> {
+                match (self.dtype(), rhs.dtype()) {
+                    (DataType::Date, DataType::Date) => {
+                        let dt = DataType::Datetime(TimeUnit::Milliseconds, None);
+                        let lhs = self.cast(&dt)?;
+                        let rhs = rhs.cast(&dt)?;
+                        lhs.subtract(&rhs)
+                    }
+                    (DataType::Date, DataType::Duration(_)) => ((&self
+                        .cast(&DataType::Datetime(TimeUnit::Milliseconds, None))
+                        .unwrap())
+                        - rhs)
+                        .cast(&DataType::Date),
+                    (dtl, dtr) => polars_bail!(opq = sub, dtl, dtr),
+                }
+            }
+            fn add_to(&self, rhs: &Series) -> PolarsResult<Series> {
+                match (self.dtype(), rhs.dtype()) {
+                    (DataType::Date, DataType::Duration(_)) => ((&self
+                        .cast(&DataType::Datetime(TimeUnit::Milliseconds, None))
+                        .unwrap())
+                        + rhs)
+                        .cast(&DataType::Date),
+                    (dtl, dtr) => polars_bail!(opq = add, dtl, dtr),
+                }
+            }
+            fn multiply(&self, rhs: &Series) -> PolarsResult<Series> {
+                polars_bail!(opq = mul, self.0.dtype(), rhs.dtype());
+            }
+            fn divide(&self, rhs: &Series) -> PolarsResult<Series> {
+                polars_bail!(opq = div, self.0.dtype(), rhs.dtype());
+            }
+            fn remainder(&self, rhs: &Series) -> PolarsResult<Series> {
+                polars_bail!(opq = rem, self.0.dtype(), rhs.dtype());
+            }
+            fn group_tuples(&self, multithreaded: bool, sorted: bool) -> PolarsResult<GroupsProxy> {
+                self.0.group_tuples(multithreaded, sorted)
+            }
+
+            fn arg_sort_multiple(&self, options: &SortMultipleOptions) -> PolarsResult<IdxCa> {
+                self.0.deref().arg_sort_multiple(options)
+            }
         }
-    }
-
-    fn vec_hash(&self, random_state: RandomState, buf: &mut Vec<u64>) -> PolarsResult<()> {
-        self.0.logical().vec_hash(random_state, buf);
-        Ok(())
-    }
-
-    fn vec_hash_combine(&self, build_hasher: RandomState, hashes: &mut [u64]) -> PolarsResult<()> {
-        self.0.logical().vec_hash_combine(build_hasher, hashes);
-        Ok(())
-    }
-
-    unsafe fn agg_list(&self, groups: &GroupsProxy) -> Series {
-        // we cannot cast and dispatch as the inner type of the list would be incorrect
-        let list = self.0.logical().agg_list(groups);
-        let mut list = list.list().unwrap().clone();
-        list.to_logical(self.dtype().clone());
-        list.into_series()
-    }
-
-    fn zip_outer_join_column(
-        &self,
-        right_column: &Series,
-        opt_join_tuples: &[(Option<IdxSize>, Option<IdxSize>)],
-    ) -> Series {
-        let new_rev_map = self
-            .0
-            .merge_categorical_map(right_column.categorical().unwrap())
-            .unwrap();
-        let left = self.0.logical();
-        let right = right_column
-            .categorical()
-            .unwrap()
-            .logical()
-            .clone()
-            .into_series();
-
-        let cats = left.zip_outer_join_column(&right, opt_join_tuples);
-        let cats = cats.u32().unwrap().clone();
 
-        unsafe {
-            CategoricalChunked::from_cats_and_rev_map_unchecked(cats, new_rev_map).into_series()
+        impl SeriesTrait for SeriesWrap<$ca> {
+            fn is_sorted_flag(&self) -> IsSorted {
+                self.0.is_sorted_flag()
+            }
+
+            fn rename(&mut self, name: &str) {
+                self.0.rename(name);
+            }
+
+            fn chunk_lengths(&self) -> ChunkIdIter {
+                self.0.chunk_id()
+            }
+            fn name(&self) -> &str {
+                self.0.name()
+            }
+
+            fn chunks(&self) -> &Vec<ArrayRef> {
+                self.0.chunks()
+            }
+
+            fn shrink_to_fit(&mut self) {
+                self.0.shrink_to_fit()
+            }
+
+            fn slice(&self, offset: i64, length: usize) -> Series {
+                self.0.slice(offset, length).$into_logical().into_series()
+            }
+
+            fn mean(&self) -> Option<f64> {
+                self.0.mean()
+            }
+
+            fn median(&self) -> Option<f64> {
+                self.0.median()
+            }
+
+            fn append(&mut self, other: &Series) -> PolarsResult<()> {
+                polars_ensure!(self.0.dtype() == other.dtype(), append);
+                let other = other.to_physical_repr();
+                // 3 refs
+                // ref Cow
+                // ref SeriesTrait
+                // ref ChunkedArray
+                self.0.append(other.as_ref().as_ref().as_ref());
+                Ok(())
+            }
+            fn extend(&mut self, other: &Series) -> PolarsResult<()> {
+                polars_ensure!(self.0.dtype() == other.dtype(), extend);
+                // 3 refs
+                // ref Cow
+                // ref SeriesTrait
+                // ref ChunkedArray
+                let other = other.to_physical_repr();
+                self.0.extend(other.as_ref().as_ref().as_ref());
+                Ok(())
+            }
+
+            fn filter(&self, filter: &BooleanChunked) -> PolarsResult<Series> {
+                self.0
+                    .filter(filter)
+                    .map(|ca| ca.$into_logical().into_series())
+            }
+
+            #[cfg(feature = "chunked_ids")]
+            unsafe fn _take_chunked_unchecked(&self, by: &[ChunkId], sorted: IsSorted) -> Series {
+                let ca = self.0.deref().take_chunked_unchecked(by, sorted);
+                ca.$into_logical().into_series()
+            }
+
+            #[cfg(feature = "chunked_ids")]
+            unsafe fn _take_opt_chunked_unchecked(&self, by: &[Option<ChunkId>]) -> Series {
+                let ca = self.0.deref().take_opt_chunked_unchecked(by);
+                ca.$into_logical().into_series()
+            }
+
+            fn take(&self, indices: &IdxCa) -> PolarsResult<Series> {
+                ChunkTake::take(self.0.deref(), indices.into())
+                    .map(|ca| ca.$into_logical().into_series())
+            }
+
+            fn take_iter(&self, iter: &mut dyn TakeIterator) -> PolarsResult<Series> {
+                ChunkTake::take(self.0.deref(), iter.into())
+                    .map(|ca| ca.$into_logical().into_series())
+            }
+
+            unsafe fn take_iter_unchecked(&self, iter: &mut dyn TakeIterator) -> Series {
+                ChunkTake::take_unchecked(self.0.deref(), iter.into())
+                    .$into_logical()
+                    .into_series()
+            }
+
+            unsafe fn take_unchecked(&self, idx: &IdxCa) -> PolarsResult<Series> {
+                let mut out = ChunkTake::take_unchecked(self.0.deref(), idx.into());
+
+                if self.0.is_sorted_ascending_flag()
+                    && (idx.is_sorted_ascending_flag() || idx.is_sorted_descending_flag())
+                {
+                    out.set_sorted_flag(idx.is_sorted_flag())
+                }
+
+                Ok(out.$into_logical().into_series())
+            }
+
+            unsafe fn take_opt_iter_unchecked(&self, iter: &mut dyn TakeIteratorNulls) -> Series {
+                ChunkTake::take_unchecked(self.0.deref(), iter.into())
+                    .$into_logical()
+                    .into_series()
+            }
+
+            #[cfg(feature = "take_opt_iter")]
+            fn take_opt_iter(&self, iter: &mut dyn TakeIteratorNulls) -> PolarsResult<Series> {
+                ChunkTake::take(self.0.deref(), iter.into())
+                    .map(|ca| ca.$into_logical().into_series())
+            }
+
+            fn len(&self) -> usize {
+                self.0.len()
+            }
+
+            fn rechunk(&self) -> Series {
+                self.0.rechunk().$into_logical().into_series()
+            }
+
+            fn new_from_index(&self, index: usize, length: usize) -> Series {
+                self.0
+                    .new_from_index(index, length)
+                    .$into_logical()
+                    .into_series()
+            }
+
+            fn cast(&self, data_type: &DataType) -> PolarsResult<Series> {
+                match (self.dtype(), data_type) {
+                    (DataType::Date, DataType::Utf8) => Ok(self
+                        .0
+                        .clone()
+                        .into_series()
+                        .date()
+                        .unwrap()
+                        .to_string("%Y-%m-%d")
+                        .into_series()),
+                    (DataType::Time, DataType::Utf8) => Ok(self
+                        .0
+                        .clone()
+                        .into_series()
+                        .time()
+                        .unwrap()
+                        .to_string("%T")
+                        .into_series()),
+                    #[cfg(feature = "dtype-datetime")]
+                    (DataType::Time, DataType::Datetime(_, _)) => {
+                        polars_bail!(
+                            ComputeError:
+                            "cannot cast `Time` to `Datetime`; consider using 'dt.combine'"
+                        );
+                    }
+                    #[cfg(feature = "dtype-datetime")]
+                    (DataType::Date, DataType::Datetime(_, _)) => {
+                        let mut out = self.0.cast(data_type)?;
+                        out.set_sorted_flag(self.0.is_sorted_flag());
+                        Ok(out)
+                    }
+                    _ => self.0.cast(data_type),
+                }
+            }
+
+            fn get(&self, index: usize) -> PolarsResult<AnyValue> {
+                self.0.get_any_value(index)
+            }
+
+            #[inline]
+            #[cfg(feature = "private")]
+            unsafe fn get_unchecked(&self, index: usize) -> AnyValue {
+                self.0.get_any_value_unchecked(index)
+            }
+
+            fn sort_with(&self, options: SortOptions) -> Series {
+                self.0.sort_with(options).$into_logical().into_series()
+            }
+
+            fn arg_sort(&self, options: SortOptions) -> IdxCa {
+                self.0.arg_sort(options)
+            }
+
+            fn null_count(&self) -> usize {
+                self.0.null_count()
+            }
+
+            fn has_validity(&self) -> bool {
+                self.0.has_validity()
+            }
+
+            fn unique(&self) -> PolarsResult<Series> {
+                self.0.unique().map(|ca| ca.$into_logical().into_series())
+            }
+
+            fn n_unique(&self) -> PolarsResult<usize> {
+                self.0.n_unique()
+            }
+
+            fn arg_unique(&self) -> PolarsResult<IdxCa> {
+                self.0.arg_unique()
+            }
+
+            fn is_null(&self) -> BooleanChunked {
+                self.0.is_null()
+            }
+
+            fn is_not_null(&self) -> BooleanChunked {
+                self.0.is_not_null()
+            }
+
+            fn reverse(&self) -> Series {
+                self.0.reverse().$into_logical().into_series()
+            }
+
+            fn as_single_ptr(&mut self) -> PolarsResult<usize> {
+                self.0.as_single_ptr()
+            }
+
+            fn shift(&self, periods: i64) -> Series {
+                self.0.shift(periods).$into_logical().into_series()
+            }
+
+            fn _sum_as_series(&self) -> Series {
+                Int32Chunked::full_null(self.name(), 1)
+                    .cast(self.dtype())
+                    .unwrap()
+                    .into()
+            }
+            fn max_as_series(&self) -> Series {
+                self.0.max_as_series().$into_logical()
+            }
+            fn min_as_series(&self) -> Series {
+                self.0.min_as_series().$into_logical()
+            }
+            fn median_as_series(&self) -> Series {
+                Int32Chunked::full_null(self.name(), 1)
+                    .cast(self.dtype())
+                    .unwrap()
+                    .into()
+            }
+            fn var_as_series(&self, _ddof: u8) -> Series {
+                Int32Chunked::full_null(self.name(), 1)
+                    .cast(self.dtype())
+                    .unwrap()
+                    .into()
+            }
+            fn std_as_series(&self, _ddof: u8) -> Series {
+                Int32Chunked::full_null(self.name(), 1)
+                    .cast(self.dtype())
+                    .unwrap()
+                    .into()
+            }
+            fn quantile_as_series(
+                &self,
+                _quantile: f64,
+                _interpol: QuantileInterpolOptions,
+            ) -> PolarsResult<Series> {
+                Ok(Int32Chunked::full_null(self.name(), 1)
+                    .cast(self.dtype())
+                    .unwrap()
+                    .into())
+            }
+
+            fn clone_inner(&self) -> Arc<dyn SeriesTrait> {
+                Arc::new(SeriesWrap(Clone::clone(&self.0)))
+            }
+
+            fn peak_max(&self) -> BooleanChunked {
+                self.0.peak_max()
+            }
+
+            fn peak_min(&self) -> BooleanChunked {
+                self.0.peak_min()
+            }
+            #[cfg(feature = "is_in")]
+            fn is_in(&self, other: &Series) -> PolarsResult<BooleanChunked> {
+                self.0.is_in(other)
+            }
+            #[cfg(feature = "repeat_by")]
+            fn repeat_by(&self, by: &IdxCa) -> PolarsResult<ListChunked> {
+                match self.0.dtype() {
+                    DataType::Date => Ok(self
+                        .0
+                        .repeat_by(by)?
+                        .cast(&DataType::List(Box::new(DataType::Date)))
+                        .unwrap()
+                        .list()
+                        .unwrap()
+                        .clone()),
+                    DataType::Time => Ok(self
+                        .0
+                        .repeat_by(by)?
+                        .cast(&DataType::List(Box::new(DataType::Time)))
+                        .unwrap()
+                        .list()
+                        .unwrap()
+                        .clone()),
+                    _ => unreachable!(),
+                }
+            }
+            #[cfg(feature = "mode")]
+            fn mode(&self) -> PolarsResult<Series> {
+                self.0.mode().map(|ca| ca.$into_logical().into_series())
+            }
         }
-    }
-    fn group_tuples(&self, multithreaded: bool, sorted: bool) -> PolarsResult<GroupsProxy> {
-        #[cfg(feature = "performant")]
-        {
-            Ok(self.0.group_tuples_perfect(multithreaded, sorted))
-        }
-        #[cfg(not(feature = "performant"))]
-        {
-            self.0.logical().group_tuples(multithreaded, sorted)
-        }
-    }
-
-    fn arg_sort_multiple(&self, by: &[Series], descending: &[bool]) -> PolarsResult<IdxCa> {
-        self.0.arg_sort_multiple(by, descending)
-    }
+    };
 }
 
-impl SeriesTrait for SeriesWrap<CategoricalChunked> {
-    fn is_sorted_flag(&self) -> IsSorted {
-        if self.0.logical().is_sorted_ascending_flag() {
-            IsSorted::Ascending
-        } else if self.0.logical().is_sorted_descending_flag() {
-            IsSorted::Descending
-        } else {
-            IsSorted::Not
+#[cfg(feature = "dtype-date")]
+impl_dyn_series!(DateChunked, into_date);
+#[cfg(feature = "dtype-time")]
+impl_dyn_series!(TimeChunked, into_time);
+
+macro_rules! impl_dyn_series_numeric {
+    ($ca: ident) => {
+        impl private::PrivateSeriesNumeric for SeriesWrap<$ca> {
+            fn bit_repr_is_large(&self) -> bool {
+                if let DataType::Time = self.dtype() {
+                    true
+                } else {
+                    false
+                }
+            }
+            fn bit_repr_large(&self) -> UInt64Chunked {
+                self.0.bit_repr_large()
+            }
+            fn bit_repr_small(&self) -> UInt32Chunked {
+                self.0.bit_repr_small()
+            }
         }
-    }
-
-    fn rename(&mut self, name: &str) {
-        self.0.logical_mut().rename(name);
-    }
-
-    fn chunk_lengths(&self) -> ChunkIdIter {
-        self.0.logical().chunk_id()
-    }
-    fn name(&self) -> &str {
-        self.0.logical().name()
-    }
-
-    fn chunks(&self) -> &Vec<ArrayRef> {
-        self.0.logical().chunks()
-    }
-    fn shrink_to_fit(&mut self) {
-        self.0.logical_mut().shrink_to_fit()
-    }
-
-    fn slice(&self, offset: i64, length: usize) -> Series {
-        self.with_state(false, |cats| cats.slice(offset, length))
-            .into_series()
-    }
-
-    fn append(&mut self, other: &Series) -> PolarsResult<()> {
-        polars_ensure!(self.0.dtype() == other.dtype(), append);
-        self.0.append(other.categorical().unwrap())
-    }
-
-    fn extend(&mut self, other: &Series) -> PolarsResult<()> {
-        polars_ensure!(self.0.dtype() == other.dtype(), extend);
-        let other = other.categorical()?;
-        self.0.logical_mut().extend(other.logical());
-        let new_rev_map = self.0.merge_categorical_map(other)?;
-        // SAFETY
-        // rev_maps are merged
-        unsafe { self.0.set_rev_map(new_rev_map, false) };
-        Ok(())
-    }
-
-    fn filter(&self, filter: &BooleanChunked) -> PolarsResult<Series> {
-        self.try_with_state(false, |cats| cats.filter(filter))
-            .map(|ca| ca.into_series())
-    }
-
-    #[cfg(feature = "chunked_ids")]
-    unsafe fn _take_chunked_unchecked(&self, by: &[ChunkId], sorted: IsSorted) -> Series {
-        let cats = self.0.logical().take_chunked_unchecked(by, sorted);
-        self.finish_with_state(false, cats).into_series()
-    }
-
-    #[cfg(feature = "chunked_ids")]
-    unsafe fn _take_opt_chunked_unchecked(&self, by: &[Option<ChunkId>]) -> Series {
-        let cats = self.0.logical().take_opt_chunked_unchecked(by);
-        self.finish_with_state(false, cats).into_series()
-    }
-
-    fn take(&self, indices: &IdxCa) -> PolarsResult<Series> {
-        let indices = if indices.chunks.len() > 1 {
-            Cow::Owned(indices.rechunk())
-        } else {
-            Cow::Borrowed(indices)
-        };
-        self.try_with_state(false, |cats| cats.take((&*indices).into()))
-            .map(|ca| ca.into_series())
-    }
-
-    fn take_iter(&self, iter: &mut dyn TakeIterator) -> PolarsResult<Series> {
-        let cats = self.0.logical().take(iter.into())?;
-        Ok(self.finish_with_state(false, cats).into_series())
-    }
-
-    fn take_every(&self, n: usize) -> Series {
-        self.with_state(true, |cats| cats.take_every(n))
-            .into_series()
-    }
-
-    unsafe fn take_iter_unchecked(&self, iter: &mut dyn TakeIterator) -> Series {
-        let cats = self.0.logical().take_unchecked(iter.into());
-        self.finish_with_state(false, cats).into_series()
-    }
-
-    unsafe fn take_unchecked(&self, idx: &IdxCa) -> PolarsResult<Series> {
-        let idx = if idx.chunks.len() > 1 {
-            Cow::Owned(idx.rechunk())
-        } else {
-            Cow::Borrowed(idx)
-        };
-        Ok(self
-            .with_state(false, |cats| cats.take_unchecked((&*idx).into()))
-            .into_series())
-    }
-
-    unsafe fn take_opt_iter_unchecked(&self, iter: &mut dyn TakeIteratorNulls) -> Series {
-        let cats = self.0.logical().take_unchecked(iter.into());
-        self.finish_with_state(false, cats).into_series()
-    }
-
-    #[cfg(feature = "take_opt_iter")]
-    fn take_opt_iter(&self, iter: &mut dyn TakeIteratorNulls) -> PolarsResult<Series> {
-        let cats = self.0.logical().take(iter.into())?;
-        Ok(self.finish_with_state(false, cats).into_series())
-    }
-
-    fn len(&self) -> usize {
-        self.0.len()
-    }
-
-    fn rechunk(&self) -> Series {
-        self.with_state(true, |ca| ca.rechunk()).into_series()
-    }
-
-    fn new_from_index(&self, index: usize, length: usize) -> Series {
-        self.with_state(true, |cats| cats.new_from_index(index, length))
-            .into_series()
-    }
-
-    fn cast(&self, data_type: &DataType) -> PolarsResult<Series> {
-        self.0.cast(data_type)
-    }
-
-    fn get(&self, index: usize) -> PolarsResult<AnyValue> {
-        self.0.get_any_value(index)
-    }
-
-    #[inline]
-    #[cfg(feature = "private")]
-    unsafe fn get_unchecked(&self, index: usize) -> AnyValue {
-        self.0.get_any_value_unchecked(index)
-    }
-
-    fn sort_with(&self, options: SortOptions) -> Series {
-        self.0.sort_with(options).into_series()
-    }
-
-    fn arg_sort(&self, options: SortOptions) -> IdxCa {
-        self.0.arg_sort(options)
-    }
-
-    fn null_count(&self) -> usize {
-        self.0.logical().null_count()
-    }
-
-    fn has_validity(&self) -> bool {
-        self.0.logical().has_validity()
-    }
-
-    fn unique(&self) -> PolarsResult<Series> {
-        self.0.unique().map(|ca| ca.into_series())
-    }
-
-    fn n_unique(&self) -> PolarsResult<usize> {
-        self.0.n_unique()
-    }
-
-    fn arg_unique(&self) -> PolarsResult<IdxCa> {
-        self.0.logical().arg_unique()
-    }
-
-    fn is_null(&self) -> BooleanChunked {
-        self.0.logical().is_null()
-    }
-
-    fn is_not_null(&self) -> BooleanChunked {
-        self.0.logical().is_not_null()
-    }
-
-    fn reverse(&self) -> Series {
-        self.with_state(true, |cats| cats.reverse()).into_series()
-    }
-
-    fn as_single_ptr(&mut self) -> PolarsResult<usize> {
-        self.0.logical_mut().as_single_ptr()
-    }
-
-    fn shift(&self, periods: i64) -> Series {
-        self.with_state(false, |ca| ca.shift(periods)).into_series()
-    }
-
-    fn _sum_as_series(&self) -> Series {
-        CategoricalChunked::full_null(self.0.logical().name(), 1).into_series()
-    }
-    fn max_as_series(&self) -> Series {
-        CategoricalChunked::full_null(self.0.logical().name(), 1).into_series()
-    }
-    fn min_as_series(&self) -> Series {
-        CategoricalChunked::full_null(self.0.logical().name(), 1).into_series()
-    }
-    fn median_as_series(&self) -> Series {
-        CategoricalChunked::full_null(self.0.logical().name(), 1).into_series()
-    }
-    fn var_as_series(&self, _ddof: u8) -> Series {
-        CategoricalChunked::full_null(self.0.logical().name(), 1).into_series()
-    }
-    fn std_as_series(&self, _ddof: u8) -> Series {
-        CategoricalChunked::full_null(self.0.logical().name(), 1).into_series()
-    }
-    fn quantile_as_series(
-        &self,
-        _quantile: f64,
-        _interpol: QuantileInterpolOptions,
-    ) -> PolarsResult<Series> {
-        Ok(CategoricalChunked::full_null(self.0.logical().name(), 1).into_series())
-    }
-
-    fn clone_inner(&self) -> Arc<dyn SeriesTrait> {
-        Arc::new(SeriesWrap(Clone::clone(&self.0)))
-    }
-
-    #[cfg(feature = "is_in")]
-    fn is_in(&self, other: &Series) -> PolarsResult<BooleanChunked> {
-        _check_categorical_src(self.dtype(), other.dtype())?;
-        self.0.logical().is_in(&other.to_physical_repr())
-    }
-    #[cfg(feature = "repeat_by")]
-    fn repeat_by(&self, by: &IdxCa) -> ListChunked {
-        let out = self.0.logical().repeat_by(by);
-        let casted = out
-            .cast(&DataType::List(Box::new(self.dtype().clone())))
-            .unwrap();
-        casted.list().unwrap().clone()
-    }
-
-    #[cfg(feature = "mode")]
-    fn mode(&self) -> PolarsResult<Series> {
-        let cats = self.0.logical().mode()?;
-        Ok(self.finish_with_state(false, cats).into_series())
-    }
+    };
 }
 
-impl private::PrivateSeriesNumeric for SeriesWrap<CategoricalChunked> {
-    fn bit_repr_is_large(&self) -> bool {
-        false
-    }
-    fn bit_repr_small(&self) -> UInt32Chunked {
-        self.0.logical().clone()
-    }
-}
+#[cfg(feature = "dtype-date")]
+impl_dyn_series_numeric!(DateChunked);
+#[cfg(feature = "dtype-time")]
+impl_dyn_series_numeric!(TimeChunked);
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/dates_time.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/mod.rs`

 * *Files 13% similar despite different names*

```diff
@@ -1,80 +1,141 @@
-//! This module exists to reduce compilation times.
-//!
-//! All the data types are backed by a physical type in memory e.g. Date -> i32, Datetime-> i64.
-//!
-//! Series lead to code implementations of all traits. Whereas there are a lot of duplicates due to
-//! data types being backed by the same physical type. In this module we reduce compile times by
-//! opting for a little more run time cost. We cast to the physical type -> apply the operation and
-//! (depending on the result) cast back to the original type
-//!
+#[cfg(feature = "dtype-array")]
+mod array;
+mod binary;
+mod boolean;
+#[cfg(feature = "dtype-categorical")]
+mod categorical;
+#[cfg(any(
+    feature = "dtype-datetime",
+    feature = "dtype-date",
+    feature = "dtype-time"
+))]
+mod dates_time;
+#[cfg(feature = "dtype-datetime")]
+mod datetime;
+#[cfg(feature = "dtype-decimal")]
+mod decimal;
+#[cfg(feature = "dtype-duration")]
+mod duration;
+mod floats;
+mod list;
+pub(crate) mod null;
+#[cfg(feature = "object")]
+mod object;
+#[cfg(feature = "dtype-struct")]
+mod struct_;
+mod utf8;
+
+#[cfg(feature = "object")]
+use std::any::Any;
 use std::borrow::Cow;
-use std::ops::{Deref, DerefMut};
+use std::ops::{BitAnd, BitOr, BitXor, Deref};
 
 use ahash::RandomState;
 use polars_arrow::prelude::QuantileInterpolOptions;
 
-use super::{private, IntoSeries, SeriesTrait, SeriesWrap, *};
+use super::{private, IntoSeries, SeriesTrait, *};
+use crate::chunked_array::comparison::*;
+use crate::chunked_array::ops::aggregate::{ChunkAggSeries, QuantileAggSeries, VarAggSeries};
+use crate::chunked_array::ops::compare_inner::{
+    IntoPartialEqInner, IntoPartialOrdInner, PartialEqInner, PartialOrdInner,
+};
 use crate::chunked_array::ops::explode::ExplodeByOffsets;
-use crate::chunked_array::ops::ToBitRepr;
 use crate::chunked_array::AsSinglePtr;
 use crate::frame::groupby::*;
-use crate::frame::hash_join::*;
+use crate::frame::hash_join::ZipOuterJoinColumn;
 use crate::prelude::*;
+#[cfg(feature = "checked_arithmetic")]
+use crate::series::arithmetic::checked::NumOpsDispatchChecked;
 
-macro_rules! impl_dyn_series {
-    ($ca: ident, $into_logical: ident) => {
-        unsafe impl IntoSeries for $ca {
-            fn into_series(self) -> Series {
-                Series(Arc::new(SeriesWrap(self)))
-            }
-        }
+// Utility wrapper struct
+pub(crate) struct SeriesWrap<T>(pub T);
+
+impl<T: PolarsDataType> From<ChunkedArray<T>> for SeriesWrap<ChunkedArray<T>> {
+    fn from(ca: ChunkedArray<T>) -> Self {
+        SeriesWrap(ca)
+    }
+}
 
+impl<T: PolarsDataType> Deref for SeriesWrap<ChunkedArray<T>> {
+    type Target = ChunkedArray<T>;
+
+    fn deref(&self) -> &Self::Target {
+        &self.0
+    }
+}
+
+unsafe impl<T: PolarsDataType + 'static> IntoSeries for ChunkedArray<T>
+where
+    SeriesWrap<ChunkedArray<T>>: SeriesTrait,
+{
+    fn into_series(self) -> Series
+    where
+        Self: Sized,
+    {
+        Series(Arc::new(SeriesWrap(self)))
+    }
+}
+
+macro_rules! impl_dyn_series {
+    ($ca: ident) => {
         impl private::PrivateSeries for SeriesWrap<$ca> {
             fn compute_len(&mut self) {
                 self.0.compute_len()
             }
+
             fn _field(&self) -> Cow<Field> {
-                Cow::Owned(self.0.field())
+                Cow::Borrowed(self.0.ref_field())
             }
+
             fn _dtype(&self) -> &DataType {
-                self.0.dtype()
+                self.0.ref_field().data_type()
             }
 
             fn explode_by_offsets(&self, offsets: &[i64]) -> Series {
-                self.0
-                    .explode_by_offsets(offsets)
-                    .$into_logical()
-                    .into_series()
+                self.0.explode_by_offsets(offsets)
             }
 
             #[cfg(feature = "cum_agg")]
             fn _cummax(&self, reverse: bool) -> Series {
-                self.0.cummax(reverse).$into_logical().into_series()
+                self.0.cummax(reverse).into_series()
             }
 
             #[cfg(feature = "cum_agg")]
             fn _cummin(&self, reverse: bool) -> Series {
-                self.0.cummin(reverse).$into_logical().into_series()
+                self.0.cummin(reverse).into_series()
             }
 
             fn _set_sorted_flag(&mut self, is_sorted: IsSorted) {
-                self.0.deref_mut().set_sorted_flag(is_sorted)
+                self.0.set_sorted_flag(is_sorted)
+            }
+
+            unsafe fn equal_element(
+                &self,
+                idx_self: usize,
+                idx_other: usize,
+                other: &Series,
+            ) -> bool {
+                self.0.equal_element(idx_self, idx_other, other)
             }
 
             #[cfg(feature = "zip_with")]
             fn zip_with_same_type(
                 &self,
                 mask: &BooleanChunked,
                 other: &Series,
             ) -> PolarsResult<Series> {
-                let other = other.to_physical_repr().into_owned();
-                self.0
-                    .zip_with(mask, &other.as_ref().as_ref())
-                    .map(|ca| ca.$into_logical().into_series())
+                ChunkZip::zip_with(&self.0, mask, other.as_ref().as_ref())
+                    .map(|ca| ca.into_series())
+            }
+            fn into_partial_eq_inner<'a>(&'a self) -> Box<dyn PartialEqInner + 'a> {
+                (&self.0).into_partial_eq_inner()
+            }
+            fn into_partial_ord_inner<'a>(&'a self) -> Box<dyn PartialOrdInner + 'a> {
+                (&self.0).into_partial_ord_inner()
             }
 
             fn vec_hash(&self, random_state: RandomState, buf: &mut Vec<u64>) -> PolarsResult<()> {
                 self.0.vec_hash(random_state, buf);
                 Ok(())
             }
 
@@ -84,420 +145,402 @@
                 hashes: &mut [u64],
             ) -> PolarsResult<()> {
                 self.0.vec_hash_combine(build_hasher, hashes);
                 Ok(())
             }
 
             unsafe fn agg_min(&self, groups: &GroupsProxy) -> Series {
-                self.0.agg_min(groups).$into_logical().into_series()
+                self.0.agg_min(groups)
             }
 
             unsafe fn agg_max(&self, groups: &GroupsProxy) -> Series {
-                self.0.agg_max(groups).$into_logical().into_series()
+                self.0.agg_max(groups)
+            }
+
+            unsafe fn agg_sum(&self, groups: &GroupsProxy) -> Series {
+                use DataType::*;
+                match self.dtype() {
+                    Int8 | UInt8 | Int16 | UInt16 => self.cast(&Int64).unwrap().agg_sum(groups),
+                    _ => self.0.agg_sum(groups),
+                }
+            }
+
+            unsafe fn agg_std(&self, groups: &GroupsProxy, ddof: u8) -> Series {
+                self.0.agg_std(groups, ddof)
+            }
+
+            unsafe fn agg_var(&self, groups: &GroupsProxy, ddof: u8) -> Series {
+                self.0.agg_var(groups, ddof)
             }
 
             unsafe fn agg_list(&self, groups: &GroupsProxy) -> Series {
-                // we cannot cast and dispatch as the inner type of the list would be incorrect
-                self.0
-                    .agg_list(groups)
-                    .cast(&DataType::List(Box::new(self.dtype().clone())))
-                    .unwrap()
+                self.0.agg_list(groups)
             }
 
             fn zip_outer_join_column(
                 &self,
                 right_column: &Series,
                 opt_join_tuples: &[(Option<IdxSize>, Option<IdxSize>)],
             ) -> Series {
-                let right_column = right_column.to_physical_repr().into_owned();
-                self.0
-                    .zip_outer_join_column(&right_column, opt_join_tuples)
-                    .$into_logical()
-                    .into_series()
+                ZipOuterJoinColumn::zip_outer_join_column(&self.0, right_column, opt_join_tuples)
             }
-
             fn subtract(&self, rhs: &Series) -> PolarsResult<Series> {
-                match (self.dtype(), rhs.dtype()) {
-                    (DataType::Date, DataType::Date) => {
-                        let dt = DataType::Datetime(TimeUnit::Milliseconds, None);
-                        let lhs = self.cast(&dt)?;
-                        let rhs = rhs.cast(&dt)?;
-                        lhs.subtract(&rhs)
-                    }
-                    (DataType::Date, DataType::Duration(_)) => ((&self
-                        .cast(&DataType::Datetime(TimeUnit::Milliseconds, None))
-                        .unwrap())
-                        - rhs)
-                        .cast(&DataType::Date),
-                    (dtl, dtr) => polars_bail!(opq = sub, dtl, dtr),
-                }
+                NumOpsDispatch::subtract(&self.0, rhs)
             }
             fn add_to(&self, rhs: &Series) -> PolarsResult<Series> {
-                match (self.dtype(), rhs.dtype()) {
-                    (DataType::Date, DataType::Duration(_)) => ((&self
-                        .cast(&DataType::Datetime(TimeUnit::Milliseconds, None))
-                        .unwrap())
-                        + rhs)
-                        .cast(&DataType::Date),
-                    (dtl, dtr) => polars_bail!(opq = add, dtl, dtr),
-                }
+                NumOpsDispatch::add_to(&self.0, rhs)
             }
             fn multiply(&self, rhs: &Series) -> PolarsResult<Series> {
-                polars_bail!(opq = mul, self.0.dtype(), rhs.dtype());
+                NumOpsDispatch::multiply(&self.0, rhs)
             }
             fn divide(&self, rhs: &Series) -> PolarsResult<Series> {
-                polars_bail!(opq = div, self.0.dtype(), rhs.dtype());
+                NumOpsDispatch::divide(&self.0, rhs)
             }
             fn remainder(&self, rhs: &Series) -> PolarsResult<Series> {
-                polars_bail!(opq = rem, self.0.dtype(), rhs.dtype());
+                NumOpsDispatch::remainder(&self.0, rhs)
             }
             fn group_tuples(&self, multithreaded: bool, sorted: bool) -> PolarsResult<GroupsProxy> {
-                self.0.group_tuples(multithreaded, sorted)
+                IntoGroupsProxy::group_tuples(&self.0, multithreaded, sorted)
             }
 
-            fn arg_sort_multiple(&self, by: &[Series], descending: &[bool]) -> PolarsResult<IdxCa> {
-                self.0.deref().arg_sort_multiple(by, descending)
+            fn arg_sort_multiple(&self, options: &SortMultipleOptions) -> PolarsResult<IdxCa> {
+                self.0.arg_sort_multiple(options)
             }
         }
 
         impl SeriesTrait for SeriesWrap<$ca> {
             fn is_sorted_flag(&self) -> IsSorted {
                 if self.0.is_sorted_ascending_flag() {
                     IsSorted::Ascending
                 } else if self.0.is_sorted_descending_flag() {
                     IsSorted::Descending
                 } else {
                     IsSorted::Not
                 }
             }
 
+            #[cfg(feature = "rolling_window")]
+            fn rolling_apply(
+                &self,
+                _f: &dyn Fn(&Series) -> Series,
+                _options: RollingOptionsFixedWindow,
+            ) -> PolarsResult<Series> {
+                ChunkRollApply::rolling_apply(&self.0, _f, _options).map(|ca| ca.into_series())
+            }
+
+            fn bitand(&self, other: &Series) -> PolarsResult<Series> {
+                let other = if other.len() == 1 {
+                    Cow::Owned(other.cast(self.dtype())?)
+                } else {
+                    Cow::Borrowed(other)
+                };
+                let other = self.0.unpack_series_matching_type(&other)?;
+                Ok(self.0.bitand(&other).into_series())
+            }
+
+            fn bitor(&self, other: &Series) -> PolarsResult<Series> {
+                let other = if other.len() == 1 {
+                    Cow::Owned(other.cast(self.dtype())?)
+                } else {
+                    Cow::Borrowed(other)
+                };
+                let other = self.0.unpack_series_matching_type(&other)?;
+                Ok(self.0.bitor(&other).into_series())
+            }
+
+            fn bitxor(&self, other: &Series) -> PolarsResult<Series> {
+                let other = if other.len() == 1 {
+                    Cow::Owned(other.cast(self.dtype())?)
+                } else {
+                    Cow::Borrowed(other)
+                };
+                let other = self.0.unpack_series_matching_type(&other)?;
+                Ok(self.0.bitxor(&other).into_series())
+            }
+
             fn rename(&mut self, name: &str) {
                 self.0.rename(name);
             }
 
             fn chunk_lengths(&self) -> ChunkIdIter {
                 self.0.chunk_id()
             }
             fn name(&self) -> &str {
                 self.0.name()
             }
 
             fn chunks(&self) -> &Vec<ArrayRef> {
                 self.0.chunks()
             }
-
             fn shrink_to_fit(&mut self) {
                 self.0.shrink_to_fit()
             }
 
             fn slice(&self, offset: i64, length: usize) -> Series {
-                self.0.slice(offset, length).$into_logical().into_series()
-            }
-
-            fn mean(&self) -> Option<f64> {
-                self.0.mean()
-            }
-
-            fn median(&self) -> Option<f64> {
-                self.0.median()
+                return self.0.slice(offset, length).into_series();
             }
 
             fn append(&mut self, other: &Series) -> PolarsResult<()> {
                 polars_ensure!(self.0.dtype() == other.dtype(), append);
-                let other = other.to_physical_repr();
-                // 3 refs
-                // ref Cow
-                // ref SeriesTrait
-                // ref ChunkedArray
-                self.0.append(other.as_ref().as_ref().as_ref());
+                self.0.append(other.as_ref().as_ref());
                 Ok(())
             }
+
             fn extend(&mut self, other: &Series) -> PolarsResult<()> {
                 polars_ensure!(self.0.dtype() == other.dtype(), extend);
-                // 3 refs
-                // ref Cow
-                // ref SeriesTrait
-                // ref ChunkedArray
-                let other = other.to_physical_repr();
-                self.0.extend(other.as_ref().as_ref().as_ref());
+                self.0.extend(other.as_ref().as_ref());
                 Ok(())
             }
 
             fn filter(&self, filter: &BooleanChunked) -> PolarsResult<Series> {
-                self.0
-                    .filter(filter)
-                    .map(|ca| ca.$into_logical().into_series())
+                ChunkFilter::filter(&self.0, filter).map(|ca| ca.into_series())
+            }
+
+            fn mean(&self) -> Option<f64> {
+                self.0.mean()
+            }
+
+            fn median(&self) -> Option<f64> {
+                self.0.median()
             }
 
             #[cfg(feature = "chunked_ids")]
             unsafe fn _take_chunked_unchecked(&self, by: &[ChunkId], sorted: IsSorted) -> Series {
-                let ca = self.0.deref().take_chunked_unchecked(by, sorted);
-                ca.$into_logical().into_series()
+                self.0.take_chunked_unchecked(by, sorted).into_series()
             }
 
             #[cfg(feature = "chunked_ids")]
             unsafe fn _take_opt_chunked_unchecked(&self, by: &[Option<ChunkId>]) -> Series {
-                let ca = self.0.deref().take_opt_chunked_unchecked(by);
-                ca.$into_logical().into_series()
+                self.0.take_opt_chunked_unchecked(by).into_series()
             }
 
             fn take(&self, indices: &IdxCa) -> PolarsResult<Series> {
-                ChunkTake::take(self.0.deref(), indices.into())
-                    .map(|ca| ca.$into_logical().into_series())
+                let indices = if indices.chunks.len() > 1 {
+                    Cow::Owned(indices.rechunk())
+                } else {
+                    Cow::Borrowed(indices)
+                };
+                Ok(ChunkTake::take(&self.0, (&*indices).into())?.into_series())
             }
 
             fn take_iter(&self, iter: &mut dyn TakeIterator) -> PolarsResult<Series> {
-                ChunkTake::take(self.0.deref(), iter.into())
-                    .map(|ca| ca.$into_logical().into_series())
-            }
-
-            fn take_every(&self, n: usize) -> Series {
-                self.0.take_every(n).$into_logical().into_series()
+                Ok(ChunkTake::take(&self.0, iter.into())?.into_series())
             }
 
             unsafe fn take_iter_unchecked(&self, iter: &mut dyn TakeIterator) -> Series {
-                ChunkTake::take_unchecked(self.0.deref(), iter.into())
-                    .$into_logical()
-                    .into_series()
+                ChunkTake::take_unchecked(&self.0, iter.into()).into_series()
             }
 
             unsafe fn take_unchecked(&self, idx: &IdxCa) -> PolarsResult<Series> {
-                let mut out = ChunkTake::take_unchecked(self.0.deref(), idx.into());
-
+                let idx = if idx.chunks.len() > 1 {
+                    Cow::Owned(idx.rechunk())
+                } else {
+                    Cow::Borrowed(idx)
+                };
+                let mut out = ChunkTake::take_unchecked(&self.0, (&*idx).into());
                 if self.0.is_sorted_ascending_flag()
                     && (idx.is_sorted_ascending_flag() || idx.is_sorted_descending_flag())
                 {
-                    out.set_sorted_flag(idx.is_sorted_flag2())
+                    out.set_sorted_flag(idx.is_sorted_flag())
                 }
-
-                Ok(out.$into_logical().into_series())
+                Ok(out.into_series())
             }
 
             unsafe fn take_opt_iter_unchecked(&self, iter: &mut dyn TakeIteratorNulls) -> Series {
-                ChunkTake::take_unchecked(self.0.deref(), iter.into())
-                    .$into_logical()
-                    .into_series()
+                ChunkTake::take_unchecked(&self.0, iter.into()).into_series()
             }
 
             #[cfg(feature = "take_opt_iter")]
             fn take_opt_iter(&self, iter: &mut dyn TakeIteratorNulls) -> PolarsResult<Series> {
-                ChunkTake::take(self.0.deref(), iter.into())
-                    .map(|ca| ca.$into_logical().into_series())
+                Ok(ChunkTake::take(&self.0, iter.into())?.into_series())
             }
 
             fn len(&self) -> usize {
                 self.0.len()
             }
 
             fn rechunk(&self) -> Series {
-                self.0.rechunk().$into_logical().into_series()
+                self.0.rechunk().into_series()
             }
 
             fn new_from_index(&self, index: usize, length: usize) -> Series {
-                self.0
-                    .new_from_index(index, length)
-                    .$into_logical()
-                    .into_series()
+                ChunkExpandAtIndex::new_from_index(&self.0, index, length).into_series()
             }
 
             fn cast(&self, data_type: &DataType) -> PolarsResult<Series> {
-                match (self.dtype(), data_type) {
-                    (DataType::Date, DataType::Utf8) => Ok(self
-                        .0
-                        .clone()
-                        .into_series()
-                        .date()
-                        .unwrap()
-                        .strftime("%Y-%m-%d")
-                        .into_series()),
-                    (DataType::Time, DataType::Utf8) => Ok(self
-                        .0
-                        .clone()
-                        .into_series()
-                        .time()
-                        .unwrap()
-                        .strftime("%T")
-                        .into_series()),
-                    #[cfg(feature = "dtype-datetime")]
-                    (DataType::Time, DataType::Datetime(_, _)) => {
-                        polars_bail!(
-                            ComputeError:
-                            "cannot cast `Time` to `Datetime`; consider using 'dt.combine'"
-                        );
-                    }
-                    _ => self.0.cast(data_type),
-                }
+                self.0.cast(data_type)
             }
 
             fn get(&self, index: usize) -> PolarsResult<AnyValue> {
                 self.0.get_any_value(index)
             }
 
             #[inline]
             #[cfg(feature = "private")]
             unsafe fn get_unchecked(&self, index: usize) -> AnyValue {
                 self.0.get_any_value_unchecked(index)
             }
 
             fn sort_with(&self, options: SortOptions) -> Series {
-                self.0.sort_with(options).$into_logical().into_series()
+                ChunkSort::sort_with(&self.0, options).into_series()
             }
 
             fn arg_sort(&self, options: SortOptions) -> IdxCa {
-                self.0.arg_sort(options)
+                ChunkSort::arg_sort(&self.0, options)
             }
 
             fn null_count(&self) -> usize {
                 self.0.null_count()
             }
 
             fn has_validity(&self) -> bool {
                 self.0.has_validity()
             }
 
             fn unique(&self) -> PolarsResult<Series> {
-                self.0.unique().map(|ca| ca.$into_logical().into_series())
+                ChunkUnique::unique(&self.0).map(|ca| ca.into_series())
             }
 
             fn n_unique(&self) -> PolarsResult<usize> {
-                self.0.n_unique()
+                ChunkUnique::n_unique(&self.0)
             }
 
             fn arg_unique(&self) -> PolarsResult<IdxCa> {
-                self.0.arg_unique()
+                ChunkUnique::arg_unique(&self.0)
             }
 
             fn is_null(&self) -> BooleanChunked {
                 self.0.is_null()
             }
 
             fn is_not_null(&self) -> BooleanChunked {
                 self.0.is_not_null()
             }
 
             fn reverse(&self) -> Series {
-                self.0.reverse().$into_logical().into_series()
+                ChunkReverse::reverse(&self.0).into_series()
             }
 
             fn as_single_ptr(&mut self) -> PolarsResult<usize> {
                 self.0.as_single_ptr()
             }
 
             fn shift(&self, periods: i64) -> Series {
-                self.0.shift(periods).$into_logical().into_series()
+                ChunkShift::shift(&self.0, periods).into_series()
             }
 
             fn _sum_as_series(&self) -> Series {
-                Int32Chunked::full_null(self.name(), 1)
-                    .cast(self.dtype())
-                    .unwrap()
-                    .into()
+                ChunkAggSeries::sum_as_series(&self.0)
             }
             fn max_as_series(&self) -> Series {
-                self.0.max_as_series().$into_logical()
+                ChunkAggSeries::max_as_series(&self.0)
             }
             fn min_as_series(&self) -> Series {
-                self.0.min_as_series().$into_logical()
+                ChunkAggSeries::min_as_series(&self.0)
             }
             fn median_as_series(&self) -> Series {
-                Int32Chunked::full_null(self.name(), 1)
-                    .cast(self.dtype())
-                    .unwrap()
-                    .into()
-            }
-            fn var_as_series(&self, _ddof: u8) -> Series {
-                Int32Chunked::full_null(self.name(), 1)
-                    .cast(self.dtype())
-                    .unwrap()
-                    .into()
-            }
-            fn std_as_series(&self, _ddof: u8) -> Series {
-                Int32Chunked::full_null(self.name(), 1)
-                    .cast(self.dtype())
-                    .unwrap()
-                    .into()
+                QuantileAggSeries::median_as_series(&self.0)
+            }
+            fn var_as_series(&self, ddof: u8) -> Series {
+                VarAggSeries::var_as_series(&self.0, ddof)
+            }
+            fn std_as_series(&self, ddof: u8) -> Series {
+                VarAggSeries::std_as_series(&self.0, ddof)
             }
             fn quantile_as_series(
                 &self,
-                _quantile: f64,
-                _interpol: QuantileInterpolOptions,
+                quantile: f64,
+                interpol: QuantileInterpolOptions,
             ) -> PolarsResult<Series> {
-                Ok(Int32Chunked::full_null(self.name(), 1)
-                    .cast(self.dtype())
-                    .unwrap()
-                    .into())
+                QuantileAggSeries::quantile_as_series(&self.0, quantile, interpol)
             }
 
             fn clone_inner(&self) -> Arc<dyn SeriesTrait> {
                 Arc::new(SeriesWrap(Clone::clone(&self.0)))
             }
 
             fn peak_max(&self) -> BooleanChunked {
                 self.0.peak_max()
             }
 
             fn peak_min(&self) -> BooleanChunked {
                 self.0.peak_min()
             }
+
             #[cfg(feature = "is_in")]
             fn is_in(&self, other: &Series) -> PolarsResult<BooleanChunked> {
-                self.0.is_in(other)
+                IsIn::is_in(&self.0, other)
             }
             #[cfg(feature = "repeat_by")]
-            fn repeat_by(&self, by: &IdxCa) -> ListChunked {
-                match self.0.dtype() {
-                    DataType::Date => self
-                        .0
-                        .repeat_by(by)
-                        .cast(&DataType::List(Box::new(DataType::Date)))
-                        .unwrap()
-                        .list()
-                        .unwrap()
-                        .clone(),
-                    DataType::Time => self
-                        .0
-                        .repeat_by(by)
-                        .cast(&DataType::List(Box::new(DataType::Time)))
-                        .unwrap()
-                        .list()
-                        .unwrap()
-                        .clone(),
-                    _ => unreachable!(),
-                }
+            fn repeat_by(&self, by: &IdxCa) -> PolarsResult<ListChunked> {
+                RepeatBy::repeat_by(&self.0, by)
+            }
+
+            #[cfg(feature = "checked_arithmetic")]
+            fn checked_div(&self, rhs: &Series) -> PolarsResult<Series> {
+                self.0.checked_div(rhs)
+            }
+
+            #[cfg(feature = "object")]
+            fn as_any(&self) -> &dyn Any {
+                &self.0
             }
             #[cfg(feature = "mode")]
             fn mode(&self) -> PolarsResult<Series> {
-                self.0.mode().map(|ca| ca.$into_logical().into_series())
+                Ok(self.0.mode()?.into_series())
             }
-        }
-    };
-}
-
-#[cfg(feature = "dtype-date")]
-impl_dyn_series!(DateChunked, into_date);
-#[cfg(feature = "dtype-time")]
-impl_dyn_series!(TimeChunked, into_time);
 
-macro_rules! impl_dyn_series_numeric {
-    ($ca: ident) => {
-        impl private::PrivateSeriesNumeric for SeriesWrap<$ca> {
-            fn bit_repr_is_large(&self) -> bool {
-                if let DataType::Time = self.dtype() {
-                    true
-                } else {
-                    false
-                }
+            #[cfg(feature = "concat_str")]
+            fn str_concat(&self, delimiter: &str) -> Utf8Chunked {
+                self.0.str_concat(delimiter)
             }
-            fn bit_repr_large(&self) -> UInt64Chunked {
-                self.0.bit_repr_large()
-            }
-            fn bit_repr_small(&self) -> UInt32Chunked {
-                self.0.bit_repr_small()
+
+            fn tile(&self, n: usize) -> Series {
+                self.0.tile(n).into_series()
             }
         }
     };
 }
 
-#[cfg(feature = "dtype-date")]
-impl_dyn_series_numeric!(DateChunked);
-#[cfg(feature = "dtype-time")]
-impl_dyn_series_numeric!(TimeChunked);
+#[cfg(feature = "dtype-u8")]
+impl_dyn_series!(UInt8Chunked);
+#[cfg(feature = "dtype-u16")]
+impl_dyn_series!(UInt16Chunked);
+impl_dyn_series!(UInt32Chunked);
+impl_dyn_series!(UInt64Chunked);
+#[cfg(feature = "dtype-i8")]
+impl_dyn_series!(Int8Chunked);
+#[cfg(feature = "dtype-i16")]
+impl_dyn_series!(Int16Chunked);
+impl_dyn_series!(Int32Chunked);
+impl_dyn_series!(Int64Chunked);
+
+impl<T: PolarsNumericType> private::PrivateSeriesNumeric for SeriesWrap<ChunkedArray<T>> {
+    fn bit_repr_is_large(&self) -> bool {
+        ChunkedArray::<T>::bit_repr_is_large()
+    }
+    fn bit_repr_large(&self) -> UInt64Chunked {
+        self.0.bit_repr_large()
+    }
+    fn bit_repr_small(&self) -> UInt32Chunked {
+        self.0.bit_repr_small()
+    }
+}
+
+impl private::PrivateSeriesNumeric for SeriesWrap<Utf8Chunked> {}
+impl private::PrivateSeriesNumeric for SeriesWrap<BinaryChunked> {}
+impl private::PrivateSeriesNumeric for SeriesWrap<ListChunked> {}
+#[cfg(feature = "dtype-array")]
+impl private::PrivateSeriesNumeric for SeriesWrap<ArrayChunked> {}
+impl private::PrivateSeriesNumeric for SeriesWrap<BooleanChunked> {
+    fn bit_repr_is_large(&self) -> bool {
+        false
+    }
+    fn bit_repr_small(&self) -> UInt32Chunked {
+        self.0
+            .cast(&DataType::UInt32)
+            .unwrap()
+            .u32()
+            .unwrap()
+            .clone()
+    }
+}
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/datetime.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/datetime.rs`

 * *Files 5% similar despite different names*

```diff
@@ -158,16 +158,16 @@
     fn remainder(&self, rhs: &Series) -> PolarsResult<Series> {
         polars_bail!(opq = rem, self.dtype(), rhs.dtype());
     }
     fn group_tuples(&self, multithreaded: bool, sorted: bool) -> PolarsResult<GroupsProxy> {
         self.0.group_tuples(multithreaded, sorted)
     }
 
-    fn arg_sort_multiple(&self, by: &[Series], descending: &[bool]) -> PolarsResult<IdxCa> {
-        self.0.deref().arg_sort_multiple(by, descending)
+    fn arg_sort_multiple(&self, options: &SortMultipleOptions) -> PolarsResult<IdxCa> {
+        self.0.deref().arg_sort_multiple(options)
     }
 }
 
 impl SeriesTrait for SeriesWrap<DatetimeChunked> {
     fn is_sorted_flag(&self) -> IsSorted {
         if self.0.is_sorted_ascending_flag() {
             IsSorted::Ascending
@@ -257,34 +257,27 @@
     fn take_iter(&self, iter: &mut dyn TakeIterator) -> PolarsResult<Series> {
         ChunkTake::take(self.0.deref(), iter.into()).map(|ca| {
             ca.into_datetime(self.0.time_unit(), self.0.time_zone().clone())
                 .into_series()
         })
     }
 
-    fn take_every(&self, n: usize) -> Series {
-        self.0
-            .take_every(n)
-            .into_datetime(self.0.time_unit(), self.0.time_zone().clone())
-            .into_series()
-    }
-
     unsafe fn take_iter_unchecked(&self, iter: &mut dyn TakeIterator) -> Series {
         ChunkTake::take_unchecked(self.0.deref(), iter.into())
             .into_datetime(self.0.time_unit(), self.0.time_zone().clone())
             .into_series()
     }
 
     unsafe fn take_unchecked(&self, idx: &IdxCa) -> PolarsResult<Series> {
         let mut out = ChunkTake::take_unchecked(self.0.deref(), idx.into());
 
         if self.0.is_sorted_ascending_flag()
             && (idx.is_sorted_ascending_flag() || idx.is_sorted_descending_flag())
         {
-            out.set_sorted_flag(idx.is_sorted_flag2())
+            out.set_sorted_flag(idx.is_sorted_flag())
         }
 
         Ok(out
             .into_datetime(self.0.time_unit(), self.0.time_zone().clone())
             .into_series())
     }
 
@@ -319,21 +312,21 @@
             .into_datetime(self.0.time_unit(), self.0.time_zone().clone())
             .into_series()
     }
 
     fn cast(&self, data_type: &DataType) -> PolarsResult<Series> {
         match (data_type, self.0.time_unit()) {
             (DataType::Utf8, TimeUnit::Milliseconds) => {
-                Ok(self.0.strftime("%F %T%.3f")?.into_series())
+                Ok(self.0.to_string("%F %T%.3f")?.into_series())
             }
             (DataType::Utf8, TimeUnit::Microseconds) => {
-                Ok(self.0.strftime("%F %T%.6f")?.into_series())
+                Ok(self.0.to_string("%F %T%.6f")?.into_series())
             }
             (DataType::Utf8, TimeUnit::Nanoseconds) => {
-                Ok(self.0.strftime("%F %T%.9f")?.into_series())
+                Ok(self.0.to_string("%F %T%.9f")?.into_series())
             }
             _ => self.0.cast(data_type),
         }
     }
 
     fn get(&self, index: usize) -> PolarsResult<AnyValue> {
         self.0.get_any_value(index)
@@ -457,25 +450,26 @@
         self.0.peak_min()
     }
     #[cfg(feature = "is_in")]
     fn is_in(&self, other: &Series) -> PolarsResult<BooleanChunked> {
         self.0.is_in(other)
     }
     #[cfg(feature = "repeat_by")]
-    fn repeat_by(&self, by: &IdxCa) -> ListChunked {
-        self.0
-            .repeat_by(by)
+    fn repeat_by(&self, by: &IdxCa) -> PolarsResult<ListChunked> {
+        Ok(self
+            .0
+            .repeat_by(by)?
             .cast(&DataType::List(Box::new(DataType::Datetime(
                 self.0.time_unit(),
                 self.0.time_zone().clone(),
             ))))
             .unwrap()
             .list()
             .unwrap()
-            .clone()
+            .clone())
     }
     #[cfg(feature = "mode")]
     fn mode(&self) -> PolarsResult<Series> {
         self.0.mode().map(|ca| {
             ca.into_datetime(self.0.time_unit(), self.0.time_zone().clone())
                 .into_series()
         })
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/decimal.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/decimal.rs`

 * *Files 2% similar despite different names*

```diff
@@ -58,15 +58,16 @@
 
     fn slice(&self, offset: i64, length: usize) -> Series {
         self.apply_logical(|ca| ca.slice(offset, length))
     }
 
     fn append(&mut self, other: &Series) -> PolarsResult<()> {
         polars_ensure!(self.0.dtype() == other.dtype(), append);
-        self.0.append(other.as_ref().as_ref());
+        let other = other.decimal()?;
+        self.0.append(&other.0);
         Ok(())
     }
 
     fn extend(&mut self, other: &Series) -> PolarsResult<()> {
         polars_ensure!(self.0.dtype() == other.dtype(), extend);
         self.0.extend(other.as_ref().as_ref());
         Ok(())
@@ -107,15 +108,15 @@
 
     unsafe fn take_unchecked(&self, idx: &IdxCa) -> PolarsResult<Series> {
         let mut out = ChunkTake::take_unchecked(self.0.deref(), idx.into());
 
         if self.0.is_sorted_ascending_flag()
             && (idx.is_sorted_ascending_flag() || idx.is_sorted_descending_flag())
         {
-            out.set_sorted_flag(idx.is_sorted_flag2())
+            out.set_sorted_flag(idx.is_sorted_flag())
         }
 
         Ok(out
             .into_decimal_unchecked(self.0.precision(), self.0.scale())
             .into_series())
     }
 
@@ -138,21 +139,14 @@
 
     fn rechunk(&self) -> Series {
         let ca = self.0.rechunk();
         ca.into_decimal_unchecked(self.0.precision(), self.0.scale())
             .into_series()
     }
 
-    fn take_every(&self, n: usize) -> Series {
-        self.0
-            .take_every(n)
-            .into_decimal_unchecked(self.0.precision(), self.0.scale())
-            .into_series()
-    }
-
     fn new_from_index(&self, index: usize, length: usize) -> Series {
         self.0
             .new_from_index(index, length)
             .into_decimal_unchecked(self.0.precision(), self.0.scale())
             .into_series()
     }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/duration.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/duration.rs`

 * *Files 2% similar despite different names*

```diff
@@ -194,16 +194,16 @@
             .into_duration(self.0.time_unit())
             .into_series())
     }
     fn group_tuples(&self, multithreaded: bool, sorted: bool) -> PolarsResult<GroupsProxy> {
         self.0.group_tuples(multithreaded, sorted)
     }
 
-    fn arg_sort_multiple(&self, by: &[Series], descending: &[bool]) -> PolarsResult<IdxCa> {
-        self.0.deref().arg_sort_multiple(by, descending)
+    fn arg_sort_multiple(&self, options: &SortMultipleOptions) -> PolarsResult<IdxCa> {
+        self.0.deref().arg_sort_multiple(options)
     }
 }
 
 impl SeriesTrait for SeriesWrap<DurationChunked> {
     fn is_sorted_flag(&self) -> IsSorted {
         if self.0.is_sorted_ascending_flag() {
             IsSorted::Ascending
@@ -286,34 +286,27 @@
     }
 
     fn take_iter(&self, iter: &mut dyn TakeIterator) -> PolarsResult<Series> {
         ChunkTake::take(self.0.deref(), iter.into())
             .map(|ca| ca.into_duration(self.0.time_unit()).into_series())
     }
 
-    fn take_every(&self, n: usize) -> Series {
-        self.0
-            .take_every(n)
-            .into_duration(self.0.time_unit())
-            .into_series()
-    }
-
     unsafe fn take_iter_unchecked(&self, iter: &mut dyn TakeIterator) -> Series {
         ChunkTake::take_unchecked(self.0.deref(), iter.into())
             .into_duration(self.0.time_unit())
             .into_series()
     }
 
     unsafe fn take_unchecked(&self, idx: &IdxCa) -> PolarsResult<Series> {
         let mut out = ChunkTake::take_unchecked(self.0.deref(), idx.into());
 
         if self.0.is_sorted_ascending_flag()
             && (idx.is_sorted_ascending_flag() || idx.is_sorted_descending_flag())
         {
-            out.set_sorted_flag(idx.is_sorted_flag2())
+            out.set_sorted_flag(idx.is_sorted_flag())
         }
 
         Ok(out.into_duration(self.0.time_unit()).into_series())
     }
 
     unsafe fn take_opt_iter_unchecked(&self, iter: &mut dyn TakeIteratorNulls) -> Series {
         ChunkTake::take_unchecked(self.0.deref(), iter.into())
@@ -460,24 +453,25 @@
         self.0.peak_min()
     }
     #[cfg(feature = "is_in")]
     fn is_in(&self, other: &Series) -> PolarsResult<BooleanChunked> {
         self.0.is_in(other)
     }
     #[cfg(feature = "repeat_by")]
-    fn repeat_by(&self, by: &IdxCa) -> ListChunked {
-        self.0
-            .repeat_by(by)
+    fn repeat_by(&self, by: &IdxCa) -> PolarsResult<ListChunked> {
+        Ok(self
+            .0
+            .repeat_by(by)?
             .cast(&DataType::List(Box::new(DataType::Duration(
                 self.0.time_unit(),
             ))))
             .unwrap()
             .list()
             .unwrap()
-            .clone()
+            .clone())
     }
     #[cfg(feature = "mode")]
     fn mode(&self) -> PolarsResult<Series> {
         self.0
             .mode()
             .map(|ca| ca.into_duration(self.0.time_unit()).into_series())
     }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/floats.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/floats.rs`

 * *Files 1% similar despite different names*

```diff
@@ -133,16 +133,16 @@
             fn remainder(&self, rhs: &Series) -> PolarsResult<Series> {
                 NumOpsDispatch::remainder(&self.0, rhs)
             }
             fn group_tuples(&self, multithreaded: bool, sorted: bool) -> PolarsResult<GroupsProxy> {
                 IntoGroupsProxy::group_tuples(&self.0, multithreaded, sorted)
             }
 
-            fn arg_sort_multiple(&self, by: &[Series], descending: &[bool]) -> PolarsResult<IdxCa> {
-                self.0.arg_sort_multiple(by, descending)
+            fn arg_sort_multiple(&self, options: &SortMultipleOptions) -> PolarsResult<IdxCa> {
+                self.0.arg_sort_multiple(options)
             }
         }
 
         impl SeriesTrait for SeriesWrap<$ca> {
             fn is_sorted_flag(&self) -> IsSorted {
                 if self.0.is_sorted_ascending_flag() {
                     IsSorted::Ascending
@@ -227,18 +227,14 @@
                 Ok(ChunkTake::take(&self.0, (&*indices).into())?.into_series())
             }
 
             fn take_iter(&self, iter: &mut dyn TakeIterator) -> PolarsResult<Series> {
                 Ok(ChunkTake::take(&self.0, iter.into())?.into_series())
             }
 
-            fn take_every(&self, n: usize) -> Series {
-                self.0.take_every(n).into_series()
-            }
-
             unsafe fn take_iter_unchecked(&self, iter: &mut dyn TakeIterator) -> Series {
                 ChunkTake::take_unchecked(&self.0, iter.into()).into_series()
             }
 
             unsafe fn take_unchecked(&self, idx: &IdxCa) -> PolarsResult<Series> {
                 let idx = if idx.chunks.len() > 1 {
                     Cow::Owned(idx.rechunk())
@@ -247,15 +243,15 @@
                 };
 
                 let mut out = ChunkTake::take_unchecked(&self.0, (&*idx).into());
 
                 if self.0.is_sorted_ascending_flag()
                     && (idx.is_sorted_ascending_flag() || idx.is_sorted_descending_flag())
                 {
-                    out.set_sorted_flag(idx.is_sorted_flag2())
+                    out.set_sorted_flag(idx.is_sorted_flag())
                 }
 
                 Ok(out.into_series())
             }
 
             unsafe fn take_opt_iter_unchecked(&self, iter: &mut dyn TakeIteratorNulls) -> Series {
                 ChunkTake::take_unchecked(&self.0, iter.into()).into_series()
@@ -379,15 +375,15 @@
             }
 
             #[cfg(feature = "is_in")]
             fn is_in(&self, other: &Series) -> PolarsResult<BooleanChunked> {
                 IsIn::is_in(&self.0, other)
             }
             #[cfg(feature = "repeat_by")]
-            fn repeat_by(&self, by: &IdxCa) -> ListChunked {
+            fn repeat_by(&self, by: &IdxCa) -> PolarsResult<ListChunked> {
                 RepeatBy::repeat_by(&self.0, by)
             }
 
             #[cfg(feature = "checked_arithmetic")]
             fn checked_div(&self, rhs: &Series) -> PolarsResult<Series> {
                 self.0.checked_div(rhs)
             }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/list.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/list.rs`

 * *Files 2% similar despite different names*

```diff
@@ -102,18 +102,14 @@
         Ok(ChunkTake::take(&self.0, (&*indices).into())?.into_series())
     }
 
     fn take_iter(&self, iter: &mut dyn TakeIterator) -> PolarsResult<Series> {
         Ok(ChunkTake::take(&self.0, iter.into())?.into_series())
     }
 
-    fn take_every(&self, n: usize) -> Series {
-        self.0.take_every(n).into_series()
-    }
-
     unsafe fn take_iter_unchecked(&self, iter: &mut dyn TakeIterator) -> Series {
         ChunkTake::take_unchecked(&self.0, iter.into()).into_series()
     }
 
     unsafe fn take_unchecked(&self, idx: &IdxCa) -> PolarsResult<Series> {
         let idx = if idx.chunks.len() > 1 {
             Cow::Owned(idx.rechunk())
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/null.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/null.rs`

 * *Files 2% similar despite different names*

```diff
@@ -109,18 +109,14 @@
         Ok(NullChunked::new(self.name.clone(), indices.len()).into_series())
     }
 
     fn len(&self) -> usize {
         self.length as usize
     }
 
-    fn take_every(&self, n: usize) -> Series {
-        NullChunked::new(self.name.clone(), self.len() / n).into_series()
-    }
-
     fn has_validity(&self) -> bool {
         true
     }
 
     fn rechunk(&self) -> Series {
         self.clone().into_series()
     }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/object.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/object.rs`

 * *Files 1% similar despite different names*

```diff
@@ -155,18 +155,14 @@
     }
 
     fn rechunk(&self) -> Series {
         // do not call normal rechunk
         self.rechunk_object().into_series()
     }
 
-    fn take_every(&self, n: usize) -> Series {
-        self.0.take_every(n).into_series()
-    }
-
     fn new_from_index(&self, index: usize, length: usize) -> Series {
         ChunkExpandAtIndex::new_from_index(&self.0, index, length).into_series()
     }
 
     fn cast(&self, data_type: &DataType) -> PolarsResult<Series> {
         if matches!(data_type, DataType::Object(_)) {
             Ok(self.0.clone().into_series())
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/struct_.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/struct_.rs`

 * *Files 5% similar despite different names*

```diff
@@ -79,18 +79,14 @@
 }
 
 impl SeriesTrait for SeriesWrap<StructChunked> {
     fn rename(&mut self, name: &str) {
         self.0.rename(name)
     }
 
-    fn take_every(&self, n: usize) -> Series {
-        self.0.apply_fields(|s| s.take_every(n)).into_series()
-    }
-
     fn has_validity(&self) -> bool {
         self.0.fields().iter().any(|s| s.has_validity())
     }
 
     /// Name of series.
     fn name(&self) -> &str {
         self.0.name()
@@ -276,29 +272,49 @@
     /// Count the null values.
     fn null_count(&self) -> usize {
         self.0.null_count()
     }
 
     /// Get unique values in the Series.
     fn unique(&self) -> PolarsResult<Series> {
-        let groups = self.group_tuples(true, false);
+        // this can called in aggregation, so this fast path can be worth a lot
+        if self.len() < 2 {
+            return Ok(self.0.clone().into_series());
+        }
+        let main_thread = POOL.current_thread_index().is_none();
+        let groups = self.group_tuples(main_thread, false);
         // safety:
         // groups are in bounds
         Ok(unsafe { self.0.clone().into_series().agg_first(&groups?) })
     }
 
     /// Get unique values in the Series.
     fn n_unique(&self) -> PolarsResult<usize> {
-        let groups = self.group_tuples(true, false)?;
-        Ok(groups.len())
+        // this can called in aggregation, so this fast path can be worth a lot
+        match self.len() {
+            0 => Ok(0),
+            1 => Ok(1),
+            _ => {
+                // TODO! try row encoding
+                let main_thread = POOL.current_thread_index().is_none();
+                let groups = self.group_tuples(main_thread, false)?;
+                Ok(groups.len())
+            }
+        }
     }
 
     /// Get first indexes of unique values.
     fn arg_unique(&self) -> PolarsResult<IdxCa> {
-        let groups = self.group_tuples(true, false)?;
+        // this can called in aggregation, so this fast path can be worth a lot
+        if self.len() == 1 {
+            return Ok(IdxCa::new_vec(self.name(), vec![0 as IdxSize]));
+        }
+        // TODO! try row encoding
+        let main_thread = POOL.current_thread_index().is_none();
+        let groups = self.group_tuples(main_thread, false)?;
         let first = groups.take_group_firsts();
         Ok(IdxCa::from_vec(self.name(), first))
     }
 
     /// Get a mask of the null values.
     fn is_null(&self) -> BooleanChunked {
         let is_null = self.0.fields().iter().map(|s| s.is_null());
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/implementations/utf8.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/implementations/utf8.rs`

 * *Files 1% similar despite different names*

```diff
@@ -91,16 +91,16 @@
     fn remainder(&self, rhs: &Series) -> PolarsResult<Series> {
         NumOpsDispatch::remainder(&self.0, rhs)
     }
     fn group_tuples(&self, multithreaded: bool, sorted: bool) -> PolarsResult<GroupsProxy> {
         IntoGroupsProxy::group_tuples(&self.0, multithreaded, sorted)
     }
 
-    fn arg_sort_multiple(&self, by: &[Series], descending: &[bool]) -> PolarsResult<IdxCa> {
-        self.0.arg_sort_multiple(by, descending)
+    fn arg_sort_multiple(&self, options: &SortMultipleOptions) -> PolarsResult<IdxCa> {
+        self.0.arg_sort_multiple(options)
     }
 }
 
 impl SeriesTrait for SeriesWrap<Utf8Chunked> {
     fn is_sorted_flag(&self) -> IsSorted {
         if self.0.is_sorted_ascending_flag() {
             IsSorted::Ascending
@@ -175,18 +175,14 @@
         Ok(ChunkTake::take(&self.0, (&*indices).into())?.into_series())
     }
 
     fn take_iter(&self, iter: &mut dyn TakeIterator) -> PolarsResult<Series> {
         Ok(ChunkTake::take(&self.0, iter.into())?.into_series())
     }
 
-    fn take_every(&self, n: usize) -> Series {
-        self.0.take_every(n).into_series()
-    }
-
     unsafe fn take_iter_unchecked(&self, iter: &mut dyn TakeIterator) -> Series {
         ChunkTake::take_unchecked(&self.0, iter.into()).into_series()
     }
 
     unsafe fn take_unchecked(&self, idx: &IdxCa) -> PolarsResult<Series> {
         let idx = if idx.chunks.len() > 1 {
             Cow::Owned(idx.rechunk())
@@ -195,15 +191,15 @@
         };
 
         let mut out = ChunkTake::take_unchecked(&self.0, (&*idx).into());
 
         if self.0.is_sorted_ascending_flag()
             && (idx.is_sorted_ascending_flag() || idx.is_sorted_descending_flag())
         {
-            out.set_sorted_flag(idx.is_sorted_flag2())
+            out.set_sorted_flag(idx.is_sorted_flag())
         }
 
         Ok(out.into_series())
     }
 
     unsafe fn take_opt_iter_unchecked(&self, iter: &mut dyn TakeIteratorNulls) -> Series {
         ChunkTake::take_unchecked(&self.0, iter.into()).into_series()
@@ -302,15 +298,15 @@
     }
 
     #[cfg(feature = "is_in")]
     fn is_in(&self, other: &Series) -> PolarsResult<BooleanChunked> {
         IsIn::is_in(&self.0, other)
     }
     #[cfg(feature = "repeat_by")]
-    fn repeat_by(&self, by: &IdxCa) -> ListChunked {
+    fn repeat_by(&self, by: &IdxCa) -> PolarsResult<ListChunked> {
         RepeatBy::repeat_by(&self.0, by)
     }
 
     #[cfg(feature = "mode")]
     fn mode(&self) -> PolarsResult<Series> {
         Ok(self.0.mode()?.into_series())
     }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/into.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/into.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/iterator.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/iterator.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/mod.rs`

 * *Files 1% similar despite different names*

```diff
@@ -278,14 +278,18 @@
             }
             dt if dt.is_numeric() => {
                 with_match_physical_numeric_polars_type!(dt, |$T| {
                     let ca: &ChunkedArray<$T> = self.as_ref().as_ref().as_ref();
                         ca.cast_unchecked(dtype)
                 })
             }
+            DataType::Binary => {
+                let ca = self.binary().unwrap();
+                ca.cast_unchecked(dtype)
+            }
             _ => self.cast(dtype),
         }
     }
 
     /// Compute the sum of all values in this Series.
     /// Returns `Some(0)` if the array is empty, and `None` if the array only
     /// contains null values.
@@ -338,20 +342,19 @@
     {
         self.max_as_series()
             .cast(&DataType::Float64)
             .ok()
             .and_then(|s| s.f64().unwrap().get(0).and_then(T::from))
     }
 
-    /// Explode a list or utf8 Series. This expands every item to a new row..
+    /// Explode a list Series. This expands every item to a new row..
     pub fn explode(&self) -> PolarsResult<Series> {
         match self.dtype() {
             DataType::List(_) => self.list().unwrap().explode(),
-            DataType::Utf8 => self.utf8().unwrap().explode(),
-            _ => polars_bail!(opq = explode, self.dtype()),
+            _ => Ok(self.clone()),
         }
     }
 
     /// Check if float value is NaN (note this is different than missing/ null)
     pub fn is_nan(&self) -> PolarsResult<BooleanChunked> {
         match self.dtype() {
             DataType::Float32 => Ok(self.f32().unwrap().is_nan()),
@@ -398,22 +401,24 @@
     /// Cast a datelike Series to their physical representation.
     /// Primitives remain unchanged
     ///
     /// * Date -> Int32
     /// * Datetime-> Int64
     /// * Time -> Int64
     /// * Categorical -> UInt32
+    /// * List(inner) -> List(physical of inner)
     ///
     pub fn to_physical_repr(&self) -> Cow<Series> {
         use DataType::*;
         match self.dtype() {
             Date => Cow::Owned(self.cast(&Int32).unwrap()),
             Datetime(_, _) | Duration(_) | Time => Cow::Owned(self.cast(&Int64).unwrap()),
             #[cfg(feature = "dtype-categorical")]
             Categorical(_) => Cow::Owned(self.cast(&UInt32).unwrap()),
+            List(inner) => Cow::Owned(self.cast(&List(Box::new(inner.to_physical()))).unwrap()),
             _ => Cow::Borrowed(self),
         }
     }
 
     fn finish_take_threaded(&self, s: Vec<Series>, rechunk: bool) -> Series {
         let s = s
             .into_iter()
@@ -1008,25 +1013,31 @@
 }
 
 impl<'a, T> AsRef<ChunkedArray<T>> for dyn SeriesTrait + 'a
 where
     T: 'static + PolarsDataType,
 {
     fn as_ref(&self) -> &ChunkedArray<T> {
-        if &T::get_dtype() == self.dtype() ||
-            // needed because we want to get ref of List no matter what the inner type is.
-            (matches!(T::get_dtype(), DataType::List(_)) && matches!(self.dtype(), DataType::List(_)))
-        {
-            unsafe { &*(self as *const dyn SeriesTrait as *const ChunkedArray<T>) }
-        } else {
-            panic!(
-                "implementation error, cannot get ref {:?} from {:?}",
-                T::get_dtype(),
-                self.dtype()
-            )
+        match T::get_dtype() {
+            #[cfg(feature = "dtype-decimal")]
+            DataType::Decimal(None, None) => panic!("impl error"),
+            _ => {
+                if &T::get_dtype() == self.dtype() ||
+                    // needed because we want to get ref of List no matter what the inner type is.
+                    (matches!(T::get_dtype(), DataType::List(_)) && matches!(self.dtype(), DataType::List(_)))
+                {
+                    unsafe { &*(self as *const dyn SeriesTrait as *const ChunkedArray<T>) }
+                } else {
+                    panic!(
+                        "implementation error, cannot get ref {:?} from {:?}",
+                        T::get_dtype(),
+                        self.dtype()
+                    );
+                }
+            }
         }
     }
 }
 
 impl<'a, T> AsMut<ChunkedArray<T>> for dyn SeriesTrait + 'a
 where
     T: 'static + PolarsDataType,
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/ops/diff.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/ops/diff.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/ops/downcast.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/ops/downcast.rs`

 * *Files 6% similar despite different names*

```diff
@@ -10,20 +10,20 @@
                 SchemaMismatch: "invalid series dtype: expected `{}`, got `{}`", $name, dt,
             ),
         }
     };
 }
 
 impl Series {
-    /// Unpack to ChunkedArray of dtype i8
+    /// Unpack to ChunkedArray of dtype `[DataType::Int8]`
     pub fn i8(&self) -> PolarsResult<&Int8Chunked> {
         unpack_chunked!(self, DataType::Int8 => Int8Chunked, "Int8")
     }
 
-    /// Unpack to ChunkedArray i16
+    /// Unpack to ChunkedArray of dtype `[DataType::Int16]`
     pub fn i16(&self) -> PolarsResult<&Int16Chunked> {
         unpack_chunked!(self, DataType::Int16 => Int16Chunked, "Int16")
     }
 
     /// Unpack to ChunkedArray
     /// ```
     /// # use polars_core::prelude::*;
@@ -34,110 +34,117 @@
     ///     .map(|opt_v| {
     ///         match opt_v {
     ///             Some(v) => Some(v * v),
     ///             None => None, // null value
     ///         }
     /// }).collect();
     /// ```
+    /// Unpack to ChunkedArray of dtype `[DataType::Int32]`
     pub fn i32(&self) -> PolarsResult<&Int32Chunked> {
         unpack_chunked!(self, DataType::Int32 => Int32Chunked, "Int32")
     }
 
-    /// Unpack to ChunkedArray of dtype i64
+    /// Unpack to ChunkedArray of dtype `[DataType::Int64]`
     pub fn i64(&self) -> PolarsResult<&Int64Chunked> {
         unpack_chunked!(self, DataType::Int64 => Int64Chunked, "Int64")
     }
 
-    /// Unpack to ChunkedArray of dtype f32
+    /// Unpack to ChunkedArray of dtype `[DataType::Float32]`
     pub fn f32(&self) -> PolarsResult<&Float32Chunked> {
         unpack_chunked!(self, DataType::Float32 => Float32Chunked, "Float32")
     }
 
-    /// Unpack to ChunkedArray of dtype f64
+    /// Unpack to ChunkedArray of dtype `[DataType::Float64]`
     pub fn f64(&self) -> PolarsResult<&Float64Chunked> {
         unpack_chunked!(self, DataType::Float64 => Float64Chunked, "Float64")
     }
 
-    /// Unpack to ChunkedArray of dtype u8
+    /// Unpack to ChunkedArray of dtype `[DataType::UInt8]`
     pub fn u8(&self) -> PolarsResult<&UInt8Chunked> {
         unpack_chunked!(self, DataType::UInt8 => UInt8Chunked, "UInt8")
     }
 
-    /// Unpack to ChunkedArray of dtype u16
+    /// Unpack to ChunkedArray of dtype `[DataType::UInt16]`
     pub fn u16(&self) -> PolarsResult<&UInt16Chunked> {
         unpack_chunked!(self, DataType::UInt16 => UInt16Chunked, "UInt16")
     }
 
-    /// Unpack to ChunkedArray of dtype u32
+    /// Unpack to ChunkedArray of dtype `[DataType::UInt32]`
     pub fn u32(&self) -> PolarsResult<&UInt32Chunked> {
         unpack_chunked!(self, DataType::UInt32 => UInt32Chunked, "UInt32")
     }
 
-    /// Unpack to ChunkedArray of dtype u64
+    /// Unpack to ChunkedArray of dtype `[DataType::UInt64]`
     pub fn u64(&self) -> PolarsResult<&UInt64Chunked> {
         unpack_chunked!(self, DataType::UInt64 => UInt64Chunked, "UInt64")
     }
 
-    /// Unpack to ChunkedArray of dtype bool
+    /// Unpack to ChunkedArray of dtype `[DataType::Boolean]`
     pub fn bool(&self) -> PolarsResult<&BooleanChunked> {
         unpack_chunked!(self, DataType::Boolean => BooleanChunked, "Boolean")
     }
 
-    /// Unpack to ChunkedArray of dtype utf8
+    /// Unpack to ChunkedArray of dtype `[DataType::Utf8]`
     pub fn utf8(&self) -> PolarsResult<&Utf8Chunked> {
         unpack_chunked!(self, DataType::Utf8 => Utf8Chunked, "Utf8")
     }
 
-    /// Unpack to ChunkedArray of dtype binary
+    /// Unpack to ChunkedArray of dtype `[DataType::Binary]`
     pub fn binary(&self) -> PolarsResult<&BinaryChunked> {
         unpack_chunked!(self, DataType::Binary => BinaryChunked, "Binary")
     }
 
-    /// Unpack to ChunkedArray of dtype Time
+    /// Unpack to ChunkedArray of dtype `[DataType::Time]`
     #[cfg(feature = "dtype-time")]
     pub fn time(&self) -> PolarsResult<&TimeChunked> {
         unpack_chunked!(self, DataType::Time => TimeChunked, "Time")
     }
 
-    /// Unpack to ChunkedArray of dtype Date
+    /// Unpack to ChunkedArray of dtype `[DataType::Date]`
     #[cfg(feature = "dtype-date")]
     pub fn date(&self) -> PolarsResult<&DateChunked> {
         unpack_chunked!(self, DataType::Date => DateChunked, "Date")
     }
 
-    /// Unpack to ChunkedArray of dtype datetime
+    /// Unpack to ChunkedArray of dtype `[DataType::Datetime]`
     #[cfg(feature = "dtype-datetime")]
     pub fn datetime(&self) -> PolarsResult<&DatetimeChunked> {
         unpack_chunked!(self, DataType::Datetime(_, _) => DatetimeChunked, "Datetime")
     }
 
-    /// Unpack to ChunkedArray of dtype duration
+    /// Unpack to ChunkedArray of dtype `[DataType::Duration]`
     #[cfg(feature = "dtype-duration")]
     pub fn duration(&self) -> PolarsResult<&DurationChunked> {
         unpack_chunked!(self, DataType::Duration(_) => DurationChunked, "Duration")
     }
 
-    /// Unpack to ChunkedArray of dtype decimal
+    /// Unpack to ChunkedArray of dtype `[DataType::Decimal]`
     #[cfg(feature = "dtype-decimal")]
     pub fn decimal(&self) -> PolarsResult<&DecimalChunked> {
         unpack_chunked!(self, DataType::Decimal(_, _) => DecimalChunked, "Decimal")
     }
 
     /// Unpack to ChunkedArray of dtype list
     pub fn list(&self) -> PolarsResult<&ListChunked> {
         unpack_chunked!(self, DataType::List(_) => ListChunked, "List")
     }
 
-    /// Unpack to ChunkedArray of dtype categorical
+    /// Unpack to ChunkedArray of dtype `[DataType::Array]`
+    #[cfg(feature = "dtype-array")]
+    pub fn array(&self) -> PolarsResult<&ArrayChunked> {
+        unpack_chunked!(self, DataType::Array(_, _) => ArrayChunked, "FixedSizeList")
+    }
+
+    /// Unpack to ChunkedArray of dtype `[DataType::Categorical]`
     #[cfg(feature = "dtype-categorical")]
     pub fn categorical(&self) -> PolarsResult<&CategoricalChunked> {
         unpack_chunked!(self, DataType::Categorical(_) => CategoricalChunked, "Categorical")
     }
 
-    /// Unpack to ChunkedArray of dtype struct
+    /// Unpack to ChunkedArray of dtype `[DataType::Struct]`
     #[cfg(feature = "dtype-struct")]
     pub fn struct_(&self) -> PolarsResult<&StructChunked> {
         #[cfg(debug_assertions)]
         {
             if let DataType::Struct(_) = self.dtype() {
                 let any = self.as_any();
                 assert!(any.is::<StructChunked>());
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/ops/ewm.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/ops/ewm.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/ops/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/ops/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/ops/moment.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/ops/moment.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/ops/null.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/ops/null.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/ops/pct_change.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/ops/pct_change.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/ops/round.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/ops/round.rs`

 * *Files 8% similar despite different names*

```diff
@@ -3,28 +3,38 @@
 
 use crate::prelude::*;
 
 impl Series {
     /// Round underlying floating point array to given decimal.
     pub fn round(&self, decimals: u32) -> PolarsResult<Self> {
         if let Ok(ca) = self.f32() {
-            // Note we do the computation on f64 floats to not loose precision
-            // when the computation is done, we cast to f32
-            let multiplier = 10.0.pow(decimals as f64);
-            let s = ca
-                .apply(|val| ((val as f64 * multiplier).round() / multiplier) as f32)
-                .into_series();
-            return Ok(s);
+            if decimals == 0 {
+                let s = ca.apply(|val| val.round()).into_series();
+                return Ok(s);
+            } else {
+                // Note we do the computation on f64 floats to not lose precision
+                // when the computation is done, we cast to f32
+                let multiplier = 10.0.pow(decimals as f64);
+                let s = ca
+                    .apply(|val| ((val as f64 * multiplier).round() / multiplier) as f32)
+                    .into_series();
+                return Ok(s);
+            }
         }
         if let Ok(ca) = self.f64() {
-            let multiplier = 10.0.pow(decimals as f64);
-            let s = ca
-                .apply(|val| (val * multiplier).round() / multiplier)
-                .into_series();
-            return Ok(s);
+            if decimals == 0 {
+                let s = ca.apply(|val| val.round()).into_series();
+                return Ok(s);
+            } else {
+                let multiplier = 10.0.pow(decimals as f64);
+                let s = ca
+                    .apply(|val| (val * multiplier).round() / multiplier)
+                    .into_series();
+                return Ok(s);
+            }
         }
         polars_bail!(opq = round, self.dtype());
     }
 
     /// Floor underlying floating point array to the lowest integers smaller or equal to the float value.
     pub fn floor(&self) -> PolarsResult<Self> {
         if let Ok(ca) = self.f32() {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/ops/to_list.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/ops/to_list.rs`

 * *Files 1% similar despite different names*

```diff
@@ -48,15 +48,15 @@
                 values.clone(),
                 None,
             )
         };
         let name = self.name();
 
         let mut ca = unsafe { ListChunked::from_chunks(name, vec![Box::new(arr)]) };
-        ca.to_logical(inner_type.clone());
+        ca.to_physical(inner_type.clone());
         ca.set_fast_explode();
 
         Ok(ca)
     }
 
     pub fn reshape(&self, dims: &[i64]) -> PolarsResult<Series> {
         if dims.is_empty() {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/ops/unique.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/ops/unique.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/series_trait.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/series_trait.rs`

 * *Files 2% similar despite different names*

```diff
@@ -13,14 +13,25 @@
 #[derive(Debug, Copy, Clone, Eq, PartialEq)]
 pub enum IsSorted {
     Ascending,
     Descending,
     Not,
 }
 
+impl IsSorted {
+    pub(crate) fn reverse(self) -> Self {
+        use IsSorted::*;
+        match self {
+            Ascending => Descending,
+            Descending => Ascending,
+            Not => Not,
+        }
+    }
+}
+
 macro_rules! invalid_operation_panic {
     ($op:ident, $s:expr) => {
         panic!(
             "`{}` operation not supported for dtype `{}`",
             stringify!($op),
             $s._dtype()
         )
@@ -163,15 +174,15 @@
             &self,
             _mask: &BooleanChunked,
             _other: &Series,
         ) -> PolarsResult<Series> {
             invalid_operation_panic!(zip_with_same_type, self)
         }
 
-        fn arg_sort_multiple(&self, _by: &[Series], _descending: &[bool]) -> PolarsResult<IdxCa> {
+        fn arg_sort_multiple(&self, _options: &SortMultipleOptions) -> PolarsResult<IdxCa> {
             polars_bail!(opq = arg_sort_multiple, self._dtype());
         }
     }
 }
 
 pub trait SeriesTrait:
     Send + Sync + private::PrivateSeries + private::PrivateSeriesNumeric
@@ -295,17 +306,14 @@
     fn is_empty(&self) -> bool {
         self.len() == 0
     }
 
     /// Aggregate all chunks to a contiguous array of memory.
     fn rechunk(&self) -> Series;
 
-    /// Take every nth value as a new Series
-    fn take_every(&self, n: usize) -> Series;
-
     /// Drop all null values and return a new Series.
     fn drop_nulls(&self) -> Series {
         if self.null_count() == 0 {
             Series(self.clone_inner())
         } else {
             self.filter(&self.is_not_null()).unwrap()
         }
@@ -497,16 +505,16 @@
 
     /// Check if elements of this Series are in the right Series, or List values of the right Series.
     #[cfg(feature = "is_in")]
     fn is_in(&self, _other: &Series) -> PolarsResult<BooleanChunked> {
         polars_bail!(opq = is_in, self._dtype());
     }
     #[cfg(feature = "repeat_by")]
-    fn repeat_by(&self, _by: &IdxCa) -> ListChunked {
-        invalid_operation_panic!(repeat_by, self)
+    fn repeat_by(&self, _by: &IdxCa) -> PolarsResult<ListChunked> {
+        polars_bail!(opq = repeat_by, self._dtype());
     }
     #[cfg(feature = "checked_arithmetic")]
     fn checked_div(&self, _rhs: &Series) -> PolarsResult<Series> {
         polars_bail!(opq = checked_div, self._dtype());
     }
 
     #[cfg(feature = "mode")]
@@ -529,14 +537,18 @@
     /// Concat the values into a string array.
     /// # Arguments
     ///
     /// * `delimiter` - A string that will act as delimiter between values.
     fn str_concat(&self, _delimiter: &str) -> Utf8Chunked {
         invalid_operation_panic!(str_concat, self);
     }
+
+    fn tile(&self, _n: usize) -> Series {
+        invalid_operation_panic!(tile, self);
+    }
 }
 
 impl<'a> (dyn SeriesTrait + 'a) {
     pub fn unpack<N: 'static>(&self) -> PolarsResult<&ChunkedArray<N>>
     where
         N: PolarsDataType,
     {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/series/unstable.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/series/unstable.rs`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,12 @@
 use std::marker::PhantomData;
 use std::ptr::NonNull;
 
+use polars_utils::unwrap::UnwrapUncheckedRelease;
+
 use crate::prelude::*;
 
 /// A wrapper type that should make it a bit more clear that we should not clone Series
 #[derive(Copy, Clone)]
 #[cfg(feature = "private")]
 pub struct UnstableSeries<'a> {
     lifetime: PhantomData<&'a Series>,
@@ -42,19 +44,21 @@
             inner: NonNull::new(inner_chunk as *const ArrayRef as *mut ArrayRef).unwrap(),
         }
     }
 
     /// Creates a new `[UnsafeSeries]`
     /// # Safety
     /// Inner chunks must be from `Series` otherwise the dtype may be incorrect and lead to UB.
+    #[inline]
     pub(crate) unsafe fn new_with_chunk(series: &'a mut Series, inner_chunk: &ArrayRef) -> Self {
         UnstableSeries {
             lifetime: PhantomData,
             container: series,
-            inner: NonNull::new(inner_chunk as *const ArrayRef as *mut ArrayRef).unwrap(),
+            inner: NonNull::new(inner_chunk as *const ArrayRef as *mut ArrayRef)
+                .unwrap_unchecked_release(),
         }
     }
 
     pub fn deep_clone(&self) -> Series {
         unsafe {
             let s = &(*self.container);
             debug_assert_eq!(s.chunks().len(), 1);
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/testing.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/testing.rs`

 * *Files 6% similar despite different names*

```diff
@@ -13,36 +13,30 @@
         }
     }
 
     /// Check if all values in series are equal where `None == None` evaluates to `true`.
     /// Two `Datetime` series are *not* equal if their timezones are different, regardless
     /// if they represent the same UTC time or not.
     pub fn series_equal_missing(&self, other: &Series) -> bool {
-        // TODO! remove this? Default behavior already includes equal missing
-        #[cfg(feature = "timezones")]
-        {
-            use crate::datatypes::DataType::Datetime;
-
-            if let Datetime(_, tz_lhs) = self.dtype() {
-                if let Datetime(_, tz_rhs) = other.dtype() {
-                    if tz_lhs != tz_rhs {
-                        return false;
-                    }
-                } else {
+        match (self.dtype(), other.dtype()) {
+            #[cfg(feature = "timezones")]
+            (DataType::Datetime(_, tz_lhs), DataType::Datetime(_, tz_rhs)) => {
+                if tz_lhs != tz_rhs {
                     return false;
                 }
             }
+            _ => {}
         }
 
         // differences from Partial::eq in that numerical dtype may be different
         self.len() == other.len()
             && self.name() == other.name()
             && self.null_count() == other.null_count()
             && {
-                let eq = self.equal(other);
+                let eq = self.equal_missing(other);
                 match eq {
                     Ok(b) => b.sum().map(|s| s as usize).unwrap_or(0) == self.len(),
                     Err(_) => false,
                 }
             }
     }
 
@@ -60,24 +54,15 @@
             unsafe { std::mem::transmute::<&dyn SeriesTrait, (usize, usize)>(object) };
         data_ptr
     }
 }
 
 impl PartialEq for Series {
     fn eq(&self, other: &Self) -> bool {
-        self.len() == other.len()
-            && self.field() == other.field()
-            && self.null_count() == other.null_count()
-            && self
-                .equal(other)
-                .unwrap()
-                .sum()
-                .map(|s| s as usize)
-                .unwrap_or(0)
-                == self.len()
+        self.series_equal_missing(other)
     }
 }
 
 impl DataFrame {
     /// Check if `DataFrames` schemas are equal.
     pub fn frame_equal_schema(&self, other: &DataFrame) -> PolarsResult<()> {
         for (lhs, rhs) in self.iter().zip(other.iter()) {
@@ -171,15 +156,15 @@
 impl PartialEq for DataFrame {
     fn eq(&self, other: &Self) -> bool {
         self.shape() == other.shape()
             && self
                 .columns
                 .iter()
                 .zip(other.columns.iter())
-                .all(|(s1, s2)| s1 == s2)
+                .all(|(s1, s2)| s1.series_equal_missing(s2))
     }
 }
 
 #[cfg(test)]
 mod test {
     use crate::prelude::*;
 
@@ -207,31 +192,14 @@
 
         let df1 = DataFrame::new(vec![a, b]).unwrap();
         let df2 = df1.clone();
         assert!(df1.frame_equal(&df2))
     }
 
     #[test]
-    fn test_series_partialeq() {
-        let s1 = Series::new("a", &[1_i32, 2_i32, 3_i32]);
-        let s1_bis = Series::new("b", &[1_i32, 2_i32, 3_i32]);
-        let s1_ter = Series::new("a", &[1.0_f64, 2.0_f64, 3.0_f64]);
-        let s2 = Series::new("", &[Some(1), Some(0)]);
-        let s3 = Series::new("", &[Some(1), None]);
-        let s4 = Series::new("", &[1.0, f64::NAN]);
-
-        assert_eq!(s1, s1);
-        assert_ne!(s1, s1_bis);
-        assert_ne!(s1, s1_ter);
-        assert_eq!(s2, s2);
-        assert_ne!(s2, s3);
-        assert_ne!(s4, s4);
-    }
-
-    #[test]
     fn test_df_partialeq() {
         let df1 = df!("a" => &[1, 2, 3],
                       "b" => &[4, 5, 6])
         .unwrap();
         let df2 = df!("b" => &[4, 5, 6],
                       "a" => &[1, 2, 3])
         .unwrap();
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/utils/flatten.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/utils/flatten.rs`

 * *Files 3% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 use polars_utils::sync::SyncPtr;
 
 use super::*;
 
-pub(super) fn flatten_df(df: &DataFrame) -> impl Iterator<Item = DataFrame> + '_ {
+pub fn flatten_df_iter(df: &DataFrame) -> impl Iterator<Item = DataFrame> + '_ {
     df.iter_chunks_physical().flat_map(|chunk| {
         let df = DataFrame::new_no_checks(
             df.iter()
                 .zip(chunk.into_arrays())
                 .map(|(s, arr)| {
                     // Safety:
                     // datatypes are correct
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/utils/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/utils/mod.rs`

 * *Files 2% similar despite different names*

```diff
@@ -27,20 +27,23 @@
     fn deref(&self) -> &Self::Target {
         &self.0
     }
 }
 
 pub fn _set_partition_size() -> usize {
     let mut n_partitions = POOL.current_num_threads();
-    // set n_partitions to closes 2^n above the no of threads.
+    if n_partitions == 1 {
+        return 1;
+    }
+    // set n_partitions to closest 2^n size
     loop {
         if n_partitions.is_power_of_two() {
             break;
         } else {
-            n_partitions += 1;
+            n_partitions -= 1;
         }
     }
     n_partitions
 }
 
 /// Just a wrapper structure. Useful for certain impl specializations
 /// This is for instance use to implement
@@ -148,15 +151,15 @@
     let chunk_size = std::cmp::max(total_len / n, 3);
 
     if df.n_chunks() == n
         && df.get_columns()[0]
             .chunk_lengths()
             .all(|len| len.abs_diff(chunk_size) < 100)
     {
-        return Ok(flatten_df(df).collect());
+        return Ok(flatten_df_iter(df).collect());
     }
 
     let mut out = Vec::with_capacity(n);
 
     for i in 0..n {
         let offset = i * chunk_size;
         let len = if i == (n - 1) {
@@ -164,15 +167,15 @@
         } else {
             chunk_size
         };
         let df = df.slice((i * chunk_size) as i64, len);
         if df.n_chunks() > 1 {
             // we add every chunk as separate dataframe. This make sure that every partition
             // deals with it.
-            out.extend(flatten_df(&df))
+            out.extend(flatten_df_iter(&df))
         } else {
             out.push(df)
         }
     }
 
     Ok(out)
 }
@@ -181,15 +184,15 @@
 #[doc(hidden)]
 /// Split a [`DataFrame`] into `n` parts. We take a `&mut` to be able to repartition/align chunks.
 pub fn split_df(df: &mut DataFrame, n: usize) -> PolarsResult<Vec<DataFrame>> {
     if n == 0 || df.height() == 0 {
         return Ok(vec![df.clone()]);
     }
     // make sure that chunks are aligned.
-    df.rechunk();
+    df.align_chunks();
     split_df_as_ref(df, n)
 }
 
 pub fn slice_slice<T>(vals: &[T], offset: i64, len: usize) -> &[T] {
     let (raw_offset, slice_len) = slice_offsets(offset, len, vals.len());
     &vals[raw_offset..raw_offset + slice_len]
 }
@@ -597,48 +600,14 @@
     let mut acc_df = iter.next().unwrap();
     for df in iter {
         acc_df.hstack_mut(df.get_columns())?;
     }
     Ok(acc_df)
 }
 
-/// Simple wrapper to parallelize functions that can be divided over threads aggregated and
-/// finally aggregated in the main thread. This can be done for sum, min, max, etc.
-#[cfg(feature = "private")]
-pub fn parallel_op_series<F>(
-    f: F,
-    s: Series,
-    n_threads: Option<usize>,
-) -> PolarsResult<Option<Series>>
-where
-    F: Fn(Series) -> PolarsResult<Series> + Send + Sync,
-{
-    let n_threads = n_threads.unwrap_or_else(|| POOL.current_num_threads());
-    let splits = _split_offsets(s.len(), n_threads);
-
-    let chunks = POOL.install(|| {
-        splits
-            .into_par_iter()
-            .map(|(offset, len)| {
-                let s = s.slice(offset as i64, len);
-                f(s)
-            })
-            .collect::<PolarsResult<Vec<_>>>()
-    })?;
-
-    let mut iter = chunks.into_iter();
-    let first = iter.next().unwrap();
-    let out = iter.fold(first, |mut acc, s| {
-        acc.append(&s).unwrap();
-        acc
-    });
-
-    f(out).map(Some)
-}
-
 pub fn align_chunks_binary<'a, T, B>(
     left: &'a ChunkedArray<T>,
     right: &'a ChunkedArray<B>,
 ) -> (Cow<'a, ChunkedArray<T>>, Cow<'a, ChunkedArray<B>>)
 where
     B: PolarsDataType,
     T: PolarsDataType,
@@ -688,15 +657,15 @@
         (1, _) => (left, right.rechunk()),
         (_, _) => (left.rechunk(), right.rechunk()),
     }
 }
 
 #[allow(clippy::type_complexity)]
 #[cfg(feature = "zip_with")]
-pub(crate) fn align_chunks_ternary<'a, A, B, C>(
+pub fn align_chunks_ternary<'a, A, B, C>(
     a: &'a ChunkedArray<A>,
     b: &'a ChunkedArray<B>,
     c: &'a ChunkedArray<C>,
 ) -> (
     Cow<'a, ChunkedArray<A>>,
     Cow<'a, ChunkedArray<B>>,
     Cow<'a, ChunkedArray<C>>,
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/utils/series.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/utils/series.rs`

 * *Files 15% similar despite different names*

```diff
@@ -27,21 +27,15 @@
 {
     let mut container = Series::full_null("", 0, dtype);
     let mut us = UnstableSeries::new(&mut container);
 
     f(&mut us)
 }
 
-pub fn ensure_sorted_arg(s: &Series, operation: &str) {
-    if matches!(s.is_sorted_flag(), IsSorted::Not) {
-        eprintln!(
-            "argument in operation '{}' is not explicitly sorted
+pub fn ensure_sorted_arg(s: &Series, operation: &str) -> PolarsResult<()> {
+    polars_ensure!(!matches!(s.is_sorted_flag(), IsSorted::Not), InvalidOperation: "argument in operation '{}' is not explicitly sorted
 
 - If your data is ALREADY sorted, set the sorted flag with: '.set_sorted()'.
 - If your data is NOT sorted, sort the 'expr/series/column' first.
-
-This might become an error in a future version.
-    ",
-            operation
-        );
-    }
+    ", operation);
+    Ok(())
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-core/src/utils/supertype.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/src/utils/supertype.rs`

 * *Files 5% similar despite different names*

```diff
@@ -213,30 +213,20 @@
                 Some(Datetime(get_time_units(lu, ru), None))
             }
             #[cfg(all(feature = "dtype-duration", feature = "dtype-date"))]
             (Duration(_), Date) | (Date, Duration(_)) => Some(Date),
             #[cfg(feature = "dtype-duration")]
             (Duration(lu), Duration(ru)) => Some(Duration(get_time_units(lu, ru))),
 
-            // None and Some("") timezones
-            // we cast from more precision to higher precision as that always fits with occasional loss of precision
-            #[cfg(feature = "dtype-datetime")]
-            (Datetime(tu_l, tz_l), Datetime(tu_r, tz_r))
-                if (tz_l.is_none() || tz_l.as_deref() == Some(""))
-                    && (tz_r.is_none() || tz_r.as_deref() == Some("")) =>
-            {
-                let tu = get_time_units(tu_l, tu_r);
-                Some(Datetime(tu, None))
-            }
-            // None and Some("<tz>") timezones
+            // both None or both Some("<tz>") timezones
             // we cast from more precision to higher precision as that always fits with occasional loss of precision
             #[cfg(feature = "dtype-datetime")]
             (Datetime(tu_l, tz_l), Datetime(tu_r, tz_r)) if
                 // both are none
-                tz_l.is_none() && tz_r.is_some()
+                (tz_l.is_none() && tz_r.is_none())
                 // both have the same time zone
                 || (tz_l.is_some() && (tz_l == tz_r)) => {
                 let tu = get_time_units(tu_l, tu_r);
                 Some(Datetime(tu, tz_r.clone()))
             }
             (List(inner_left), List(inner_right)) => {
                 let st = get_supertype(inner_left, inner_right)?;
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/Cargo.toml` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/Cargo.toml`

 * *Files 10% similar despite different names*

```diff
@@ -1,27 +1,27 @@
 [package]
 name = "polars-time"
-version= "0.28.0"
+version= "0.30.0"
 authors = ["ritchie46 <ritchie46@gmail.com>"]
 edition = "2021"
 license = "MIT"
 description = "Time related code for the polars dataframe library"
 
 # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html
 
 [dependencies]
 atoi = "2.0.0"
 chrono = { version = "0.4", default-features = false, features = ["std"] }
 chrono-tz = { version = "0.8", optional = true }
 now = "0.1"
 once_cell= "1"
-polars-arrow = { version = "0.28.0", path = "../polars-arrow", features = ["compute", "temporal"] }
-polars-core = { version = "0.28.0", path = "../polars-core", default-features = false, features = ["private", "dtype-datetime", "dtype-duration", "dtype-time", "dtype-date"] }
-polars-ops = { version = "0.28.0", path = "../polars-ops" }
-polars-utils = { version = "0.28.0", path = "../polars-utils" }
+polars-arrow = { version = "0.30.0", path = "../polars-arrow", features = ["compute", "temporal"] }
+polars-core = { version = "0.30.0", path = "../polars-core", default-features = false, features = ["private", "dtype-datetime", "dtype-duration", "dtype-time", "dtype-date"] }
+polars-ops = { version = "0.30.0", path = "../polars-ops" }
+polars-utils = { version = "0.30.0", path = "../polars-utils" }
 regex = "1.7.1"
 serde = { version = "1", features = ["derive"], optional = true }
 smartstring= { version = "1" }
 
 [features]
 dtype-date = ["polars-core/dtype-date", "polars-core/temporal"]
 dtype-datetime = ["polars-core/dtype-date", "polars-core/temporal"]
@@ -35,18 +35,18 @@
 test = ["dtype-date", "dtype-datetime", "polars-core/fmt"]
 
 default = ["private"]
 
 [dependencies.arrow]
 package = "arrow2"
 # git = "https://github.com/jorgecarleitao/arrow2"
-git = "https://github.com/ritchie46/arrow2"
+# git = "https://github.com/ritchie46/arrow2"
 # rev = "1491c6e8f4fd100f53c358e4f3ef1536d9e75090"
 # path = "../arrow2"
-branch = "polars_2023-04-20"
+# branch = "polars_2023-05-25"
 version = "0.17"
 default-features = false
 features = [
   "compute_aggregate",
   "compute_arithmetics",
   "compute_boolean",
   "compute_boolean_kleene",
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/LICENSE` & `polars_lts_cpu-0.18.0/local_dependencies/polars/LICENSE`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/date.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/date.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/datetime.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/datetime.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/duration.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/duration.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/kernels.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/kernels.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/rolling_window/floats.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/rolling_window/floats.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/rolling_window/ints.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/rolling_window/ints.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/rolling_window/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/rolling_window/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/time.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/time.rs`

 * *Files 11% similar despite different names*

```diff
@@ -1,23 +1,11 @@
-use chrono::Timelike;
-use polars_arrow::export::arrow::temporal_conversions::NANOSECONDS;
+use polars_core::chunked_array::temporal::time_to_time64ns;
 
 use super::*;
 
-const SECONDS_IN_MINUTE: i64 = 60;
-const SECONDS_IN_HOUR: i64 = 3_600;
-
-pub(crate) fn time_to_time64ns(time: &NaiveTime) -> i64 {
-    (time.hour() as i64 * SECONDS_IN_HOUR
-        + time.minute() as i64 * SECONDS_IN_MINUTE
-        + time.second() as i64)
-        * NANOSECONDS
-        + time.nanosecond() as i64
-}
-
 pub trait TimeMethods {
     /// Extract hour from underlying NaiveDateTime representation.
     /// Returns the hour number from 0 to 23.
     fn hour(&self) -> UInt32Chunked;
 
     /// Extract minute from underlying NaiveDateTime representation.
     /// Returns the minute number from 0 to 59.
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/utf8/infer.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/utf8/infer.rs`

 * *Files 16% similar despite different names*

```diff
@@ -1,18 +1,17 @@
-use chrono::{DateTime, FixedOffset, NaiveDate, NaiveDateTime};
+use chrono::{DateTime, NaiveDate, NaiveDateTime};
 use once_cell::sync::Lazy;
 use polars_arrow::export::arrow::array::PrimitiveArray;
 use polars_core::prelude::*;
 use polars_core::utils::arrow::types::NativeType;
 use regex::Regex;
 
-use super::patterns::{self, PatternWithOffset};
+use super::patterns::{self, Pattern};
 #[cfg(feature = "dtype-date")]
 use crate::chunkedarray::date::naive_date_to_date;
-use crate::chunkedarray::utf8::patterns::Pattern;
 use crate::chunkedarray::utf8::strptime;
 use crate::prelude::utf8::strptime::StrpTimeState;
 
 const DATETIME_DMY_PATTERN: &str = r#"(?x)
         ^
         ['"]?                        # optional quotes
         (?:\d{1,2})                  # day
@@ -203,139 +202,118 @@
                 })
         }
     }
 }
 
 #[derive(Clone)]
 pub struct DatetimeInfer<T> {
-    pub pattern_with_offset: PatternWithOffset,
+    pub pattern: Pattern,
     patterns: &'static [&'static str],
     latest_fmt: &'static str,
-    transform: fn(&str, &str, Option<FixedOffset>, bool) -> Option<T>,
+    transform: fn(&str, &str) -> Option<T>,
     transform_bytes: StrpTimeState,
     fmt_len: u16,
     pub logical_type: DataType,
-    utc: bool,
 }
 
 #[cfg(feature = "dtype-datetime")]
 impl TryFrom<Pattern> for DatetimeInfer<i64> {
     type Error = PolarsError;
 
     fn try_from(value: Pattern) -> PolarsResult<Self> {
         match value {
             Pattern::DatetimeDMY => Ok(DatetimeInfer {
-                pattern_with_offset: PatternWithOffset {
-                    pattern: Pattern::DatetimeDMY,
-                    offset: None,
-                },
+                pattern: Pattern::DatetimeDMY,
                 patterns: patterns::DATETIME_D_M_Y,
                 latest_fmt: patterns::DATETIME_D_M_Y[0],
                 transform: transform_datetime_us,
                 transform_bytes: StrpTimeState::default(),
                 fmt_len: 0,
                 logical_type: DataType::Datetime(TimeUnit::Microseconds, None),
-                utc: false,
             }),
             Pattern::DatetimeYMD => Ok(DatetimeInfer {
-                pattern_with_offset: PatternWithOffset {
-                    pattern: Pattern::DatetimeYMD,
-                    offset: None,
-                },
+                pattern: Pattern::DatetimeYMD,
                 patterns: patterns::DATETIME_Y_M_D,
                 latest_fmt: patterns::DATETIME_Y_M_D[0],
                 transform: transform_datetime_us,
                 transform_bytes: StrpTimeState::default(),
                 fmt_len: 0,
                 logical_type: DataType::Datetime(TimeUnit::Microseconds, None),
-                utc: false,
             }),
             Pattern::DatetimeYMDZ => Ok(DatetimeInfer {
-                pattern_with_offset: PatternWithOffset {
-                    pattern: Pattern::DatetimeYMDZ,
-                    offset: None,
-                },
+                pattern: Pattern::DatetimeYMDZ,
                 patterns: patterns::DATETIME_Y_M_D_Z,
                 latest_fmt: patterns::DATETIME_Y_M_D_Z[0],
                 transform: transform_tzaware_datetime_us,
                 transform_bytes: StrpTimeState::default(),
                 fmt_len: 0,
                 logical_type: DataType::Datetime(TimeUnit::Microseconds, None),
-                utc: false,
             }),
             _ => polars_bail!(ComputeError: "could not convert pattern"),
         }
     }
 }
 
 #[cfg(feature = "dtype-date")]
 impl TryFrom<Pattern> for DatetimeInfer<i32> {
     type Error = PolarsError;
 
     fn try_from(value: Pattern) -> PolarsResult<Self> {
         match value {
             Pattern::DateDMY => Ok(DatetimeInfer {
-                pattern_with_offset: PatternWithOffset {
-                    pattern: Pattern::DateDMY,
-                    offset: None,
-                },
+                pattern: Pattern::DateDMY,
                 patterns: patterns::DATE_D_M_Y,
                 latest_fmt: patterns::DATE_D_M_Y[0],
                 transform: transform_date,
                 transform_bytes: StrpTimeState::default(),
                 fmt_len: 0,
                 logical_type: DataType::Date,
-                utc: false,
             }),
             Pattern::DateYMD => Ok(DatetimeInfer {
-                pattern_with_offset: PatternWithOffset {
-                    pattern: Pattern::DateYMD,
-                    offset: None,
-                },
+                pattern: Pattern::DateYMD,
                 patterns: patterns::DATE_Y_M_D,
                 latest_fmt: patterns::DATE_Y_M_D[0],
                 transform: transform_date,
                 transform_bytes: StrpTimeState::default(),
                 fmt_len: 0,
                 logical_type: DataType::Date,
-                utc: false,
             }),
             _ => polars_bail!(ComputeError: "could not convert pattern"),
         }
     }
 }
 
 impl<T: NativeType> DatetimeInfer<T> {
-    pub fn parse(&mut self, val: &str, offset: Option<FixedOffset>) -> Option<T> {
-        match (self.transform)(val, self.latest_fmt, offset, self.utc) {
+    pub fn parse(&mut self, val: &str) -> Option<T> {
+        match (self.transform)(val, self.latest_fmt) {
             Some(parsed) => Some(parsed),
             // try other patterns
             None => {
-                if !self.pattern_with_offset.pattern.is_inferable(val) {
+                if !self.pattern.is_inferable(val) {
                     return None;
                 }
                 for fmt in self.patterns {
                     self.fmt_len = 0;
-                    if let Some(parsed) = (self.transform)(val, fmt, offset, self.utc) {
+                    if let Some(parsed) = (self.transform)(val, fmt) {
                         self.latest_fmt = fmt;
                         return Some(parsed);
                     }
                 }
                 None
             }
         }
     }
 
-    fn coerce_utf8(&mut self, ca: &Utf8Chunked, offset: Option<FixedOffset>) -> Series {
+    fn coerce_utf8(&mut self, ca: &Utf8Chunked) -> Series {
         let chunks = ca
             .downcast_iter()
             .map(|array| {
                 let iter = array
                     .into_iter()
-                    .map(|opt_val| opt_val.and_then(|val| self.parse(val, offset)));
+                    .map(|opt_val| opt_val.and_then(|val| self.parse(val)));
                 Box::new(PrimitiveArray::from_trusted_len_iter(iter)) as ArrayRef
             })
             .collect();
         let mut out = match self.logical_type {
             DataType::Date => unsafe { Int32Chunked::from_chunks(ca.name(), chunks) }
                 .into_series()
                 .cast(&self.logical_type)
@@ -348,239 +326,173 @@
         };
         out.rename(ca.name());
         out
     }
 }
 
 #[cfg(feature = "dtype-date")]
-fn transform_date(val: &str, fmt: &str, _offset: Option<FixedOffset>, _utc: bool) -> Option<i32> {
+fn transform_date(val: &str, fmt: &str) -> Option<i32> {
     NaiveDate::parse_from_str(val, fmt)
         .ok()
         .map(naive_date_to_date)
 }
 
 #[cfg(feature = "dtype-datetime")]
-pub(crate) fn transform_datetime_ns(
-    val: &str,
-    fmt: &str,
-    _offset: Option<FixedOffset>,
-    _utc: bool,
-) -> Option<i64> {
+pub(crate) fn transform_datetime_ns(val: &str, fmt: &str) -> Option<i64> {
     let out = NaiveDateTime::parse_from_str(val, fmt)
         .ok()
         .map(datetime_to_timestamp_ns);
     out.or_else(|| {
         NaiveDate::parse_from_str(val, fmt)
             .ok()
             .map(|nd| datetime_to_timestamp_ns(nd.and_hms_opt(0, 0, 0).unwrap()))
     })
 }
 
-fn transform_tzaware_datetime_ns(
-    val: &str,
-    fmt: &str,
-    offset: Option<FixedOffset>,
-    utc: bool,
-) -> Option<i64> {
+fn transform_tzaware_datetime_ns(val: &str, fmt: &str) -> Option<i64> {
     let dt = DateTime::parse_from_str(val, fmt);
-    match utc {
-        true => dt.ok().map(|dt| datetime_to_timestamp_ns(dt.naive_utc())),
-        false => match Some(dt.ok()?.timezone()) == offset {
-            true => dt.ok().map(|dt| datetime_to_timestamp_ns(dt.naive_utc())),
-            false => None,
-        },
-    }
+    dt.ok().map(|dt| datetime_to_timestamp_ns(dt.naive_utc()))
 }
 
 #[cfg(feature = "dtype-datetime")]
-pub(crate) fn transform_datetime_us(
-    val: &str,
-    fmt: &str,
-    _offset: Option<FixedOffset>,
-    _utc: bool,
-) -> Option<i64> {
+pub(crate) fn transform_datetime_us(val: &str, fmt: &str) -> Option<i64> {
     let out = NaiveDateTime::parse_from_str(val, fmt)
         .ok()
         .map(datetime_to_timestamp_us);
     out.or_else(|| {
         NaiveDate::parse_from_str(val, fmt)
             .ok()
             .map(|nd| datetime_to_timestamp_us(nd.and_hms_opt(0, 0, 0).unwrap()))
     })
 }
 
-fn transform_tzaware_datetime_us(
-    val: &str,
-    fmt: &str,
-    offset: Option<FixedOffset>,
-    utc: bool,
-) -> Option<i64> {
+fn transform_tzaware_datetime_us(val: &str, fmt: &str) -> Option<i64> {
     let dt = DateTime::parse_from_str(val, fmt);
-    match utc {
-        true => dt.ok().map(|dt| datetime_to_timestamp_us(dt.naive_utc())),
-        false => match Some(dt.ok()?.timezone()) == offset {
-            true => dt.ok().map(|dt| datetime_to_timestamp_us(dt.naive_utc())),
-            false => None,
-        },
-    }
+    dt.ok().map(|dt| datetime_to_timestamp_us(dt.naive_utc()))
 }
 
 #[cfg(feature = "dtype-datetime")]
-pub(crate) fn transform_datetime_ms(
-    val: &str,
-    fmt: &str,
-    _offset: Option<FixedOffset>,
-    _utc: bool,
-) -> Option<i64> {
+pub(crate) fn transform_datetime_ms(val: &str, fmt: &str) -> Option<i64> {
     let out = NaiveDateTime::parse_from_str(val, fmt)
         .ok()
         .map(datetime_to_timestamp_ms);
     out.or_else(|| {
         NaiveDate::parse_from_str(val, fmt)
             .ok()
             .map(|nd| datetime_to_timestamp_ms(nd.and_hms_opt(0, 0, 0).unwrap()))
     })
 }
 
-fn transform_tzaware_datetime_ms(
-    val: &str,
-    fmt: &str,
-    offset: Option<FixedOffset>,
-    utc: bool,
-) -> Option<i64> {
+fn transform_tzaware_datetime_ms(val: &str, fmt: &str) -> Option<i64> {
     let dt = DateTime::parse_from_str(val, fmt);
-    match utc {
-        true => dt.ok().map(|dt| datetime_to_timestamp_ms(dt.naive_utc())),
-        false => match Some(dt.ok()?.timezone()) == offset {
-            true => dt.ok().map(|dt| datetime_to_timestamp_ms(dt.naive_utc())),
-            false => None,
-        },
-    }
+    dt.ok().map(|dt| datetime_to_timestamp_ms(dt.naive_utc()))
 }
 
-pub fn infer_pattern_single(val: &str) -> Option<PatternWithOffset> {
+pub fn infer_pattern_single(val: &str) -> Option<Pattern> {
     // Dates come first, because we see datetimes as superset of dates
     infer_pattern_date_single(val).or_else(|| infer_pattern_datetime_single(val))
 }
 
-fn infer_pattern_datetime_single(val: &str) -> Option<PatternWithOffset> {
+fn infer_pattern_datetime_single(val: &str) -> Option<Pattern> {
     if patterns::DATETIME_D_M_Y.iter().any(|fmt| {
         NaiveDateTime::parse_from_str(val, fmt).is_ok()
             || NaiveDate::parse_from_str(val, fmt).is_ok()
     }) {
-        Some(PatternWithOffset {
-            pattern: Pattern::DatetimeDMY,
-            offset: None,
-        })
+        Some(Pattern::DatetimeDMY)
     } else if patterns::DATETIME_Y_M_D.iter().any(|fmt| {
         NaiveDateTime::parse_from_str(val, fmt).is_ok()
             || NaiveDate::parse_from_str(val, fmt).is_ok()
     }) {
-        Some(PatternWithOffset {
-            pattern: Pattern::DatetimeYMD,
-            offset: None,
-        })
+        Some(Pattern::DatetimeYMD)
+    } else if patterns::DATETIME_Y_M_D_Z
+        .iter()
+        .any(|fmt| NaiveDateTime::parse_from_str(val, fmt).is_ok())
+    {
+        Some(Pattern::DatetimeYMDZ)
     } else {
-        patterns::DATETIME_Y_M_D_Z
-            .iter()
-            .find_map(|fmt| DateTime::parse_from_str(val, fmt).ok())
-            .map(|dt| PatternWithOffset {
-                pattern: Pattern::DatetimeYMDZ,
-                offset: Some(dt.timezone()),
-            })
+        None
     }
 }
 
-fn infer_pattern_date_single(val: &str) -> Option<PatternWithOffset> {
+fn infer_pattern_date_single(val: &str) -> Option<Pattern> {
     if patterns::DATE_D_M_Y
         .iter()
         .any(|fmt| NaiveDate::parse_from_str(val, fmt).is_ok())
     {
-        Some(PatternWithOffset {
-            pattern: Pattern::DateDMY,
-            offset: None,
-        })
+        Some(Pattern::DateDMY)
     } else if patterns::DATE_Y_M_D
         .iter()
         .any(|fmt| NaiveDate::parse_from_str(val, fmt).is_ok())
     {
-        Some(PatternWithOffset {
-            pattern: Pattern::DateYMD,
-            offset: None,
-        })
+        Some(Pattern::DateYMD)
     } else {
         None
     }
 }
 
 #[cfg(feature = "dtype-datetime")]
 pub(crate) fn to_datetime(
     ca: &Utf8Chunked,
     tu: TimeUnit,
     tz: Option<&TimeZone>,
-    utc: bool,
 ) -> PolarsResult<DatetimeChunked> {
     match ca.first_non_null() {
         None => Ok(Int64Chunked::full_null(ca.name(), ca.len()).into_datetime(tu, tz.cloned())),
         Some(idx) => {
             let subset = ca.slice(idx as i64, ca.len());
-            let pattern_with_offset = subset
+            let pattern = subset
                 .into_iter()
                 .find_map(|opt_val| opt_val.and_then(infer_pattern_datetime_single))
                 .ok_or_else(|| polars_err!(parse_fmt_idk = "date"))?;
-            let mut infer = DatetimeInfer::<i64>::try_from(pattern_with_offset.pattern)?;
-            match (tu, pattern_with_offset.offset) {
-                (TimeUnit::Nanoseconds, None) => infer.transform = transform_datetime_ns,
-                (TimeUnit::Microseconds, None) => infer.transform = transform_datetime_us,
-                (TimeUnit::Milliseconds, None) => infer.transform = transform_datetime_ms,
-                (TimeUnit::Nanoseconds, _) => infer.transform = transform_tzaware_datetime_ns,
-                (TimeUnit::Microseconds, _) => infer.transform = transform_tzaware_datetime_us,
-                (TimeUnit::Milliseconds, _) => infer.transform = transform_tzaware_datetime_ms,
+            let mut infer = DatetimeInfer::<i64>::try_from(pattern)?;
+            match (tu, pattern) {
+                (TimeUnit::Nanoseconds, Pattern::DatetimeYMDZ) => {
+                    infer.transform = transform_tzaware_datetime_ns
+                }
+                (TimeUnit::Microseconds, Pattern::DatetimeYMDZ) => {
+                    infer.transform = transform_tzaware_datetime_us
+                }
+                (TimeUnit::Milliseconds, Pattern::DatetimeYMDZ) => {
+                    infer.transform = transform_tzaware_datetime_ms
+                }
+                (TimeUnit::Nanoseconds, _) => infer.transform = transform_datetime_ns,
+                (TimeUnit::Microseconds, _) => infer.transform = transform_datetime_us,
+                (TimeUnit::Milliseconds, _) => infer.transform = transform_datetime_ms,
             }
-            infer.utc = utc;
-            if tz.is_some() && pattern_with_offset.offset.is_some() {
+            if tz.is_some() && pattern == Pattern::DatetimeYMDZ {
                 polars_bail!(ComputeError: "cannot parse tz-aware values with tz-aware dtype - please drop the time zone from the dtype.")
             }
-            match pattern_with_offset.offset {
+            match pattern {
                 #[cfg(feature = "timezones")]
-                Some(offset) => infer.coerce_utf8(ca, Some(offset)).datetime().map(|ca| {
+                Pattern::DatetimeYMDZ => infer.coerce_utf8(ca).datetime().map(|ca| {
                     let mut ca = ca.clone();
                     ca.set_time_unit(tu);
-                    match utc {
-                        true => Ok(ca.replace_time_zone(Some("UTC"), None)?),
-                        false => Ok(ca
-                            .replace_time_zone(Some("UTC"), None)?
-                            .convert_time_zone(offset.to_string())?),
-                    }
+                    ca.replace_time_zone(Some("UTC"), None)
                 })?,
-                _ => infer.coerce_utf8(ca, None).datetime().map(|ca| {
+                _ => infer.coerce_utf8(ca).datetime().map(|ca| {
                     let mut ca = ca.clone();
                     ca.set_time_unit(tu);
-                    match (tz, utc) {
-                        #[cfg(feature = "timezones")]
-                        (Some(tz), false) => Ok(ca.replace_time_zone(Some(tz), None)?),
-                        #[cfg(feature = "timezones")]
-                        (None, true) => Ok(ca.replace_time_zone(Some("UTC"), None)?),
+                    match tz {
                         #[cfg(feature = "timezones")]
-                        (Some(_), true) => unreachable!(), // has already been validated in strptime
+                        Some(tz) => ca.replace_time_zone(Some(tz), None),
                         _ => Ok(ca),
                     }
                 })?,
             }
         }
     }
 }
 #[cfg(feature = "dtype-date")]
 pub(crate) fn to_date(ca: &Utf8Chunked) -> PolarsResult<DateChunked> {
     match ca.first_non_null() {
         None => Ok(Int32Chunked::full_null(ca.name(), ca.len()).into_date()),
         Some(idx) => {
             let subset = ca.slice(idx as i64, ca.len());
-            let pattern_with_offset = subset
+            let pattern = subset
                 .into_iter()
                 .find_map(|opt_val| opt_val.and_then(infer_pattern_date_single))
                 .ok_or_else(|| polars_err!(parse_fmt_idk = "date"))?;
-            let mut infer = DatetimeInfer::<i32>::try_from(pattern_with_offset.pattern).unwrap();
-            infer.coerce_utf8(ca, None).date().cloned()
+            let mut infer = DatetimeInfer::<i32>::try_from(pattern).unwrap();
+            infer.coerce_utf8(ca).date().cloned()
         }
     }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/utf8/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/utf8/mod.rs`

 * *Files 3% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 pub mod infer;
 mod patterns;
 mod strptime;
 
 use chrono::ParseError;
 pub use patterns::{Pattern, PatternWithOffset};
+#[cfg(feature = "dtype-time")]
+use polars_core::chunked_array::temporal::time_to_time64ns;
 
 use super::*;
 #[cfg(feature = "dtype-date")]
 use crate::chunkedarray::date::naive_date_to_date;
-#[cfg(feature = "dtype-time")]
-use crate::chunkedarray::time::time_to_time64ns;
 use crate::prelude::utf8::strptime::StrpTimeState;
 
 #[cfg(feature = "dtype-time")]
 fn time_pattern<F, K>(val: &str, convert: F) -> Option<&'static str>
 // (string, fmt) -> PolarsResult
 where
     F: Fn(&str, &str) -> chrono::ParseResult<K>,
@@ -307,15 +307,15 @@
     fn as_date(&self, fmt: Option<&str>, cache: bool) -> PolarsResult<DateChunked> {
         let utf8_ca = self.as_utf8();
         let fmt = match fmt {
             Some(fmt) => fmt,
             None => return infer::to_date(utf8_ca),
         };
         let cache = cache && utf8_ca.len() > 50;
-        let fmt = strptime::compile_fmt(fmt);
+        let fmt = strptime::compile_fmt(fmt)?;
         let mut cache_map = PlHashMap::new();
 
         // we can use the fast parser
         let mut ca: Int32Chunked = if let Some(fmt_len) = strptime::fmt_len(fmt.as_bytes()) {
             let mut strptime_cache = StrpTimeState::default();
             let mut convert = |s: &str| {
                 // Safety:
@@ -382,91 +382,69 @@
     /// Parsing string values and return a [`DatetimeChunked`]
     fn as_datetime(
         &self,
         fmt: Option<&str>,
         tu: TimeUnit,
         cache: bool,
         tz_aware: bool,
-        utc: bool,
         tz: Option<&TimeZone>,
     ) -> PolarsResult<DatetimeChunked> {
         let utf8_ca = self.as_utf8();
         let fmt = match fmt {
             Some(fmt) => fmt,
-            None => return infer::to_datetime(utf8_ca, tu, tz, utc),
+            None => return infer::to_datetime(utf8_ca, tu, tz),
         };
-        let fmt = strptime::compile_fmt(fmt);
+        let fmt = strptime::compile_fmt(fmt)?;
         let cache = cache && utf8_ca.len() > 50;
 
         let func = match tu {
             TimeUnit::Nanoseconds => datetime_to_timestamp_ns,
             TimeUnit::Microseconds => datetime_to_timestamp_us,
             TimeUnit::Milliseconds => datetime_to_timestamp_ms,
         };
 
         if tz_aware {
             #[cfg(feature = "timezones")]
             {
                 use chrono::DateTime;
                 use polars_arrow::export::hashbrown::hash_map::Entry;
                 let mut cache_map = PlHashMap::new();
-                let mut tz = None;
 
-                let mut convert = |s: &str| {
+                let convert = |s: &str| {
                     DateTime::parse_from_str(s, &fmt)
                         .ok()
-                        .map(|dt| {
-                            if !utc {
-                                if let Some(tz_found) = tz {
-                                    polars_ensure!(
-                                        tz_found == dt.timezone(),
-                                        ComputeError: "different timezones found during 'strptime' \
-                                        operation (you might want to use `utc=True` and then set \
-                                        the time zone after parsing"
-                                );
-                                } else {
-                                    tz = Some(dt.timezone());
-                                }
-                            }
-                            Ok(func(dt.naive_utc()))
-                        })
-                        .transpose()
+                        .map(|dt| func(dt.naive_utc()))
                 };
 
                 let mut ca: Int64Chunked = utf8_ca
                     .into_iter()
                     .map(|opt_s| {
                         opt_s
                             .map(|s| {
                                 let out = if cache {
                                     match cache_map.entry(s) {
                                         Entry::Vacant(entry) => {
-                                            let value = convert(s)?;
+                                            let value = convert(s);
                                             entry.insert(value);
                                             value
                                         }
                                         Entry::Occupied(val) => *val.get(),
                                     }
                                 } else {
-                                    convert(s)?
+                                    convert(s)
                                 };
                                 Ok(out)
                             })
                             .transpose()
                             .map(|options| options.flatten())
                     })
                     .collect::<PolarsResult<_>>()?;
 
                 ca.rename(utf8_ca.name());
-                if !utc {
-                    let tz = tz.map(|of| format!("{of}"));
-                    Ok(ca.into_datetime(tu, tz))
-                } else {
-                    Ok(ca.into_datetime(tu, Some("UTC".to_string())))
-                }
+                Ok(ca.into_datetime(tu, Some("UTC".to_string())))
             }
             #[cfg(not(feature = "timezones"))]
             {
                 panic!("activate 'timezones' feature")
             }
         } else {
             let mut cache_map = PlHashMap::new();
@@ -480,15 +458,15 @@
                 self::strptime::fmt_len(fmt.as_bytes())
             {
                 let mut strptime_cache = StrpTimeState::default();
                 let mut convert = |s: &str| {
                     // Safety:
                     // fmt_len is correct, it was computed with this `fmt` str.
                     match unsafe { strptime_cache.parse(s.as_bytes(), fmt.as_bytes(), fmt_len) } {
-                        None => transform(s, &fmt, None, utc),
+                        None => transform(s, &fmt),
                         Some(ndt) => Some(func(ndt)),
                     }
                 };
                 if utf8_ca.null_count() == 0 {
                     utf8_ca
                         .into_no_null_iter()
                         .map(|val| {
@@ -516,34 +494,26 @@
             } else {
                 let mut cache_map = PlHashMap::new();
                 utf8_ca
                     .into_iter()
                     .map(|opt_s| {
                         opt_s.and_then(|s| {
                             if cache {
-                                *cache_map
-                                    .entry(s)
-                                    .or_insert_with(|| transform(s, &fmt, None, false))
+                                *cache_map.entry(s).or_insert_with(|| transform(s, &fmt))
                             } else {
-                                transform(s, &fmt, None, false)
+                                transform(s, &fmt)
                             }
                         })
                     })
                     .collect_trusted()
             };
             ca.rename(utf8_ca.name());
-            match (tz, utc) {
-                #[cfg(feature = "timezones")]
-                (Some(tz), false) => ca.into_datetime(tu, None).replace_time_zone(Some(tz), None),
-                #[cfg(feature = "timezones")]
-                (None, true) => ca
-                    .into_datetime(tu, None)
-                    .replace_time_zone(Some("UTC"), None),
+            match tz {
                 #[cfg(feature = "timezones")]
-                (Some(_), true) => unreachable!(), // has already been validated in strptime
+                Some(tz) => ca.into_datetime(tu, None).replace_time_zone(Some(tz), None),
                 _ => Ok(ca.into_datetime(tu, None)),
             }
         }
     }
 }
 
 pub trait AsUtf8 {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/utf8/patterns.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/utf8/patterns.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/chunkedarray/utf8/strptime.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/chunkedarray/utf8/strptime.rs`

 * *Files 10% similar despite different names*

```diff
@@ -1,12 +1,19 @@
 //! Much more opinionated, but also much faster strptrime than the one given in Chrono.
 //!
 use atoi::FromRadix10;
 use chrono::{NaiveDate, NaiveDateTime};
+use once_cell::sync::Lazy;
 use polars_utils::slice::GetSaferUnchecked;
+use regex::Regex;
+
+use crate::chunkedarray::{polars_bail, PolarsResult};
+
+static HOUR_PATTERN: Lazy<Regex> = Lazy::new(|| Regex::new(r"%[HI]").unwrap());
+static MINUTE_PATTERN: Lazy<Regex> = Lazy::new(|| Regex::new(r"%M").unwrap());
 
 #[inline]
 fn update_and_parse<T: atoi::FromRadix10>(
     incr: usize,
     offset: usize,
     vals: &[u8],
 ) -> Option<(T, usize)> {
@@ -40,20 +47,29 @@
         _ => None,
     }
 }
 
 /// Tries to convert a chrono `fmt` to a `fmt` that the polars parser consumes.
 /// E.g. chrono supports single letter date identifiers like %F, whereas polars only consumes
 /// year, day, month distinctively with %Y, %d, %m.
-pub(super) fn compile_fmt(fmt: &str) -> String {
-    fmt.replace("%D", "%m/%d/%y")
+pub(super) fn compile_fmt(fmt: &str) -> PolarsResult<String> {
+    if HOUR_PATTERN.is_match(fmt) & !MINUTE_PATTERN.is_match(fmt) {
+        // (hopefully) temporary hack. Ideally, chrono would return a ParseKindError indicating
+        // if `fmt` is too long for NaiveDate. If that's implemented, then this check could
+        // be removed, and that error could be matched against in `transform_datetime_*s`
+        // See https://github.com/chronotope/chrono/issues/1075.
+        polars_bail!(ComputeError: "Invalid format string: found hour, but no minute directive. \
+            Please either specify both or neither.");
+    }
+    Ok(fmt
+        .replace("%D", "%m/%d/%y")
         .replace("%R", "%H:%M")
         .replace("%T", "%H:%M:%S")
         .replace("%X", "%H:%M:%S")
-        .replace("%F", "%Y-%m-%d")
+        .replace("%F", "%Y-%m-%d"))
 }
 
 #[derive(Default, Clone)]
 pub(super) struct StrpTimeState {}
 
 impl StrpTimeState {
     #[inline]
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/date_range.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/month_start.rs`

 * *Files 23% similar despite different names*

```diff
@@ -1,88 +1,98 @@
-#[cfg(feature = "timezones")]
-use arrow::temporal_conversions::parse_offset;
-use chrono::{Datelike, NaiveDateTime};
+use chrono::{Datelike, NaiveDate, NaiveDateTime, NaiveTime, Timelike};
+use polars_arrow::time_zone::Tz;
 use polars_core::prelude::*;
-use polars_core::series::IsSorted;
+use polars_core::utils::arrow::temporal_conversions::{
+    timestamp_ms_to_datetime, timestamp_ns_to_datetime, timestamp_us_to_datetime, MILLISECONDS,
+    SECONDS_IN_DAY,
+};
 
-use crate::prelude::*;
 #[cfg(feature = "timezones")]
-use crate::utils::localize_timestamp;
-
-pub fn in_nanoseconds_window(ndt: &NaiveDateTime) -> bool {
-    // ~584 year around 1970
-    !(ndt.year() > 2554 || ndt.year() < 1386)
-}
+use crate::utils::{localize_datetime, unlocalize_datetime};
 
-#[cfg(feature = "private")]
-#[doc(hidden)]
-pub fn date_range_impl(
-    name: &str,
-    start: i64,
-    stop: i64,
-    every: Duration,
-    closed: ClosedWindow,
-    tu: TimeUnit,
-    _tz: Option<&TimeZone>,
-) -> PolarsResult<DatetimeChunked> {
-    if start > stop {
-        polars_bail!(ComputeError: "'start' cannot be greater than 'stop'")
-    }
-    if every.negative {
-        polars_bail!(ComputeError: "'interval' cannot be negative")
-    }
-    let mut out = match _tz {
+// roll backward to the first day of the month
+pub(crate) fn roll_backward(
+    t: i64,
+    tz: Option<&Tz>,
+    timestamp_to_datetime: fn(i64) -> NaiveDateTime,
+    datetime_to_timestamp: fn(NaiveDateTime) -> i64,
+) -> PolarsResult<i64> {
+    let ts = match tz {
         #[cfg(feature = "timezones")]
-        Some(tz) => match tz.parse::<chrono_tz::Tz>() {
-            Ok(tz) => {
-                let start = localize_timestamp(start, tu, tz);
-                let stop = localize_timestamp(stop, tu, tz);
-                Int64Chunked::new_vec(
-                    name,
-                    date_range_vec(start?, stop?, every, closed, tu, Some(&tz))?,
-                )
-                .into_datetime(tu, _tz.cloned())
-            }
-            Err(_) => match parse_offset(tz) {
-                Ok(tz) => {
-                    let start = localize_timestamp(start, tu, tz);
-                    let stop = localize_timestamp(stop, tu, tz);
-                    Int64Chunked::new_vec(
-                        name,
-                        date_range_vec(start?, stop?, every, closed, tu, Some(&tz))?,
-                    )
-                    .into_datetime(tu, _tz.cloned())
-                }
-                _ => polars_bail!(ComputeError: "unable to parse time zone: '{}'", tz),
-            },
-        },
-        _ => Int64Chunked::new_vec(
-            name,
-            date_range_vec(start, stop, every, closed, tu, NO_TIMEZONE)?,
-        )
-        .into_datetime(tu, None),
+        Some(tz) => unlocalize_datetime(timestamp_to_datetime(t), tz),
+        _ => timestamp_to_datetime(t),
+    };
+    let date = NaiveDate::from_ymd_opt(ts.year(), ts.month(), 1).ok_or(polars_err!(
+        ComputeError: format!("Could not construct date {}-{}-1", ts.year(), ts.month())
+    ))?;
+    let time = NaiveTime::from_hms_nano_opt(
+        ts.hour(),
+        ts.minute(),
+        ts.second(),
+        ts.timestamp_subsec_nanos(),
+    )
+    .ok_or(polars_err!(
+        ComputeError:
+            format!(
+                "Could not construct time {}:{}:{}.{}",
+                ts.hour(),
+                ts.minute(),
+                ts.second(),
+                ts.timestamp_subsec_nanos()
+            )
+    ))?;
+    let ndt = NaiveDateTime::new(date, time);
+    let t = match tz {
+        #[cfg(feature = "timezones")]
+        Some(tz) => datetime_to_timestamp(localize_datetime(ndt, tz)?),
+        _ => datetime_to_timestamp(ndt),
     };
+    Ok(t)
+}
 
-    out.set_sorted_flag(IsSorted::Ascending);
-    Ok(out)
+pub trait PolarsMonthStart {
+    fn month_start(&self, time_zone: Option<&Tz>) -> PolarsResult<Self>
+    where
+        Self: Sized;
 }
 
-/// Create a [`DatetimeChunked`] from a given `start` and `stop` date and a given `every` interval.
-pub fn date_range(
-    name: &str,
-    start: NaiveDateTime,
-    stop: NaiveDateTime,
-    every: Duration,
-    closed: ClosedWindow,
-    tu: TimeUnit,
-    tz: Option<TimeZone>,
-) -> PolarsResult<DatetimeChunked> {
-    let (start, stop) = match tu {
-        TimeUnit::Nanoseconds => (start.timestamp_nanos(), stop.timestamp_nanos()),
-        TimeUnit::Microseconds => (
-            start.timestamp() + start.timestamp_subsec_micros() as i64,
-            stop.timestamp() + stop.timestamp_subsec_millis() as i64,
-        ),
-        TimeUnit::Milliseconds => (start.timestamp_millis(), stop.timestamp_millis()),
-    };
-    date_range_impl(name, start, stop, every, closed, tu, tz.as_ref())
+impl PolarsMonthStart for DatetimeChunked {
+    fn month_start(&self, tz: Option<&Tz>) -> PolarsResult<Self> {
+        let timestamp_to_datetime: fn(i64) -> NaiveDateTime;
+        let datetime_to_timestamp: fn(NaiveDateTime) -> i64;
+        match self.time_unit() {
+            TimeUnit::Nanoseconds => {
+                timestamp_to_datetime = timestamp_ns_to_datetime;
+                datetime_to_timestamp = datetime_to_timestamp_ns;
+            }
+            TimeUnit::Microseconds => {
+                timestamp_to_datetime = timestamp_us_to_datetime;
+                datetime_to_timestamp = datetime_to_timestamp_us;
+            }
+            TimeUnit::Milliseconds => {
+                timestamp_to_datetime = timestamp_ms_to_datetime;
+                datetime_to_timestamp = datetime_to_timestamp_ms;
+            }
+        };
+        Ok(self
+            .0
+            .try_apply(|t| roll_backward(t, tz, timestamp_to_datetime, datetime_to_timestamp))?
+            .into_datetime(self.time_unit(), self.time_zone().clone()))
+    }
+}
+
+impl PolarsMonthStart for DateChunked {
+    fn month_start(&self, _tz: Option<&Tz>) -> PolarsResult<Self> {
+        const MSECS_IN_DAY: i64 = MILLISECONDS * SECONDS_IN_DAY;
+        Ok(self
+            .0
+            .try_apply(|t| {
+                Ok((roll_backward(
+                    MSECS_IN_DAY * t as i64,
+                    None,
+                    timestamp_ms_to_datetime,
+                    datetime_to_timestamp_ms,
+                )? / MSECS_IN_DAY) as i32)
+            })?
+            .into_date())
+    }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/groupby/dynamic.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/groupby/dynamic.rs`

 * *Files 6% similar despite different names*

```diff
@@ -1,20 +1,17 @@
-#[cfg(feature = "timezones")]
-use arrow::temporal_conversions::parse_offset;
-#[cfg(feature = "timezones")]
-use chrono_tz::Tz;
-use polars_arrow::time_zone::PolarsTimeZone;
+use polars_arrow::time_zone::Tz;
 use polars_arrow::utils::CustomIterTools;
 use polars_core::export::rayon::prelude::*;
 use polars_core::frame::groupby::GroupsProxy;
 use polars_core::prelude::*;
 use polars_core::series::IsSorted;
 use polars_core::utils::ensure_sorted_arg;
 use polars_core::utils::flatten::flatten_par;
 use polars_core::POOL;
+use polars_utils::slice::SortedSlice;
 #[cfg(feature = "serde")]
 use serde::{Deserialize, Serialize};
 use smartstring::alias::String as SmartString;
 
 use crate::prelude::*;
 
 #[repr(transparent)]
@@ -29,44 +26,70 @@
     pub every: Duration,
     /// window duration
     pub period: Duration,
     /// offset window boundaries
     pub offset: Duration,
     /// truncate the time column values to the window
     pub truncate: bool,
-    // add the boundaries to the dataframe
+    /// add the boundaries to the dataframe
     pub include_boundaries: bool,
     pub closed_window: ClosedWindow,
     pub start_by: StartBy,
+    /// In cases sortedness cannot be checked by
+    /// the sorted flag, traverse the data to
+    /// check sortedness
+    pub check_sorted: bool,
 }
 
 impl Default for DynamicGroupOptions {
     fn default() -> Self {
         Self {
             index_column: "".into(),
             every: Duration::new(1),
             period: Duration::new(1),
             offset: Duration::new(1),
             truncate: true,
             include_boundaries: false,
             closed_window: ClosedWindow::Left,
             start_by: Default::default(),
+            check_sorted: true,
         }
     }
 }
 
 #[derive(Clone, Debug, PartialEq, Eq)]
 #[cfg_attr(feature = "serde", derive(Serialize, Deserialize))]
 pub struct RollingGroupOptions {
     /// Time or index column
     pub index_column: SmartString,
     /// window duration
     pub period: Duration,
     pub offset: Duration,
     pub closed_window: ClosedWindow,
+    /// In cases sortedness cannot be checked by
+    /// the sorted flag, traverse the data to
+    /// check sortedness
+    pub check_sorted: bool,
+}
+
+impl Default for RollingGroupOptions {
+    fn default() -> Self {
+        Self {
+            index_column: "".into(),
+            period: Duration::new(1),
+            offset: Duration::new(1),
+            closed_window: ClosedWindow::Left,
+            check_sorted: true,
+        }
+    }
+}
+
+fn check_sortedness_slice(v: &[i64]) -> PolarsResult<()> {
+    polars_ensure!(v.is_sorted_ascending(), ComputeError: "input data is not sorted");
+    Ok(())
 }
 
 const LB_NAME: &str = "_lower_boundary";
 const UP_NAME: &str = "_upper_boundary";
 
 pub trait PolarsTemporalGroupby {
     fn groupby_rolling(
@@ -103,17 +126,18 @@
 impl Wrap<&DataFrame> {
     fn groupby_rolling(
         &self,
         by: Vec<Series>,
         options: &RollingGroupOptions,
     ) -> PolarsResult<(Series, Vec<Series>, GroupsProxy)> {
         let time = self.0.column(&options.index_column)?.clone();
-        if by.is_empty() && !options.period.parsed_int {
+        if by.is_empty() {
             // if by is given, the column must be sorted in the 'by' arg, which we can not check now
-            ensure_sorted_arg(&time, "groupby_rolling");
+            // this will be checked when the groups are materialized
+            ensure_sorted_arg(&time, "groupby_rolling")?;
         }
         let time_type = time.dtype();
 
         polars_ensure!(time.null_count() == 0, ComputeError: "null values in dynamic groupby not supported, fill nulls.");
 
         use DataType::*;
         let (dt, tu, tz): (Series, TimeUnit, Option<TimeZone>) = match time_type {
@@ -127,50 +151,46 @@
                 let time_type = Datetime(TimeUnit::Nanoseconds, None);
                 let dt = time.cast(&Int64).unwrap().cast(&time_type).unwrap();
                 let (out, by, gt) = self.impl_groupby_rolling(
                     dt,
                     by,
                     options,
                     TimeUnit::Nanoseconds,
-                    NO_TIMEZONE.copied(),
+                    None,
                     &time_type,
                 )?;
                 let out = out.cast(&Int64).unwrap().cast(&Int32).unwrap();
                 return Ok((out, by, gt));
             }
             Int64 => {
                 let time_type = Datetime(TimeUnit::Nanoseconds, None);
                 let dt = time.cast(&time_type).unwrap();
                 let (out, by, gt) = self.impl_groupby_rolling(
                     dt,
                     by,
                     options,
                     TimeUnit::Nanoseconds,
-                    NO_TIMEZONE.copied(),
+                    None,
                     &time_type,
                 )?;
                 let out = out.cast(&Int64).unwrap();
                 return Ok((out, by, gt));
             }
             dt => polars_bail!(
                 ComputeError:
                 "expected any of the following dtypes: {{ Date, Datetime, Int32, Int64 }}, got {}",
                 dt
             ),
         };
         match tz {
             #[cfg(feature = "timezones")]
-            Some(tz) => match tz.parse::<Tz>() {
-                Ok(tz) => self.impl_groupby_rolling(dt, by, options, tu, Some(tz), time_type),
-                Err(_) => match parse_offset(&tz) {
-                    Ok(tz) => self.impl_groupby_rolling(dt, by, options, tu, Some(tz), time_type),
-                    Err(_) => unreachable!(),
-                },
-            },
-            _ => self.impl_groupby_rolling(dt, by, options, tu, NO_TIMEZONE.copied(), time_type),
+            Some(tz) => {
+                self.impl_groupby_rolling(dt, by, options, tu, tz.parse::<Tz>().ok(), time_type)
+            }
+            _ => self.impl_groupby_rolling(dt, by, options, tu, None, time_type),
         }
     }
 
     /// Returns: time_keys, keys, groupsproxy
     fn groupby_dynamic(
         &self,
         by: Vec<Series>,
@@ -182,17 +202,18 @@
                     && (options.every.parsed_int || options.every.is_zero())
                     && (options.period.parsed_int || options.period.is_zero())),
                 ComputeError: "you cannot combine time durations like '2h' with integer durations like '3i'"
             )
         }
 
         let time = self.0.column(&options.index_column)?.rechunk();
-        if by.is_empty() && !options.period.parsed_int {
+        if by.is_empty() {
             // if by is given, the column must be sorted in the 'by' arg, which we can not check now
-            ensure_sorted_arg(&time, "groupby_dynamic");
+            // this will be checked when the groups are materialized
+            ensure_sorted_arg(&time, "groupby_dynamic")?;
         }
         let time_type = time.dtype();
 
         polars_ensure!(time.null_count() == 0, ComputeError: "null values in dynamic groupby not supported, fill nulls.");
 
         use DataType::*;
         let (dt, tu) = match time_type {
@@ -294,57 +315,65 @@
                 tu,
                 tz,
                 include_lower_bound,
                 include_upper_bound,
                 options.start_by,
             );
             update_bounds(lower, upper);
-            GroupsProxy::Slice {
+            PolarsResult::Ok(GroupsProxy::Slice {
                 groups,
                 rolling: false,
-            }
+            })
         } else {
             let groups = self
                 .0
                 .groupby_with_series(by.clone(), true, true)?
                 .take_groups();
 
             // include boundaries cannot be parallel (easily)
             if include_lower_bound {
                 POOL.install(|| match groups {
                     GroupsProxy::Idx(groups) => {
-                        let mut ir = groups
+                        let ir = groups
                             .par_iter()
                             .map(|base_g| {
                                 let dt = unsafe { dt.take_unchecked(base_g.1.into()) };
-
                                 let vals = dt.downcast_iter().next().unwrap();
                                 let ts = vals.values().as_slice();
+                                if options.check_sorted
+                                    && !matches!(dt.is_sorted_flag(), IsSorted::Ascending)
+                                {
+                                    check_sortedness_slice(ts)?
+                                }
                                 let (sub_groups, lower, upper) = groupby_windows(
                                     w,
                                     ts,
                                     options.closed_window,
                                     tu,
                                     tz,
                                     include_lower_bound,
                                     include_upper_bound,
                                     options.start_by,
                                 );
 
-                                (lower, upper, update_subgroups_idx(&sub_groups, base_g))
+                                Ok((lower, upper, update_subgroups_idx(&sub_groups, base_g)))
                             })
-                            .collect::<Vec<_>>();
+                            .collect::<PolarsResult<Vec<_>>>()?;
 
-                        ir.iter_mut().for_each(|(lower, upper, _)| {
-                            let lower = std::mem::take(lower);
-                            let upper = std::mem::take(upper);
-                            update_bounds(lower, upper)
+                        // flatten in 2 stages
+                        // first flatten the groups
+                        let mut groups = Vec::with_capacity(ir.len());
+
+                        ir.into_iter().for_each(|(lower, upper, g)| {
+                            update_bounds(lower, upper);
+                            groups.push(g);
                         });
 
-                        GroupsProxy::Idx(ir.into_iter().flat_map(|(_, _, groups)| groups).collect())
+                        // then parallelize the flatten in the `from` impl
+                        Ok(GroupsProxy::Idx(GroupsIdx::from(groups)))
                     }
                     GroupsProxy::Slice { groups, .. } => {
                         let mut ir = groups
                             .par_iter()
                             .map(|base_g| {
                                 let dt = dt.slice(base_g[0] as i64, base_g[1] as usize);
                                 let vals = dt.downcast_iter().next().unwrap();
@@ -359,54 +388,64 @@
                                     include_upper_bound,
                                     options.start_by,
                                 );
                                 (lower, upper, update_subgroups_slice(&sub_groups, *base_g))
                             })
                             .collect::<Vec<_>>();
 
-                        ir.iter_mut().for_each(|(lower, upper, _)| {
+                        let mut capacity = 0;
+                        ir.iter_mut().for_each(|(lower, upper, g)| {
                             let lower = std::mem::take(lower);
                             let upper = std::mem::take(upper);
-                            update_bounds(lower, upper)
+                            update_bounds(lower, upper);
+                            capacity += g.len();
                         });
 
-                        GroupsProxy::Slice {
-                            groups: ir.into_iter().flat_map(|(_, _, groups)| groups).collect(),
+                        let mut groups = Vec::with_capacity(capacity);
+                        ir.iter().for_each(|(_, _, g)| groups.extend_from_slice(g));
+
+                        Ok(GroupsProxy::Slice {
+                            groups,
                             rolling: false,
-                        }
+                        })
                     }
                 })
             } else {
                 POOL.install(|| match groups {
                     GroupsProxy::Idx(groups) => {
                         let groupsidx = groups
                             .par_iter()
-                            .flat_map(|base_g| {
+                            .map(|base_g| {
                                 let dt = unsafe { dt.take_unchecked(base_g.1.into()) };
                                 let vals = dt.downcast_iter().next().unwrap();
                                 let ts = vals.values().as_slice();
+                                if options.check_sorted
+                                    && !matches!(dt.is_sorted_flag(), IsSorted::Ascending)
+                                {
+                                    check_sortedness_slice(ts)?
+                                }
                                 let (sub_groups, _, _) = groupby_windows(
                                     w,
                                     ts,
                                     options.closed_window,
                                     tu,
                                     tz,
                                     include_lower_bound,
                                     include_upper_bound,
                                     options.start_by,
                                 );
-                                update_subgroups_idx(&sub_groups, base_g)
+                                Ok(update_subgroups_idx(&sub_groups, base_g))
                             })
-                            .collect();
-                        GroupsProxy::Idx(groupsidx)
+                            .collect::<PolarsResult<Vec<_>>>()?;
+                        Ok(GroupsProxy::Idx(GroupsIdx::from(groupsidx)))
                     }
                     GroupsProxy::Slice { groups, .. } => {
                         let groups = groups
                             .par_iter()
-                            .flat_map(|base_g| {
+                            .map(|base_g| {
                                 let dt = dt.slice(base_g[0] as i64, base_g[1] as usize);
                                 let vals = dt.downcast_iter().next().unwrap();
                                 let ts = vals.values().as_slice();
                                 let (sub_groups, _, _) = groupby_windows(
                                     w,
                                     ts,
                                     options.closed_window,
@@ -414,45 +453,59 @@
                                     tz,
                                     include_lower_bound,
                                     include_upper_bound,
                                     options.start_by,
                                 );
                                 update_subgroups_slice(&sub_groups, *base_g)
                             })
-                            .collect();
-                        GroupsProxy::Slice {
+                            .collect::<Vec<_>>();
+
+                        let groups = flatten_par(&groups);
+
+                        Ok(GroupsProxy::Slice {
                             groups,
                             rolling: false,
-                        }
+                        })
                     }
                 })
             }
-        };
+        }?;
+        // note that if 'by' is empty we can be sure that the index column, the lower column and the
+        // upper column remain/are sorted
 
         let dt = unsafe { dt.clone().into_series().agg_first(&groups) };
         let mut dt = dt.datetime().unwrap().as_ref().clone();
         for key in by.iter_mut() {
             *key = unsafe { key.agg_first(&groups) };
         }
 
         let lower = lower_bound.map(|lower| Int64Chunked::new_vec(LB_NAME, lower));
 
         if options.truncate {
             let mut lower = lower.clone().unwrap();
+            if by.is_empty() {
+                lower.set_sorted_flag(IsSorted::Ascending)
+            }
             lower.rename(dt.name());
             dt = lower;
         }
 
-        if let (true, Some(lower), Some(higher)) = (options.include_boundaries, lower, upper_bound)
+        if let (true, Some(mut lower), Some(upper)) =
+            (options.include_boundaries, lower, upper_bound)
         {
-            by.push(lower.into_datetime(tu, tz.clone()).into_series());
-            let s = Int64Chunked::new_vec(UP_NAME, higher)
+            let mut upper = Int64Chunked::new_vec(UP_NAME, upper)
                 .into_datetime(tu, tz.clone())
                 .into_series();
-            by.push(s);
+
+            if by.is_empty() {
+                lower.set_sorted_flag(IsSorted::Ascending);
+                upper.set_sorted_flag(IsSorted::Ascending);
+            }
+            by.push(lower.into_datetime(tu, tz.clone()).into_series());
+            by.push(upper);
         }
 
         dt.into_datetime(tu, None)
             .into_series()
             .cast(time_type)
             .map(|s| (s, by, groups))
     }
@@ -460,23 +513,23 @@
     /// Returns: time_keys, keys, groupsproxy
     fn impl_groupby_rolling(
         &self,
         dt: Series,
         by: Vec<Series>,
         options: &RollingGroupOptions,
         tu: TimeUnit,
-        tz: Option<impl PolarsTimeZone>,
+        tz: Option<Tz>,
         time_type: &DataType,
     ) -> PolarsResult<(Series, Vec<Series>, GroupsProxy)> {
         let mut dt = dt.rechunk();
-        // a requirement for the index
-        // so we can set this such that downstream code has this info
-        dt.set_sorted_flag(IsSorted::Ascending);
 
         let groups = if by.is_empty() {
+            // a requirement for the index
+            // so we can set this such that downstream code has this info
+            dt.set_sorted_flag(IsSorted::Ascending);
             let dt = dt.datetime().unwrap();
             let vals = dt.downcast_iter().next().unwrap();
             let ts = vals.values().as_slice();
             PolarsResult::Ok(GroupsProxy::Slice {
                 groups: groupby_values(
                     options.period,
                     options.offset,
@@ -505,21 +558,27 @@
                 GroupsProxy::Idx(groups) => {
                     let idx = groups
                         .par_iter()
                         .map(|base_g| {
                             let dt = unsafe { dt_local.take_unchecked(base_g.1.into()) };
                             let vals = dt.downcast_iter().next().unwrap();
                             let ts = vals.values().as_slice();
+                            if options.check_sorted
+                                && !matches!(dt.is_sorted_flag(), IsSorted::Ascending)
+                            {
+                                check_sortedness_slice(ts)?
+                            }
+
                             let sub_groups = groupby_values(
                                 options.period,
                                 options.offset,
                                 ts,
                                 options.closed_window,
                                 tu,
-                                tz.clone(),
+                                tz,
                             )?;
                             Ok(update_subgroups_idx(&sub_groups, base_g))
                         })
                         .collect::<PolarsResult<Vec<_>>>()?;
 
                     Ok(GroupsProxy::Idx(GroupsIdx::from(idx)))
                 }
@@ -532,15 +591,15 @@
                             let ts = vals.values().as_slice();
                             let sub_groups = groupby_values(
                                 options.period,
                                 options.offset,
                                 ts,
                                 options.closed_window,
                                 tu,
-                                tz.clone(),
+                                tz,
                             )?;
                             Ok(update_subgroups_slice(&sub_groups, *base_g))
                         })
                         .collect::<PolarsResult<Vec<_>>>()?;
 
                     let slice_groups = flatten_par(&slice_groups);
                     Ok(GroupsProxy::Slice {
@@ -599,76 +658,81 @@
     fn test_rolling_groupby_tu() -> PolarsResult<()> {
         // test multiple time units
         for tu in [
             TimeUnit::Nanoseconds,
             TimeUnit::Microseconds,
             TimeUnit::Milliseconds,
         ] {
-            let date = Utf8Chunked::new(
+            let mut date = Utf8Chunked::new(
                 "dt",
                 [
                     "2020-01-01 13:45:48",
                     "2020-01-01 16:42:13",
                     "2020-01-01 16:45:09",
                     "2020-01-02 18:12:48",
                     "2020-01-03 19:45:32",
                     "2020-01-08 23:16:43",
                 ],
             )
-            .as_datetime(None, tu, false, false, false, None)?
+            .as_datetime(None, tu, false, false, None)?
             .into_series();
+            date.set_sorted_flag(IsSorted::Ascending);
             let a = Series::new("a", [3, 7, 5, 9, 2, 1]);
             let df = DataFrame::new(vec![date, a.clone()])?;
 
             let (_, _, groups) = df
                 .groupby_rolling(
                     vec![],
                     &RollingGroupOptions {
                         index_column: "dt".into(),
                         period: Duration::parse("2d"),
                         offset: Duration::parse("-2d"),
                         closed_window: ClosedWindow::Right,
+                        ..Default::default()
                     },
                 )
                 .unwrap();
 
             let sum = unsafe { a.agg_sum(&groups) };
             let expected = Series::new("", [3, 10, 15, 24, 11, 1]);
             assert_eq!(sum, expected);
         }
 
         Ok(())
     }
 
     #[test]
     fn test_rolling_groupby_aggs() -> PolarsResult<()> {
-        let date = Utf8Chunked::new(
+        let mut date = Utf8Chunked::new(
             "dt",
             [
                 "2020-01-01 13:45:48",
                 "2020-01-01 16:42:13",
                 "2020-01-01 16:45:09",
                 "2020-01-02 18:12:48",
                 "2020-01-03 19:45:32",
                 "2020-01-08 23:16:43",
             ],
         )
-        .as_datetime(None, TimeUnit::Milliseconds, false, false, false, None)?
+        .as_datetime(None, TimeUnit::Milliseconds, false, false, None)?
         .into_series();
+        date.set_sorted_flag(IsSorted::Ascending);
+
         let a = Series::new("a", [3, 7, 5, 9, 2, 1]);
         let df = DataFrame::new(vec![date, a.clone()])?;
 
         let (_, _, groups) = df
             .groupby_rolling(
                 vec![],
                 &RollingGroupOptions {
                     index_column: "dt".into(),
                     period: Duration::parse("2d"),
                     offset: Duration::parse("-2d"),
                     closed_window: ClosedWindow::Right,
+                    ..Default::default()
                 },
             )
             .unwrap();
 
         let nulls = Series::new("", [Some(3), Some(7), None, Some(9), Some(2), Some(1)]);
 
         let min = unsafe { a.agg_min(&groups) };
@@ -742,14 +806,15 @@
                     every: Duration::parse("1h"),
                     period: Duration::parse("1h"),
                     offset: Duration::parse("0h"),
                     truncate: true,
                     include_boundaries: true,
                     closed_window: ClosedWindow::Both,
                     start_by: Default::default(),
+                    ..Default::default()
                 },
             )
             .unwrap();
 
         let ca = time_key.datetime().unwrap();
         let years = ca.year();
         assert_eq!(years.get(0), Some(2021i32));
@@ -856,14 +921,15 @@
                     every: Duration::parse("6d"),
                     period: Duration::parse("6d"),
                     offset: Duration::parse("0h"),
                     truncate: true,
                     include_boundaries: true,
                     closed_window: ClosedWindow::Both,
                     start_by: Default::default(),
+                    ..Default::default()
                 },
             )
             .unwrap();
         let mut lower_bound = keys[1].clone();
         time_key.rename("");
         lower_bound.rename("");
         assert!(time_key.series_equal(&lower_bound));
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/lib.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/lib.rs`

 * *Files 16% similar despite different names*

```diff
@@ -16,11 +16,11 @@
 #[cfg(any(feature = "dtype-date", feature = "dtype-datetime"))]
 pub use groupby::dynamic::*;
 pub use month_end::*;
 pub use month_start::*;
 pub use round::*;
 pub use truncate::*;
 pub use upsample::*;
-pub use windows::calendar::date_range as date_range_vec;
+pub use windows::calendar::temporal_range as temporal_range_vec;
 pub use windows::duration::Duration;
 pub use windows::groupby::ClosedWindow;
 pub use windows::window::Window;
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/month_end.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/month_end.rs`

 * *Files 16% similar despite different names*

```diff
@@ -1,42 +1,42 @@
 use chrono::NaiveDateTime;
-use polars_arrow::time_zone::{PolarsTimeZone, NO_TIMEZONE};
+use polars_arrow::time_zone::Tz;
 use polars_core::prelude::*;
 use polars_core::utils::arrow::temporal_conversions::{
     timestamp_ms_to_datetime, timestamp_ns_to_datetime, timestamp_us_to_datetime, MILLISECONDS,
     SECONDS_IN_DAY,
 };
 
 use crate::month_start::roll_backward;
 use crate::windows::duration::Duration;
 
 // roll forward to the last day of the month
-fn roll_forward<T: PolarsTimeZone>(
+fn roll_forward(
     t: i64,
-    time_zone: Option<&T>,
+    time_zone: Option<&Tz>,
     timestamp_to_datetime: fn(i64) -> NaiveDateTime,
     datetime_to_timestamp: fn(NaiveDateTime) -> i64,
-    offset_fn: fn(&Duration, i64, Option<&T>) -> PolarsResult<i64>,
+    offset_fn: fn(&Duration, i64, Option<&Tz>) -> PolarsResult<i64>,
 ) -> PolarsResult<i64> {
     let t = roll_backward(t, time_zone, timestamp_to_datetime, datetime_to_timestamp)?;
     let t = offset_fn(&Duration::parse("1mo"), t, time_zone)?;
     offset_fn(&Duration::parse("-1d"), t, time_zone)
 }
 
 pub trait PolarsMonthEnd {
-    fn month_end<T: PolarsTimeZone>(&self, time_zone: Option<&T>) -> PolarsResult<Self>
+    fn month_end(&self, time_zone: Option<&Tz>) -> PolarsResult<Self>
     where
         Self: Sized;
 }
 
 impl PolarsMonthEnd for DatetimeChunked {
-    fn month_end<T: PolarsTimeZone>(&self, time_zone: Option<&T>) -> PolarsResult<Self> {
+    fn month_end(&self, time_zone: Option<&Tz>) -> PolarsResult<Self> {
         let timestamp_to_datetime: fn(i64) -> NaiveDateTime;
         let datetime_to_timestamp: fn(NaiveDateTime) -> i64;
-        let offset_fn: fn(&Duration, i64, Option<&T>) -> PolarsResult<i64>;
+        let offset_fn: fn(&Duration, i64, Option<&Tz>) -> PolarsResult<i64>;
         match self.time_unit() {
             TimeUnit::Nanoseconds => {
                 timestamp_to_datetime = timestamp_ns_to_datetime;
                 datetime_to_timestamp = datetime_to_timestamp_ns;
                 offset_fn = Duration::add_ns;
             }
             TimeUnit::Microseconds => {
@@ -62,22 +62,22 @@
                 )
             })?
             .into_datetime(self.time_unit(), self.time_zone().clone()))
     }
 }
 
 impl PolarsMonthEnd for DateChunked {
-    fn month_end<T: PolarsTimeZone>(&self, _time_zone: Option<&T>) -> PolarsResult<Self> {
+    fn month_end(&self, _time_zone: Option<&Tz>) -> PolarsResult<Self> {
         const MSECS_IN_DAY: i64 = MILLISECONDS * SECONDS_IN_DAY;
         Ok(self
             .0
             .try_apply(|t| {
                 Ok((roll_forward(
                     MSECS_IN_DAY * t as i64,
-                    NO_TIMEZONE,
+                    None,
                     timestamp_ms_to_datetime,
                     datetime_to_timestamp_ms,
                     Duration::add_ms,
                 )? / MSECS_IN_DAY) as i32)
             })?
             .into_date())
     }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/round.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/groupby_dynamic.rs`

 * *Files 26% similar despite different names*

```diff
@@ -1,55 +1,62 @@
-use polars_arrow::export::arrow::temporal_conversions::{MILLISECONDS, SECONDS_IN_DAY};
-use polars_arrow::time_zone::PolarsTimeZone;
-use polars_core::prelude::*;
+// used only if feature="temporal", "dtype-date", "dynamic_groupby"
+#[allow(unused_imports)]
+use polars::export::chrono::prelude::*;
 
-use crate::prelude::*;
+// used only if feature="temporal", "dtype-date", "dynamic_groupby"
+#[allow(unused_imports)]
+use super::*;
 
-pub trait PolarsRound {
-    fn round(
-        &self,
-        every: Duration,
-        offset: Duration,
-        tz: Option<&impl PolarsTimeZone>,
-    ) -> PolarsResult<Self>
-    where
-        Self: Sized;
-}
-
-#[cfg(feature = "dtype-datetime")]
-impl PolarsRound for DatetimeChunked {
-    fn round(
-        &self,
-        every: Duration,
-        offset: Duration,
-        tz: Option<&impl PolarsTimeZone>,
-    ) -> PolarsResult<Self> {
-        let w = Window::new(every, every, offset);
+#[test]
+#[cfg(all(
+    feature = "temporal",
+    feature = "dtype-date",
+    feature = "dynamic_groupby"
+))]
+fn test_groupby_dynamic_week_bounds() -> PolarsResult<()> {
+    let start = NaiveDate::from_ymd_opt(2022, 2, 1)
+        .unwrap()
+        .and_hms_opt(0, 0, 0)
+        .unwrap();
+    let stop = NaiveDate::from_ymd_opt(2022, 2, 14)
+        .unwrap()
+        .and_hms_opt(0, 0, 0)
+        .unwrap();
+    let range = polars_time::date_range(
+        "dt",
+        start,
+        stop,
+        Duration::parse("1d"),
+        ClosedWindow::Left,
+        TimeUnit::Milliseconds,
+        None,
+    )?
+    .into_series();
 
-        let func = match self.time_unit() {
-            TimeUnit::Nanoseconds => Window::round_ns,
-            TimeUnit::Microseconds => Window::round_us,
-            TimeUnit::Milliseconds => Window::round_ms,
-        };
-        Ok(self
-            .try_apply(|t| func(&w, t, tz))?
-            .into_datetime(self.time_unit(), self.time_zone().clone()))
-    }
-}
+    let a = Int32Chunked::full("a", 1, range.len());
+    let df = df![
+        "dt" => range,
+        "a" => a
+    ]?;
 
-#[cfg(feature = "dtype-date")]
-impl PolarsRound for DateChunked {
-    fn round(
-        &self,
-        every: Duration,
-        offset: Duration,
-        _tz: Option<&impl PolarsTimeZone>,
-    ) -> PolarsResult<Self> {
-        let w = Window::new(every, every, offset);
-        Ok(self
-            .try_apply(|t| {
-                const MSECS_IN_DAY: i64 = MILLISECONDS * SECONDS_IN_DAY;
-                Ok((w.round_ms(MSECS_IN_DAY * t as i64, NO_TIMEZONE)? / MSECS_IN_DAY) as i32)
-            })?
-            .into_date())
-    }
+    let out = df
+        .lazy()
+        .groupby_dynamic(
+            col("dt"),
+            [],
+            DynamicGroupOptions {
+                every: Duration::parse("1w"),
+                period: Duration::parse("1w"),
+                offset: Duration::parse("0w"),
+                closed_window: ClosedWindow::Left,
+                truncate: false,
+                include_boundaries: true,
+                ..Default::default()
+            },
+        )
+        .agg([col("a").sum()])
+        .collect()?;
+    let a = out.column("a")?;
+    assert_eq!(a.get(0)?, AnyValue::Int32(7));
+    assert_eq!(a.get(1)?, AnyValue::Int32(6));
+    Ok(())
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/series/_trait.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/series/_trait.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/series/implementations/floats.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/series/implementations/floats.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/series/implementations/integers.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/series/implementations/integers.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/series/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/series/mod.rs`

 * *Files 4% similar despite different names*

```diff
@@ -300,30 +300,39 @@
             DataType::Date => s.date().map(|ca| ca.month()),
             #[cfg(feature = "dtype-datetime")]
             DataType::Datetime(_, _) => s.datetime().map(|ca| ca.month()),
             dt => polars_bail!(opq = month, dt),
         }
     }
 
-    /// Format Date/Datetimewith a `format` rule. See [chrono strftime/strptime](https://docs.rs/chrono/0.4.19/chrono/format/strftime/index.html).
-    fn strftime(&self, format: &str) -> PolarsResult<Series> {
+    /// Convert Time into Utf8 with the given format.
+    /// See [chrono strftime/strptime](https://docs.rs/chrono/0.4.19/chrono/format/strftime/index.html).
+    fn to_string(&self, format: &str) -> PolarsResult<Series> {
         let s = self.as_series();
         match s.dtype() {
             #[cfg(feature = "dtype-date")]
-            DataType::Date => s.date().map(|ca| ca.strftime(format).into_series()),
+            DataType::Date => s.date().map(|ca| ca.to_string(format).into_series()),
             #[cfg(feature = "dtype-datetime")]
             DataType::Datetime(_, _) => s
                 .datetime()
-                .map(|ca| Ok(ca.strftime(format)?.into_series()))?,
+                .map(|ca| Ok(ca.to_string(format)?.into_series()))?,
             #[cfg(feature = "dtype-time")]
-            DataType::Time => s.time().map(|ca| ca.strftime(format).into_series()),
-            dt => polars_bail!(opq = strftime, dt),
+            DataType::Time => s.time().map(|ca| ca.to_string(format).into_series()),
+            dt => polars_bail!(opq = to_string, dt),
         }
     }
 
+    /// Convert from Time into Utf8 with the given format.
+    /// See [chrono strftime/strptime](https://docs.rs/chrono/0.4.19/chrono/format/strftime/index.html).
+    ///
+    /// Alias for `to_string`.
+    fn strftime(&self, format: &str) -> PolarsResult<Series> {
+        self.to_string(format)
+    }
+
     #[cfg(all(feature = "dtype-date", feature = "dtype-datetime"))]
     /// Convert date(time) object to timestamp in [`TimeUnit`].
     fn timestamp(&self, tu: TimeUnit) -> PolarsResult<Int64Chunked> {
         let s = self.as_series();
         if matches!(s.dtype(), DataType::Time) {
             polars_bail!(opq = timestamp, s.dtype());
         } else {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/truncate.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/truncate.rs`

 * *Files 22% similar despite different names*

```diff
@@ -1,33 +1,23 @@
 #[cfg(feature = "dtype-date")]
 use polars_arrow::export::arrow::temporal_conversions::{MILLISECONDS, SECONDS_IN_DAY};
-use polars_arrow::time_zone::PolarsTimeZone;
+use polars_arrow::time_zone::Tz;
 use polars_core::prelude::*;
 
 use crate::prelude::*;
 
 pub trait PolarsTruncate {
-    fn truncate(
-        &self,
-        every: Duration,
-        offset: Duration,
-        tz: Option<&impl PolarsTimeZone>,
-    ) -> PolarsResult<Self>
+    fn truncate(&self, every: Duration, offset: Duration, tz: Option<&Tz>) -> PolarsResult<Self>
     where
         Self: Sized;
 }
 
 #[cfg(feature = "dtype-datetime")]
 impl PolarsTruncate for DatetimeChunked {
-    fn truncate(
-        &self,
-        every: Duration,
-        offset: Duration,
-        tz: Option<&impl PolarsTimeZone>,
-    ) -> PolarsResult<Self> {
+    fn truncate(&self, every: Duration, offset: Duration, tz: Option<&Tz>) -> PolarsResult<Self> {
         let w = Window::new(every, every, offset);
 
         let func = match self.time_unit() {
             TimeUnit::Nanoseconds => Window::truncate_ns,
             TimeUnit::Microseconds => Window::truncate_us,
             TimeUnit::Milliseconds => Window::truncate_ms,
         };
@@ -36,22 +26,17 @@
             .try_apply(|t| func(&w, t, tz))?
             .into_datetime(self.time_unit(), self.time_zone().clone()))
     }
 }
 
 #[cfg(feature = "dtype-date")]
 impl PolarsTruncate for DateChunked {
-    fn truncate(
-        &self,
-        every: Duration,
-        offset: Duration,
-        _tz: Option<&impl PolarsTimeZone>,
-    ) -> PolarsResult<Self> {
+    fn truncate(&self, every: Duration, offset: Duration, _tz: Option<&Tz>) -> PolarsResult<Self> {
         let w = Window::new(every, every, offset);
         Ok(self
             .try_apply(|t| {
                 const MSECS_IN_DAY: i64 = MILLISECONDS * SECONDS_IN_DAY;
-                Ok((w.truncate_ms(MSECS_IN_DAY * t as i64, NO_TIMEZONE)? / MSECS_IN_DAY) as i32)
+                Ok((w.truncate_ms(MSECS_IN_DAY * t as i64, None)? / MSECS_IN_DAY) as i32)
             })?
             .into_date())
     }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/upsample.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/upsample.rs`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 #[cfg(feature = "timezones")]
-use arrow::temporal_conversions::parse_offset;
+use chrono_tz::Tz;
 use polars_core::prelude::*;
 use polars_core::utils::ensure_sorted_arg;
 use polars_ops::prelude::*;
 
 use crate::prelude::*;
 #[cfg(feature = "timezones")]
 use crate::utils::unlocalize_timestamp;
@@ -106,15 +106,15 @@
     by: Vec<String>,
     index_column: &str,
     every: Duration,
     offset: Duration,
     stable: bool,
 ) -> PolarsResult<DataFrame> {
     let s = source.column(index_column)?;
-    ensure_sorted_arg(s, "upsample");
+    ensure_sorted_arg(s, "upsample")?;
     if matches!(s.dtype(), DataType::Date) {
         let mut df = source.clone();
         df.try_apply(index_column, |s| {
             s.cast(&DataType::Datetime(TimeUnit::Milliseconds, None))
         })
         .unwrap();
         let mut out = upsample_impl(&df, by, index_column, every, offset, stable).unwrap();
@@ -153,33 +153,24 @@
             let ca = s.i64().unwrap();
             let first = ca.into_iter().flatten().next();
             let last = ca.into_iter().flatten().next_back();
             match (first, last) {
                 (Some(first), Some(last)) => {
                     let (first, last) = match tz {
                         #[cfg(feature = "timezones")]
-                        Some(tz) => match tz.parse::<chrono_tz::Tz>() {
-                            Ok(tz) => (
-                                unlocalize_timestamp(first, *tu, tz),
-                                unlocalize_timestamp(last, *tu, tz),
-                            ),
-                            Err(_) => match parse_offset(tz) {
-                                Ok(tz) => (
-                                    unlocalize_timestamp(first, *tu, tz),
-                                    unlocalize_timestamp(last, *tu, tz),
-                                ),
-                                Err(_) => unreachable!(),
-                            },
-                        },
+                        Some(tz) => (
+                            unlocalize_timestamp(first, *tu, tz.parse::<Tz>().unwrap()),
+                            unlocalize_timestamp(last, *tu, tz.parse::<Tz>().unwrap()),
+                        ),
                         _ => (first, last),
                     };
                     let first = match tu {
-                        TimeUnit::Nanoseconds => offset.add_ns(first, NO_TIMEZONE)?,
-                        TimeUnit::Microseconds => offset.add_us(first, NO_TIMEZONE)?,
-                        TimeUnit::Milliseconds => offset.add_ms(first, NO_TIMEZONE)?,
+                        TimeUnit::Nanoseconds => offset.add_ns(first, None)?,
+                        TimeUnit::Microseconds => offset.add_us(first, None)?,
+                        TimeUnit::Milliseconds => offset.add_ms(first, None)?,
                     };
                     let range = date_range_impl(
                         index_col_name,
                         first,
                         last,
                         every,
                         ClosedWindow::Both,
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/utils.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/utils.rs`

 * *Files 4% similar despite different names*

```diff
@@ -1,23 +1,22 @@
 #[cfg(feature = "timezones")]
 use arrow::temporal_conversions::{
     timestamp_ms_to_datetime, timestamp_ns_to_datetime, timestamp_us_to_datetime,
 };
 #[cfg(feature = "timezones")]
+use chrono::TimeZone;
+#[cfg(feature = "timezones")]
 use chrono::{LocalResult, NaiveDateTime};
 #[cfg(feature = "timezones")]
-use polars_arrow::time_zone::PolarsTimeZone;
+use polars_arrow::time_zone::Tz;
 #[cfg(feature = "timezones")]
 use polars_core::prelude::{polars_bail, PolarsResult, TimeUnit};
 
 #[cfg(feature = "timezones")]
-pub(crate) fn localize_datetime(
-    ndt: NaiveDateTime,
-    tz: &impl PolarsTimeZone,
-) -> PolarsResult<NaiveDateTime> {
+pub(crate) fn localize_datetime(ndt: NaiveDateTime, tz: &Tz) -> PolarsResult<NaiveDateTime> {
     // e.g. '2021-01-01 03:00' -> '2021-01-01 03:00CDT'
     match tz.from_local_datetime(&ndt) {
         LocalResult::Single(tz) => Ok(tz.naive_utc()),
         LocalResult::Ambiguous(_, _) => {
             polars_bail!(
                 ComputeError: format!("datetime '{}' is ambiguous in time zone '{}'. Ambiguous datetimes are not yet supported", ndt, tz)
             )
@@ -27,40 +26,36 @@
                 ComputeError: format!("datetime '{}' is non-existent in time zone '{}'. Non-existent datetimes are not yet supported", ndt, tz)
             )
         }
     }
 }
 
 #[cfg(feature = "timezones")]
-pub(crate) fn unlocalize_datetime(ndt: NaiveDateTime, tz: &impl PolarsTimeZone) -> NaiveDateTime {
+pub(crate) fn unlocalize_datetime(ndt: NaiveDateTime, tz: &Tz) -> NaiveDateTime {
     // e.g. '2021-01-01 03:00CDT' -> '2021-01-01 03:00'
     tz.from_utc_datetime(&ndt).naive_local()
 }
 
 #[cfg(feature = "timezones")]
-pub(crate) fn localize_timestamp<T: PolarsTimeZone>(
-    timestamp: i64,
-    tu: TimeUnit,
-    tz: T,
-) -> PolarsResult<i64> {
+pub(crate) fn localize_timestamp(timestamp: i64, tu: TimeUnit, tz: Tz) -> PolarsResult<i64> {
     match tu {
         TimeUnit::Nanoseconds => {
             Ok(localize_datetime(timestamp_ns_to_datetime(timestamp), &tz)?.timestamp_nanos())
         }
         TimeUnit::Microseconds => {
             Ok(localize_datetime(timestamp_us_to_datetime(timestamp), &tz)?.timestamp_micros())
         }
         TimeUnit::Milliseconds => {
             Ok(localize_datetime(timestamp_ms_to_datetime(timestamp), &tz)?.timestamp_millis())
         }
     }
 }
 
 #[cfg(feature = "timezones")]
-pub(crate) fn unlocalize_timestamp<T: PolarsTimeZone>(timestamp: i64, tu: TimeUnit, tz: T) -> i64 {
+pub(crate) fn unlocalize_timestamp(timestamp: i64, tu: TimeUnit, tz: Tz) -> i64 {
     match tu {
         TimeUnit::Nanoseconds => {
             unlocalize_datetime(timestamp_ns_to_datetime(timestamp), &tz).timestamp_nanos()
         }
         TimeUnit::Microseconds => {
             unlocalize_datetime(timestamp_us_to_datetime(timestamp), &tz).timestamp_micros()
         }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/windows/bounds.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/windows/bounds.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/windows/calendar.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/windows/calendar.rs`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-use polars_arrow::time_zone::PolarsTimeZone;
+use polars_arrow::time_zone::Tz;
 use polars_core::prelude::*;
 
 use crate::prelude::*;
 
 const LAST_DAYS_MONTH: [u32; 12] = [
     31, // January:   31,
     28, // February:  28,
@@ -31,24 +31,25 @@
 pub const NS_MILLISECOND: i64 = 1_000_000;
 pub const NS_SECOND: i64 = 1_000_000_000;
 pub const NS_MINUTE: i64 = 60 * NS_SECOND;
 pub const NS_HOUR: i64 = 60 * NS_MINUTE;
 pub const NS_DAY: i64 = 24 * NS_HOUR;
 pub const NS_WEEK: i64 = 7 * NS_DAY;
 
-pub fn date_range<T: PolarsTimeZone>(
+/// vector of i64 representing temporal values
+pub fn temporal_range(
     start: i64,
     stop: i64,
     every: Duration,
     closed: ClosedWindow,
     tu: TimeUnit,
-    tz: Option<&T>,
+    tz: Option<&Tz>,
 ) -> PolarsResult<Vec<i64>> {
     let size: usize;
-    let offset_fn: fn(&Duration, i64, Option<&T>) -> PolarsResult<i64>;
+    let offset_fn: fn(&Duration, i64, Option<&Tz>) -> PolarsResult<i64>;
 
     match tu {
         TimeUnit::Nanoseconds => {
             size = ((stop - start) / every.duration_ns() + 1) as usize;
             offset_fn = Duration::add_ns;
         }
         TimeUnit::Microseconds => {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/windows/duration.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/windows/duration.rs`

 * *Files 5% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 use std::ops::Mul;
 
 use chrono::{Datelike, NaiveDate, NaiveDateTime, NaiveTime, Timelike, Weekday};
 use polars_arrow::error::polars_err;
 use polars_arrow::export::arrow::temporal_conversions::{
     timestamp_ms_to_datetime, timestamp_ns_to_datetime, timestamp_us_to_datetime, MILLISECONDS,
 };
-use polars_arrow::time_zone::PolarsTimeZone;
+use polars_arrow::time_zone::Tz;
 use polars_core::export::arrow::temporal_conversions::MICROSECONDS;
 use polars_core::prelude::{
     datetime_to_timestamp_ms, datetime_to_timestamp_ns, datetime_to_timestamp_us, polars_bail,
     PolarsResult,
 };
 use polars_core::utils::arrow::temporal_conversions::NANOSECONDS;
 #[cfg(feature = "serde")]
@@ -356,19 +356,82 @@
     #[cfg(feature = "private")]
     #[doc(hidden)]
     pub const fn duration_ms(&self) -> i64 {
         self.months * 28 * 24 * 3600 * MILLISECONDS
             + (self.weeks * NS_WEEK + self.nsecs + self.days * NS_DAY) / 1_000_000
     }
 
+    #[cfg(feature = "private")]
+    #[doc(hidden)]
+    fn add_month(
+        ts: NaiveDateTime,
+        n_months: i64,
+        negative: bool,
+        saturating: bool,
+    ) -> PolarsResult<NaiveDateTime> {
+        let mut months = n_months;
+        if negative {
+            months = -months;
+        }
+
+        // Retrieve the current date and increment the values
+        // based on the number of months
+        let mut year = ts.year();
+        let mut month = ts.month() as i32;
+        let mut day = ts.day();
+        year += (months / 12) as i32;
+        month += (months % 12) as i32;
+
+        // if the month overflowed or underflowed, adjust the year
+        // accordingly. Because we add the modulo for the months
+        // the year will only adjust by one
+        if month > 12 {
+            year += 1;
+            month -= 12;
+        } else if month <= 0 {
+            year -= 1;
+            month += 12;
+        }
+
+        if saturating {
+            // Normalize the day if we are past the end of the month.
+            let mut last_day_of_month = last_day_of_month(month);
+            if month == (chrono::Month::February.number_from_month() as i32) && is_leap_year(year) {
+                last_day_of_month += 1;
+            }
+
+            if day > last_day_of_month {
+                day = last_day_of_month
+            }
+        }
+
+        // Retrieve the original time and construct a data
+        // with the new year, month and day
+        let hour = ts.hour();
+        let minute = ts.minute();
+        let sec = ts.second();
+        let nsec = ts.nanosecond();
+        new_datetime(year, month as u32, day, hour, minute, sec, nsec).ok_or(
+            polars_err!(
+                ComputeError: format!(
+                    "cannot advance '{}' by {} month(s). \
+                        If you were trying to get the last day of each month, you may want to try `.dt.month_end` \
+                        or append \"_saturating\" to your duration string.",
+                        ts,
+                        if negative {-n_months} else {n_months}
+                )
+            ),
+        )
+    }
+
     #[inline]
     pub fn truncate_impl<F, G, J>(
         &self,
         t: i64,
-        tz: Option<&impl PolarsTimeZone>,
+        tz: Option<&Tz>,
         nsecs_to_unit: F,
         timestamp_to_datetime: G,
         datetime_to_timestamp: J,
     ) -> PolarsResult<i64>
     where
         F: Fn(i64) -> i64,
         G: Fn(i64) -> NaiveDateTime,
@@ -474,121 +537,71 @@
                 polars_bail!(ComputeError: "duration may not mix month, weeks and nanosecond units")
             }
         }
     }
 
     // Truncate the given ns timestamp by the window boundary.
     #[inline]
-    pub fn truncate_ns(&self, t: i64, tz: Option<&impl PolarsTimeZone>) -> PolarsResult<i64> {
+    pub fn truncate_ns(&self, t: i64, tz: Option<&Tz>) -> PolarsResult<i64> {
         self.truncate_impl(
             t,
             tz,
             |nsecs| nsecs,
             timestamp_ns_to_datetime,
             datetime_to_timestamp_ns,
         )
     }
 
     // Truncate the given ns timestamp by the window boundary.
     #[inline]
-    pub fn truncate_us(&self, t: i64, tz: Option<&impl PolarsTimeZone>) -> PolarsResult<i64> {
+    pub fn truncate_us(&self, t: i64, tz: Option<&Tz>) -> PolarsResult<i64> {
         self.truncate_impl(
             t,
             tz,
             |nsecs| nsecs / 1000,
             timestamp_us_to_datetime,
             datetime_to_timestamp_us,
         )
     }
 
     // Truncate the given ms timestamp by the window boundary.
     #[inline]
-    pub fn truncate_ms(&self, t: i64, tz: Option<&impl PolarsTimeZone>) -> PolarsResult<i64> {
+    pub fn truncate_ms(&self, t: i64, tz: Option<&Tz>) -> PolarsResult<i64> {
         self.truncate_impl(
             t,
             tz,
             |nsecs| nsecs / 1_000_000,
             timestamp_ms_to_datetime,
             datetime_to_timestamp_ms,
         )
     }
 
     fn add_impl_month_week_or_day<F, G, J>(
         &self,
         t: i64,
-        tz: Option<&impl PolarsTimeZone>,
+        tz: Option<&Tz>,
         nsecs_to_unit: F,
         timestamp_to_datetime: G,
         datetime_to_timestamp: J,
     ) -> PolarsResult<i64>
     where
         F: Fn(i64) -> i64,
         G: Fn(i64) -> NaiveDateTime,
         J: Fn(NaiveDateTime) -> i64,
     {
         let d = self;
         let mut new_t = t;
 
         if d.months > 0 {
-            let mut months = d.months;
-            if d.negative {
-                months = -months;
-            }
-
-            // Retrieve the current date and increment the values
-            // based on the number of months
             let ts = match tz {
                 #[cfg(feature = "timezones")]
                 Some(tz) => unlocalize_datetime(timestamp_to_datetime(t), tz),
                 _ => timestamp_to_datetime(t),
             };
-            let mut year = ts.year();
-            let mut month = ts.month() as i32;
-            let mut day = ts.day();
-            year += (months / 12) as i32;
-            month += (months % 12) as i32;
-
-            // if the month overflowed or underflowed, adjust the year
-            // accordingly. Because we add the modulo for the months
-            // the year will only adjust by one
-            if month > 12 {
-                year += 1;
-                month -= 12;
-            } else if month <= 0 {
-                year -= 1;
-                month += 12;
-            }
-
-            if d.saturating {
-                // Normalize the day if we are past the end of the month.
-                let mut last_day_of_month = last_day_of_month(month);
-                if month == (chrono::Month::February.number_from_month() as i32)
-                    && is_leap_year(year)
-                {
-                    last_day_of_month += 1;
-                }
-
-                if day > last_day_of_month {
-                    day = last_day_of_month
-                }
-            }
-
-            // Retrieve the original time and construct a data
-            // with the new year, month and day
-            let hour = ts.hour();
-            let minute = ts.minute();
-            let sec = ts.second();
-            let nsec = ts.nanosecond();
-            let dt = new_datetime(year, month as u32, day, hour, minute, sec, nsec).ok_or(
-                polars_err!(
-                    ComputeError: format!(
-                        "cannot advance '{}' by {} month(s). \
-                         If you were trying to get the last day of each month, you may want to try `.dt.month_end`", ts, if d.negative {-d.months} else {d.months})
-                ),
-            )?;
+            let dt = Self::add_month(ts, d.months, d.negative, d.saturating)?;
             new_t = match tz {
                 #[cfg(feature = "timezones")]
                 Some(tz) => datetime_to_timestamp(localize_datetime(dt, tz)?),
                 _ => datetime_to_timestamp(dt),
             };
         }
 
@@ -621,41 +634,41 @@
                 _ => new_t += if d.negative { -t_days } else { t_days },
             };
         }
 
         Ok(new_t)
     }
 
-    pub fn add_ns(&self, t: i64, tz: Option<&impl PolarsTimeZone>) -> PolarsResult<i64> {
+    pub fn add_ns(&self, t: i64, tz: Option<&Tz>) -> PolarsResult<i64> {
         let d = self;
         let new_t = self.add_impl_month_week_or_day(
             t,
             tz,
             |nsecs| nsecs,
             timestamp_ns_to_datetime,
             datetime_to_timestamp_ns,
         );
         let nsecs = if d.negative { -d.nsecs } else { d.nsecs };
         Ok(new_t? + nsecs)
     }
 
-    pub fn add_us(&self, t: i64, tz: Option<&impl PolarsTimeZone>) -> PolarsResult<i64> {
+    pub fn add_us(&self, t: i64, tz: Option<&Tz>) -> PolarsResult<i64> {
         let d = self;
         let new_t = self.add_impl_month_week_or_day(
             t,
             tz,
             |nsecs| nsecs / 1000,
             timestamp_us_to_datetime,
             datetime_to_timestamp_us,
         );
         let nsecs = if d.negative { -d.nsecs } else { d.nsecs };
         Ok(new_t? + nsecs / 1_000)
     }
 
-    pub fn add_ms(&self, t: i64, tz: Option<&impl PolarsTimeZone>) -> PolarsResult<i64> {
+    pub fn add_ms(&self, t: i64, tz: Option<&Tz>) -> PolarsResult<i64> {
         let d = self;
         let new_t = self.add_impl_month_week_or_day(
             t,
             tz,
             |nsecs| nsecs / 1_000_000,
             timestamp_ms_to_datetime,
             datetime_to_timestamp_ms,
@@ -714,30 +727,29 @@
         assert!(out.negative);
         let out = Duration::parse("5w");
         assert_eq!(out.weeks(), 5);
     }
 
     #[test]
     fn test_add_ns() {
-        use polars_arrow::time_zone::NO_TIMEZONE;
         let t = 1;
         let seven_days = Duration::parse("7d");
         let one_week = Duration::parse("1w");
 
         // add_ns can only error if a time zone is passed, so it's
         // safe to unwrap here
         assert_eq!(
-            seven_days.add_ns(t, NO_TIMEZONE).unwrap(),
-            one_week.add_ns(t, NO_TIMEZONE).unwrap()
+            seven_days.add_ns(t, None).unwrap(),
+            one_week.add_ns(t, None).unwrap()
         );
 
         let seven_days_negative = Duration::parse("-7d");
         let one_week_negative = Duration::parse("-1w");
 
         // add_ns can only error if a time zone is passed, so it's
         // safe to unwrap here
         assert_eq!(
-            seven_days_negative.add_ns(t, NO_TIMEZONE).unwrap(),
-            one_week_negative.add_ns(t, NO_TIMEZONE).unwrap()
+            seven_days_negative.add_ns(t, None).unwrap(),
+            one_week_negative.add_ns(t, None).unwrap()
         );
     }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/windows/groupby.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/windows/groupby.rs`

 * *Files 13% similar despite different names*

```diff
@@ -1,21 +1,14 @@
-use std::cmp::Ordering;
-
-#[cfg(feature = "timezones")]
-use arrow::temporal_conversions::parse_offset;
-#[cfg(feature = "timezones")]
-use chrono_tz::Tz;
-use polars_arrow::time_zone::PolarsTimeZone;
+use polars_arrow::time_zone::Tz;
 use polars_arrow::trusted_len::TrustedLen;
-use polars_arrow::utils::CustomIterTools;
 use polars_core::export::rayon::prelude::*;
 use polars_core::prelude::*;
 use polars_core::utils::_split_offsets;
+use polars_core::utils::flatten::flatten_par;
 use polars_core::POOL;
-use polars_utils::flatten;
 #[cfg(feature = "serde")]
 use serde::{Deserialize, Serialize};
 
 use crate::prelude::*;
 
 #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash)]
 #[cfg_attr(feature = "serde", derive(Serialize, Deserialize))]
@@ -29,25 +22,46 @@
 #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash)]
 #[cfg_attr(feature = "serde", derive(Serialize, Deserialize))]
 pub enum StartBy {
     WindowBound,
     DataPoint,
     /// only useful if periods are weekly
     Monday,
+    Tuesday,
+    Wednesday,
+    Thursday,
+    Friday,
+    Saturday,
+    Sunday,
 }
 
 impl Default for StartBy {
     fn default() -> Self {
         Self::WindowBound
     }
 }
 
+impl StartBy {
+    pub fn weekday(&self) -> Option<u32> {
+        match self {
+            StartBy::Monday => Some(0),
+            StartBy::Tuesday => Some(1),
+            StartBy::Wednesday => Some(2),
+            StartBy::Thursday => Some(3),
+            StartBy::Friday => Some(4),
+            StartBy::Saturday => Some(5),
+            StartBy::Sunday => Some(6),
+            _ => None,
+        }
+    }
+}
+
 #[allow(clippy::too_many_arguments)]
-fn update_groups_and_bounds<T: PolarsTimeZone>(
-    bounds_iter: BoundsIter<'_, T>,
+fn update_groups_and_bounds(
+    bounds_iter: BoundsIter<'_>,
     mut start_offset: usize,
     time: &[i64],
     closed_window: ClosedWindow,
     include_lower_bound: bool,
     include_upper_bound: bool,
     lower_bound: &mut Vec<i64>,
     upper_bound: &mut Vec<i64>,
@@ -165,53 +179,38 @@
     let mut upper_bound = Vec::with_capacity(size_upper);
 
     let mut groups = Vec::with_capacity(size);
     let start_offset = 0;
 
     match tz {
         #[cfg(feature = "timezones")]
-        Some(tz) => match tz.parse::<Tz>() {
-            Ok(tz) => {
-                update_groups_and_bounds(
-                    window
-                        .get_overlapping_bounds_iter(boundary, tu, Some(&tz), start_by)
-                        .unwrap(),
-                    start_offset,
-                    time,
-                    closed_window,
-                    include_lower_bound,
-                    include_upper_bound,
-                    &mut lower_bound,
-                    &mut upper_bound,
-                    &mut groups,
-                );
-            }
-            Err(_) => match parse_offset(tz) {
-                Ok(tz) => {
-                    update_groups_and_bounds(
-                        window
-                            .get_overlapping_bounds_iter(boundary, tu, Some(&tz), start_by)
-                            .unwrap(),
-                        start_offset,
-                        time,
-                        closed_window,
-                        include_lower_bound,
-                        include_upper_bound,
-                        &mut lower_bound,
-                        &mut upper_bound,
-                        &mut groups,
-                    );
-                }
-                _ => unreachable!(),
-            },
-        },
+        Some(tz) => {
+            update_groups_and_bounds(
+                window
+                    .get_overlapping_bounds_iter(
+                        boundary,
+                        tu,
+                        tz.parse::<Tz>().ok().as_ref(),
+                        start_by,
+                    )
+                    .unwrap(),
+                start_offset,
+                time,
+                closed_window,
+                include_lower_bound,
+                include_upper_bound,
+                &mut lower_bound,
+                &mut upper_bound,
+                &mut groups,
+            );
+        }
         _ => {
             update_groups_and_bounds(
                 window
-                    .get_overlapping_bounds_iter(boundary, tu, NO_TIMEZONE, start_by)
+                    .get_overlapping_bounds_iter(boundary, tu, None, start_by)
                     .unwrap(),
                 start_offset,
                 time,
                 closed_window,
                 include_lower_bound,
                 include_upper_bound,
                 &mut lower_bound,
@@ -221,41 +220,36 @@
         }
     };
 
     (groups, lower_bound, upper_bound)
 }
 
 // this assumes that the starting point is alwa
-pub(crate) fn groupby_values_iter_full_lookbehind<'a>(
+pub(crate) fn groupby_values_iter_full_lookbehind(
     period: Duration,
     offset: Duration,
-    time: &'a [i64],
+    time: &[i64],
     closed_window: ClosedWindow,
     tu: TimeUnit,
-    tz: Option<impl PolarsTimeZone + 'a>,
+    tz: Option<Tz>,
     start_offset: usize,
-) -> impl Iterator<Item = PolarsResult<(IdxSize, IdxSize)>> + TrustedLen + 'a {
+) -> impl Iterator<Item = PolarsResult<(IdxSize, IdxSize)>> + TrustedLen + '_ {
     debug_assert!(offset.duration_ns() >= period.duration_ns());
     debug_assert!(offset.negative);
     let add = match tu {
         TimeUnit::Nanoseconds => Duration::add_ns,
         TimeUnit::Microseconds => Duration::add_us,
         TimeUnit::Milliseconds => Duration::add_ms,
     };
 
     let mut last_lookbehind_i = 0;
-    let mut last = i64::MIN;
     time[start_offset..]
         .iter()
         .enumerate()
         .map(move |(mut i, lower)| {
-            if *lower < last {
-                panic!("index column of 'groupby_rolling' must be sorted in ascending order!")
-            }
-            last = *lower;
             i += start_offset;
             let lower = add(&offset, *lower, tz.as_ref())?;
             let upper = add(&period, lower, tz.as_ref())?;
 
             let b = Bounds::new(lower, upper);
 
             // we have a complete lookbehind so we know that `i` is the upper bound.
@@ -283,22 +277,22 @@
             }
 
             Ok((lookbehind_i as IdxSize, len as IdxSize))
         })
 }
 
 // this one is correct for all lookbehind/lookaheads, but is slower
-pub(crate) fn groupby_values_iter_window_behind_t<'a>(
+pub(crate) fn groupby_values_iter_window_behind_t(
     period: Duration,
     offset: Duration,
-    time: &'a [i64],
+    time: &[i64],
     closed_window: ClosedWindow,
     tu: TimeUnit,
-    tz: Option<impl PolarsTimeZone + 'a>,
-) -> impl Iterator<Item = PolarsResult<(IdxSize, IdxSize)>> + TrustedLen + 'a {
+    tz: Option<Tz>,
+) -> impl Iterator<Item = PolarsResult<(IdxSize, IdxSize)>> + TrustedLen + '_ {
     let add = match tu {
         TimeUnit::Nanoseconds => Duration::add_ns,
         TimeUnit::Microseconds => Duration::add_us,
         TimeUnit::Milliseconds => Duration::add_ms,
     };
 
     let mut lagging_offset = 0;
@@ -333,22 +327,22 @@
 
             Ok((lagging_offset as IdxSize, len as IdxSize))
         }
     })
 }
 
 // this one is correct for all lookbehind/lookaheads, but is slower
-pub(crate) fn groupby_values_iter_partial_lookbehind<'a>(
+pub(crate) fn groupby_values_iter_partial_lookbehind(
     period: Duration,
     offset: Duration,
-    time: &'a [i64],
+    time: &[i64],
     closed_window: ClosedWindow,
     tu: TimeUnit,
-    tz: Option<impl PolarsTimeZone + 'a>,
-) -> impl Iterator<Item = PolarsResult<(IdxSize, IdxSize)>> + TrustedLen + 'a {
+    tz: Option<Tz>,
+) -> impl Iterator<Item = PolarsResult<(IdxSize, IdxSize)>> + TrustedLen + '_ {
     let add = match tu {
         TimeUnit::Nanoseconds => Duration::add_ns,
         TimeUnit::Microseconds => Duration::add_us,
         TimeUnit::Milliseconds => Duration::add_ms,
     };
 
     let mut lagging_offset = 0;
@@ -371,24 +365,24 @@
         let len = slice.partition_point(|v| b.is_member(*v, closed_window));
 
         Ok((lagging_offset as IdxSize, len as IdxSize))
     })
 }
 
 #[allow(clippy::too_many_arguments)]
-pub(crate) fn groupby_values_iter_full_lookahead<'a>(
+pub(crate) fn groupby_values_iter_partial_lookahead(
     period: Duration,
     offset: Duration,
-    time: &'a [i64],
+    time: &[i64],
     closed_window: ClosedWindow,
     tu: TimeUnit,
-    tz: Option<impl PolarsTimeZone + 'a>,
+    tz: Option<Tz>,
     start_offset: usize,
     upper_bound: Option<usize>,
-) -> impl Iterator<Item = PolarsResult<(IdxSize, IdxSize)>> + TrustedLen + 'a {
+) -> impl Iterator<Item = PolarsResult<(IdxSize, IdxSize)>> + TrustedLen + '_ {
     let upper_bound = upper_bound.unwrap_or(time.len());
     debug_assert!(!offset.negative);
 
     let add = match tu {
         TimeUnit::Nanoseconds => Duration::add_ns,
         TimeUnit::Microseconds => Duration::add_us,
         TimeUnit::Milliseconds => Duration::add_ms,
@@ -407,109 +401,161 @@
             debug_assert!(i < time.len());
             let slice = unsafe { time.get_unchecked(i..) };
             let len = slice.partition_point(|v| b.is_member(*v, closed_window));
 
             Ok((i as IdxSize, len as IdxSize))
         })
 }
+#[allow(clippy::too_many_arguments)]
+pub(crate) fn groupby_values_iter_full_lookahead(
+    period: Duration,
+    offset: Duration,
+    time: &[i64],
+    closed_window: ClosedWindow,
+    tu: TimeUnit,
+    tz: Option<Tz>,
+    start_offset: usize,
+    upper_bound: Option<usize>,
+) -> impl Iterator<Item = PolarsResult<(IdxSize, IdxSize)>> + TrustedLen + '_ {
+    let upper_bound = upper_bound.unwrap_or(time.len());
+    debug_assert!(!offset.negative);
 
-pub(crate) fn partially_check_sorted(time: &[i64]) {
-    // check sortedness of a small subslice.
-    if time.len() > 1 {
-        assert!(time[..std::cmp::min(time.len(), 10)].windows(2).filter_map(|w| match w[0].cmp(&w[1]) {
-            Ordering::Equal => None,
-            t => Some(t)
-        }).all_equal(), "Subslice check showed that the values in `groupby_rolling/groupby_dynamic` were not sorted. Pleasure ensure the index column is sorted.");
-    }
+    let add = match tu {
+        TimeUnit::Nanoseconds => Duration::add_ns,
+        TimeUnit::Microseconds => Duration::add_us,
+        TimeUnit::Milliseconds => Duration::add_ms,
+    };
+
+    time[start_offset..upper_bound]
+        .iter()
+        .enumerate()
+        .map(move |(mut i, lower)| {
+            i += start_offset;
+            let lower = add(&offset, *lower, tz.as_ref())?;
+            let upper = add(&period, lower, tz.as_ref())?;
+
+            let b = Bounds::new(lower, upper);
+
+            // find starting point of window
+            for &t in &time[i..] {
+                if b.is_member(t, closed_window) {
+                    break;
+                }
+                i += 1;
+            }
+            if i >= time.len() {
+                return Ok((i as IdxSize, 0));
+            }
+
+            let slice = unsafe { time.get_unchecked(i..) };
+            let len = slice.partition_point(|v| b.is_member(*v, closed_window));
+
+            Ok((i as IdxSize, len as IdxSize))
+        })
 }
 
 #[cfg(feature = "rolling_window")]
 pub(crate) fn groupby_values_iter<'a>(
     period: Duration,
     offset: Duration,
     time: &'a [i64],
     closed_window: ClosedWindow,
     tu: TimeUnit,
-    tz: Option<impl PolarsTimeZone + 'a>,
+    tz: Option<Tz>,
 ) -> Box<dyn TrustedLen<Item = PolarsResult<(IdxSize, IdxSize)>> + 'a> {
-    partially_check_sorted(time);
     // we have a (partial) lookbehind window
     if offset.negative {
         // only lookbehind
         if offset.nanoseconds() == period.nanoseconds() {
             let iter =
                 groupby_values_iter_full_lookbehind(period, offset, time, closed_window, tu, tz, 0);
             Box::new(iter)
         }
         // partial lookbehind
         else {
             let iter =
                 groupby_values_iter_partial_lookbehind(period, offset, time, closed_window, tu, tz);
             Box::new(iter)
         }
-    } else {
+    } else if offset != Duration::parse("0ns")
+        || closed_window == ClosedWindow::Right
+        || closed_window == ClosedWindow::None
+    {
+        // only lookahead
         let iter = groupby_values_iter_full_lookahead(
             period,
             offset,
             time,
             closed_window,
             tu,
             tz,
             0,
             None,
         );
         Box::new(iter)
+    } else {
+        // partial lookahead
+        let iter = groupby_values_iter_partial_lookahead(
+            period,
+            offset,
+            time,
+            closed_window,
+            tu,
+            tz,
+            0,
+            None,
+        );
+        Box::new(iter)
     }
 }
 
 /// Different from `groupby_windows`, where define window buckets and search which values fit that
 /// pre-defined bucket, this function defines every window based on the:
 ///     - timestamp (lower bound)
 ///     - timestamp + period (upper bound)
 /// where timestamps are the individual values in the array `time`
-pub fn groupby_values<'a>(
+pub fn groupby_values(
     period: Duration,
     offset: Duration,
-    time: &'a [i64],
+    time: &[i64],
     closed_window: ClosedWindow,
     tu: TimeUnit,
-    tz: Option<impl PolarsTimeZone + 'a>,
+    tz: Option<Tz>,
 ) -> PolarsResult<GroupsSlice> {
-    partially_check_sorted(time);
     let thread_offsets = _split_offsets(time.len(), POOL.current_num_threads());
 
     // we have a (partial) lookbehind window
     if offset.negative {
         if offset.duration_ns() >= period.duration_ns() {
             // lookbehind
             // window is within 2 periods length of t
             // ------t---
             // [------]
             if offset.duration_ns() < period.duration_ns() * 2 {
-                let vals = POOL.install(|| {
-                    thread_offsets
+                POOL.install(|| {
+                    let vals = thread_offsets
                         .par_iter()
                         .copied()
                         .map(|(base_offset, len)| {
                             let upper_bound = base_offset + len;
                             let iter = groupby_values_iter_full_lookbehind(
                                 period,
                                 offset,
                                 &time[..upper_bound],
                                 closed_window,
                                 tu,
-                                tz.clone(),
+                                tz,
                                 base_offset,
                             );
                             iter.map(|result| result.map(|(offset, len)| [offset, len]))
                                 .collect::<PolarsResult<Vec<_>>>()
                         })
-                        .collect::<PolarsResult<Vec<_>>>()
-                })?;
-                Ok(flatten(&vals, Some(time.len())))
+                        .collect::<PolarsResult<Vec<_>>>()?;
+                    Ok(flatten_par(&vals))
+                })
             }
             // window is completely behind t and t itself is not a member
             // ---------------t---
             //  [---]
             else {
                 let iter = groupby_values_iter_window_behind_t(
                     period,
@@ -531,33 +577,67 @@
         //  [---]
         else {
             let iter =
                 groupby_values_iter_partial_lookbehind(period, offset, time, closed_window, tu, tz);
             iter.map(|result| result.map(|(offset, len)| [offset, len]))
                 .collect::<PolarsResult<_>>()
         }
-    } else {
-        let vals = POOL.install(|| {
-            thread_offsets
+    } else if offset != Duration::parse("0ns")
+        || closed_window == ClosedWindow::Right
+        || closed_window == ClosedWindow::None
+    {
+        // window is completely ahead of t and t itself is not a member
+        // --t-----------
+        //        [---]
+        POOL.install(|| {
+            let vals = thread_offsets
                 .par_iter()
                 .copied()
                 .map(|(base_offset, len)| {
                     let lower_bound = base_offset;
                     let upper_bound = base_offset + len;
                     let iter = groupby_values_iter_full_lookahead(
                         period,
                         offset,
                         time,
                         closed_window,
                         tu,
-                        tz.clone(),
+                        tz,
                         lower_bound,
                         Some(upper_bound),
                     );
                     iter.map(|result| result.map(|(offset, len)| [offset as IdxSize, len]))
                         .collect::<PolarsResult<Vec<_>>>()
                 })
-                .collect::<PolarsResult<Vec<_>>>()
-        })?;
-        Ok(flatten(&vals, Some(time.len())))
+                .collect::<PolarsResult<Vec<_>>>()?;
+            Ok(flatten_par(&vals))
+        })
+    } else {
+        // Duration is 0 and window is closed on the left:
+        // it must be that the window starts at t and t is a member
+        // --t-----------
+        //  [---]
+        POOL.install(|| {
+            let vals = thread_offsets
+                .par_iter()
+                .copied()
+                .map(|(base_offset, len)| {
+                    let lower_bound = base_offset;
+                    let upper_bound = base_offset + len;
+                    let iter = groupby_values_iter_partial_lookahead(
+                        period,
+                        offset,
+                        time,
+                        closed_window,
+                        tu,
+                        tz,
+                        lower_bound,
+                        Some(upper_bound),
+                    );
+                    iter.map(|result| result.map(|(offset, len)| [offset as IdxSize, len]))
+                        .collect::<PolarsResult<Vec<_>>>()
+                })
+                .collect::<PolarsResult<Vec<_>>>()?;
+            Ok(flatten_par(&vals))
+        })
     }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/windows/test.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/windows/test.rs`

 * *Files 9% similar despite different names*

```diff
@@ -11,21 +11,21 @@
         .unwrap()
         .and_hms_opt(0, 0, 0)
         .unwrap();
     let end = NaiveDate::from_ymd_opt(2022, 4, 1)
         .unwrap()
         .and_hms_opt(0, 0, 0)
         .unwrap();
-    let dates = date_range_vec(
+    let dates = temporal_range_vec(
         start.timestamp_nanos(),
         end.timestamp_nanos(),
         Duration::parse("1mo"),
         ClosedWindow::Both,
         TimeUnit::Nanoseconds,
-        NO_TIMEZONE,
+        None,
     )
     .unwrap(); // unwrapping as we pass None as the time zone
     let expected = [
         NaiveDate::from_ymd_opt(2022, 1, 1).unwrap(),
         NaiveDate::from_ymd_opt(2022, 2, 1).unwrap(),
         NaiveDate::from_ymd_opt(2022, 3, 1).unwrap(),
         NaiveDate::from_ymd_opt(2022, 4, 1).unwrap(),
@@ -42,21 +42,21 @@
         .unwrap()
         .and_hms_opt(0, 0, 0)
         .unwrap();
     let end = NaiveDate::from_ymd_opt(2022, 3, 1)
         .unwrap()
         .and_hms_opt(0, 0, 0)
         .unwrap();
-    let dates = date_range_vec(
+    let dates = temporal_range_vec(
         start.timestamp_nanos(),
         end.timestamp_nanos(),
         Duration::parse("1mo"),
         ClosedWindow::Both,
         TimeUnit::Nanoseconds,
-        NO_TIMEZONE,
+        None,
     )
     .unwrap(); // unwrapping as we pass None as the time zone
     let expected = [
         NaiveDate::from_ymd_opt(2022, 2, 1).unwrap(),
         NaiveDate::from_ymd_opt(2022, 3, 1).unwrap(),
     ]
     .iter()
@@ -143,15 +143,15 @@
         .timestamp_nanos();
     let w = Window::new(
         Duration::parse("5m"),
         Duration::parse("5m"),
         Duration::parse("-2m"),
     );
 
-    let b = w.get_earliest_bounds_ns(t, NO_TIMEZONE).unwrap();
+    let b = w.get_earliest_bounds_ns(t, None).unwrap();
     let start = NaiveDate::from_ymd_opt(2020, 1, 1)
         .unwrap()
         .and_hms_opt(23, 58, 0)
         .unwrap()
         .timestamp_nanos();
     assert_eq!(b.start, start);
 }
@@ -163,35 +163,35 @@
         .and_hms_opt(0, 0, 0)
         .unwrap();
     let stop = NaiveDate::from_ymd_opt(2021, 12, 16)
         .unwrap()
         .and_hms_opt(3, 0, 0)
         .unwrap();
 
-    let ts = date_range_vec(
+    let ts = temporal_range_vec(
         start.timestamp_nanos(),
         stop.timestamp_nanos(),
         Duration::parse("30m"),
         ClosedWindow::Both,
         TimeUnit::Nanoseconds,
-        NO_TIMEZONE,
+        None,
     )
     .unwrap(); // unwrapping as we pass None as the time zone
 
     // window:
     // every 2h
     // period 1h
     let w = Window::new(
         Duration::parse("1h"),
         Duration::parse("1h"),
         Duration::parse("0ns"),
     );
 
     // earliest bound is first datapoint: 2021-12-16 00:00:00
-    let b = w.get_earliest_bounds_ns(ts[0], NO_TIMEZONE).unwrap();
+    let b = w.get_earliest_bounds_ns(ts[0], None).unwrap();
     assert_eq!(b.start, start.timestamp_nanos());
 
     // test closed: "both" (includes both ends of the interval)
     let (groups, lower, higher) = groupby_windows(
         w,
         &ts,
         ClosedWindow::Both,
@@ -339,35 +339,35 @@
         .and_hms_opt(0, 0, 0)
         .unwrap();
     let stop = NaiveDate::from_ymd_opt(2021, 12, 16)
         .unwrap()
         .and_hms_opt(4, 0, 0)
         .unwrap();
 
-    let ts = date_range_vec(
+    let ts = temporal_range_vec(
         start.timestamp_nanos(),
         stop.timestamp_nanos(),
         Duration::parse("30m"),
         ClosedWindow::Both,
         TimeUnit::Nanoseconds,
-        NO_TIMEZONE,
+        None,
     )
     .unwrap(); // unwrapping as we pass None as the time zone
 
     print_ns(&ts);
 
     // window:
     // every 2h
     // period 1h
     // offset 30m
     let offset = Duration::parse("30m");
     let w = Window::new(Duration::parse("2h"), Duration::parse("1h"), offset);
 
     // earliest bound is first datapoint: 2021-12-16 00:00:00 + 30m offset: 2021-12-16 00:30:00
-    let b = w.get_earliest_bounds_ns(ts[0], NO_TIMEZONE).unwrap();
+    let b = w.get_earliest_bounds_ns(ts[0], None).unwrap();
 
     assert_eq!(b.start, start.timestamp_nanos() + offset.duration_ns());
 
     let (groups, lower, higher) = groupby_windows(
         w,
         &ts,
         ClosedWindow::Left,
@@ -447,35 +447,35 @@
         .and_hms_opt(0, 0, 0)
         .unwrap();
     let stop = NaiveDate::from_ymd_opt(2021, 12, 16)
         .unwrap()
         .and_hms_opt(3, 0, 0)
         .unwrap();
 
-    let ts = date_range_vec(
+    let ts = temporal_range_vec(
         start.timestamp_millis(),
         stop.timestamp_millis(),
         Duration::parse("30m"),
         ClosedWindow::Both,
         TimeUnit::Milliseconds,
-        NO_TIMEZONE,
+        None,
     )
     .unwrap(); // unwrapping as we pass None as the time zone
 
     // window:
     // every 2h
     // period 1h
     let w = Window::new(
         Duration::parse("1h"),
         Duration::parse("1h"),
         Duration::parse("0ns"),
     );
 
     // earliest bound is first datapoint: 2021-12-16 00:00:00
-    let b = w.get_earliest_bounds_ms(ts[0], NO_TIMEZONE).unwrap();
+    let b = w.get_earliest_bounds_ms(ts[0], None).unwrap();
     assert_eq!(b.start, start.timestamp_millis());
 
     // test closed: "both" (includes both ends of the interval)
     let (groups, lower, higher) = groupby_windows(
         w,
         &ts,
         ClosedWindow::Both,
@@ -623,32 +623,32 @@
         .unwrap()
         .and_hms_opt(0, 0, 0)
         .unwrap();
     let end = NaiveDate::from_ymd_opt(1970, 1, 16)
         .unwrap()
         .and_hms_opt(4, 0, 0)
         .unwrap();
-    let dates = date_range_vec(
+    let dates = temporal_range_vec(
         start.timestamp_millis(),
         end.timestamp_millis(),
         Duration::parse("30m"),
         ClosedWindow::Both,
         TimeUnit::Milliseconds,
-        NO_TIMEZONE,
+        None,
     )
     .unwrap(); // unwrapping as we pass None as the time zone
 
     // full lookbehind
     let groups = groupby_values(
         Duration::parse("2h"),
         Duration::parse("-2h"),
         &dates,
         ClosedWindow::Right,
         TimeUnit::Milliseconds,
-        NO_TIMEZONE.copied(),
+        None,
     )
     .unwrap();
     assert_eq!(dates.len(), groups.len());
     assert_eq!(groups[0], [0, 1]); // bound: 22:00 -> 24:00     time: 24:00
     assert_eq!(groups[1], [0, 2]); // bound: 22:30 -> 00:30     time: 00:30
     assert_eq!(groups[2], [0, 3]); // bound: 23:00 -> 01:00     time: 01:00
     assert_eq!(groups[3], [0, 4]); // bound: 23:30 -> 01:30     time: 01:30
@@ -661,15 +661,15 @@
     // partial lookbehind
     let groups = groupby_values(
         Duration::parse("2h"),
         Duration::parse("-1h"),
         &dates,
         ClosedWindow::Right,
         TimeUnit::Milliseconds,
-        NO_TIMEZONE.copied(),
+        None,
     )
     .unwrap();
     assert_eq!(dates.len(), groups.len());
     assert_eq!(groups[0], [0, 3]);
     assert_eq!(groups[1], [0, 4]);
     assert_eq!(groups[2], [1, 4]);
     assert_eq!(groups[3], [2, 4]);
@@ -682,83 +682,45 @@
     // no lookbehind
     let groups = groupby_values(
         Duration::parse("2h"),
         Duration::parse("0h"),
         &dates,
         ClosedWindow::Right,
         TimeUnit::Milliseconds,
-        NO_TIMEZONE.copied(),
+        None,
     )
     .unwrap();
     assert_eq!(dates.len(), groups.len());
-    assert_eq!(groups[0], [0, 5]);
-    assert_eq!(groups[1], [1, 5]);
-    assert_eq!(groups[2], [2, 5]);
-    assert_eq!(groups[3], [3, 5]);
-    assert_eq!(groups[4], [4, 5]);
-    assert_eq!(groups[5], [5, 4]);
-    assert_eq!(groups[6], [6, 3]);
-    assert_eq!(groups[7], [7, 2]);
-    assert_eq!(groups[8], [8, 0]);
+    assert_eq!(groups[0], [1, 4]); // (00:00, 02:00]
+    assert_eq!(groups[1], [2, 4]); // (00:30, 02:30]
+    assert_eq!(groups[2], [3, 4]); // (01:00, 03:00]
+    assert_eq!(groups[3], [4, 4]); // (01:30, 03:30]
+    assert_eq!(groups[4], [5, 4]); // (02:00, 04:00]
+    assert_eq!(groups[5], [6, 3]); // (02:30, 04:30]
+    assert_eq!(groups[6], [7, 2]); // (03:00, 05:00]
+    assert_eq!(groups[7], [8, 1]); // (03:30, 05:30]
+    assert_eq!(groups[8], [9, 0]); // (04:00, 06:00]
 
     let period = Duration::parse("2h");
     let tu = TimeUnit::Milliseconds;
     for closed_window in [
         ClosedWindow::Left,
         ClosedWindow::Right,
         ClosedWindow::Both,
         ClosedWindow::None,
     ] {
-        let offset = Duration::parse("0h");
-        let g0 = groupby_values_iter_full_lookahead(
-            period,
-            offset,
-            &dates,
-            closed_window,
-            tu,
-            NO_TIMEZONE.copied(),
-            0,
-            None,
-        )
-        .collect::<PolarsResult<Vec<_>>>()
-        .unwrap();
-        let g1 = groupby_values_iter_partial_lookbehind(
-            period,
-            offset,
-            &dates,
-            closed_window,
-            tu,
-            NO_TIMEZONE.copied(),
-        )
-        .collect::<PolarsResult<Vec<_>>>()
-        .unwrap();
-        assert_eq!(g0, g1);
-
         let offset = Duration::parse("-2h");
-        let g0 = groupby_values_iter_full_lookbehind(
-            period,
-            offset,
-            &dates,
-            closed_window,
-            tu,
-            NO_TIMEZONE.copied(),
-            0,
-        )
-        .collect::<PolarsResult<Vec<_>>>()
-        .unwrap();
-        let g1 = groupby_values_iter_partial_lookbehind(
-            period,
-            offset,
-            &dates,
-            closed_window,
-            tu,
-            NO_TIMEZONE.copied(),
-        )
-        .collect::<PolarsResult<Vec<_>>>()
-        .unwrap();
+        let g0 =
+            groupby_values_iter_full_lookbehind(period, offset, &dates, closed_window, tu, None, 0)
+                .collect::<PolarsResult<Vec<_>>>()
+                .unwrap();
+        let g1 =
+            groupby_values_iter_partial_lookbehind(period, offset, &dates, closed_window, tu, None)
+                .collect::<PolarsResult<Vec<_>>>()
+                .unwrap();
         assert_eq!(g0, g1);
     }
 }
 
 #[test]
 fn test_end_membership() {
     let time = [
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-time/src/windows/window.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/src/windows/window.rs`

 * *Files 16% similar despite different names*

```diff
@@ -1,13 +1,13 @@
-#[cfg(feature = "timezones")]
 use chrono::NaiveDateTime;
 #[cfg(feature = "timezones")]
+use chrono::TimeZone;
 use now::DateTimeNow;
 use polars_arrow::export::arrow::temporal_conversions::*;
-use polars_arrow::time_zone::PolarsTimeZone;
+use polars_arrow::time_zone::Tz;
 use polars_core::prelude::*;
 use polars_core::utils::arrow::temporal_conversions::{timeunit_scale, SECONDS_IN_DAY};
 
 use crate::prelude::*;
 
 /// Represents a window in time
 #[derive(Copy, Clone)]
@@ -27,70 +27,58 @@
             every,
             period,
             offset,
         }
     }
 
     /// Truncate the given ns timestamp by the window boundary.
-    pub fn truncate_ns(&self, t: i64, tz: Option<&impl PolarsTimeZone>) -> PolarsResult<i64> {
+    pub fn truncate_ns(&self, t: i64, tz: Option<&Tz>) -> PolarsResult<i64> {
         let t = self.every.truncate_ns(t, tz)?;
         self.offset.add_ns(t, tz)
     }
 
-    pub fn truncate_no_offset_ns(
-        &self,
-        t: i64,
-        tz: Option<&impl PolarsTimeZone>,
-    ) -> PolarsResult<i64> {
+    pub fn truncate_no_offset_ns(&self, t: i64, tz: Option<&Tz>) -> PolarsResult<i64> {
         self.every.truncate_ns(t, tz)
     }
 
     /// Truncate the given us timestamp by the window boundary.
-    pub fn truncate_us(&self, t: i64, tz: Option<&impl PolarsTimeZone>) -> PolarsResult<i64> {
+    pub fn truncate_us(&self, t: i64, tz: Option<&Tz>) -> PolarsResult<i64> {
         let t = self.every.truncate_us(t, tz)?;
         self.offset.add_us(t, tz)
     }
 
-    pub fn truncate_no_offset_us(
-        &self,
-        t: i64,
-        tz: Option<&impl PolarsTimeZone>,
-    ) -> PolarsResult<i64> {
+    pub fn truncate_no_offset_us(&self, t: i64, tz: Option<&Tz>) -> PolarsResult<i64> {
         self.every.truncate_us(t, tz)
     }
 
-    pub fn truncate_ms(&self, t: i64, tz: Option<&impl PolarsTimeZone>) -> PolarsResult<i64> {
+    pub fn truncate_ms(&self, t: i64, tz: Option<&Tz>) -> PolarsResult<i64> {
         let t = self.every.truncate_ms(t, tz)?;
         self.offset.add_ms(t, tz)
     }
 
     #[inline]
-    pub fn truncate_no_offset_ms(
-        &self,
-        t: i64,
-        tz: Option<&impl PolarsTimeZone>,
-    ) -> PolarsResult<i64> {
+    pub fn truncate_no_offset_ms(&self, t: i64, tz: Option<&Tz>) -> PolarsResult<i64> {
         self.every.truncate_ms(t, tz)
     }
 
     /// Round the given ns timestamp by the window boundary.
-    pub fn round_ns(&self, t: i64, tz: Option<&impl PolarsTimeZone>) -> PolarsResult<i64> {
+    pub fn round_ns(&self, t: i64, tz: Option<&Tz>) -> PolarsResult<i64> {
         let t = t + self.every.duration_ns() / 2_i64;
         self.truncate_ns(t, tz)
     }
 
     /// Round the given us timestamp by the window boundary.
-    pub fn round_us(&self, t: i64, tz: Option<&impl PolarsTimeZone>) -> PolarsResult<i64> {
+    pub fn round_us(&self, t: i64, tz: Option<&Tz>) -> PolarsResult<i64> {
         let t = t + self.every.duration_ns()
             / (2 * timeunit_scale(ArrowTimeUnit::Nanosecond, ArrowTimeUnit::Microsecond) as i64);
         self.truncate_us(t, tz)
     }
 
     /// Round the given ms timestamp by the window boundary.
-    pub fn round_ms(&self, t: i64, tz: Option<&impl PolarsTimeZone>) -> PolarsResult<i64> {
+    pub fn round_ms(&self, t: i64, tz: Option<&Tz>) -> PolarsResult<i64> {
         let t = t + self.every.duration_ns()
             / (2 * timeunit_scale(ArrowTimeUnit::Nanosecond, ArrowTimeUnit::Millisecond) as i64);
         self.truncate_ms(t, tz)
     }
 
     /// returns the bounds for the earliest window bounds
     /// that contains the given time t.  For underlapping windows that
@@ -101,54 +89,42 @@
     /// Below 1 day, it make sense to truncate to:
     /// - days
     /// - hours
     /// - 15 minutes
     /// - etc.
     ///
     /// But for 2w3d, it does not make sense to start it on a different lower bound, so we start at `t`
-    pub fn get_earliest_bounds_ns(
-        &self,
-        t: i64,
-        tz: Option<&impl PolarsTimeZone>,
-    ) -> PolarsResult<Bounds> {
+    pub fn get_earliest_bounds_ns(&self, t: i64, tz: Option<&Tz>) -> PolarsResult<Bounds> {
         let start = if !self.every.months_only()
             && self.every.duration_ns() > NANOSECONDS * SECONDS_IN_DAY
         {
             self.offset.add_ns(t, tz)?
         } else {
             // offset is translated in the truncate
             self.truncate_ns(t, tz)?
         };
 
         let stop = self.period.add_ns(start, tz)?;
 
         Ok(Bounds::new_checked(start, stop))
     }
 
-    pub fn get_earliest_bounds_us(
-        &self,
-        t: i64,
-        tz: Option<&impl PolarsTimeZone>,
-    ) -> PolarsResult<Bounds> {
+    pub fn get_earliest_bounds_us(&self, t: i64, tz: Option<&Tz>) -> PolarsResult<Bounds> {
         let start = if !self.every.months_only()
             && self.every.duration_us() > MICROSECONDS * SECONDS_IN_DAY
         {
             self.offset.add_us(t, tz)?
         } else {
             self.truncate_us(t, tz)?
         };
         let stop = self.period.add_us(start, tz)?;
         Ok(Bounds::new_checked(start, stop))
     }
 
-    pub fn get_earliest_bounds_ms(
-        &self,
-        t: i64,
-        tz: Option<&impl PolarsTimeZone>,
-    ) -> PolarsResult<Bounds> {
+    pub fn get_earliest_bounds_ms(&self, t: i64, tz: Option<&Tz>) -> PolarsResult<Bounds> {
         let start = if !self.every.months_only()
             && self.every.duration_ms() > MILLISECONDS * SECONDS_IN_DAY
         {
             self.offset.add_ms(t, tz)?
         } else {
             self.truncate_ms(t, tz)?
         };
@@ -169,40 +145,40 @@
     }
 
     pub(crate) fn estimate_overlapping_bounds_ms(&self, boundary: Bounds) -> usize {
         (boundary.duration() / self.every.duration_ms()
             + self.period.duration_ms() / self.every.duration_ms()) as usize
     }
 
-    pub fn get_overlapping_bounds_iter<'a, T: PolarsTimeZone>(
-        &self,
+    pub fn get_overlapping_bounds_iter<'a>(
+        &'a self,
         boundary: Bounds,
         tu: TimeUnit,
-        tz: Option<&'a T>,
+        tz: Option<&'a Tz>,
         start_by: StartBy,
-    ) -> PolarsResult<BoundsIter<'a, T>> {
+    ) -> PolarsResult<BoundsIter> {
         BoundsIter::new(*self, boundary, tu, tz, start_by)
     }
 }
 
-pub struct BoundsIter<'a, T: PolarsTimeZone> {
+pub struct BoundsIter<'a> {
     window: Window,
     // wrapping boundary
     boundary: Bounds,
     // boundary per window iterator
     bi: Bounds,
     tu: TimeUnit,
-    tz: Option<&'a T>,
+    tz: Option<&'a Tz>,
 }
-impl<'a, T: PolarsTimeZone> BoundsIter<'a, T> {
+impl<'a> BoundsIter<'a> {
     fn new(
         window: Window,
         boundary: Bounds,
         tu: TimeUnit,
-        tz: Option<&'a T>,
+        tz: Option<&'a Tz>,
         start_by: StartBy,
     ) -> PolarsResult<Self> {
         let bi = match start_by {
             StartBy::DataPoint => {
                 let mut boundary = boundary;
                 let offset_fn = match tu {
                     TimeUnit::Nanoseconds => Duration::add_ns,
@@ -213,22 +189,21 @@
                 boundary
             }
             StartBy::WindowBound => match tu {
                 TimeUnit::Nanoseconds => window.get_earliest_bounds_ns(boundary.start, tz)?,
                 TimeUnit::Microseconds => window.get_earliest_bounds_us(boundary.start, tz)?,
                 TimeUnit::Milliseconds => window.get_earliest_bounds_ms(boundary.start, tz)?,
             },
-            StartBy::Monday => {
-                #[cfg(feature = "timezones")]
+            _ => {
                 {
                     #[allow(clippy::type_complexity)]
                     let (from, to, offset): (
                         fn(i64) -> NaiveDateTime,
                         fn(NaiveDateTime) -> i64,
-                        fn(&Duration, i64, Option<&'a T>) -> PolarsResult<i64>,
+                        fn(&Duration, i64, Option<&Tz>) -> PolarsResult<i64>,
                     ) = match tu {
                         TimeUnit::Nanoseconds => (
                             timestamp_ns_to_datetime,
                             datetime_to_timestamp_ns,
                             Duration::add_ns,
                         ),
                         TimeUnit::Microseconds => (
@@ -242,57 +217,67 @@
                             Duration::add_ms,
                         ),
                     };
                     // find beginning of the week.
                     let mut boundary = boundary;
                     let dt = from(boundary.start);
                     (boundary.start, boundary.stop) = match tz {
+                        #[cfg(feature = "timezones")]
                         Some(tz) => {
-                            let dt = dt.and_local_timezone(tz.clone()).unwrap();
+                            let dt = tz.from_utc_datetime(&dt);
                             let dt = dt.beginning_of_week();
                             let dt = dt.naive_utc();
                             let start = to(dt);
+                            // adjust start of the week based on given day of the week
+                            let start = offset(
+                                &Duration::parse(&format!("{}d", start_by.weekday().unwrap())),
+                                start,
+                                Some(tz),
+                            )?;
                             // apply the 'offset'
                             let start = offset(&window.offset, start, Some(tz))?;
                             // and compute the end of the window defined by the 'period'
                             let stop = offset(&window.period, start, Some(tz))?;
                             (start, stop)
                         }
-                        None => {
+                        _ => {
                             let tz = chrono::Utc;
                             let dt = dt.and_local_timezone(tz).unwrap();
                             let dt = dt.beginning_of_week();
                             let dt = dt.naive_utc();
                             let start = to(dt);
+                            // adjust start of the week based on given day of the week
+                            let start = offset(
+                                &Duration::parse(&format!("{}d", start_by.weekday().unwrap())),
+                                start,
+                                None,
+                            )
+                            .unwrap();
                             // apply the 'offset'
-                            let start = offset(&window.offset, start, None::<&T>).unwrap();
+                            let start = offset(&window.offset, start, None).unwrap();
                             // and compute the end of the window defined by the 'period'
-                            let stop = offset(&window.period, start, None::<&T>).unwrap();
+                            let stop = offset(&window.period, start, None).unwrap();
                             (start, stop)
                         }
                     };
                     boundary
                 }
-                #[cfg(not(feature = "timezones"))]
-                {
-                    panic!("activate 'timezones' feature")
-                }
             }
         };
         Ok(Self {
             window,
             boundary,
             bi,
             tu,
             tz,
         })
     }
 }
 
-impl<'a, T: PolarsTimeZone> Iterator for BoundsIter<'a, T> {
+impl<'a> Iterator for BoundsIter<'a> {
     type Item = Bounds;
 
     fn next(&mut self) -> Option<Self::Item> {
         if self.bi.start < self.boundary.stop {
             let out = self.bi;
             match self.tu {
                 // TODO: find some way to propagate error instead of unwrapping?
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-algo/Cargo.toml` & `polars_lts_cpu-0.18.0/local_dependencies/polars-algo/Cargo.toml`

 * *Files 3% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 [package]
 name = "polars-algo"
-version= "0.28.0"
+version= "0.30.0"
 edition = "2021"
 license = "MIT"
 repository = "https://github.com/pola-rs/polars"
 description = "Algorithms built upon Polars primitives"
 
 # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html
 
 [dependencies]
-polars-core = { version = "0.28.0", path = "../polars-core", features = ["private", "dtype-categorical", "asof_join"], default-features = false }
-polars-lazy = { version = "0.28.0", path = "../polars-lazy", features = ["asof_join", "concat_str", "strings"] }
-polars-ops = { version = "0.28.0", path = "../polars-ops", features = ["dtype-categorical", "asof_join"], default-features = false }
+polars-core = { version = "0.30.0", path = "../polars-core", features = ["private", "dtype-categorical", "asof_join"], default-features = false }
+polars-lazy = { version = "0.30.0", path = "../polars-lazy", features = ["asof_join", "concat_str", "strings"] }
+polars-ops = { version = "0.30.0", path = "../polars-ops", features = ["dtype-categorical", "asof_join"], default-features = false }
 
 [package.metadata.docs.rs]
 all-features = true
 # defines the configuration attribute `docsrs`
 rustdoc-args = ["--cfg", "docsrs"]
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-algo/LICENSE` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/LICENSE`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-algo/src/algo.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-algo/src/algo.rs`

 * *Files 2% similar despite different names*

```diff
@@ -71,15 +71,17 @@
         )
         .collect()?;
 
     let cuts = cuts_df
         .lazy()
         .with_columns([
             col(category_str).cast(DataType::Categorical(None)),
-            col(breakpoint_str).cast(s.dtype().to_owned()),
+            col(breakpoint_str)
+                .cast(s.dtype().to_owned())
+                .set_sorted_flag(IsSorted::Ascending),
         ])
         .collect()?;
 
     let out = s.clone().into_frame().join_asof(
         &cuts,
         s.name(),
         breakpoint_str,
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-sql/Cargo.toml` & `polars_lts_cpu-0.18.0/local_dependencies/polars-sql/Cargo.toml`

 * *Files 19% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 [package]
 name = "polars-sql"
-version = "0.28.0"
+version= "0.30.0"
 edition = "2021"
 license = "MIT"
 repository = "https://github.com/pola-rs/polars"
 description = "SQL transpiler for polars. Converts SQL to polars logical plans"
 
 # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html
 [features]
@@ -12,14 +12,15 @@
 json = ["polars-lazy/json"]
 default = []
 ipc = ["polars-lazy/ipc"]
 parquet = ["polars-lazy/parquet"]
 private = []
 
 [dependencies]
-polars-arrow = { version = "0.28.0", path = "../polars-arrow", features = ["like"] }
-polars-core = { version = "0.28.0", path = "../polars-core", features = [] }
-polars-lazy = { version = "0.28.0", path = "../polars-lazy", features = ["compile", "strings", "cross_join", "trigonometry", "abs", "round_series", "log", "regex", "is_in", "meta", "cum_agg"] }
-polars-plan = { version = "0.28.0", path = "../polars-plan", features = ["compile"] }
+polars-arrow = { version = "0.30.0", path = "../polars-arrow", features = ["like"] }
+polars-core = { version = "0.30.0", path = "../polars-core", features = [] }
+polars-lazy = { version = "0.30.0", path = "../polars-lazy", features = ["compile", "strings", "cross_join", "trigonometry", "abs", "round_series", "log", "regex", "is_in", "meta", "cum_agg"] }
+polars-plan = { version = "0.30.0", path = "../polars-plan", features = ["compile"] }
 serde = "1"
 serde_json = { version = "1" }
-sqlparser = { version = "0.30" }
+# sqlparser = { git = "https://github.com/sqlparser-rs/sqlparser-rs.git", rev = "ae3b5844c839072c235965fe0d1bddc473dced87" }
+sqlparser = "0.34"
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-sql/LICENSE` & `polars_lts_cpu-0.18.0/local_dependencies/polars-row/LICENSE`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-sql/src/context.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-sql/src/context.rs`

 * *Files 16% similar despite different names*

```diff
@@ -3,69 +3,78 @@
 
 use polars_arrow::error::to_compute_err;
 use polars_core::prelude::*;
 use polars_lazy::prelude::*;
 use polars_plan::prelude::*;
 use polars_plan::utils::expressions_to_schema;
 use sqlparser::ast::{
-    Expr as SqlExpr, FunctionArg, JoinOperator, ObjectName, OrderByExpr, Query, Select, SelectItem,
-    SetExpr, Statement, TableAlias, TableFactor, TableWithJoins, Value as SQLValue,
+    Distinct, ExcludeSelectItem, Expr as SqlExpr, FunctionArg, JoinOperator, ObjectName, Offset,
+    OrderByExpr, Query, Select, SelectItem, SetExpr, SetOperator, SetQuantifier, Statement,
+    TableAlias, TableFactor, TableWithJoins, Value as SQLValue, WildcardAdditionalOptions,
 };
 use sqlparser::dialect::GenericDialect;
-use sqlparser::parser::Parser;
+use sqlparser::parser::{Parser, ParserOptions};
 
 use crate::sql_expr::{parse_sql_expr, process_join_constraint};
 use crate::table_functions::PolarsTableFunctions;
 
 /// The SQLContext is the main entry point for executing SQL queries.
 #[derive(Default, Clone)]
 pub struct SQLContext {
     pub(crate) table_map: PlHashMap<String, LazyFrame>,
-    pub(crate) tables: Vec<String>,
     cte_map: RefCell<PlHashMap<String, LazyFrame>>,
 }
 
 impl SQLContext {
     /// Create a new SQLContext
     /// ```rust
     /// # use polars_sql::SQLContext;
     /// # fn main() {
     /// let ctx = SQLContext::new();
     /// # }
     /// ```
     pub fn new() -> Self {
         Self {
             table_map: PlHashMap::new(),
-            tables: vec![],
             cte_map: RefCell::new(PlHashMap::new()),
         }
     }
 
-    /// Register a DataFrame as a table in the SQLContext.
+    /// Get the names of all registered tables, in sorted order.
+    pub fn get_tables(&self) -> Vec<String> {
+        let mut tables = Vec::from_iter(self.table_map.keys().cloned());
+        tables.sort_unstable();
+        tables
+    }
+
+    /// Register a LazyFrame as a table in the SQLContext.
     /// ```rust
     /// # use polars_sql::SQLContext;
     /// # use polars_core::prelude::*;
     /// # use polars_lazy::prelude::*;
     /// # fn main() {
     ///
     /// let mut ctx = SQLContext::new();
-    ///
     /// let df = df! {
     ///    "a" =>  [1, 2, 3],
     /// }.unwrap().lazy();
     ///
     /// ctx.register("df", df);
     /// # }
     ///```
     pub fn register(&mut self, name: &str, lf: LazyFrame) {
         self.table_map.insert(name.to_owned(), lf);
-        self.tables.push(name.to_owned());
     }
 
-    /// Execute a sql query and return the result as a LazyFrame.
+    /// Unregister a LazyFrame table from the SQLContext.
+    pub fn unregister(&mut self, name: &str) {
+        self.table_map.remove(&name.to_owned());
+    }
+
+    /// Execute a SQL query, returning a LazyFrame.
     /// ```rust
     /// # use polars_sql::SQLContext;
     /// # use polars_core::prelude::*;
     /// # use polars_lazy::prelude::*;
     /// # fn main() {
     ///
     /// let mut ctx = SQLContext::new();
@@ -76,15 +85,24 @@
     ///
     /// ctx.register("df", df.clone().lazy());
     /// let sql_df = ctx.execute("SELECT * FROM df").unwrap().collect().unwrap();
     /// assert!(sql_df.frame_equal(&df));
     /// # }
     ///```
     pub fn execute(&mut self, query: &str) -> PolarsResult<LazyFrame> {
-        let ast = Parser::parse_sql(&GenericDialect::default(), query).map_err(to_compute_err)?;
+        let mut parser = Parser::new(&GenericDialect);
+        parser = parser.with_options(ParserOptions {
+            trailing_commas: true,
+        });
+
+        let ast = parser
+            .try_with_sql(query)
+            .map_err(to_compute_err)?
+            .parse_statements()
+            .map_err(to_compute_err)?;
         polars_ensure!(ast.len() == 1, ComputeError: "One and only one statement at a time please");
         let res = self.execute_statement(ast.get(0).unwrap());
         // every execution should clear the cte map
         self.cte_map.borrow_mut().clear();
         res
     }
 }
@@ -104,47 +122,88 @@
 
     pub(crate) fn execute_statement(&mut self, stmt: &Statement) -> PolarsResult<LazyFrame> {
         let ast = stmt;
         Ok(match ast {
             Statement::Query(query) => self.execute_query(query)?,
             stmt @ Statement::ShowTables { .. } => self.execute_show_tables(stmt)?,
             stmt @ Statement::CreateTable { .. } => self.execute_create_table(stmt)?,
+            stmt @ Statement::Explain { .. } => self.execute_explain(stmt)?,
             _ => polars_bail!(
                 ComputeError: "SQL statement type {:?} is not supported", ast,
             ),
         })
     }
 
     pub(crate) fn execute_query(&mut self, query: &Query) -> PolarsResult<LazyFrame> {
         self.register_ctes(query)?;
-        let mut lf = match &query.body.as_ref() {
-            SetExpr::Select(select_stmt) => self.execute_select(select_stmt)?,
-            SetExpr::Query(query) => self.execute_query(query)?,
-            _ => polars_bail!(ComputeError: "INSERT, UPDATE is not supported"),
-        };
 
-        if !query.order_by.is_empty() {
-            lf = self.process_order_by(lf, &query.order_by)?;
+        let lf = self.process_set_expr(&query.body, query)?;
+
+        self.process_limit_offset(lf, &query.limit, &query.offset)
+    }
+
+    fn process_set_expr(&mut self, expr: &SetExpr, query: &Query) -> PolarsResult<LazyFrame> {
+        match expr {
+            SetExpr::Select(select_stmt) => self.execute_select(select_stmt, query),
+            SetExpr::Query(query) => self.execute_query(query),
+            SetExpr::SetOperation {
+                op: SetOperator::Union,
+                set_quantifier,
+                left,
+                right,
+            } => self.process_union(left, right, set_quantifier, query),
+            SetExpr::SetOperation { op, .. } => {
+                polars_bail!(InvalidOperation: "{} operation not yet supported", op)
+            }
+            op => polars_bail!(InvalidOperation: "{} operation not yet supported", op),
         }
-        match &query.limit {
-            Some(SqlExpr::Value(SQLValue::Number(nrow, _))) => {
-                let nrow = nrow
-                    .parse()
-                    .map_err(|e| polars_err!(ComputeError: "conversion error: {}", e))?;
-                Ok(lf.limit(nrow))
+    }
+
+    fn process_union(
+        &mut self,
+        left: &SetExpr,
+        right: &SetExpr,
+        quantifier: &SetQuantifier,
+        query: &Query,
+    ) -> PolarsResult<LazyFrame> {
+        match quantifier {
+            // UNION ALL
+            SetQuantifier::All => {
+                let left = self.process_set_expr(left, query)?;
+                let right = self.process_set_expr(right, query)?;
+                polars_lazy::dsl::concat(vec![left, right], false, true)
             }
-            None => Ok(lf),
-            _ => polars_bail!(
-                ComputeError: "non-number arguments to LIMIT clause are not supported",
-            ),
+            // UNION DISTINCT | UNION
+            _ => {
+                let left = self.process_set_expr(left, query)?;
+                let right = self.process_set_expr(right, query)?;
+                Ok(polars_lazy::dsl::concat(vec![left, right], true, true)?
+                    .unique(None, UniqueKeepStrategy::Any))
+            }
+        }
+    }
+    // EXPLAIN SELECT * FROM DF
+    fn execute_explain(&mut self, stmt: &Statement) -> PolarsResult<LazyFrame> {
+        match stmt {
+            Statement::Explain { statement, .. } => {
+                let lf = self.execute_statement(statement)?;
+                let plan = lf.describe_optimized_plan()?;
+                let mut plan = plan.split('\n').collect::<Series>();
+                plan.rename("Logical Plan");
+
+                let df = DataFrame::new(vec![plan])?;
+                Ok(df.lazy())
+            }
+            _ => unreachable!(),
         }
     }
 
+    /// SHOW TABLES
     fn execute_show_tables(&mut self, _: &Statement) -> PolarsResult<LazyFrame> {
-        let tables = Series::new("name", self.tables.clone());
+        let tables = Series::new("name", self.get_tables());
         let df = DataFrame::new(vec![tables])?;
         Ok(df.lazy())
     }
 
     fn register_ctes(&mut self, query: &Query) -> PolarsResult<()> {
         if let Some(with) = &query.with {
             if with.recursive {
@@ -180,59 +239,63 @@
                         let (left_on, right_on) =
                             process_join_constraint(constraint, &tbl_name, &join_tbl_name)?;
                         lf = lf.outer_join(join_tbl, left_on, right_on)
                     }
                     JoinOperator::CrossJoin => lf = lf.cross_join(join_tbl),
                     join_type => {
                         polars_bail!(
-                            ComputeError:
+                            InvalidOperation:
                             "join type '{:?}' not yet supported by polars-sql", join_type
                         );
                     }
                 }
             }
         };
 
         Ok(lf)
     }
-
     /// execute the 'SELECT' part of the query
-    fn execute_select(&mut self, select_stmt: &Select) -> PolarsResult<LazyFrame> {
+    fn execute_select(&mut self, select_stmt: &Select, query: &Query) -> PolarsResult<LazyFrame> {
         // Determine involved dataframe
         // Implicit join require some more work in query parsers, Explicit join are preferred for now.
         let sql_tbl: &TableWithJoins = select_stmt
             .from
             .get(0)
             .ok_or_else(|| polars_err!(ComputeError: "no table name provided in query"))?;
 
-        let lf = self.execute_from_statement(sql_tbl)?;
+        let mut lf = self.execute_from_statement(sql_tbl)?;
         let mut contains_wildcard = false;
 
         // Filter Expression
-        let lf = match select_stmt.selection.as_ref() {
+        lf = match select_stmt.selection.as_ref() {
             Some(expr) => {
                 let filter_expression = parse_sql_expr(expr, self)?;
                 lf.filter(filter_expression)
             }
             None => lf,
         };
+
         // Column Projections
         let projections: Vec<_> = select_stmt
             .projection
             .iter()
             .map(|select_item| {
                 Ok(match select_item {
                     SelectItem::UnnamedExpr(expr) => parse_sql_expr(expr, self)?,
                     SelectItem::ExprWithAlias { expr, alias } => {
                         let expr = parse_sql_expr(expr, self)?;
                         expr.alias(&alias.value)
                     }
-                    SelectItem::QualifiedWildcard { .. } | SelectItem::Wildcard { .. } => {
+                    SelectItem::QualifiedWildcard(oname, wildcard_options) => {
+                        self.process_qualified_wildcard(oname, wildcard_options)?
+                    }
+                    SelectItem::Wildcard(wildcard_options) => {
                         contains_wildcard = true;
-                        col("*")
+                        let e = col("*");
+                        self.process_wildcard_additional_options(e, wildcard_options)?
                     }
                 })
             })
             .collect::<PolarsResult<_>>()?;
 
         // Check for group by
         // After projection since there might be number.
@@ -256,17 +319,58 @@
                     "groupby error: a positive number or an expression expected",
                 )),
                 _ => parse_sql_expr(e, self),
             })
             .collect::<PolarsResult<_>>()?;
 
         if groupby_keys.is_empty() {
-            Ok(lf.select(projections))
+            lf = lf.select(projections)
         } else {
-            self.process_groupby(lf, contains_wildcard, &groupby_keys, &projections)
+            lf = self.process_groupby(lf, contains_wildcard, &groupby_keys, &projections)?;
+
+            // Apply optional 'having' clause, post-aggregation
+            lf = match select_stmt.having.as_ref() {
+                Some(expr) => lf.filter(parse_sql_expr(expr, self)?),
+                None => lf,
+            };
+        };
+
+        // Apply optional 'distinct' clause
+        lf = match &select_stmt.distinct {
+            Some(Distinct::Distinct) => lf.unique(None, UniqueKeepStrategy::Any),
+            Some(Distinct::On(exprs)) => {
+                // TODO: support exprs in `unique` see https://github.com/pola-rs/polars/issues/5760
+                let cols = exprs
+                    .iter()
+                    .map(|e| {
+                        let expr = parse_sql_expr(e, self)?;
+                        if let Expr::Column(name) = expr {
+                            Ok(name.to_string())
+                        } else {
+                            Err(polars_err!(
+                                ComputeError:
+                                "DISTINCT ON only supports column names"
+                            ))
+                        }
+                    })
+                    .collect::<PolarsResult<Vec<_>>>()?;
+
+                // DISTINCT ON applies the ORDER BY before the operation.
+                if !query.order_by.is_empty() {
+                    lf = self.process_order_by(lf, &query.order_by)?;
+                }
+                return Ok(lf.unique_stable(Some(cols), UniqueKeepStrategy::First));
+            }
+            None => lf,
+        };
+
+        if query.order_by.is_empty() {
+            Ok(lf)
+        } else {
+            self.process_order_by(lf, &query.order_by)
         }
     }
 
     fn execute_create_table(&mut self, stmt: &Statement) -> PolarsResult<LazyFrame> {
         if let Statement::CreateTable {
             if_not_exists,
             name,
@@ -408,33 +512,113 @@
                     col(name)
                 }
             })
             .collect::<Vec<_>>();
 
         Ok(aggregated.select(&final_projection))
     }
+
+    fn process_limit_offset(
+        &mut self,
+        lf: LazyFrame,
+        limit: &Option<SqlExpr>,
+        offset: &Option<Offset>,
+    ) -> PolarsResult<LazyFrame> {
+        match (offset, limit) {
+            (
+                Some(Offset {
+                    value: SqlExpr::Value(SQLValue::Number(offset, _)),
+                    ..
+                }),
+                Some(SqlExpr::Value(SQLValue::Number(limit, _))),
+            ) => Ok(lf.slice(
+                offset
+                    .parse()
+                    .map_err(|e| polars_err!(ComputeError: "OFFSET conversion error: {}", e))?,
+                limit
+                    .parse()
+                    .map_err(|e| polars_err!(ComputeError: "LIMIT conversion error: {}", e))?,
+            )),
+            (
+                Some(Offset {
+                    value: SqlExpr::Value(SQLValue::Number(offset, _)),
+                    ..
+                }),
+                None,
+            ) => Ok(lf.slice(
+                offset
+                    .parse()
+                    .map_err(|e| polars_err!(ComputeError: "OFFSET conversion error: {}", e))?,
+                IdxSize::MAX,
+            )),
+            (None, Some(SqlExpr::Value(SQLValue::Number(limit, _)))) => Ok(lf.limit(
+                limit
+                    .parse()
+                    .map_err(|e| polars_err!(ComputeError: "LIMIT conversion error: {}", e))?,
+            )),
+            (None, None) => Ok(lf),
+            _ => polars_bail!(
+                ComputeError: "non-numeric arguments for LIMIT/OFFSET are not supported",
+            ),
+        }
+    }
+
+    fn process_qualified_wildcard(
+        &mut self,
+        ObjectName(idents): &ObjectName,
+        options: &WildcardAdditionalOptions,
+    ) -> PolarsResult<Expr> {
+        let idents = idents.as_slice();
+        let e = match idents {
+            [tbl_name] => {
+                let lf = self.table_map.get(&tbl_name.value).ok_or_else(|| {
+                    polars_err!(
+                        ComputeError: "no table named '{}' found",
+                        tbl_name
+                    )
+                })?;
+                let schema = lf.schema()?;
+                cols(schema.iter_names())
+            }
+            e => polars_bail!(
+                ComputeError: "Invalid wildcard expression: {:?}",
+                e
+            ),
+        };
+        self.process_wildcard_additional_options(e, options)
+    }
+
+    fn process_wildcard_additional_options(
+        &mut self,
+        expr: Expr,
+        options: &WildcardAdditionalOptions,
+    ) -> PolarsResult<Expr> {
+        if options.opt_except.is_some() {
+            polars_bail!(InvalidOperation: "EXCEPT not supported. Use EXCLUDE instead")
+        }
+        Ok(match &options.opt_exclude {
+            Some(ExcludeSelectItem::Single(ident)) => expr.exclude(vec![&ident.value]),
+            Some(ExcludeSelectItem::Multiple(idents)) => {
+                expr.exclude(idents.iter().map(|i| &i.value))
+            }
+            _ => expr,
+        })
+    }
 }
 
 #[cfg(feature = "private")]
 impl SQLContext {
-    /// get all registered tables. For internal use only.
-    pub fn get_tables(&self) -> Vec<String> {
-        self.tables.clone()
-    }
-    /// get internal table map. For internal use only.
+    /// Get internal table map. For internal use only.
     #[cfg(feature = "private")]
     pub fn get_table_map(&self) -> PlHashMap<String, LazyFrame> {
         self.table_map.clone()
     }
-    /// Create a new SQLContext from a table map and a list of tables. For internal use only
+
+    /// Create a new SQLContext from a table map. For internal use only
     #[cfg(feature = "private")]
-    pub fn new_from_tables_and_map(
-        tables: Vec<String>,
-        table_map: PlHashMap<String, LazyFrame>,
-    ) -> Self {
+    pub fn new_from_table_map(table_map: PlHashMap<String, LazyFrame>) -> Self {
         Self {
             table_map,
-            tables,
             cte_map: RefCell::new(PlHashMap::new()),
         }
     }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-sql/src/functions.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-sql/src/functions.rs`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 use polars_core::prelude::{polars_bail, polars_err, PolarsError, PolarsResult};
 use polars_lazy::dsl::Expr;
 use polars_plan::dsl::count;
 use sqlparser::ast::{
     Expr as SqlExpr, Function as SQLFunction, FunctionArg, FunctionArgExpr, Value as SqlValue,
-    WindowSpec,
+    WindowSpec, WindowType,
 };
 
 use crate::sql_expr::parse_sql_expr;
 use crate::SQLContext;
 
 pub(crate) struct SqlFunctionVisitor<'a> {
     pub(crate) func: &'a SQLFunction,
@@ -234,51 +234,53 @@
     ArrayContains,
 }
 impl PolarsSqlFunctions {
     pub(crate) fn keywords() -> &'static [&'static str] {
         &[
             "abs",
             "acos",
+            "array_contains",
+            "array_get",
+            "array_length",
+            "array_lower",
+            "array_mean",
+            "array_reverse",
+            "array_sum",
+            "array_unique",
+            "array_upper",
             "asin",
             "atan",
+            "avg",
             "ceil",
             "ceiling",
+            "count",
+            "ends_with",
             "exp",
+            "first",
             "floor",
+            "last",
+            "len",
+            "length",
             "ln",
-            "log2",
-            "log10",
             "log",
+            "log10",
             "log1p",
-            "pow",
+            "log2",
             "lower",
-            "upper",
             "ltrim",
+            "max",
+            "min",
+            "pow",
             "rtrim",
             "starts_with",
-            "ends_with",
-            "count",
-            "sum",
-            "min",
-            "max",
-            "avg",
             "stddev",
-            "variance",
-            "first",
-            "last",
-            "array_length",
-            "array_lower",
-            "array_upper",
-            "array_sum",
-            "array_mean",
-            "array_reverse",
-            "array_unique",
+            "sum",
             "unnest",
-            "array_get",
-            "array_contains",
+            "upper",
+            "variance",
         ]
     }
 }
 
 impl TryFrom<&'_ SQLFunction> for PolarsSqlFunctions {
     type Error = PolarsError;
     fn try_from(function: &'_ SQLFunction) -> Result<Self, Self::Error> {
@@ -291,53 +293,53 @@
             "acos" => Self::Acos,
             "asin" => Self::Asin,
             "atan" => Self::Atan,
             "ceil" | "ceiling" => Self::Ceil,
             "exp" => Self::Exp,
             "floor" => Self::Floor,
             "ln" => Self::Ln,
-            "log2" => Self::Log2,
-            "log10" => Self::Log10,
             "log" => Self::Log,
+            "log10" => Self::Log10,
             "log1p" => Self::Log1p,
+            "log2" => Self::Log2,
             "pow" => Self::Pow,
             // ----
             // String functions
             // ----
+            "ends_with" => Self::EndsWith,
             "lower" => Self::Lower,
-            "upper" => Self::Upper,
             "ltrim" => Self::LTrim,
             "rtrim" => Self::RTrim,
             "starts_with" => Self::StartsWith,
-            "ends_with" => Self::EndsWith,
+            "upper" => Self::Upper,
             // ----
             // Aggregate functions
             // ----
+            "avg" => Self::Avg,
             "count" => Self::Count,
-            "sum" => Self::Sum,
-            "min" => Self::Min,
+            "first" => Self::First,
+            "last" => Self::Last,
             "max" => Self::Max,
-            "avg" => Self::Avg,
+            "min" => Self::Min,
             "stddev" | "stddev_samp" => Self::StdDev,
+            "sum" => Self::Sum,
             "variance" | "var_samp" => Self::Variance,
-            "first" => Self::First,
-            "last" => Self::Last,
             // ----
             // Array functions
             // ----
+            "array_contains" => Self::ArrayContains,
+            "array_get" => Self::ArrayGet,
             "array_length" => Self::ArrayLength,
             "array_lower" => Self::ArrayMin,
-            "array_upper" => Self::ArrayMax,
-            "array_sum" => Self::ArraySum,
             "array_mean" => Self::ArrayMean,
             "array_reverse" => Self::ArrayReverse,
+            "array_sum" => Self::ArraySum,
             "array_unique" => Self::ArrayUnique,
+            "array_upper" => Self::ArrayMax,
             "unnest" => Self::Explode,
-            "array_get" => Self::ArrayGet,
-            "array_contains" => Self::ArrayContains,
             other => polars_bail!(InvalidOperation: "unsupported SQL function: {}", other),
         })
     }
 }
 
 impl SqlFunctionVisitor<'_> {
     pub(crate) fn visit_function(&self) -> PolarsResult<Expr> {
@@ -354,24 +356,24 @@
             Acos => self.visit_unary(Expr::arccos),
             Asin => self.visit_unary(Expr::arcsin),
             Atan => self.visit_unary(Expr::arctan),
             Ceil => self.visit_unary(Expr::ceil),
             Exp => self.visit_unary(Expr::exp),
             Floor => self.visit_unary(Expr::floor),
             Ln => self.visit_unary(|e| e.log(std::f64::consts::E)),
-            Log2 => self.visit_unary(|e| e.log(2.0)),
-            Log10 => self.visit_unary(|e| e.log(10.0)),
             Log => self.visit_binary(Expr::log),
+            Log10 => self.visit_unary(|e| e.log(10.0)),
             Log1p => self.visit_unary(Expr::log1p),
+            Log2 => self.visit_unary(|e| e.log(2.0)),
             Pow => self.visit_binary::<Expr>(Expr::pow),
             // ----
             // String functions
             // ----
+            EndsWith => self.visit_binary(|e, s| e.str().ends_with(s)),
             Lower => self.visit_unary(|e| e.str().to_lowercase()),
-            Upper => self.visit_unary(|e| e.str().to_uppercase()),
             LTrim => match function.args.len() {
                 1 => self.visit_unary(|e| e.str().lstrip(None)),
                 2 => self.visit_binary(|e, s| e.str().lstrip(Some(s))),
                 _ => panic!(
                     "Invalid number of arguments for LTrim: {}",
                     function.args.len()
                 ),
@@ -381,40 +383,40 @@
                 2 => self.visit_binary(|e, s| e.str().rstrip(Some(s))),
                 _ => panic!(
                     "Invalid number of arguments for RTrim: {}",
                     function.args.len()
                 ),
             },
             StartsWith => self.visit_binary(|e, s| e.str().starts_with(s)),
-            EndsWith => self.visit_binary(|e, s| e.str().ends_with(s)),
+            Upper => self.visit_unary(|e| e.str().to_uppercase()),
             // ----
             // Aggregate functions
             // ----
+            Avg => self.visit_unary(Expr::mean),
             Count => self.visit_count(),
-            Sum => self.visit_unary_with_opt_cumulative(Expr::sum, Expr::cumsum),
-            Min => self.visit_unary_with_opt_cumulative(Expr::min, Expr::cummin),
+            First => self.visit_unary(Expr::first),
+            Last => self.visit_unary(Expr::last),
             Max => self.visit_unary_with_opt_cumulative(Expr::max, Expr::cummax),
-            Avg => self.visit_unary(Expr::mean),
+            Min => self.visit_unary_with_opt_cumulative(Expr::min, Expr::cummin),
             StdDev => self.visit_unary(|e| e.std(1)),
+            Sum => self.visit_unary_with_opt_cumulative(Expr::sum, Expr::cumsum),
             Variance => self.visit_unary(|e| e.var(1)),
-            First => self.visit_unary(Expr::first),
-            Last => self.visit_unary(Expr::last),
             // ----
             // Array functions
             // ----
-            ArrayLength => self.visit_unary(|e| e.arr().lengths()),
-            ArrayMin => self.visit_unary(|e| e.arr().min()),
-            ArrayMax => self.visit_unary(|e| e.arr().max()),
-            ArraySum => self.visit_unary(|e| e.arr().sum()),
-            ArrayMean => self.visit_unary(|e| e.arr().mean()),
-            ArrayReverse => self.visit_unary(|e| e.arr().reverse()),
-            ArrayUnique => self.visit_unary(|e| e.arr().unique()),
+            ArrayContains => self.visit_binary::<Expr>(|e, s| e.list().contains(s)),
+            ArrayGet => self.visit_binary(|e, i| e.list().get(i)),
+            ArrayLength => self.visit_unary(|e| e.list().lengths()),
+            ArrayMax => self.visit_unary(|e| e.list().max()),
+            ArrayMean => self.visit_unary(|e| e.list().mean()),
+            ArrayMin => self.visit_unary(|e| e.list().min()),
+            ArrayReverse => self.visit_unary(|e| e.list().reverse()),
+            ArraySum => self.visit_unary(|e| e.list().sum()),
+            ArrayUnique => self.visit_unary(|e| e.list().unique()),
             Explode => self.visit_unary(|e| e.explode()),
-            ArrayContains => self.visit_binary::<Expr>(|e, s| e.arr().contains(s)),
-            ArrayGet => self.visit_binary(|e, i| e.arr().get(i)),
         }
     }
 
     fn visit_unary(&self, f: impl Fn(Expr) -> Expr) -> PolarsResult<Expr> {
         self.visit_unary_no_window(f)
             .and_then(|e| self.apply_window_spec(e, &self.func.over))
     }
@@ -426,15 +428,21 @@
     /// otherwise it will apply the function
     fn visit_unary_with_opt_cumulative(
         &self,
         f: impl Fn(Expr) -> Expr,
         cumulative_f: impl Fn(Expr, bool) -> Expr,
     ) -> PolarsResult<Expr> {
         match self.func.over.as_ref() {
-            Some(spec) => self.apply_cumulative_window(f, cumulative_f, spec),
+            Some(WindowType::WindowSpec(spec)) => {
+                self.apply_cumulative_window(f, cumulative_f, spec)
+            }
+            Some(WindowType::NamedWindow(named_window)) => polars_bail!(
+                InvalidOperation: "Named windows are not supported yet. Got {:?}",
+                named_window
+            ),
             _ => self.visit_unary(f),
         }
     }
     /// Window specs without partition bys are essentially cumulative functions
     /// e.g. SUM(a) OVER (ORDER BY b DESC) -> CUMSUM(a, false)
     fn apply_cumulative_window(
         &self,
@@ -528,18 +536,18 @@
             _ => return not_supported_error("count", &args),
         })
     }
 
     fn apply_window_spec(
         &self,
         expr: Expr,
-        window_spec: &Option<WindowSpec>,
+        window_type: &Option<WindowType>,
     ) -> PolarsResult<Expr> {
-        Ok(match &window_spec {
-            Some(window_spec) => {
+        Ok(match &window_type {
+            Some(WindowType::WindowSpec(window_spec)) => {
                 if window_spec.partition_by.is_empty() {
                     let exprs = window_spec
                         .order_by
                         .iter()
                         .map(|o| {
                             let e = parse_sql_expr(&o.expr, self.ctx)?;
                             match o.asc {
@@ -554,16 +562,19 @@
                     let partition_by = window_spec
                         .partition_by
                         .iter()
                         .map(|p| parse_sql_expr(p, self.ctx))
                         .collect::<PolarsResult<Vec<_>>>()?;
                     expr.over(partition_by)
                 }
-                // Order by and Row range may not be supported at the moment
             }
+            Some(WindowType::NamedWindow(named_window)) => polars_bail!(
+                InvalidOperation: "Named windows are not supported yet. Got: {:?}",
+                named_window
+            ),
             None => expr,
         })
     }
 }
 
 fn not_supported_error(function_name: &str, args: &Vec<&FunctionArgExpr>) -> PolarsResult<Expr> {
     polars_bail!(
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-sql/src/keywords.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-sql/src/keywords.rs`

 * *Files 12% similar despite different names*

```diff
@@ -11,52 +11,59 @@
 /// Get all keywords that are supported by Polars SQL
 pub fn all_keywords() -> Vec<&'static str> {
     let mut keywords = vec![];
     keywords.extend_from_slice(PolarsTableFunctions::keywords());
     keywords.extend_from_slice(PolarsSqlFunctions::keywords());
     use sqlparser::keywords;
     let sql_keywords = &[
-        keywords::SELECT,
+        keywords::AND,
+        keywords::ARRAY,
+        keywords::AS,
+        keywords::AS,
+        keywords::ASC,
+        keywords::BOOLEAN,
+        keywords::BY,
+        keywords::CREATE,
+        keywords::DATE,
+        keywords::DATETIME,
+        keywords::DESC,
+        keywords::DISTINCT,
+        keywords::DOUBLE,
+        keywords::FLOAT,
         keywords::FROM,
-        keywords::WHERE,
+        keywords::FULL,
         keywords::GROUP,
-        keywords::BY,
-        keywords::ORDER,
+        keywords::HAVING,
+        keywords::IN,
+        keywords::INNER,
+        keywords::INT,
+        keywords::JOIN,
+        keywords::LEFT,
         keywords::LIMIT,
+        keywords::NOT,
+        keywords::NULL,
         keywords::OFFSET,
-        keywords::AND,
-        keywords::OR,
-        keywords::AS,
         keywords::ON,
-        keywords::INNER,
-        keywords::LEFT,
-        keywords::RIGHT,
-        keywords::FULL,
+        keywords::OR,
+        keywords::ORDER,
         keywords::OUTER,
-        keywords::JOIN,
-        keywords::CREATE,
-        keywords::TABLE,
+        keywords::RIGHT,
+        keywords::SELECT,
         keywords::SHOW,
+        keywords::TABLE,
         keywords::TABLES,
-        keywords::AS,
-        keywords::VARCHAR,
-        keywords::INT,
-        keywords::FLOAT,
-        keywords::DOUBLE,
-        keywords::BOOLEAN,
-        keywords::DATE,
         keywords::TIME,
-        keywords::DATETIME,
-        keywords::ARRAY,
-        keywords::ASC,
-        keywords::DESC,
-        keywords::NULL,
-        keywords::NOT,
-        keywords::IN,
+        keywords::USING,
+        keywords::VARCHAR,
+        keywords::WHERE,
         keywords::WITH,
+        keywords::CASE,
+        keywords::WHEN,
+        keywords::THEN,
+        keywords::EXCLUDE,
     ];
     keywords.extend_from_slice(sql_keywords);
     keywords
 }
 
 /// Get a list of all function names that are supported by Polars SQL
 pub fn all_functions() -> Vec<&'static str> {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-sql/src/sql_expr.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-sql/src/sql_expr.rs`

 * *Files 17% similar despite different names*

```diff
@@ -1,123 +1,140 @@
 use polars_core::prelude::*;
 use polars_lazy::dsl::Expr;
 use polars_lazy::prelude::*;
+use polars_plan::prelude::{col, when};
 use sqlparser::ast::{
     ArrayAgg, BinaryOperator as SQLBinaryOperator, BinaryOperator, DataType as SQLDataType,
     Expr as SqlExpr, Function as SQLFunction, JoinConstraint, OrderByExpr, TrimWhereField,
     UnaryOperator, Value as SqlValue,
 };
 
 use crate::functions::SqlFunctionVisitor;
 use crate::SQLContext;
 
 pub(crate) fn map_sql_polars_datatype(data_type: &SQLDataType) -> PolarsResult<DataType> {
     Ok(match data_type {
+        SQLDataType::Array(Some(inner_type)) => {
+            DataType::List(Box::new(map_sql_polars_datatype(inner_type)?))
+        }
+        SQLDataType::BigInt(_) => DataType::Int64,
+        SQLDataType::Boolean => DataType::Boolean,
         SQLDataType::Char(_)
         | SQLDataType::Varchar(_)
         | SQLDataType::Uuid
         | SQLDataType::Clob(_)
         | SQLDataType::Text
         | SQLDataType::String => DataType::Utf8,
+        SQLDataType::Date => DataType::Date,
+        SQLDataType::Double => DataType::Float64,
         SQLDataType::Float(_) => DataType::Float32,
+        SQLDataType::Int(_) => DataType::Int32,
+        SQLDataType::Interval => DataType::Duration(TimeUnit::Milliseconds),
         SQLDataType::Real => DataType::Float32,
-        SQLDataType::Double => DataType::Float64,
-        SQLDataType::TinyInt(_) => DataType::Int8,
-        SQLDataType::UnsignedTinyInt(_) => DataType::UInt8,
         SQLDataType::SmallInt(_) => DataType::Int16,
-        SQLDataType::UnsignedSmallInt(_) => DataType::UInt16,
-        SQLDataType::Int(_) => DataType::Int32,
-        SQLDataType::UnsignedInt(_) => DataType::UInt32,
-        SQLDataType::BigInt(_) => DataType::Int64,
-        SQLDataType::UnsignedBigInt(_) => DataType::UInt64,
-        SQLDataType::Boolean => DataType::Boolean,
-        SQLDataType::Date => DataType::Date,
         SQLDataType::Time { .. } => DataType::Time,
         SQLDataType::Timestamp { .. } => DataType::Datetime(TimeUnit::Milliseconds, None),
-        SQLDataType::Interval => DataType::Duration(TimeUnit::Milliseconds),
-        SQLDataType::Array(Some(inner_type)) => {
-            DataType::List(Box::new(map_sql_polars_datatype(inner_type)?))
-        }
+        SQLDataType::TinyInt(_) => DataType::Int8,
+        SQLDataType::UnsignedBigInt(_) => DataType::UInt64,
+        SQLDataType::UnsignedInt(_) => DataType::UInt32,
+        SQLDataType::UnsignedSmallInt(_) => DataType::UInt16,
+        SQLDataType::UnsignedTinyInt(_) => DataType::UInt8,
+
         _ => polars_bail!(ComputeError: "SQL datatype {:?} is not yet supported", data_type),
     })
 }
 
 /// Recursively walks a SQL Expr to create a polars Expr
 pub(crate) struct SqlExprVisitor<'a> {
     ctx: &'a SQLContext,
 }
 
 impl SqlExprVisitor<'_> {
     fn visit_expr(&self, expr: &SqlExpr) -> PolarsResult<Expr> {
         match expr {
-            SqlExpr::CompoundIdentifier(idents) => self.visit_compound_identifier(idents),
-            SqlExpr::Identifier(ident) => self.visit_identifier(ident),
-            SqlExpr::BinaryOp { left, op, right } => self.visit_binary_op(left, op, right),
-            SqlExpr::Function(function) => self.visit_function(function),
-            SqlExpr::Cast { expr, data_type } => self.visit_cast(expr, data_type),
-            SqlExpr::Value(value) => self.visit_literal(value),
-            SqlExpr::IsNull(expr) => Ok(self.visit_expr(expr)?.is_null()),
-            SqlExpr::IsNotNull(expr) => Ok(self.visit_expr(expr)?.is_not_null()),
-            SqlExpr::Floor { expr, .. } => Ok(self.visit_expr(expr)?.floor()),
-            SqlExpr::Ceil { expr, .. } => Ok(self.visit_expr(expr)?.ceil()),
+            SqlExpr::AllOp(_) => Ok(self.visit_expr(expr)?.all()),
+            SqlExpr::AnyOp(expr) => Ok(self.visit_expr(expr)?.any()),
             SqlExpr::ArrayAgg(expr) => self.visit_arr_agg(expr),
             SqlExpr::Between {
                 expr,
                 negated,
                 low,
                 high,
             } => self.visit_between(expr, *negated, low, high),
-            SqlExpr::Trim {
+            SqlExpr::BinaryOp { left, op, right } => self.visit_binary_op(left, op, right),
+            SqlExpr::Cast { expr, data_type } => self.visit_cast(expr, data_type),
+            SqlExpr::Ceil { expr, .. } => Ok(self.visit_expr(expr)?.ceil()),
+            SqlExpr::CompoundIdentifier(idents) => self.visit_compound_identifier(idents),
+            SqlExpr::Floor { expr, .. } => Ok(self.visit_expr(expr)?.floor()),
+            SqlExpr::Function(function) => self.visit_function(function),
+            SqlExpr::Identifier(ident) => self.visit_identifier(ident),
+            SqlExpr::InList {
                 expr,
-                trim_where,
-                trim_what,
-            } => self.visit_trim(expr, trim_where, trim_what),
+                list,
+                negated,
+            } => self.visit_is_in(expr, list, *negated),
             SqlExpr::IsFalse(expr) => Ok(self.visit_expr(expr)?.eq(lit(false))),
             SqlExpr::IsNotFalse(expr) => Ok(self.visit_expr(expr)?.eq(lit(false)).not()),
-            SqlExpr::IsTrue(expr) => Ok(self.visit_expr(expr)?.eq(lit(true))),
+            SqlExpr::IsNotNull(expr) => Ok(self.visit_expr(expr)?.is_not_null()),
             SqlExpr::IsNotTrue(expr) => Ok(self.visit_expr(expr)?.eq(lit(true)).not()),
-            SqlExpr::AnyOp(expr) => Ok(self.visit_expr(expr)?.any()),
-            SqlExpr::AllOp(_) => Ok(self.visit_expr(expr)?.all()),
+            SqlExpr::IsNull(expr) => Ok(self.visit_expr(expr)?.is_null()),
+            SqlExpr::IsTrue(expr) => Ok(self.visit_expr(expr)?.eq(lit(true))),
             SqlExpr::Nested(expr) => self.visit_expr(expr),
-            SqlExpr::UnaryOp { op, expr } => self.visit_unary_op(op, expr),
-            SqlExpr::InList {
+            SqlExpr::Trim {
                 expr,
-                list,
-                negated,
-            } => self.visit_is_in(expr, list, *negated),
-            other => polars_bail!(ComputeError: "SQL expression {:?} is not yet supported", other),
+                trim_where,
+                trim_what,
+            } => self.visit_trim(expr, trim_where, trim_what),
+            SqlExpr::UnaryOp { op, expr } => self.visit_unary_op(op, expr),
+            SqlExpr::Value(value) => self.visit_literal(value),
+            e @ SqlExpr::Case { .. } => self.visit_when_then(e),
+            other => {
+                polars_bail!(InvalidOperation: "SQL expression {:?} is not yet supported", other)
+            }
         }
     }
 
     /// Visit a compound identifier
     ///
     /// e.g. df.column or "df"."column"
     fn visit_compound_identifier(&self, idents: &[sqlparser::ast::Ident]) -> PolarsResult<Expr> {
-        polars_ensure!(
-            idents.len() == 2,
-            ComputeError: "compound identifier {:?} is not yet supported", idents,
-        );
-        let tbl_name = &idents[0].value;
-        let refers_main_table =
-            { self.ctx.table_map.len() == 1 && self.ctx.table_map.contains_key(tbl_name) };
-        polars_ensure!(
-            refers_main_table, ComputeError:
-            "compound identifier {:?} is not yet supported if multiple tables are registered",
-            idents
-        );
-        Ok(col(&idents[1].value))
+        match idents {
+            [tbl_name, column_name] => {
+                let lf = self.ctx.table_map.get(&tbl_name.value).ok_or_else(|| {
+                    polars_err!(
+                        ComputeError: "no table named '{}' found",
+                        tbl_name
+                    )
+                })?;
+
+                let schema = lf.schema()?;
+                if let Some((_, name, _)) = schema.get_full(&column_name.value) {
+                    Ok(col(name))
+                } else {
+                    polars_bail!(
+                        ColumnNotFound: "no column named '{}' found in table '{}'",
+                        column_name,
+                        tbl_name
+                    )
+                }
+            }
+            _ => polars_bail!(
+                ComputeError: "Invalid identifier {:?}",
+                idents
+            ),
+        }
     }
 
     fn visit_unary_op(&self, op: &UnaryOperator, expr: &SqlExpr) -> PolarsResult<Expr> {
         let expr = self.visit_expr(expr)?;
         Ok(match op {
             UnaryOperator::Plus => lit(0) + expr,
             UnaryOperator::Minus => lit(0) - expr,
             UnaryOperator::Not => expr.not(),
-            other => polars_bail!(ComputeError: "Unary operator {:?} is not supported", other),
+            other => polars_bail!(InvalidOperation: "Unary operator {:?} is not supported", other),
         })
     }
 
     /// Visit a single identifier
     ///
     /// e.g. column
     fn visit_identifier(&self, ident: &sqlparser::ast::Ident) -> PolarsResult<Expr> {
@@ -132,30 +149,30 @@
         left: &SqlExpr,
         op: &BinaryOperator,
         right: &SqlExpr,
     ) -> PolarsResult<Expr> {
         let left = self.visit_expr(left)?;
         let right = self.visit_expr(right)?;
         Ok(match op {
-            SQLBinaryOperator::Plus => left + right,
-            SQLBinaryOperator::Minus => left - right,
-            SQLBinaryOperator::Multiply => left * right,
+            SQLBinaryOperator::And => left.and(right),
             SQLBinaryOperator::Divide => left / right,
-            SQLBinaryOperator::Modulo => left % right,
-            SQLBinaryOperator::StringConcat => {
-                left.cast(DataType::Utf8) + right.cast(DataType::Utf8)
-            }
+            SQLBinaryOperator::Eq => left.eq(right),
             SQLBinaryOperator::Gt => left.gt(right),
-            SQLBinaryOperator::Lt => left.lt(right),
             SQLBinaryOperator::GtEq => left.gt_eq(right),
+            SQLBinaryOperator::Lt => left.lt(right),
             SQLBinaryOperator::LtEq => left.lt_eq(right),
-            SQLBinaryOperator::Eq => left.eq(right),
+            SQLBinaryOperator::Minus => left - right,
+            SQLBinaryOperator::Modulo => left % right,
+            SQLBinaryOperator::Multiply => left * right,
             SQLBinaryOperator::NotEq => left.eq(right).not(),
-            SQLBinaryOperator::And => left.and(right),
             SQLBinaryOperator::Or => left.or(right),
+            SQLBinaryOperator::Plus => left + right,
+            SQLBinaryOperator::StringConcat => {
+                left.cast(DataType::Utf8) + right.cast(DataType::Utf8)
+            }
             SQLBinaryOperator::Xor => left.xor(right),
             other => polars_bail!(ComputeError: "SQL operator {:?} is not yet supported", other),
         })
     }
 
     /// Visit a SQL function
     ///
@@ -183,51 +200,51 @@
     /// Visit a SQL literal
     ///
     /// e.g. 1, 'foo', 1.0, NULL
     ///
     /// See [SqlValue] and [LiteralValue] for more details
     fn visit_literal(&self, value: &SqlValue) -> PolarsResult<Expr> {
         Ok(match value {
+            SqlValue::Boolean(b) => lit(*b),
+            SqlValue::DoubleQuotedString(s) => lit(s.clone()),
+            SqlValue::HexStringLiteral(s) => lit(s.clone()),
+            SqlValue::NationalStringLiteral(s) => lit(s.clone()),
+            SqlValue::Null => Expr::Literal(LiteralValue::Null),
             SqlValue::Number(s, _) => {
                 // Check for existence of decimal separator dot
                 if s.contains('.') {
                     s.parse::<f64>().map(lit).map_err(|_| ())
                 } else {
                     s.parse::<i64>().map(lit).map_err(|_| ())
                 }
                 .map_err(|_| polars_err!(ComputeError: "cannot parse literal: {:?}"))?
             }
             SqlValue::SingleQuotedString(s) => lit(s.clone()),
-            SqlValue::NationalStringLiteral(s) => lit(s.clone()),
-            SqlValue::HexStringLiteral(s) => lit(s.clone()),
-            SqlValue::DoubleQuotedString(s) => lit(s.clone()),
-            SqlValue::Boolean(b) => lit(*b),
-            SqlValue::Null => Expr::Literal(LiteralValue::Null),
             other => polars_bail!(ComputeError: "SQL value {:?} is not yet supported", other),
         })
     }
 
     // similar to visit_literal, but returns an AnyValue instead of Expr
     fn visit_anyvalue(&self, value: &SqlValue) -> PolarsResult<AnyValue> {
         Ok(match value {
+            SqlValue::Boolean(b) => AnyValue::Boolean(*b),
+            SqlValue::Null => AnyValue::Null,
             SqlValue::Number(s, _) => {
                 // Check for existence of decimal separator dot
                 if s.contains('.') {
                     s.parse::<f64>().map(AnyValue::Float64).map_err(|_| ())
                 } else {
                     s.parse::<i64>().map(AnyValue::Int64).map_err(|_| ())
                 }
                 .map_err(|_| polars_err!(ComputeError: "cannot parse literal: {:?}"))?
             }
             SqlValue::SingleQuotedString(s)
             | SqlValue::NationalStringLiteral(s)
             | SqlValue::HexStringLiteral(s)
             | SqlValue::DoubleQuotedString(s) => AnyValue::Utf8Owned(s.into()),
-            SqlValue::Boolean(b) => AnyValue::Boolean(*b),
-            SqlValue::Null => AnyValue::Null,
             other => polars_bail!(ComputeError: "SQL value {:?} is not yet supported", other),
         })
     }
 
     /// Visit a SQL `BETWEEN` expression
     /// See [sqlparser::ast::Expr::Between] for more details
     fn visit_between(
@@ -240,15 +257,15 @@
         let expr = self.visit_expr(expr)?;
         let low = self.visit_expr(low)?;
         let high = self.visit_expr(high)?;
 
         if negated {
             Ok(expr.clone().lt(low).or(expr.gt(high)))
         } else {
-            Ok(expr.clone().gt(low).and(expr.lt(high)))
+            Ok(expr.clone().gt_eq(low).and(expr.lt_eq(high)))
         }
     }
 
     /// Visit a SQL 'TRIM' function
     /// See [sqlparser::ast::Expr::Trim] for more details
     fn visit_trim(
         &self,
@@ -276,15 +293,15 @@
 
     /// Visit a SQL `ARRAY_AGG` expression
     fn visit_arr_agg(&self, expr: &ArrayAgg) -> PolarsResult<Expr> {
         let mut base = self.visit_expr(&expr.expr)?;
 
         if let Some(order_by) = expr.order_by.as_ref() {
             let (order_by, descending) = self.visit_order_by(order_by)?;
-            base = base.sort_by(vec![order_by], vec![descending]);
+            base = base.sort_by(order_by, descending);
         }
 
         if let Some(limit) = &expr.limit {
             let limit = match self.visit_expr(limit)? {
                 Expr::Literal(LiteralValue::UInt32(n)) => n as usize,
                 Expr::Literal(LiteralValue::UInt64(n)) => n as usize,
                 Expr::Literal(LiteralValue::Int32(n)) => n as usize,
@@ -324,20 +341,87 @@
         if negated {
             Ok(expr.is_in(lit(s)).not())
         } else {
             Ok(expr.is_in(lit(s)))
         }
     }
 
-    fn visit_order_by(&self, order_by: &OrderByExpr) -> PolarsResult<(Expr, bool)> {
-        let expr = self.visit_expr(&order_by.expr)?;
-        let descending = order_by.asc.unwrap_or(false);
+    fn visit_order_by(&self, order_by: &[OrderByExpr]) -> PolarsResult<(Vec<Expr>, Vec<bool>)> {
+        let mut expr = Vec::with_capacity(order_by.len());
+        let mut descending = Vec::with_capacity(order_by.len());
+        for order_by_expr in order_by {
+            let e = self.visit_expr(&order_by_expr.expr)?;
+            expr.push(e);
+            let desc = order_by_expr.asc.unwrap_or(false);
+            descending.push(desc);
+        }
+
         Ok((expr, descending))
     }
 
+    fn visit_when_then(&self, expr: &SqlExpr) -> PolarsResult<Expr> {
+        if let SqlExpr::Case {
+            operand,
+            conditions,
+            results,
+            else_result,
+        } = expr
+        {
+            if operand.is_some() {
+                polars_bail!(ComputeError: "CASE operand is not yet supported");
+            }
+
+            polars_ensure!(
+                conditions.len() == results.len(),
+                ComputeError: "WHEN and THEN expressions must have the same length"
+            );
+
+            polars_ensure!(
+                !conditions.is_empty(),
+                ComputeError: "WHEN and THEN expressions must have at least one element"
+            );
+
+            let mut when_thens = conditions.iter().zip(results.iter());
+            let first = when_thens.next();
+
+            if first.is_none() {
+                polars_bail!(ComputeError: "WHEN and THEN expressions must have at least one element");
+            }
+
+            let else_res = match else_result {
+                Some(else_res) => self.visit_expr(else_res)?,
+                None => polars_bail!(ComputeError: "ELSE expression is required"),
+            };
+
+            let first = first.unwrap();
+            let first_cond = self.visit_expr(first.0)?;
+            let first_then = self.visit_expr(first.1)?;
+            let expr = when(first_cond).then(first_then);
+            let next = when_thens.next();
+
+            let mut when_then = if let Some((cond, res)) = next {
+                let cond = self.visit_expr(cond)?;
+                let res = self.visit_expr(res)?;
+                expr.when(cond).then(res)
+            } else {
+                return Ok(expr.otherwise(else_res));
+            };
+
+            for (cond, res) in when_thens {
+                let cond = self.visit_expr(cond)?;
+                let res = self.visit_expr(res)?;
+                when_then = when_then.when(cond).then(res);
+            }
+
+            Ok(when_then.otherwise(else_res))
+        } else {
+            unreachable!()
+        }
+    }
+
     fn err(&self, expr: &Expr) -> PolarsResult<Expr> {
         polars_bail!(ComputeError: "SQL expression {:?} is not yet supported", expr);
     }
 }
 
 pub(crate) fn parse_sql_expr(expr: &SqlExpr, ctx: &SQLContext) -> PolarsResult<Expr> {
     let visitor = SqlExprVisitor { ctx };
@@ -351,15 +435,14 @@
 ) -> PolarsResult<(Expr, Expr)> {
     if let JoinConstraint::On(SqlExpr::BinaryOp { left, op, right }) = constraint {
         match (left.as_ref(), right.as_ref()) {
             (SqlExpr::CompoundIdentifier(left), SqlExpr::CompoundIdentifier(right)) => {
                 if left.len() == 2 && right.len() == 2 {
                     let tbl_a = &left[0].value;
                     let col_a = &left[1].value;
-
                     let tbl_b = &right[0].value;
                     let col_b = &right[1].value;
 
                     if let BinaryOperator::Eq = op {
                         if left_name == tbl_a && right_name == tbl_b {
                             return Ok((col(col_a), col(col_b)));
                         } else if left_name == tbl_b && right_name == tbl_a {
@@ -370,9 +453,15 @@
             }
             (SqlExpr::Identifier(left), SqlExpr::Identifier(right)) => {
                 return Ok((col(&left.value), col(&right.value)))
             }
             _ => {}
         }
     }
-    polars_bail!(ComputeError: "SQL join constraint {:?} is not yet supported", constraint);
+    if let JoinConstraint::Using(idents) = constraint {
+        if !idents.is_empty() {
+            let cols = &idents[0].value;
+            return Ok((col(cols), col(cols)));
+        }
+    }
+    polars_bail!(InvalidOperation: "SQL join constraint {:?} is not yet supported", constraint);
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-sql/src/table_functions.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-sql/src/table_functions.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-sql/tests/functions_cumulative.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-sql/tests/functions_cumulative.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-sql/tests/functions_io.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-sql/tests/functions_io.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-sql/tests/functions_math.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-sql/tests/functions_math.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-sql/tests/functions_string.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-sql/tests/functions_string.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-sql/tests/iss_7436.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-sql/tests/iss_7436.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-sql/tests/iss_7437.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-sql/tests/iss_7437.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-sql/tests/iss_7440.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-sql/tests/iss_7440.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-sql/tests/iss_8395.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-sql/tests/iss_8395.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-sql/tests/iss_8419.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-sql/tests/iss_8419.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-sql/tests/simple_exprs.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-sql/tests/simple_exprs.rs`

 * *Files 2% similar despite different names*

```diff
@@ -496,7 +496,31 @@
             false,
         )
         .limit(2);
     let expected = expected.collect()?;
     assert!(df_sql.frame_equal(&expected));
     Ok(())
 }
+
+#[test]
+fn test_case_expr() {
+    let df = create_sample_df().unwrap().head(Some(10));
+    let mut context = SQLContext::new();
+    context.register("df", df.clone().lazy());
+    let sql = r#"
+        SELECT
+            CASE
+                WHEN (a > 5 AND a < 8) THEN 'gt_5 and lt_8'
+                WHEN a <= 5 THEN 'lteq_5'
+                ELSE 'no match'
+            END AS sign
+        FROM df"#;
+    let df_sql = context.execute(sql).unwrap().collect().unwrap();
+    let case_expr = when(col("a").gt(lit(5)).and(col("a").lt(lit(8))))
+        .then(lit("gt_5 and lt_8"))
+        .when(col("a").lt_eq(lit(5)))
+        .then(lit("lteq_5"))
+        .otherwise(lit("no match"))
+        .alias("sign");
+    let df_pl = df.lazy().select(&[case_expr]).collect().unwrap();
+    assert!(df_sql.frame_equal(&df_pl));
+}
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/Cargo.toml` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/Cargo.toml`

 * *Files 3% similar despite different names*

```diff
@@ -1,29 +1,29 @@
 [package]
 name = "polars-plan"
-version= "0.28.0"
+version= "0.30.0"
 edition = "2021"
 license = "MIT"
 repository = "https://github.com/pola-rs/polars"
 description = "Lazy query engine for the Polars DataFrame library"
 
 # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html
 
 [dependencies]
 ahash= "0.8"
 chrono = { version = "0.4", optional = true }
 chrono-tz = { version = "0.8", optional = true }
 futures = { version = "0.3.25", optional = true }
 once_cell= "1"
-polars-arrow = { version = "0.28.0", path = "../polars-arrow" }
-polars-core = { version = "0.28.0", path = "../polars-core", features = ["lazy", "private", "zip_with", "random"], default-features = false }
-polars-io = { version = "0.28.0", path = "../polars-io", features = ["lazy", "csv", "private"], default-features = false }
-polars-ops = { version = "0.28.0", path = "../polars-ops", default-features = false }
-polars-time = { version = "0.28.0", path = "../polars-time", optional = true }
-polars-utils = { version = "0.28.0", path = "../polars-utils" }
+polars-arrow = { version = "0.30.0", path = "../polars-arrow" }
+polars-core = { version = "0.30.0", path = "../polars-core", features = ["lazy", "private", "zip_with", "random"], default-features = false }
+polars-io = { version = "0.30.0", path = "../polars-io", features = ["lazy", "csv", "private"], default-features = false }
+polars-ops = { version = "0.30.0", path = "../polars-ops", default-features = false }
+polars-time = { version = "0.30.0", path = "../polars-time", optional = true }
+polars-utils = { version = "0.30.0", path = "../polars-utils" }
 pyo3 = { version = "0.18", optional = true }
 rayon= "1.6"
 regex = { version = "1.6", optional = true }
 serde = { version = "1", features = ["derive", "rc"], optional = true }
 smartstring= { version = "1" }
 
 [features]
@@ -50,14 +50,15 @@
 dtype-i8 = ["polars-core/dtype-i8"]
 dtype-i16 = ["polars-core/dtype-i16"]
 dtype-decimal = ["polars-core/dtype-decimal"]
 dtype-date = ["polars-core/dtype-date", "polars-time/dtype-date", "temporal"]
 dtype-datetime = ["polars-core/dtype-datetime", "polars-time/dtype-datetime", "temporal"]
 dtype-duration = ["polars-core/dtype-duration", "polars-time/dtype-duration", "temporal"]
 dtype-time = ["polars-core/dtype-time", "polars-time/dtype-time"]
+dtype-array = ["polars-core/dtype-array"]
 dtype-categorical = ["polars-core/dtype-categorical"]
 dtype-struct = ["polars-core/dtype-struct"]
 object = ["polars-core/object"]
 date_offset = ["polars-time", "chrono"]
 list_take = ["polars-ops/list_take"]
 list_count = ["polars-ops/list_count"]
 trigonometry = []
@@ -71,15 +72,14 @@
 is_in = ["polars-core/is_in"]
 repeat_by = ["polars-core/repeat_by"]
 round_series = ["polars-core/round_series"]
 is_first = ["polars-core/is_first", "polars-ops/is_first"]
 is_unique = ["polars-ops/is_unique"]
 cross_join = ["polars-core/cross_join"]
 asof_join = ["polars-core/asof_join", "polars-time", "polars-ops/asof_join"]
-dot_product = ["polars-core/dot_product"]
 concat_str = ["polars-core/concat_str"]
 arange = []
 mode = ["polars-core/mode"]
 cum_agg = ["polars-core/cum_agg"]
 interpolate = ["polars-ops/interpolate"]
 rolling_window = [
   "polars-core/rolling_window",
@@ -110,14 +110,15 @@
 meta = []
 pivot = ["polars-core/rows", "polars-ops/pivot"]
 top_k = ["polars-ops/top_k"]
 semi_anti_join = ["polars-core/semi_anti_join", "polars-ops/semi_anti_join"]
 cse = []
 propagate_nans = ["polars-ops/propagate_nans"]
 coalesce = []
+fused = []
 
 # no guarantees whatsoever
 private = ["polars-time/private"]
 
 bigidx = ["polars-arrow/bigidx", "polars-core/bigidx", "polars-utils/bigidx"]
 
 panic_on_schema = []
@@ -126,18 +127,18 @@
 all-features = true
 # defines the configuration attribute `docsrs`
 rustdoc-args = ["--cfg", "docsrs"]
 
 [dependencies.arrow]
 package = "arrow2"
 # git = "https://github.com/jorgecarleitao/arrow2"
-git = "https://github.com/ritchie46/arrow2"
+# git = "https://github.com/ritchie46/arrow2"
 # rev = "1491c6e8f4fd100f53c358e4f3ef1536d9e75090"
 # path = "../arrow2"
-branch = "polars_2023-04-20"
+# branch = "polars_2023-05-25"
 version = "0.17"
 default-features = false
 features = [
   "compute_aggregate",
   "compute_arithmetics",
   "compute_boolean",
   "compute_boolean_kleene",
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/LICENSE` & `polars_lts_cpu-0.18.0/local_dependencies/polars-sql/LICENSE`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dot.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dot.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/arithmetic.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/arithmetic.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/binary.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/binary.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/cat.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/cat.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/dt.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/dt.rs`

 * *Files 3% similar despite different names*

```diff
@@ -3,22 +3,30 @@
 use super::*;
 use crate::prelude::function_expr::TemporalFunction;
 
 /// Specialized expressions for [`Series`] with dates/datetimes.
 pub struct DateLikeNameSpace(pub(crate) Expr);
 
 impl DateLikeNameSpace {
-    /// Format Date/datetime with a formatting rule
+    /// Convert from Date/Time/Datetime into Utf8 with the given format.
     /// See [chrono strftime/strptime](https://docs.rs/chrono/0.4.19/chrono/format/strftime/index.html).
-    pub fn strftime(self, format: &str) -> Expr {
+    pub fn to_string(self, format: &str) -> Expr {
         let format = format.to_string();
-        let function = move |s: Series| s.strftime(&format).map(Some);
+        let function = move |s: Series| TemporalMethods::to_string(&s, &format).map(Some);
         self.0
             .map(function, GetOutput::from_type(DataType::Utf8))
-            .with_fmt("strftime")
+            .with_fmt("to_string")
+    }
+
+    /// Convert from Date/Time/Datetime into Utf8 with the given format.
+    /// See [chrono strftime/strptime](https://docs.rs/chrono/0.4.19/chrono/format/strftime/index.html).
+    ///
+    /// Alias for `to_string`.
+    pub fn strftime(self, format: &str) -> Expr {
+        self.to_string(format)
     }
 
     /// Change the underlying [`TimeUnit`]. And update the data accordingly.
     pub fn cast_time_unit(self, tu: TimeUnit) -> Expr {
         self.0.map(
             move |s| match s.dtype() {
                 DataType::Datetime(_, _) => {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/expr.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/expr.rs`

 * *Files 3% similar despite different names*

```diff
@@ -391,14 +391,18 @@
         input: Vec<Expr>,
         /// function to apply
         function: SpecialEq<Arc<dyn SeriesUdf>>,
         /// output dtype of the function
         output_type: GetOutput,
         options: FunctionOptions,
     },
+    Cache {
+        input: Box<Expr>,
+        id: usize,
+    },
 }
 
 // TODO! derive. This is only a temporary fix
 // Because PartialEq will have a lot of `false`, e.g. on Function
 // Types, this may lead to many file reads, as we use predicate comparison
 // to check if we can cache a file
 #[allow(clippy::derived_hash_with_manual_eq)]
@@ -443,15 +447,17 @@
     }
 }
 
 #[derive(Copy, Clone, PartialEq, Eq)]
 #[cfg_attr(feature = "serde", derive(Serialize, Deserialize))]
 pub enum Operator {
     Eq,
+    EqValidity,
     NotEq,
+    NotEqValidity,
     Lt,
     LtEq,
     Gt,
     GtEq,
     Plus,
     Minus,
     Multiply,
@@ -465,15 +471,17 @@
 }
 
 impl Display for Operator {
     fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
         use Operator::*;
         let tkn = match self {
             Eq => "==",
+            EqValidity => "==v",
             NotEq => "!=",
+            NotEqValidity => "!=v",
             Lt => "<",
             LtEq => "<=",
             Gt => ">",
             GtEq => ">=",
             Plus => "+",
             Minus => "-",
             Multiply => "*",
@@ -498,14 +506,16 @@
                 | Self::Lt
                 | Self::LtEq
                 | Self::Gt
                 | Self::GtEq
                 | Self::And
                 | Self::Or
                 | Self::Xor
+                | Self::EqValidity
+                | Self::NotEqValidity
         )
     }
 
     pub(crate) fn is_arithmetic(&self) -> bool {
         !(self.is_comparison())
     }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/from.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/from.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/arg_where.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/arg_where.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/binary.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/binary.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/boolean.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/boolean.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/bounds.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/bounds.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/cat.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/cat.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/cum.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/cum.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/dispatch.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/dispatch.rs`

 * *Files 3% similar despite different names*

```diff
@@ -18,11 +18,7 @@
     s.diff(n, null_behavior)
 }
 
 #[cfg(feature = "interpolate")]
 pub(super) fn interpolate(s: &Series, method: InterpolationMethod) -> PolarsResult<Series> {
     Ok(polars_ops::prelude::interpolate(s, method))
 }
-#[cfg(feature = "dot_product")]
-pub(super) fn dot_impl(s: &[Series]) -> PolarsResult<Series> {
-    Ok((&s[0] * &s[1]).sum_as_series())
-}
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/fill_null.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/fill_null.rs`

 * *Files 4% similar despite different names*

```diff
@@ -1,24 +1,24 @@
 use super::*;
 
 pub(super) fn fill_null(s: &[Series], super_type: &DataType) -> PolarsResult<Series> {
-    let array = &s[0];
+    let series = &s[0];
     let fill_value = &s[1];
 
     let (series, fill_value) = if matches!(super_type, DataType::Unknown) {
-        let fill_value = fill_value.cast(array.dtype()).map_err(|_| {
+        let fill_value = fill_value.cast(series.dtype()).map_err(|_| {
             polars_err!(
                 SchemaMismatch:
                 "`fill_null` supertype could not be determined; set correct literal value or \
                 ensure the type of the expression is known"
             )
         })?;
-        (array.clone(), fill_value)
+        (series.clone(), fill_value)
     } else {
-        (array.cast(super_type)?, fill_value.cast(super_type)?)
+        (series.cast(super_type)?, fill_value.cast(super_type)?)
     };
     // nothing to fill, so return early
     // this is done after casting as the output type must be correct
     if series.null_count() == 0 {
         return Ok(series);
     }
 
@@ -32,15 +32,15 @@
         series.zip_with_same_type(&mask, &fill_value)
     }
 
     match series.dtype() {
         #[cfg(feature = "dtype-categorical")]
         // for Categoricals we first need to check if the category already exist
         DataType::Categorical(Some(rev_map)) => {
-            if fill_value.len() == 1 && fill_value.null_count() == 0 {
+            if rev_map.is_local() && fill_value.len() == 1 && fill_value.null_count() == 0 {
                 let fill_av = fill_value.get(0).unwrap();
                 let fill_str = fill_av.get_str().unwrap();
 
                 if let Some(idx) = rev_map.find(fill_str) {
                     let cats = series.to_physical_repr();
                     let mask = cats.is_not_null();
                     let out = cats
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/list.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/list.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/log.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/log.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/mod.rs`

 * *Files 2% similar despite different names*

```diff
@@ -1,23 +1,27 @@
 #[cfg(feature = "abs")]
 mod abs;
 #[cfg(feature = "arg_where")]
 mod arg_where;
+#[cfg(feature = "dtype-array")]
+mod array;
 mod binary;
 mod boolean;
 mod bounds;
 #[cfg(feature = "dtype-categorical")]
 mod cat;
 #[cfg(feature = "round_series")]
 mod clip;
 mod cum;
 #[cfg(feature = "temporal")]
 mod datetime;
 mod dispatch;
 mod fill_null;
+#[cfg(feature = "fused")]
+mod fused;
 mod list;
 #[cfg(feature = "log")]
 mod log;
 mod nan;
 mod pow;
 #[cfg(all(feature = "rolling_window", feature = "moment"))]
 mod rolling;
@@ -40,14 +44,18 @@
 mod temporal;
 #[cfg(feature = "trigonometry")]
 mod trigonometry;
 mod unique;
 
 use std::fmt::{Display, Formatter};
 
+#[cfg(feature = "dtype-array")]
+pub(super) use array::ArrayFunction;
+#[cfg(feature = "fused")]
+pub(crate) use fused::FusedOperator;
 pub(super) use list::ListFunction;
 use polars_core::prelude::*;
 use schema::FieldsMapper;
 #[cfg(feature = "serde")]
 use serde::{Deserialize, Serialize};
 
 pub(crate) use self::binary::BinaryFunction;
@@ -103,14 +111,16 @@
     DropNans,
     #[cfg(feature = "round_series")]
     Clip {
         min: Option<AnyValue<'static>>,
         max: Option<AnyValue<'static>>,
     },
     ListExpr(ListFunction),
+    #[cfg(feature = "dtype-array")]
+    ArrayExpr(ArrayFunction),
     #[cfg(feature = "dtype-struct")]
     StructExpr(StructFunction),
     #[cfg(feature = "top_k")]
     TopK {
         k: usize,
         descending: bool,
     },
@@ -138,16 +148,14 @@
     Categorical(CategoricalFunction),
     Coalesce,
     ShrinkType,
     #[cfg(feature = "diff")]
     Diff(i64, NullBehavior),
     #[cfg(feature = "interpolate")]
     Interpolate(InterpolationMethod),
-    #[cfg(feature = "dot_product")]
-    Dot,
     #[cfg(feature = "log")]
     Entropy {
         base: f64,
         normalize: bool,
     },
     #[cfg(feature = "log")]
     Log {
@@ -164,14 +172,16 @@
     },
     #[cfg(feature = "round_series")]
     Floor,
     #[cfg(feature = "round_series")]
     Ceil,
     UpperBound,
     LowerBound,
+    #[cfg(feature = "fused")]
+    Fused(fused::FusedOperator),
 }
 
 impl Display for FunctionExpr {
     fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
         use FunctionExpr::*;
 
         let s = match self {
@@ -227,16 +237,14 @@
             Categorical(func) => return write!(f, "{func}"),
             Coalesce => "coalesce",
             ShrinkType => "shrink_dtype",
             #[cfg(feature = "diff")]
             Diff(_, _) => "diff",
             #[cfg(feature = "interpolate")]
             Interpolate(_) => "interpolate",
-            #[cfg(feature = "dot_product")]
-            Dot => "dot",
             #[cfg(feature = "log")]
             Entropy { .. } => "entropy",
             #[cfg(feature = "log")]
             Log { .. } => "log",
             #[cfg(feature = "log")]
             Log1p => "log1p",
             #[cfg(feature = "log")]
@@ -252,14 +260,18 @@
             Round { .. } => "round",
             #[cfg(feature = "round_series")]
             Floor => "floor",
             #[cfg(feature = "round_series")]
             Ceil => "ceil",
             UpperBound => "upper_bound",
             LowerBound => "lower_bound",
+            #[cfg(feature = "fused")]
+            Fused(fused) => return Display::fmt(fused, f),
+            #[cfg(feature = "dtype-array")]
+            ArrayExpr(af) => return Display::fmt(af, f),
         };
         write!(f, "{s}")
     }
 }
 
 #[macro_export]
 macro_rules! wrap {
@@ -407,14 +419,23 @@
                     #[cfg(feature = "list_take")]
                     Take(null_ob_oob) => map_as_slice!(list::take, null_ob_oob),
                     #[cfg(feature = "list_count")]
                     CountMatch => map_as_slice!(list::count_match),
                     Sum => map!(list::sum),
                 }
             }
+            #[cfg(feature = "dtype-array")]
+            ArrayExpr(lf) => {
+                use ArrayFunction::*;
+                match lf {
+                    Min => map!(array::min),
+                    Max => map!(array::max),
+                    Sum => map!(array::sum),
+                }
+            }
             #[cfg(feature = "dtype-struct")]
             StructExpr(sf) => {
                 use StructFunction::*;
                 match sf {
                     FieldByIndex(index) => map!(struct_::get_by_index, index),
                     FieldByName(name) => map!(struct_::get_by_name, name.clone()),
                 }
@@ -439,18 +460,14 @@
             ShrinkType => map_owned!(shrink_type::shrink),
             #[cfg(feature = "diff")]
             Diff(n, null_behavior) => map!(dispatch::diff, n, null_behavior),
             #[cfg(feature = "interpolate")]
             Interpolate(method) => {
                 map!(dispatch::interpolate, method)
             }
-            #[cfg(feature = "dot_product")]
-            Dot => {
-                map_as_slice!(dispatch::dot_impl)
-            }
             #[cfg(feature = "log")]
             Entropy { base, normalize } => map!(log::entropy, base, normalize),
             #[cfg(feature = "log")]
             Log { base } => map!(log::log, base),
             #[cfg(feature = "log")]
             Log1p => map!(log::log1p),
             #[cfg(feature = "log")]
@@ -460,14 +477,16 @@
             Round { decimals } => map!(round::round, decimals),
             #[cfg(feature = "round_series")]
             Floor => map!(round::floor),
             #[cfg(feature = "round_series")]
             Ceil => map!(round::ceil),
             UpperBound => map!(bounds::upper_bound),
             LowerBound => map!(bounds::lower_bound),
+            #[cfg(feature = "fused")]
+            Fused(op) => map_as_slice!(fused::fused, op),
         }
     }
 }
 
 #[cfg(feature = "strings")]
 impl From<StringFunction> for SpecialEq<Arc<dyn SeriesUdf>> {
     fn from(func: StringFunction) -> Self {
@@ -495,16 +514,16 @@
                 map!(strings::ljust, width, fillchar)
             }
             #[cfg(feature = "string_justify")]
             RJust { width, fillchar } => {
                 map!(strings::rjust, width, fillchar)
             }
             #[cfg(feature = "temporal")]
-            Strptime(options) => {
-                map!(strings::strptime, &options)
+            Strptime(dtype, options) => {
+                map!(strings::strptime, dtype.clone(), &options)
             }
             #[cfg(feature = "concat_str")]
             ConcatVertical(delimiter) => map!(strings::concat, &delimiter),
             #[cfg(feature = "concat_str")]
             ConcatHorizontal(delimiter) => map_as_slice!(strings::concat_hor, &delimiter),
             #[cfg(feature = "regex")]
             Replace { n, literal } => map_as_slice!(strings::replace, literal, n),
@@ -512,14 +531,17 @@
             Lowercase => map!(strings::lowercase),
             Strip(matches) => map!(strings::strip, matches.as_deref()),
             LStrip(matches) => map!(strings::lstrip, matches.as_deref()),
             RStrip(matches) => map!(strings::rstrip, matches.as_deref()),
             #[cfg(feature = "string_from_radix")]
             FromRadix(radix, strict) => map!(strings::from_radix, radix, strict),
             Slice(start, length) => map!(strings::str_slice, start, length),
+            Explode => map!(strings::explode),
+            #[cfg(feature = "dtype-decimal")]
+            ToDecimal(infer_len) => map!(strings::to_decimal, infer_len),
         }
     }
 }
 
 impl From<BinaryFunction> for SpecialEq<Arc<dyn SeriesUdf>> {
     fn from(func: BinaryFunction) -> Self {
         use BinaryFunction::*;
@@ -571,24 +593,28 @@
             #[cfg(feature = "timezones")]
             CastTimezone(tz, use_earliest) => {
                 map!(datetime::replace_timezone, tz.as_deref(), use_earliest)
             }
             #[cfg(feature = "timezones")]
             TzLocalize(tz) => map!(datetime::tz_localize, &tz),
             Combine(tu) => map_as_slice!(temporal::combine, tu),
-            DateRange {
-                name,
-                every,
-                closed,
-                tz,
-            } => {
+            DateRange { every, closed, tz } => {
                 map_as_slice!(
-                    datetime::date_range_dispatch,
-                    name.as_ref(),
+                    temporal::temporal_range_dispatch,
+                    "date",
                     every,
                     closed,
                     tz.clone()
                 )
             }
+            TimeRange { every, closed } => {
+                map_as_slice!(
+                    temporal::temporal_range_dispatch,
+                    "time",
+                    every,
+                    closed,
+                    None
+                )
+            }
         }
     }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/pow.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/pow.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/schema.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/schema.rs`

 * *Files 7% similar despite different names*

```diff
@@ -56,14 +56,15 @@
                     #[cfg(feature = "timezones")]
                     CastTimezone(tz, _use_earliest) => {
                         return mapper.map_datetime_dtype_timezone(tz.as_ref())
                     }
                     #[cfg(feature = "timezones")]
                     TzLocalize(tz) => return mapper.map_datetime_dtype_timezone(Some(tz)),
                     DateRange { .. } => return mapper.map_to_supertype(),
+                    TimeRange { .. } => DataType::Time,
                     Combine(tu) => match mapper.with_same_dtype().unwrap().dtype {
                         DataType::Datetime(_, tz) => DataType::Datetime(*tu, tz),
                         DataType::Date => DataType::Datetime(*tu, None),
                         dtype => {
                             polars_bail!(ComputeError: "expected Date or Datetime, got {}", dtype)
                         }
                     },
@@ -92,26 +93,23 @@
                     Contains => mapper.with_dtype(DataType::Boolean),
                     Slice => mapper.with_same_dtype(),
                     Get => mapper.map_to_list_inner_dtype(),
                     #[cfg(feature = "list_take")]
                     Take(_) => mapper.with_same_dtype(),
                     #[cfg(feature = "list_count")]
                     CountMatch => mapper.with_dtype(IDX_DTYPE),
-                    Sum => {
-                        let mut first = fields[0].clone();
-                        use DataType::*;
-                        let dt = first.data_type().inner_dtype().cloned().unwrap_or(Unknown);
-
-                        if matches!(dt, UInt8 | Int8 | Int16 | UInt16) {
-                            first.coerce(Int64);
-                        } else {
-                            first.coerce(dt);
-                        }
-                        Ok(first)
-                    }
+                    Sum => mapper.nested_sum_type(),
+                }
+            }
+            #[cfg(feature = "dtype-array")]
+            ArrayExpr(af) => {
+                use ArrayFunction::*;
+                match af {
+                    Min | Max => mapper.with_same_dtype(),
+                    Sum => mapper.nested_sum_type(),
                 }
             }
             #[cfg(feature = "dtype-struct")]
             StructExpr(s) => {
                 use polars_core::utils::slice_offsets;
                 use StructFunction::*;
                 match s {
@@ -183,28 +181,22 @@
                             DataType::UInt8
                         }
                     } else {
                         dt.clone()
                     }
                 })
             }
-            #[cfg(feature = "dot_product")]
-            Dot => mapper.map_dtype(|dt| {
-                use DataType::*;
-                match dt {
-                    Int8 | Int16 | UInt16 | UInt8 => Int64,
-                    _ => dt.clone(),
-                }
-            }),
             #[cfg(feature = "log")]
             Entropy { .. } | Log { .. } | Log1p | Exp => mapper.map_to_float_dtype(),
             Unique(_) => mapper.with_same_dtype(),
             #[cfg(feature = "round_series")]
             Round { .. } | Floor | Ceil => mapper.with_same_dtype(),
             UpperBound | LowerBound => mapper.with_same_dtype(),
+            #[cfg(feature = "fused")]
+            Fused(_) => mapper.map_to_supertype(),
         }
     }
 }
 
 pub(super) struct FieldsMapper<'a> {
     fields: &'a [Field],
 }
@@ -315,8 +307,21 @@
             if let DataType::Datetime(tu, _) = dt {
                 Ok(DataType::Datetime(*tu, tz.cloned()))
             } else {
                 polars_bail!(op = "cast-timezone", got = dt, expected = "Datetime");
             }
         })
     }
+
+    fn nested_sum_type(&self) -> PolarsResult<Field> {
+        let mut first = self.fields[0].clone();
+        use DataType::*;
+        let dt = first.data_type().inner_dtype().cloned().unwrap_or(Unknown);
+
+        if matches!(dt, UInt8 | Int8 | Int16 | UInt16) {
+            first.coerce(Int64);
+        } else {
+            first.coerce(dt);
+        }
+        Ok(first)
+    }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/shift_and_fill.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/shift_and_fill.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/shrink_type.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/shrink_type.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/sign.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/sign.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/strings.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/strings.rs`

 * *Files 5% similar despite different names*

```diff
@@ -39,15 +39,15 @@
     RJust {
         width: usize,
         fillchar: char,
     },
     ExtractAll,
     CountMatch(String),
     #[cfg(feature = "temporal")]
-    Strptime(StrpTimeOptions),
+    Strptime(DataType, StrptimeOptions),
     #[cfg(feature = "concat_str")]
     ConcatVertical(String),
     #[cfg(feature = "concat_str")]
     ConcatHorizontal(String),
     #[cfg(feature = "regex")]
     Replace {
         // negative is replace all
@@ -59,14 +59,17 @@
     Lowercase,
     Strip(Option<String>),
     RStrip(Option<String>),
     LStrip(Option<String>),
     #[cfg(feature = "string_from_radix")]
     FromRadix(u32, bool),
     Slice(i64, Option<u64>),
+    Explode,
+    #[cfg(feature = "dtype-decimal")]
+    ToDecimal(usize),
 }
 
 impl StringFunction {
     pub(super) fn get_field(&self, mapper: FieldsMapper) -> PolarsResult<Field> {
         use StringFunction::*;
         match self {
             #[cfg(feature = "regex")]
@@ -74,31 +77,33 @@
             EndsWith | StartsWith => mapper.with_dtype(DataType::Boolean),
             Extract { .. } => mapper.with_same_dtype(),
             ExtractAll => mapper.with_dtype(DataType::List(Box::new(DataType::Utf8))),
             CountMatch(_) => mapper.with_dtype(DataType::UInt32),
             #[cfg(feature = "string_justify")]
             Zfill { .. } | LJust { .. } | RJust { .. } => mapper.with_same_dtype(),
             #[cfg(feature = "temporal")]
-            Strptime(options) => mapper.with_dtype(options.date_dtype.clone()),
+            Strptime(dtype, _) => mapper.with_dtype(dtype.clone()),
             #[cfg(feature = "concat_str")]
             ConcatVertical(_) | ConcatHorizontal(_) => mapper.with_dtype(DataType::Utf8),
             #[cfg(feature = "regex")]
             Replace { .. } => mapper.with_dtype(DataType::Utf8),
             Uppercase | Lowercase | Strip(_) | LStrip(_) | RStrip(_) | Slice(_, _) => {
                 mapper.with_dtype(DataType::Utf8)
             }
             #[cfg(feature = "string_from_radix")]
             FromRadix { .. } => mapper.with_dtype(DataType::Int32),
+            Explode => mapper.with_same_dtype(),
+            #[cfg(feature = "dtype-decimal")]
+            ToDecimal(_) => mapper.with_dtype(DataType::Decimal(None, None)),
         }
     }
 }
 
 impl Display for StringFunction {
     fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
-        use self::*;
         let s = match self {
             #[cfg(feature = "regex")]
             StringFunction::Contains { .. } => "contains",
             StringFunction::StartsWith { .. } => "starts_with",
             StringFunction::EndsWith { .. } => "ends_with",
             StringFunction::Extract { .. } => "extract",
             #[cfg(feature = "string_justify")]
@@ -106,29 +111,32 @@
             #[cfg(feature = "string_justify")]
             StringFunction::LJust { .. } => "str.ljust",
             #[cfg(feature = "string_justify")]
             StringFunction::RJust { .. } => "rjust",
             StringFunction::ExtractAll => "extract_all",
             StringFunction::CountMatch(_) => "count_match",
             #[cfg(feature = "temporal")]
-            StringFunction::Strptime(_) => "strptime",
+            StringFunction::Strptime(_, _) => "strptime",
             #[cfg(feature = "concat_str")]
             StringFunction::ConcatVertical(_) => "concat_vertical",
             #[cfg(feature = "concat_str")]
             StringFunction::ConcatHorizontal(_) => "concat_horizontal",
             #[cfg(feature = "regex")]
             StringFunction::Replace { .. } => "replace",
             StringFunction::Uppercase => "uppercase",
             StringFunction::Lowercase => "lowercase",
             StringFunction::Strip(_) => "strip",
             StringFunction::LStrip(_) => "lstrip",
             StringFunction::RStrip(_) => "rstrip",
             #[cfg(feature = "string_from_radix")]
             StringFunction::FromRadix { .. } => "from_radix",
             StringFunction::Slice(_, _) => "str_slice",
+            StringFunction::Explode => "explode",
+            #[cfg(feature = "dtype-decimal")]
+            StringFunction::ToDecimal(_) => "to_decimal",
         };
 
         write!(f, "str.{s}")
     }
 }
 
 pub(super) fn uppercase(s: &Series) -> PolarsResult<Series> {
@@ -340,85 +348,125 @@
     let pat = pat.to_string();
 
     let ca = s.utf8()?;
     ca.count_match(&pat).map(|ca| ca.into_series())
 }
 
 #[cfg(feature = "temporal")]
-pub(super) fn strptime(s: &Series, options: &StrpTimeOptions) -> PolarsResult<Series> {
-    let tz_aware = match (options.tz_aware, &options.format) {
-        (true, Some(_)) => true,
-        (true, None) => polars_bail!(
-            ComputeError:
-            "passing 'tz_aware=True' without 'format' is not yet supported, please specify 'format'"
-        ),
-        #[cfg(feature = "timezones")]
-        (false, Some(format)) => TZ_AWARE_RE.is_match(format),
-        (false, _) => false,
-    };
-    let ca = s.utf8()?;
-
-    let out = match &options.date_dtype {
-        DataType::Date => {
-            if options.exact {
-                ca.as_date(options.format.as_deref(), options.cache)?
-                    .into_series()
-            } else {
-                ca.as_date_not_exact(options.format.as_deref())?
-                    .into_series()
-            }
+pub(super) fn strptime(
+    s: &Series,
+    dtype: DataType,
+    options: &StrptimeOptions,
+) -> PolarsResult<Series> {
+    match dtype {
+        DataType::Date => to_date(s, options),
+        DataType::Datetime(time_unit, time_zone) => {
+            to_datetime(s, &time_unit, time_zone.as_ref(), options)
         }
-        DataType::Datetime(tu, tz) => {
-            match (tz, tz_aware, options.utc) {
-                (Some(_), true, _) => polars_bail!(
-                    ComputeError:
-                    "cannot use strptime with both a tz-aware format and a tz-aware dtype, \
-                    please drop time zone from the dtype"
-                ),
-                (Some(_), _, true) => polars_bail!(
-                    ComputeError:
-                    "cannot use strptime with both 'utc=True' and tz-aware dtype, \
-                    please drop time zone from the dtype"
-                ),
-                _ => (),
-            };
-            if options.exact {
-                ca.as_datetime(
-                    options.format.as_deref(),
-                    *tu,
-                    options.cache,
-                    tz_aware,
-                    options.utc,
-                    tz.as_ref(),
-                )?
+        DataType::Time => to_time(s, options),
+        dt => polars_bail!(ComputeError: "not implemented for dtype {}", dt),
+    }
+}
+
+#[cfg(feature = "dtype-date")]
+fn to_date(s: &Series, options: &StrptimeOptions) -> PolarsResult<Series> {
+    let ca = s.utf8()?;
+    let out = {
+        if options.exact {
+            ca.as_date(options.format.as_deref(), options.cache)?
                 .into_series()
-            } else {
-                ca.as_datetime_not_exact(options.format.as_deref(), *tu, tz.as_ref())?
-                    .into_series()
-            }
-        }
-        dt @ DataType::Time => {
-            polars_ensure!(
-                options.exact, ComputeError: "non-exact not implemented for datatype {}", dt,
-            );
-            ca.as_time(options.format.as_deref(), options.cache)?
+        } else {
+            ca.as_date_not_exact(options.format.as_deref())?
                 .into_series()
         }
-        dt => polars_bail!(ComputeError: "not implemented for dtype {}", dt),
     };
+
+    if options.strict {
+        polars_ensure!(
+            out.null_count() == ca.null_count(),
+            ComputeError:
+            "strict conversion to dates failed.\n\
+            \n\
+            You might want to try:\n\
+            - setting `strict=False`\n\
+            - explicitly specifying a `format`"
+        );
+    }
+    Ok(out.into_series())
+}
+
+#[cfg(feature = "dtype-datetime")]
+fn to_datetime(
+    s: &Series,
+    time_unit: &TimeUnit,
+    time_zone: Option<&TimeZone>,
+    options: &StrptimeOptions,
+) -> PolarsResult<Series> {
+    let tz_aware = match &options.format {
+        #[cfg(feature = "timezones")]
+        Some(format) => TZ_AWARE_RE.is_match(format),
+        _ => false,
+    };
+    if let (Some(_), true) = (time_zone, tz_aware) {
+        polars_bail!(
+            ComputeError:
+            "cannot use strptime with both a tz-aware format and a tz-aware dtype, \
+            please drop time zone from the dtype"
+        )
+    };
+
+    let ca = s.utf8()?;
+    let out = if options.exact {
+        ca.as_datetime(
+            options.format.as_deref(),
+            *time_unit,
+            options.cache,
+            tz_aware,
+            time_zone,
+        )?
+        .into_series()
+    } else {
+        ca.as_datetime_not_exact(options.format.as_deref(), *time_unit, time_zone)?
+            .into_series()
+    };
+
     if options.strict {
         polars_ensure!(
             out.null_count() == ca.null_count(),
             ComputeError:
-            "strict conversion to date(time)s failed.\n\
+            "strict conversion to datetimes failed.\n\
             \n\
             You might want to try:\n\
-            - setting `strict=False`,\n\
-            - explicitly specifying a `format`,\n\
-            - setting `utc=True` (if you are parsing datetimes with multiple offsets)."
+            - setting `strict=False`\n\
+            - explicitly specifying a `format`"
+        );
+    }
+    Ok(out.into_series())
+}
+
+#[cfg(feature = "dtype-time")]
+fn to_time(s: &Series, options: &StrptimeOptions) -> PolarsResult<Series> {
+    polars_ensure!(
+        options.exact, ComputeError: "non-exact not implemented for Time data type"
+    );
+
+    let ca = s.utf8()?;
+    let out = ca
+        .as_time(options.format.as_deref(), options.cache)?
+        .into_series();
+
+    if options.strict {
+        polars_ensure!(
+            out.null_count() == ca.null_count(),
+            ComputeError:
+            "strict conversion to times failed.\n\
+            \n\
+            You might want to try:\n\
+            - setting `strict=False`\n\
+            - explicitly specifying a `format`"
         );
     }
     Ok(out.into_series())
 }
 
 #[cfg(feature = "concat_str")]
 pub(super) fn concat(s: &Series, delimiter: &str) -> PolarsResult<Series> {
@@ -598,7 +646,18 @@
     let ca = s.utf8()?;
     ca.parse_int(radix, strict).map(|ok| ok.into_series())
 }
 pub(super) fn str_slice(s: &Series, start: i64, length: Option<u64>) -> PolarsResult<Series> {
     let ca = s.utf8()?;
     ca.str_slice(start, length).map(|ca| ca.into_series())
 }
+
+pub(super) fn explode(s: &Series) -> PolarsResult<Series> {
+    let ca = s.utf8()?;
+    ca.explode()
+}
+
+#[cfg(feature = "dtype-decimal")]
+pub(super) fn to_decimal(s: &Series, infer_len: usize) -> PolarsResult<Series> {
+    let ca = s.utf8()?;
+    ca.to_decimal(infer_len)
+}
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/struct_.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/struct_.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/function_expr/trigonometry.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/function_expr/trigonometry.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/functions.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/functions.rs`

 * *Files 6% similar despite different names*

```diff
@@ -236,31 +236,21 @@
     )
 }
 
 /// Find the indexes that would sort these series in order of appearance.
 /// That means that the first `Series` will be used to determine the ordering
 /// until duplicates are found. Once duplicates are found, the next `Series` will
 /// be used and so on.
+#[cfg(feature = "arange")]
 pub fn arg_sort_by<E: AsRef<[Expr]>>(by: E, descending: &[bool]) -> Expr {
-    let descending = descending.to_vec();
-    let function = SpecialEq::new(Arc::new(move |by: &mut [Series]| {
-        polars_core::functions::arg_sort_by(by, &descending).map(|ca| Some(ca.into_series()))
-    }) as Arc<dyn SeriesUdf>);
-
-    Expr::AnonymousFunction {
-        input: by.as_ref().to_vec(),
-        function,
-        output_type: GetOutput::from_type(IDX_DTYPE),
-        options: FunctionOptions {
-            collect_groups: ApplyOptions::ApplyGroups,
-            input_wildcard_expansion: true,
-            fmt_str: "arg_sort_by",
-            ..Default::default()
-        },
-    }
+    let e = &by.as_ref()[0];
+    let name = expr_output_name(e).unwrap();
+    arange(lit(0 as IdxSize), count().cast(IDX_DTYPE), 1)
+        .sort_by(by, descending)
+        .alias(name.as_ref())
 }
 
 #[cfg(all(feature = "concat_str", feature = "strings"))]
 /// Horizontally concat string columns in linear time
 pub fn concat_str<E: AsRef<[Expr]>>(s: E, separator: &str) -> Expr {
     let input = s.as_ref().to_vec();
     let separator = separator.to_string();
@@ -304,15 +294,15 @@
         }
     }
 
     Ok(concat_str(exprs, ""))
 }
 
 /// Concat lists entries.
-pub fn concat_lst<E: AsRef<[IE]>, IE: Into<Expr> + Clone>(s: E) -> PolarsResult<Expr> {
+pub fn concat_list<E: AsRef<[IE]>, IE: Into<Expr> + Clone>(s: E) -> PolarsResult<Expr> {
     let s: Vec<_> = s.as_ref().iter().map(|e| e.clone().into()).collect();
 
     polars_ensure!(!s.is_empty(), ComputeError: "`concat_list` needs one or more expressions");
 
     Ok(Expr::Function {
         input: s,
         function: FunctionExpr::ListExpr(ListFunction::Concat),
@@ -321,19 +311,50 @@
             input_wildcard_expansion: true,
             fmt_str: "concat_list",
             ..Default::default()
         },
     })
 }
 
+#[cfg(feature = "arange")]
+fn arange_impl<T>(start: T::Native, end: T::Native, step: i64) -> PolarsResult<Option<Series>>
+where
+    T: PolarsNumericType,
+    ChunkedArray<T>: IntoSeries,
+    std::ops::Range<T::Native>: Iterator<Item = T::Native>,
+    std::ops::RangeInclusive<T::Native>: DoubleEndedIterator<Item = T::Native>,
+{
+    let mut ca = match step {
+        1 => ChunkedArray::<T>::from_iter_values("arange", start..end),
+        2.. => ChunkedArray::<T>::from_iter_values("arange", (start..end).step_by(step as usize)),
+        _ => {
+            polars_ensure!(start > end, InvalidOperation: "range must be decreasing if 'step' is negative");
+            ChunkedArray::<T>::from_iter_values(
+                "arange",
+                (end..=start).rev().step_by(step.unsigned_abs() as usize),
+            )
+        }
+    };
+    let is_sorted = if end < start {
+        IsSorted::Descending
+    } else {
+        IsSorted::Ascending
+    };
+    ca.set_sorted_flag(is_sorted);
+    Ok(Some(ca.into_series()))
+}
+
+// TODO! rewrite this with the apply_private architecture
 /// Create list entries that are range arrays
 /// - if `start` and `end` are a column, every element will expand into an array in a list column.
 /// - if `start` and `end` are literals the output will be of `Int64`.
 #[cfg(feature = "arange")]
 pub fn arange(start: Expr, end: Expr, step: i64) -> Expr {
+    let output_name = "arange";
+
     let has_col_without_agg = |e: &Expr| {
         has_expr(e, |ae| matches!(ae, Expr::Column(_)))
             &&
             // check if there is no aggregation
             !has_expr(e, |ae| {
                 matches!(
                     ae,
@@ -363,52 +384,64 @@
     let any_column_no_agg = has_col_without_agg(&start) || has_col_without_agg(&end);
     let literal_start = has_lit(&start);
     let literal_end = has_lit(&end);
 
     if (literal_start || literal_end) && !any_column_no_agg {
         let f = move |sa: Series, sb: Series| {
             polars_ensure!(step != 0, InvalidOperation: "step must not be zero");
-            let sa = sa.cast(&DataType::Int64)?;
-            let sb = sb.cast(&DataType::Int64)?;
-            let start = sa
-                .i64()?
-                .get(0)
-                .ok_or_else(|| polars_err!(NoData: "no data in `start` evaluation"))?;
-            let end = sb
-                .i64()?
-                .get(0)
-                .ok_or_else(|| polars_err!(NoData: "no data in `end` evaluation"))?;
-
-            let mut ca = match step {
-                1 => Int64Chunked::from_iter_values("arange", start..end),
-                2.. => {
-                    Int64Chunked::from_iter_values("arange", (start..end).step_by(step as usize))
+
+            match sa.dtype() {
+                dt if dt == &IDX_DTYPE => {
+                    let start = sa
+                        .idx()?
+                        .get(0)
+                        .ok_or_else(|| polars_err!(NoData: "no data in `start` evaluation"))?;
+                    let sb = sb.cast(&IDX_DTYPE)?;
+                    let end = sb
+                        .idx()?
+                        .get(0)
+                        .ok_or_else(|| polars_err!(NoData: "no data in `end` evaluation"))?;
+                    #[cfg(feature = "bigidx")]
+                    {
+                        arange_impl::<UInt64Type>(start, end, step)
+                    }
+                    #[cfg(not(feature = "bigidx"))]
+                    {
+                        arange_impl::<UInt32Type>(start, end, step)
+                    }
                 }
                 _ => {
-                    polars_ensure!(start > end, InvalidOperation: "range must be decreasing if 'step' is negative");
-                    Int64Chunked::from_iter_values(
-                        "arange",
-                        (end..=start).rev().step_by(step.unsigned_abs() as usize),
-                    )
+                    let sa = sa.cast(&DataType::Int64)?;
+                    let sb = sb.cast(&DataType::Int64)?;
+                    let start = sa
+                        .i64()?
+                        .get(0)
+                        .ok_or_else(|| polars_err!(NoData: "no data in `start` evaluation"))?;
+                    let end = sb
+                        .i64()?
+                        .get(0)
+                        .ok_or_else(|| polars_err!(NoData: "no data in `end` evaluation"))?;
+                    arange_impl::<Int64Type>(start, end, step)
                 }
-            };
-            let is_sorted = if end < start {
-                IsSorted::Descending
-            } else {
-                IsSorted::Ascending
-            };
-            ca.set_sorted_flag(is_sorted);
-            Ok(Some(ca.into_series()))
+            }
         };
         apply_binary(
             start,
             end,
             f,
-            GetOutput::map_field(|_| Field::new("arange", DataType::Int64)),
+            GetOutput::map_field(|input| {
+                let dtype = if input.data_type() == &IDX_DTYPE {
+                    IDX_DTYPE
+                } else {
+                    DataType::Int64
+                };
+                Field::new(output_name, dtype)
+            }),
         )
+        .alias(output_name)
     } else {
         let f = move |sa: Series, sb: Series| {
             polars_ensure!(step != 0, InvalidOperation: "step must not be zero");
             let mut sa = sa.cast(&DataType::Int64)?;
             let mut sb = sb.cast(&DataType::Int64)?;
 
             if sa.len() != sb.len() {
@@ -425,15 +458,15 @@
                     );
                 }
             }
 
             let start = sa.i64()?;
             let end = sb.i64()?;
             let mut builder = ListPrimitiveChunkedBuilder::<Int64Type>::new(
-                "arange",
+                output_name,
                 start.len(),
                 start.len() * 3,
                 DataType::Int64,
             );
 
             for (opt_start, opt_end) in start.into_iter().zip(end.into_iter()) {
                 match (opt_start, opt_end) {
@@ -459,16 +492,19 @@
 
             Ok(Some(builder.finish().into_series()))
         };
         apply_binary(
             start,
             end,
             f,
-            GetOutput::map_field(|_| Field::new("arange", DataType::List(DataType::Int64.into()))),
+            GetOutput::map_field(|_| {
+                Field::new(output_name, DataType::List(DataType::Int64.into()))
+            }),
         )
+        .alias(output_name)
     }
 }
 
 macro_rules! impl_unit_setter {
     ($fn_name:ident($field:ident)) => {
         #[doc = concat!("Set the ", stringify!($field))]
         pub fn $fn_name(mut self, n: Expr) -> Self {
@@ -1266,23 +1302,23 @@
         options
     })
 }
 
 /// Create a column of length `n` containing `n` copies of the literal `value`. Generally you won't need this function,
 /// as `lit(value)` already represents a column containing only `value` whose length is automatically set to the correct
 /// number of rows.
-pub fn repeat<L: Literal>(value: L, n_times: Expr) -> Expr {
+pub fn repeat<L: Literal>(value: L, n: Expr) -> Expr {
     let function = |s: Series, n: Series| {
         let n =
             n.get(0).unwrap().extract::<usize>().ok_or_else(
                 || polars_err!(ComputeError: "could not extract a size from {:?}", n),
             )?;
         Ok(Some(s.new_from_index(0, n)))
     };
-    apply_binary(lit(value), n_times, function, GetOutput::same_type())
+    apply_binary(lit(value), n, function, GetOutput::same_type()).alias("repeat")
 }
 
 #[cfg(feature = "arg_where")]
 /// Get the indices where `condition` evaluates `true`.
 pub fn arg_where<E: Into<Expr>>(condition: E) -> Expr {
     let condition = condition.into();
     Expr::Function {
@@ -1308,34 +1344,110 @@
             cast_to_supertypes: true,
             input_wildcard_expansion: true,
             ..Default::default()
         },
     }
 }
 
-/// Create a date range, named `name`, from a `start` and `stop` expression.
+/// Create a date range from a `start` and `stop` expression.
 #[cfg(feature = "temporal")]
 pub fn date_range(
-    name: String,
     start: Expr,
     end: Expr,
     every: Duration,
     closed: ClosedWindow,
     tz: Option<TimeZone>,
 ) -> Expr {
     let input = vec![start, end];
 
     Expr::Function {
         input,
-        function: FunctionExpr::TemporalExpr(TemporalFunction::DateRange {
-            name,
-            every,
-            closed,
-            tz,
-        }),
+        function: FunctionExpr::TemporalExpr(TemporalFunction::DateRange { every, closed, tz }),
         options: FunctionOptions {
             collect_groups: ApplyOptions::ApplyGroups,
             cast_to_supertypes: true,
+            allow_rename: true,
             ..Default::default()
         },
     }
 }
+
+/// Create a time range from a `start` and `stop` expression.
+#[cfg(feature = "temporal")]
+pub fn time_range(start: Expr, end: Expr, every: Duration, closed: ClosedWindow) -> Expr {
+    let input = vec![start, end];
+
+    Expr::Function {
+        input,
+        function: FunctionExpr::TemporalExpr(TemporalFunction::TimeRange { every, closed }),
+        options: FunctionOptions {
+            collect_groups: ApplyOptions::ApplyGroups,
+            cast_to_supertypes: false,
+            allow_rename: true,
+            ..Default::default()
+        },
+    }
+}
+
+#[cfg(feature = "rolling_window")]
+pub fn rolling_corr(x: Expr, y: Expr, options: RollingCovOptions) -> Expr {
+    let x = x.cache();
+    let y = y.cache();
+    // see: https://github.com/pandas-dev/pandas/blob/v1.5.1/pandas/core/window/rolling.py#L1780-L1804
+    let rolling_options = RollingOptions {
+        window_size: Duration::new(options.window_size as i64),
+        min_periods: options.min_periods as usize,
+        ..Default::default()
+    };
+
+    let mean_x_y = (x.clone() * y.clone()).rolling_mean(rolling_options.clone());
+    let mean_x = x.clone().rolling_mean(rolling_options.clone());
+    let mean_y = y.clone().rolling_mean(rolling_options.clone());
+    let var_x = x.clone().rolling_var(rolling_options.clone());
+    let var_y = y.clone().rolling_var(rolling_options);
+
+    let rolling_options_count = RollingOptions {
+        window_size: Duration::new(options.window_size as i64),
+        min_periods: 0,
+        ..Default::default()
+    };
+    let ddof = options.ddof as f64;
+    let count_x_y = (x + y)
+        .is_not_null()
+        .cast(DataType::Float64)
+        .rolling_sum(rolling_options_count)
+        .cache();
+    let numerator = (mean_x_y - mean_x * mean_y) * (count_x_y.clone() / (count_x_y - lit(ddof)));
+    let denominator = (var_x * var_y).pow(lit(0.5));
+
+    numerator / denominator
+}
+
+#[cfg(feature = "rolling_window")]
+pub fn rolling_cov(x: Expr, y: Expr, options: RollingCovOptions) -> Expr {
+    let x = x.cache();
+    let y = y.cache();
+    // see: https://github.com/pandas-dev/pandas/blob/91111fd99898d9dcaa6bf6bedb662db4108da6e6/pandas/core/window/rolling.py#L1700
+    let rolling_options = RollingOptions {
+        window_size: Duration::new(options.window_size as i64),
+        min_periods: options.min_periods as usize,
+        ..Default::default()
+    };
+
+    let mean_x_y = (x.clone() * y.clone()).rolling_mean(rolling_options.clone());
+    let mean_x = x.clone().rolling_mean(rolling_options.clone());
+    let mean_y = y.clone().rolling_mean(rolling_options);
+    let rolling_options_count = RollingOptions {
+        window_size: Duration::new(options.window_size as i64),
+        min_periods: 0,
+        ..Default::default()
+    };
+    let count_x_y = (x + y)
+        .is_not_null()
+        .cast(DataType::Float64)
+        .rolling_sum(rolling_options_count)
+        .cache();
+
+    let ddof = options.ddof as f64;
+
+    (mean_x_y - mean_x * mean_y) * (count_x_y.clone() / (count_x_y - lit(ddof)))
+}
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/list.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/list.rs`

 * *Files 2% similar despite different names*

```diff
@@ -18,15 +18,15 @@
     pub fn lengths(self) -> Expr {
         let function = |s: Series| {
             let ca = s.list()?;
             Ok(Some(ca.lst_lengths().into_series()))
         };
         self.0
             .map(function, GetOutput::from_type(IDX_DTYPE))
-            .with_fmt("arr.len")
+            .with_fmt("list.len")
     }
 
     /// Compute the maximum of the items in every sublist.
     pub fn max(self) -> Expr {
         self.0
             .map(
                 |s| Ok(Some(s.list()?.lst_max())),
@@ -35,15 +35,15 @@
                         Field::new(f.name(), *adt.clone())
                     } else {
                         // inner type
                         f.clone()
                     }
                 }),
             )
-            .with_fmt("arr.max")
+            .with_fmt("list.max")
     }
 
     /// Compute the minimum of the items in every sublist.
     pub fn min(self) -> Expr {
         self.0
             .map(
                 |s| Ok(Some(s.list()?.lst_min())),
@@ -52,15 +52,15 @@
                         Field::new(f.name(), *adt.clone())
                     } else {
                         // inner type
                         f.clone()
                     }
                 }),
             )
-            .with_fmt("arr.min")
+            .with_fmt("list.min")
     }
 
     /// Compute the sum the items in every sublist.
     pub fn sum(self) -> Expr {
         self.0
             .map_private(FunctionExpr::ListExpr(ListFunction::Sum))
     }
@@ -68,55 +68,55 @@
     /// Compute the mean of every sublist and return a `Series` of dtype `Float64`
     pub fn mean(self) -> Expr {
         self.0
             .map(
                 |s| Ok(Some(s.list()?.lst_mean().into_series())),
                 GetOutput::from_type(DataType::Float64),
             )
-            .with_fmt("arr.mean")
+            .with_fmt("list.mean")
     }
 
     /// Sort every sublist.
     pub fn sort(self, options: SortOptions) -> Expr {
         self.0
             .map(
                 move |s| Ok(Some(s.list()?.lst_sort(options).into_series())),
                 GetOutput::same_type(),
             )
-            .with_fmt("arr.sort")
+            .with_fmt("list.sort")
     }
 
     /// Reverse every sublist
     pub fn reverse(self) -> Expr {
         self.0
             .map(
                 move |s| Ok(Some(s.list()?.lst_reverse().into_series())),
                 GetOutput::same_type(),
             )
-            .with_fmt("arr.reverse")
+            .with_fmt("list.reverse")
     }
 
     /// Keep only the unique values in every sublist.
     pub fn unique(self) -> Expr {
         self.0
             .map(
                 move |s| Ok(Some(s.list()?.lst_unique()?.into_series())),
                 GetOutput::same_type(),
             )
-            .with_fmt("arr.unique")
+            .with_fmt("list.unique")
     }
 
     /// Keep only the unique values in every sublist.
     pub fn unique_stable(self) -> Expr {
         self.0
             .map(
                 move |s| Ok(Some(s.list()?.lst_unique_stable()?.into_series())),
                 GetOutput::same_type(),
             )
-            .with_fmt("arr.unique_stable")
+            .with_fmt("list.unique_stable")
     }
 
     /// Get items in every sublist by index.
     pub fn get(self, index: Expr) -> Expr {
         self.0
             .map_many_private(FunctionExpr::ListExpr(ListFunction::Get), &[index], false)
     }
@@ -155,56 +155,56 @@
                 move |s| {
                     s.list()?
                         .lst_join(&separator)
                         .map(|ca| Some(ca.into_series()))
                 },
                 GetOutput::from_type(DataType::Utf8),
             )
-            .with_fmt("arr.join")
+            .with_fmt("list.join")
     }
 
     /// Return the index of the minimal value of every sublist
     pub fn arg_min(self) -> Expr {
         self.0
             .map(
                 |s| Ok(Some(s.list()?.lst_arg_min().into_series())),
                 GetOutput::from_type(IDX_DTYPE),
             )
-            .with_fmt("arr.arg_min")
+            .with_fmt("list.arg_min")
     }
 
     /// Return the index of the maximum value of every sublist
     pub fn arg_max(self) -> Expr {
         self.0
             .map(
                 |s| Ok(Some(s.list()?.lst_arg_max().into_series())),
                 GetOutput::from_type(IDX_DTYPE),
             )
-            .with_fmt("arr.arg_max")
+            .with_fmt("list.arg_max")
     }
 
     /// Diff every sublist.
     #[cfg(feature = "diff")]
     pub fn diff(self, n: i64, null_behavior: NullBehavior) -> Expr {
         self.0
             .map(
                 move |s| Ok(Some(s.list()?.lst_diff(n, null_behavior)?.into_series())),
                 GetOutput::same_type(),
             )
-            .with_fmt("arr.diff")
+            .with_fmt("list.diff")
     }
 
     /// Shift every sublist.
     pub fn shift(self, periods: i64) -> Expr {
         self.0
             .map(
                 move |s| Ok(Some(s.list()?.lst_shift(periods).into_series())),
                 GetOutput::same_type(),
             )
-            .with_fmt("arr.shift")
+            .with_fmt("list.shift")
     }
 
     /// Slice every sublist.
     pub fn slice(self, offset: Expr, length: Expr) -> Expr {
         self.0.map_many_private(
             FunctionExpr::ListExpr(ListFunction::Slice),
             &[offset, length],
@@ -215,15 +215,15 @@
     /// Get the head of every sublist
     pub fn head(self, n: Expr) -> Expr {
         self.slice(lit(0), n)
     }
 
     /// Get the tail of every sublist
     pub fn tail(self, n: Expr) -> Expr {
-        self.slice(lit(0) - n.clone().cast(DataType::Int64), n)
+        self.slice(lit(0i64) - n.clone().cast(DataType::Int64), n)
     }
 
     #[cfg(feature = "list_to_struct")]
     #[allow(clippy::wrong_self_convention)]
     /// Convert this `List` to a `Series` of type `Struct`. The width will be determined according to
     /// `ListToStructWidthStrategy` and the names of the fields determined by the given `name_generator`.
     ///
@@ -271,15 +271,15 @@
 
                             *lock = Some(dt.clone());
                             dt
                         }
                     }
                 }),
             )
-            .with_fmt("arr.to_struct")
+            .with_fmt("list.to_struct")
     }
 
     #[cfg(feature = "is_in")]
     /// Check if the list array contain an element
     pub fn contains<E: Into<Expr>>(self, other: E) -> Expr {
         let other = other.into();
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/meta.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/meta.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/mod.rs`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,17 @@
 #![allow(ambiguous_glob_reexports)]
 //! Domain specific language for the Lazy API.
 #[cfg(feature = "dtype-categorical")]
 pub mod cat;
 #[cfg(feature = "dtype-categorical")]
 pub use cat::*;
 mod arithmetic;
+mod arity;
+#[cfg(feature = "dtype-array")]
+mod array;
 pub mod binary;
 #[cfg(feature = "temporal")]
 pub mod dt;
 mod expr;
 mod from;
 pub(crate) mod function_expr;
 #[cfg(feature = "compile")]
@@ -22,14 +25,15 @@
 pub mod string;
 #[cfg(feature = "dtype-struct")]
 mod struct_;
 
 use std::fmt::Debug;
 use std::sync::Arc;
 
+pub use arity::*;
 pub use expr::*;
 pub use function_expr::*;
 pub use functions::*;
 pub use list::*;
 pub use options::*;
 use polars_arrow::prelude::QuantileInterpolOptions;
 use polars_core::prelude::*;
@@ -43,154 +47,14 @@
 use crate::constants::MAP_LIST_NAME;
 pub use crate::logical_plan::lit;
 use crate::prelude::*;
 use crate::utils::has_expr;
 #[cfg(feature = "is_in")]
 use crate::utils::has_root_literal_expr;
 
-/// Compute `op(l, r)` (or equivalently `l op r`). `l` and `r` must have types compatible with the Operator.
-pub fn binary_expr(l: Expr, op: Operator, r: Expr) -> Expr {
-    Expr::BinaryExpr {
-        left: Box::new(l),
-        op,
-        right: Box::new(r),
-    }
-}
-
-/// Intermediate state of `when(..).then(..).otherwise(..)` expr.
-#[derive(Clone)]
-pub struct When {
-    predicate: Expr,
-}
-
-/// Intermediate state of `when(..).then(..).otherwise(..)` expr.
-#[derive(Clone)]
-pub struct WhenThen {
-    predicate: Expr,
-    then: Expr,
-}
-
-/// Intermediate state of chain when then exprs.
-///
-/// ```text
-/// when(..).then(..)
-/// when(..).then(..)
-/// when(..).then(..)
-/// .otherwise(..)`
-/// ```
-#[derive(Clone)]
-#[must_use]
-pub struct WhenThenThen {
-    predicates: Vec<Expr>,
-    thens: Vec<Expr>,
-}
-
-impl When {
-    pub fn then<E: Into<Expr>>(self, expr: E) -> WhenThen {
-        WhenThen {
-            predicate: self.predicate,
-            then: expr.into(),
-        }
-    }
-}
-
-impl WhenThen {
-    pub fn when<E: Into<Expr>>(self, predicate: E) -> WhenThenThen {
-        WhenThenThen {
-            predicates: vec![self.predicate, predicate.into()],
-            thens: vec![self.then],
-        }
-    }
-
-    pub fn otherwise<E: Into<Expr>>(self, expr: E) -> Expr {
-        Expr::Ternary {
-            predicate: Box::new(self.predicate),
-            truthy: Box::new(self.then),
-            falsy: Box::new(expr.into()),
-        }
-    }
-}
-
-impl WhenThenThen {
-    pub fn then(mut self, expr: Expr) -> Self {
-        self.thens.push(expr);
-        self
-    }
-
-    pub fn when(mut self, predicate: Expr) -> Self {
-        self.predicates.push(predicate);
-        self
-    }
-
-    pub fn otherwise(self, expr: Expr) -> Expr {
-        // we iterate the preds/ exprs last in first out
-        // and nest them.
-        //
-        // // this expr:
-        //   when((col('x') == 'a')).then(1)
-        //         .when(col('x') == 'a').then(2)
-        //         .when(col('x') == 'b').then(3)
-        //         .otherwise(4)
-        //
-        // needs to become:
-        //       when((col('x') == 'a')).then(1)                        -
-        //         .otherwise(                                           |
-        //             when(col('x') == 'a').then(2)            -        |
-        //             .otherwise(                               |       |
-        //                 pl.when(col('x') == 'b').then(3)      |       |
-        //                 .otherwise(4)                         | inner | outer
-        //             )                                         |       |
-        //         )                                            _|      _|
-        //
-        // by iterating lifo we first create
-        // `inner` and then assign that to `otherwise`,
-        // which will be used in the next layer `outer`
-        //
-
-        let pred_iter = self.predicates.into_iter().rev();
-        let mut then_iter = self.thens.into_iter().rev();
-
-        let mut otherwise = expr;
-
-        for e in pred_iter {
-            otherwise = Expr::Ternary {
-                predicate: Box::new(e),
-                truthy: Box::new(
-                    then_iter
-                        .next()
-                        .expect("expr expected, did you call when().then().otherwise?"),
-                ),
-                falsy: Box::new(otherwise),
-            }
-        }
-        if then_iter.next().is_some() {
-            panic!(
-                "this expr is not properly constructed. \
-            Every `when` should have an accompanied `then` call."
-            )
-        }
-        otherwise
-    }
-}
-
-/// Start a when-then-otherwise expression
-pub fn when<E: Into<Expr>>(predicate: E) -> When {
-    When {
-        predicate: predicate.into(),
-    }
-}
-
-pub fn ternary_expr(predicate: Expr, truthy: Expr, falsy: Expr) -> Expr {
-    Expr::Ternary {
-        predicate: Box::new(predicate),
-        truthy: Box::new(truthy),
-        falsy: Box::new(falsy),
-    }
-}
-
 impl Expr {
     /// Modify the Options passed to the `Function` node.
     pub(crate) fn with_function_options<F>(self, func: F) -> Expr
     where
         F: Fn(FunctionOptions) -> FunctionOptions,
     {
         match self {
@@ -237,19 +101,29 @@
     }
 
     /// Compare `Expr` with other `Expr` on equality
     pub fn eq<E: Into<Expr>>(self, other: E) -> Expr {
         binary_expr(self, Operator::Eq, other.into())
     }
 
+    /// Compare `Expr` with other `Expr` on equality where `None == None`
+    pub fn eq_missing<E: Into<Expr>>(self, other: E) -> Expr {
+        binary_expr(self, Operator::EqValidity, other.into())
+    }
+
     /// Compare `Expr` with other `Expr` on non-equality
     pub fn neq<E: Into<Expr>>(self, other: E) -> Expr {
         binary_expr(self, Operator::NotEq, other.into())
     }
 
+    /// Compare `Expr` with other `Expr` on non-equality where `None == None`
+    pub fn neq_missing<E: Into<Expr>>(self, other: E) -> Expr {
+        binary_expr(self, Operator::NotEqValidity, other.into())
+    }
+
     /// Check if `Expr` < `Expr`
     pub fn lt<E: Into<Expr>>(self, other: E) -> Expr {
         binary_expr(self, Operator::Lt, other.into())
     }
 
     /// Check if `Expr` > `Expr`
     pub fn gt<E: Into<Expr>>(self, other: E) -> Expr {
@@ -388,42 +262,15 @@
     /// Alias for explode
     pub fn flatten(self) -> Self {
         self.explode()
     }
 
     /// Explode the utf8/ list column
     pub fn explode(self) -> Self {
-        let has_filter = has_expr(&self, |e| matches!(e, Expr::Filter { .. }));
-
-        // if we explode right after a window function we don't self join, but just flatten
-        // the expression
-        if let Expr::Window {
-            function,
-            partition_by,
-            order_by,
-            mut options,
-        } = self
-        {
-            if has_filter {
-                panic!("A Filter of a window function is not allowed in combination with explode/flatten.\
-                The resulting column may not fit the DataFrame/ or the groups
-                ")
-            }
-
-            options.explode = true;
-
-            Expr::Explode(Box::new(Expr::Window {
-                function,
-                partition_by,
-                order_by,
-                options,
-            }))
-        } else {
-            Expr::Explode(Box::new(self))
-        }
+        Expr::Explode(Box::new(self))
     }
 
     /// Slice the Series.
     /// `offset` may be negative.
     pub fn slice<E: Into<Expr>, F: Into<Expr>>(self, offset: E, length: F) -> Self {
         Expr::Slice {
             input: Box::new(self),
@@ -651,34 +498,26 @@
 
         Expr::AnonymousFunction {
             input: vec![self],
             function: SpecialEq::new(Arc::new(f)),
             output_type,
             options: FunctionOptions {
                 collect_groups: ApplyOptions::ApplyFlat,
-                input_wildcard_expansion: false,
-                auto_explode: false,
                 fmt_str: "map",
-                cast_to_supertypes: false,
-                allow_rename: false,
-                pass_name_to_apply: false,
+                ..Default::default()
             },
         }
     }
 
     fn map_private(self, function_expr: FunctionExpr) -> Self {
         Expr::Function {
             input: vec![self],
             function: function_expr,
             options: FunctionOptions {
                 collect_groups: ApplyOptions::ApplyFlat,
-                input_wildcard_expansion: false,
-                auto_explode: false,
-                cast_to_supertypes: false,
-                allow_rename: false,
                 ..Default::default()
             },
         }
     }
 
     /// Apply a function/closure once the logical plan get executed with many arguments
     ///
@@ -1055,24 +894,32 @@
     ///  2       13     
     ///  3       15     
     ///  3       15     
     ///  1       16     
     /// 
     /// ```
     pub fn over<E: AsRef<[IE]>, IE: Into<Expr> + Clone>(self, partition_by: E) -> Self {
+        self.over_with_options(partition_by, Default::default())
+    }
+
+    pub fn over_with_options<E: AsRef<[IE]>, IE: Into<Expr> + Clone>(
+        self,
+        partition_by: E,
+        options: WindowOptions,
+    ) -> Self {
         let partition_by = partition_by
             .as_ref()
             .iter()
             .map(|e| e.clone().into())
             .collect();
         Expr::Window {
             function: Box::new(self),
             partition_by,
             order_by: None,
-            options: WindowOptions { explode: false },
+            options,
         }
     }
 
     fn fill_null_impl(self, fill_value: Expr) -> Self {
         let input = vec![self, fill_value];
 
         Expr::Function {
@@ -1216,15 +1063,15 @@
 
     #[cfg(feature = "repeat_by")]
     fn repeat_by_impl(self, by: Expr) -> Expr {
         let function = |s: &mut [Series]| {
             let by = &s[1];
             let s = &s[0];
             let by = by.cast(&IDX_DTYPE)?;
-            Ok(Some(s.repeat_by(by.idx()?).into_series()))
+            Ok(Some(s.repeat_by(by.idx()?)?.into_series()))
         };
 
         self.apply_many(
             function,
             &[by],
             GetOutput::map_dtype(|dt| DataType::List(dt.clone().into())),
         )
@@ -1241,20 +1088,18 @@
     #[cfg(feature = "is_first")]
     #[allow(clippy::wrong_self_convention)]
     /// Get a mask of the first unique value.
     pub fn is_first(self) -> Expr {
         self.apply_private(BooleanFunction::IsFirst.into())
     }
 
-    #[cfg(feature = "dot_product")]
     fn dot_impl(self, other: Expr) -> Expr {
-        self.apply_many_private(FunctionExpr::Dot, &[other], true, true)
+        (self * other).sum()
     }
 
-    #[cfg(feature = "dot_product")]
     pub fn dot<E: Into<Expr>>(self, other: E) -> Expr {
         self.dot_impl(other.into())
     }
 
     #[cfg(feature = "mode")]
     /// Compute the mode(s) of this column. This is the most occurring value.
     pub fn mode(self) -> Expr {
@@ -1881,14 +1726,28 @@
                 s.set_sorted_flag(sorted);
                 Ok(Some(s))
             },
             GetOutput::same_type(),
         )
     }
 
+    /// Cache this expression, so that it is executed only once per context.
+    pub fn cache(self) -> Expr {
+        match &self {
+            // don't cache cheap no-ops
+            Expr::Column(_) => self,
+            Expr::Alias(input, _) if matches!(**input, Expr::Column(_)) => self,
+            _ => {
+                let input = Box::new(self);
+                let id = input.as_ref() as *const Expr as usize;
+                Self::Cache { input, id }
+            }
+        }
+    }
+
     #[cfg(feature = "row_hash")]
     /// Compute the hash of every element
     pub fn hash(self, k0: u64, k1: u64, k2: u64, k3: u64) -> Expr {
         self.map_private(FunctionExpr::Hash(k0, k1, k2, k3))
     }
 
     #[cfg(feature = "strings")]
@@ -1900,25 +1759,38 @@
         binary::BinaryNameSpace(self)
     }
 
     #[cfg(feature = "temporal")]
     pub fn dt(self) -> dt::DateLikeNameSpace {
         dt::DateLikeNameSpace(self)
     }
-    pub fn arr(self) -> list::ListNameSpace {
+
+    pub fn list(self) -> list::ListNameSpace {
         list::ListNameSpace(self)
     }
+
+    /// Get the [`ArrayNameSpace`]
+    #[cfg(feature = "dtype-array")]
+    pub fn arr(self) -> array::ArrayNameSpace {
+        array::ArrayNameSpace(self)
+    }
+
+    /// Get the [`CategoricalNameSpace`]
     #[cfg(feature = "dtype-categorical")]
     pub fn cat(self) -> cat::CategoricalNameSpace {
         cat::CategoricalNameSpace(self)
     }
+
+    /// Get the [`StructNameSpace`]
     #[cfg(feature = "dtype-struct")]
     pub fn struct_(self) -> struct_::StructNameSpace {
         struct_::StructNameSpace(self)
     }
+
+    /// Get the [`MetaNameSpace`]
     #[cfg(feature = "meta")]
     pub fn meta(self) -> meta::MetaNameSpace {
         meta::MetaNameSpace(self)
     }
 }
 
 /// Apply a function/closure over multiple columns once the logical plan get executed.
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/options.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/options.rs`

 * *Files 12% similar despite different names*

```diff
@@ -1,44 +1,43 @@
 use std::borrow::Cow;
 
-use polars_core::datatypes::DataType;
-use polars_core::prelude::{JoinType, TimeUnit};
+use polars_core::prelude::JoinType;
+use polars_utils::IdxSize;
 #[cfg(feature = "serde")]
 use serde::{Deserialize, Serialize};
 
+#[derive(Copy, Clone, PartialEq, Debug, Eq, Hash)]
+#[cfg_attr(feature = "serde", derive(Serialize, Deserialize))]
+pub struct RollingCovOptions {
+    pub window_size: IdxSize,
+    pub min_periods: IdxSize,
+    pub ddof: u8,
+}
+
 #[derive(Clone, PartialEq, Debug, Eq, Hash)]
 #[cfg_attr(feature = "serde", derive(Serialize, Deserialize))]
-pub struct StrpTimeOptions {
-    /// DataType to parse in. One of {Date, Datetime}
-    pub date_dtype: DataType,
+pub struct StrptimeOptions {
     /// Formatting string
     pub format: Option<String>,
     /// If set then polars will return an error if any date parsing fails
     pub strict: bool,
     /// If polars may parse matches that not contain the whole string
     /// e.g. "foo-2021-01-01-bar" could match "2021-01-01"
     pub exact: bool,
     /// use a cache of unique, converted dates to apply the datetime conversion.
     pub cache: bool,
-    /// Parse a timezone aware timestamp
-    pub tz_aware: bool,
-    /// Convert timezone aware to UTC
-    pub utc: bool,
 }
 
-impl Default for StrpTimeOptions {
+impl Default for StrptimeOptions {
     fn default() -> Self {
-        StrpTimeOptions {
-            date_dtype: DataType::Datetime(TimeUnit::Microseconds, None),
+        StrptimeOptions {
             format: None,
-            strict: false,
-            exact: false,
+            strict: true,
+            exact: true,
             cache: true,
-            tz_aware: false,
-            utc: false,
         }
     }
 }
 
 #[derive(Clone, Debug, PartialEq, Eq)]
 #[cfg_attr(feature = "serde", derive(Serialize, Deserialize))]
 pub struct JoinOptions {
@@ -62,7 +61,29 @@
             suffix: "_right".into(),
             slice: None,
             rows_left: (None, usize::MAX),
             rows_right: (None, usize::MAX),
         }
     }
 }
+
+#[derive(Copy, Clone, Debug, PartialEq, Eq, Default)]
+#[cfg_attr(feature = "serde", derive(Serialize, Deserialize))]
+pub struct WindowOptions {
+    /// Explode the aggregated list and just do a hstack instead of a join
+    /// this requires the groups to be sorted to make any sense
+    pub mapping: WindowMapping,
+}
+
+#[derive(Copy, Clone, Debug, PartialEq, Eq, Default)]
+#[cfg_attr(feature = "serde", derive(Serialize, Deserialize))]
+pub enum WindowMapping {
+    /// Map the group values to the position
+    #[default]
+    GroupsToRows,
+    /// Explode the aggregated list and just do a hstack instead of a join
+    /// this requires the groups to be sorted to make any sense
+    Explode,
+    /// Join the groups as 'List<group_dtype>' to the row positions.
+    /// warning: this can be memory intensive
+    Join,
+}
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/string.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/string.rs`

 * *Files 8% similar despite different names*

```diff
@@ -97,18 +97,68 @@
 
     /// Count all successive non-overlapping regex matches.
     pub fn count_match(self, pat: &str) -> Expr {
         let pat = pat.to_string();
         self.0.map_private(StringFunction::CountMatch(pat).into())
     }
 
-    /// Construct a `Datetime` column by parsing this string column as datetimes, using the provided `options`.
+    /// Convert a Utf8 column into a Date/Datetime/Time column.
     #[cfg(feature = "temporal")]
-    pub fn strptime(self, options: StrpTimeOptions) -> Expr {
-        self.0.map_private(StringFunction::Strptime(options).into())
+    pub fn strptime(self, dtype: DataType, options: StrptimeOptions) -> Expr {
+        self.0
+            .map_private(StringFunction::Strptime(dtype, options).into())
+    }
+
+    /// Convert a Utf8 column into a Date column.
+    #[cfg(feature = "dtype-date")]
+    pub fn to_date(self, options: StrptimeOptions) -> Expr {
+        self.strptime(DataType::Date, options)
+    }
+
+    /// Convert a Utf8 column into a Datetime column.
+    #[cfg(feature = "dtype-datetime")]
+    pub fn to_datetime(
+        self,
+        time_unit: Option<TimeUnit>,
+        time_zone: Option<TimeZone>,
+        options: StrptimeOptions,
+    ) -> Expr {
+        // If time_unit is None, try to infer it from the format or set a default
+        let time_unit = match (&options.format, time_unit) {
+            (_, Some(time_unit)) => time_unit,
+            (Some(format), None) => {
+                if format.contains("%.9f")
+                    || format.contains("%9f")
+                    || format.contains("%f")
+                    || format.contains("%.f")
+                {
+                    TimeUnit::Nanoseconds
+                } else if format.contains("%.3f") || format.contains("%3f") {
+                    TimeUnit::Milliseconds
+                } else {
+                    TimeUnit::Microseconds
+                }
+            }
+            (None, None) => TimeUnit::Microseconds,
+        };
+
+        self.strptime(DataType::Datetime(time_unit, time_zone), options)
+    }
+
+    /// Convert a Utf8 column into a Time column.
+    #[cfg(feature = "dtype-time")]
+    pub fn to_time(self, options: StrptimeOptions) -> Expr {
+        self.strptime(DataType::Time, options)
+    }
+
+    /// Convert a Utf8 column into a Decimal column.
+    #[cfg(feature = "dtype-decimal")]
+    pub fn to_decimal(self, infer_length: usize) -> Expr {
+        self.0
+            .map_private(StringFunction::ToDecimal(infer_length).into())
     }
 
     /// Concat the values into a string array.
     /// # Arguments
     ///
     /// * `delimiter` - A string that will act as delimiter between values.
     #[cfg(feature = "concat_str")]
@@ -410,8 +460,13 @@
     /// Slice the string values.
     pub fn str_slice(self, start: i64, length: Option<u64>) -> Expr {
         self.0
             .map_private(FunctionExpr::StringExpr(StringFunction::Slice(
                 start, length,
             )))
     }
+
+    pub fn explode(self) -> Expr {
+        self.0
+            .apply_private(FunctionExpr::StringExpr(StringFunction::Explode))
+    }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/dsl/struct_.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/dsl/struct_.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/frame/opt_state.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/frame/opt_state.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/aexpr/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/aexpr/mod.rs`

 * *Files 22% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 mod schema;
 
 use std::sync::Arc;
 
 use polars_arrow::prelude::QuantileInterpolOptions;
+use polars_core::frame::groupby::GroupByMethod;
 use polars_core::prelude::*;
 use polars_core::utils::{get_time_units, try_get_supertype};
 use polars_utils::arena::{Arena, Node};
 
 use crate::dsl::function_expr::FunctionExpr;
 use crate::logical_plan::Context;
 use crate::prelude::aexpr::NodeInputs::Single;
@@ -37,14 +38,48 @@
     Sum(Node),
     Count(Node),
     Std(Node, u8),
     Var(Node, u8),
     AggGroups(Node),
 }
 
+impl From<AAggExpr> for GroupByMethod {
+    fn from(value: AAggExpr) -> Self {
+        use AAggExpr::*;
+        match value {
+            Min { propagate_nans, .. } => {
+                if propagate_nans {
+                    GroupByMethod::NanMin
+                } else {
+                    GroupByMethod::Min
+                }
+            }
+            Max { propagate_nans, .. } => {
+                if propagate_nans {
+                    GroupByMethod::NanMax
+                } else {
+                    GroupByMethod::Max
+                }
+            }
+            Median(_) => GroupByMethod::Median,
+            NUnique(_) => GroupByMethod::NUnique,
+            First(_) => GroupByMethod::First,
+            Last(_) => GroupByMethod::Last,
+            Mean(_) => GroupByMethod::Mean,
+            Implode(_) => GroupByMethod::Implode,
+            Sum(_) => GroupByMethod::Sum,
+            Count(_) => GroupByMethod::Count,
+            Std(_, ddof) => GroupByMethod::Std(ddof),
+            Var(_, ddof) => GroupByMethod::Var(ddof),
+            AggGroups(_) => GroupByMethod::Groups,
+            Quantile { .. } => unreachable!(),
+        }
+    }
+}
+
 // AExpr representation of Nodes which are allocated in an Arena
 #[derive(Clone, Debug, Default)]
 pub enum AExpr {
     Explode(Node),
     Alias(Node, Arc<str>),
     Column(Arc<str>),
     Literal(LiteralValue),
@@ -105,14 +140,18 @@
     Slice {
         input: Node,
         offset: Node,
         length: Node,
     },
     Count,
     Nth(i64),
+    Cache {
+        input: Node,
+        id: usize,
+    },
 }
 
 impl AExpr {
     /// Any expression that is sensitive to the number of elements in a group
     /// - Aggregations
     /// - Sorts
     /// - Counts
@@ -138,14 +177,15 @@
             | Literal(_)
             // a caller should traverse binary and ternary
             // to determine if the whole expr. is group sensitive
             | BinaryExpr { .. }
             | Ternary { .. }
             | Wildcard
             | Cast { .. }
+            | Cache{..}
             | Filter { .. } => false,
         }
     }
 
     /// This should be a 1 on 1 copy of the get_type method of Expr until Expr is completely phased out.
     pub fn get_type(
         &self,
@@ -214,22 +254,29 @@
                     out.push(*a);
                 }
                 out.extend(partition_by);
                 Many(out)
             }
             Wildcard => panic!("no wildcard expected"),
             Slice { input, .. } => Single(*input),
+            Cache { input, .. } => Single(*input),
             Count => Leaf,
             Nth(_) => Leaf,
         }
     }
+    pub(crate) fn is_leaf(&self) -> bool {
+        matches!(
+            self,
+            AExpr::Column(_) | AExpr::Literal(_) | AExpr::Count | AExpr::Nth(_)
+        )
+    }
 }
 
 impl AAggExpr {
-    pub(crate) fn get_input(&self) -> NodeInputs {
+    pub fn get_input(&self) -> NodeInputs {
         use AAggExpr::*;
         match self {
             Min { input, .. } => Single(*input),
             Max { input, .. } => Single(*input),
             Median(input) => Single(*input),
             NUnique(input) => Single(*input),
             First(input) => Single(*input),
@@ -242,22 +289,22 @@
             Std(input, _) => Single(*input),
             Var(input, _) => Single(*input),
             AggGroups(input) => Single(*input),
         }
     }
 }
 
-pub(crate) enum NodeInputs {
+pub enum NodeInputs {
     Leaf,
     Single(Node),
     Many(Vec<Node>),
 }
 
 impl NodeInputs {
-    pub(crate) fn first(&self) -> Node {
+    pub fn first(&self) -> Node {
         match self {
             Single(node) => *node,
             NodeInputs::Many(nodes) => nodes[0],
             NodeInputs::Leaf => panic!(),
         }
     }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/aexpr/schema.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/aexpr/schema.rs`

 * *Files 1% similar despite different names*

```diff
@@ -135,26 +135,26 @@
                             arena.get(*expr).to_field(schema, Context::Default, arena)?;
                         float_type(&mut field);
                         Ok(field)
                     }
                     NUnique(expr) => {
                         let mut field =
                             arena.get(*expr).to_field(schema, Context::Default, arena)?;
-                        field.coerce(DataType::UInt32);
+                        field.coerce(IDX_DTYPE);
                         Ok(field)
                     }
                     Count(expr) => {
                         let mut field =
                             arena.get(*expr).to_field(schema, Context::Default, arena)?;
                         field.coerce(IDX_DTYPE);
                         Ok(field)
                     }
                     AggGroups(expr) => {
                         let mut field = arena.get(*expr).to_field(schema, ctxt, arena)?;
-                        field.coerce(DataType::List(IDX_DTYPE.into()));
+                        field.coerce(List(IDX_DTYPE.into()));
                         Ok(field)
                     }
                     Quantile { expr, .. } => {
                         let mut field =
                             arena.get(*expr).to_field(schema, Context::Default, arena)?;
                         float_type(&mut field);
                         Ok(field)
@@ -196,14 +196,15 @@
                     .iter()
                     // default context because `col()` would return a list in aggregation context
                     .map(|node| arena.get(*node).to_field(schema, Context::Default, arena))
                     .collect::<PolarsResult<Vec<_>>>()?;
                 function.get_field(schema, ctxt, &fields)
             }
             Slice { input, .. } => arena.get(*input).to_field(schema, ctxt, arena),
+            Cache { input, .. } => arena.get(*input).to_field(schema, ctxt, arena),
             Wildcard => panic!("should be no wildcard at this point"),
             Nth(_) => panic!("should be no nth at this point"),
         }
     }
 }
 
 fn get_arithmetic_field(
@@ -234,14 +235,20 @@
             match (&left_field.dtype, right_type) {
                 // T - T != T if T is a datetime / date
                 (Datetime(tul, _), Datetime(tur, _)) => Duration(get_time_units(tul, &tur)),
                 (Date, Date) => Duration(TimeUnit::Milliseconds),
                 (left, right) => try_get_supertype(left, &right)?,
             }
         }
+        Operator::Plus
+            if left_field.dtype == Boolean
+                && right_ae.get_type(schema, Context::Default, arena)? == Boolean =>
+        {
+            IDX_DTYPE
+        }
         _ => {
             match (left_ae, right_ae) {
                 (AExpr::Literal(_), AExpr::Literal(_)) => {}
                 (AExpr::Literal(_), _) => {
                     // literal will be coerced to match right type
                     let right_type = right_ae.get_type(schema, ctxt, arena)?;
                     left_field.coerce(right_type);
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/alp.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/alp.rs`

 * *Files 1% similar despite different names*

```diff
@@ -601,15 +601,16 @@
         let mut inputs = Vec::new();
         self.copy_inputs(&mut inputs);
         inputs
     }
     /// panics if more than one input
     #[cfg(any(
         all(feature = "strings", feature = "concat_str"),
-        feature = "streaming"
+        feature = "streaming",
+        feature = "fused"
     ))]
     pub(crate) fn get_input(&self) -> Option<Node> {
         let mut inputs = [None, None];
         self.copy_inputs(&mut inputs);
         inputs[0]
     }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/anonymous_scan.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/anonymous_scan.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/apply.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/apply.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/builder.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/builder.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/conversion.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/conversion.rs`

 * *Files 0% similar despite different names*

```diff
@@ -141,14 +141,18 @@
             offset,
             length,
         } => AExpr::Slice {
             input: to_aexpr(*input, arena),
             offset: to_aexpr(*offset, arena),
             length: to_aexpr(*length, arena),
         },
+        Expr::Cache { input, id } => AExpr::Cache {
+            input: to_aexpr(*input, arena),
+            id,
+        },
         Expr::Wildcard => AExpr::Wildcard,
         Expr::Count => AExpr::Count,
         Expr::Nth(i) => AExpr::Nth(i),
         Expr::KeepName(_) => panic!("no keep_name expected at this point"),
         Expr::Exclude(_, _) => panic!("no exclude expected at this point"),
         Expr::RenameAlias { .. } => panic!("no `rename_alias` expected at this point"),
         Expr::Columns { .. } => panic!("no `columns` expected at this point"),
@@ -618,14 +622,18 @@
             offset,
             length,
         } => Expr::Slice {
             input: Box::new(node_to_expr(input, expr_arena)),
             offset: Box::new(node_to_expr(offset, expr_arena)),
             length: Box::new(node_to_expr(length, expr_arena)),
         },
+        AExpr::Cache { input, id } => Expr::Cache {
+            input: Box::new(node_to_expr(input, expr_arena)),
+            id,
+        },
         AExpr::Count => Expr::Count,
         AExpr::Nth(i) => Expr::Nth(i),
         AExpr::Wildcard => Expr::Wildcard,
     }
 }
 
 fn nodes_to_exprs(nodes: &[Node], expr_arena: &Arena<AExpr>) -> Vec<Expr> {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/format.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/format.rs`

 * *Files 14% similar despite different names*

```diff
@@ -1,22 +1,24 @@
 use std::borrow::Cow;
 use std::fmt;
-use std::fmt::{Debug, Display, Formatter};
+use std::fmt::{Debug, Display, Formatter, Write};
 use std::path::Path;
 
 use crate::prelude::*;
 
+#[allow(clippy::too_many_arguments)]
 fn write_scan<P: Display>(
     f: &mut Formatter,
     name: &str,
     path: &Path,
     indent: usize,
     n_columns: i64,
     total_columns: usize,
     predicate: &Option<P>,
+    n_rows: Option<usize>,
 ) -> fmt::Result {
     if indent != 0 {
         writeln!(f)?;
     }
     write!(f, "{:indent$}{} SCAN {}", "", name, path.display())?;
     if n_columns > 0 {
         write!(
@@ -26,14 +28,17 @@
         )?;
     } else {
         write!(f, "\n{:indent$}PROJECT */{total_columns} COLUMNS", "",)?;
     }
     if let Some(predicate) = predicate {
         write!(f, "\n{:indent$}SELECTION: {predicate}", "")?;
     }
+    if let Some(n_rows) = n_rows {
+        write!(f, "\n{:indent$}N_ROWS: {n_rows}", "")?;
+    }
     Ok(())
 }
 
 impl LogicalPlan {
     fn _format(&self, f: &mut Formatter, indent: usize) -> fmt::Result {
         if indent != 0 {
             writeln!(f)?;
@@ -54,14 +59,15 @@
                     f,
                     "PYTHON",
                     Path::new(""),
                     sub_indent,
                     n_columns,
                     total_columns,
                     &options.predicate,
+                    options.n_rows,
                 )
             }
             AnonymousScan {
                 file_info,
                 predicate,
                 options,
                 ..
@@ -75,23 +81,31 @@
                     f,
                     options.fmt_str,
                     Path::new(""),
                     sub_indent,
                     n_columns,
                     file_info.schema.len(),
                     predicate,
+                    options.n_rows,
                 )
             }
-            Union { inputs, .. } => {
-                write!(f, "{:indent$}UNION:", "")?;
+            Union { inputs, options } => {
+                let mut name = String::new();
+                let name = if let Some(slice) = options.slice {
+                    write!(name, "SLICED UNION: {:?}", slice)?;
+                    name.as_str()
+                } else {
+                    "UNION"
+                };
+                write!(f, "{:indent$}{}", "", name)?;
                 for (i, plan) in inputs.iter().enumerate() {
                     write!(f, "\n{:indent$}PLAN {i}:", "")?;
                     plan._format(f, sub_indent)?;
                 }
-                write!(f, "\n{:indent$}END UNION", "")
+                write!(f, "\n{:indent$}END {}", "", name)
             }
             Cache { input, id, count } => {
                 write!(f, "{:indent$}CACHE[id: {:x}, count: {}]", "", *id, *count)?;
                 input._format(f, sub_indent)
             }
             #[cfg(feature = "parquet")]
             ParquetScan {
@@ -110,14 +124,15 @@
                     f,
                     "PARQUET",
                     path,
                     sub_indent,
                     n_columns,
                     file_info.schema.len(),
                     predicate,
+                    options.n_rows,
                 )
             }
             #[cfg(feature = "ipc")]
             IpcScan {
                 path,
                 file_info,
                 options,
@@ -133,14 +148,15 @@
                     f,
                     "IPC",
                     path,
                     sub_indent,
                     n_columns,
                     file_info.schema.len(),
                     predicate,
+                    options.n_rows,
                 )
             }
             Selection { predicate, input } => {
                 write!(f, "{:indent$}FILTER {predicate:?} FROM", "")?;
                 input._format(f, indent)
             }
             #[cfg(feature = "csv")]
@@ -160,14 +176,15 @@
                     f,
                     "CSV",
                     path,
                     sub_indent,
                     n_columns,
                     file_info.schema.len(),
                     predicate,
+                    options.n_rows,
                 )
             }
             DataFrameScan {
                 schema,
                 projection,
                 selection,
                 ..
@@ -395,40 +412,22 @@
             } => write!(f, "{input:?}.slice(offset={offset:?}, length={length:?})",),
             Wildcard => write!(f, "*"),
             Exclude(column, names) => write!(f, "{column:?}, EXCEPT {names:?}"),
             KeepName(e) => write!(f, "KEEP NAME {e:?}"),
             RenameAlias { expr, .. } => write!(f, "RENAME_ALIAS {expr:?}"),
             Columns(names) => write!(f, "COLUMNS({names:?})"),
             DtypeColumn(dt) => write!(f, "COLUMN OF DTYPE: {dt:?}"),
+            Cache { input, .. } => write!(f, "CACHE {input:?}"),
         }
     }
 }
 
 impl Debug for Operator {
     fn fmt(&self, f: &mut Formatter<'_>) -> fmt::Result {
-        use Operator::*;
-        let s = match self {
-            Eq => "==",
-            NotEq => "!=",
-            Lt => "<",
-            LtEq => "<=",
-            Gt => ">",
-            GtEq => ">=",
-            Plus => "+",
-            Minus => "-",
-            Multiply => "*",
-            Divide => "/",
-            TrueDivide => "/",
-            FloorDivide => "//",
-            Modulus => "%",
-            And => "&",
-            Or => "|",
-            Xor => "^",
-        };
-        write!(f, "{s}")
+        Display::fmt(self, f)
     }
 }
 
 impl Debug for LiteralValue {
     fn fmt(&self, f: &mut Formatter<'_>) -> fmt::Result {
         use LiteralValue::*;
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/functions/drop.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/functions/drop.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/functions/merge_sorted.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/functions/merge_sorted.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/functions/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/functions/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/functions/rename.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/functions/rename.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/iterator.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/iterator.rs`

 * *Files 1% similar despite different names*

```diff
@@ -85,14 +85,15 @@
                 $push(length);
                 $push(offset);
                 // latest, so that it is popped first
                 $push(input);
             }
             Exclude(e, _) => $push(e),
             KeepName(e) => $push(e),
+            Cache { input, .. } => $push(input),
             RenameAlias { expr, .. } => $push(expr),
         }
     }};
 }
 
 impl Expr {
     /// Expr::mutate().apply(fn())
@@ -168,14 +169,15 @@
             Alias(e, _) => push(e),
             BinaryExpr { left, op: _, right } => {
                 // reverse order so that left is popped first
                 push(right);
                 push(left);
             }
             Cast { expr, .. } => push(expr),
+            Cache { input, .. } => push(input),
             Sort { expr, .. } => push(expr),
             Take { expr, idx } => {
                 push(idx);
                 // latest, so that it is popped first
                 push(expr);
             }
             SortBy { expr, by, .. } => {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/lit.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/lit.rs`

 * *Files 2% similar despite different names*

```diff
@@ -287,14 +287,20 @@
 
 impl Literal for Series {
     fn lit(self) -> Expr {
         Expr::Literal(LiteralValue::Series(SpecialEq::new(self)))
     }
 }
 
+impl Literal for LiteralValue {
+    fn lit(self) -> Expr {
+        Expr::Literal(self)
+    }
+}
+
 /// Create a Literal Expression from `L`. A literal expression behaves like a column that contains a single distinct
 /// value.
 ///
 /// The column is automatically of the "correct" length to make the operations work. Often this is determined by the
 /// length of the `LazyFrame` it is being used with. For instance, `lazy_df.with_column(lit(5).alias("five"))` creates a
 /// new column named "five" that is the length of the Dataframe (at the time `collect` is called), where every value in
 /// the column is `5`.
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/cache_states.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/cache_states.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/cse.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/cse.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/delay_rechunk.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/delay_rechunk.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/drop_nulls.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/drop_nulls.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/fast_projection.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/fast_projection.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/file_caching.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/file_caching.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/flatten_union.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/flatten_union.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/mod.rs`

 * *Files 2% similar despite different names*

```diff
@@ -4,18 +4,21 @@
 use crate::prelude::*;
 
 mod cache_states;
 #[cfg(feature = "cse")]
 mod cse;
 mod delay_rechunk;
 mod drop_nulls;
+
 mod fast_projection;
 #[cfg(any(feature = "ipc", feature = "parquet", feature = "csv", feature = "cse"))]
 pub(crate) mod file_caching;
 mod flatten_union;
+#[cfg(feature = "fused")]
+mod fused;
 mod predicate_pushdown;
 mod projection_pushdown;
 mod simplify_expr;
 mod slice_pushdown_expr;
 mod slice_pushdown_lp;
 mod stack_opt;
 mod type_coercion;
@@ -65,14 +68,15 @@
 
     #[allow(unused_variables)]
     let agg_scan_projection = opt_state.file_caching;
 
     // gradually fill the rules passed to the optimizer
     let opt = StackOptimizer {};
     let mut rules: Vec<Box<dyn OptimizationRule>> = Vec::with_capacity(8);
+
     // during debug we check if the optimizations have not modified the final schema
     #[cfg(debug_assertions)]
     let prev_schema = logical_plan.schema()?.into_owned();
 
     let mut lp_top = to_alp(logical_plan, expr_arena, lp_arena)?;
 
     #[cfg(feature = "cse")]
@@ -85,14 +89,16 @@
     };
     #[cfg(not(feature = "cse"))]
     let cse_changed = false;
 
     // we do simplification
     if simplify_expr {
         rules.push(Box::new(SimplifyExprRule {}));
+        #[cfg(feature = "fused")]
+        rules.push(Box::new(fused::FusedArithmetic {}));
     }
 
     // should be run before predicate pushdown
     if projection_pushdown {
         let mut projection_pushdown_opt = ProjectionPushDown::new();
         let alp = lp_arena.take(lp_top);
         let alp = projection_pushdown_opt.optimize(alp, lp_arena, expr_arena)?;
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/predicate_pushdown/keys.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/predicate_pushdown/keys.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/predicate_pushdown/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/predicate_pushdown/mod.rs`

 * *Files 1% similar despite different names*

```diff
@@ -574,16 +574,30 @@
                     } else {
                         true
                     }
                 });
                 let lp = self.pushdown_and_continue(lp, acc_predicates, lp_arena, expr_arena, false)?;
                 Ok(self.optional_apply_predicate(lp, local_predicates, lp_arena, expr_arena))
             }
+            lp @ Sort{..} => {
+                let mut local_predicates = vec![];
+                acc_predicates.retain(|_, predicate| {
+                    if predicate_is_sort_boundary(*predicate, expr_arena) {
+                        local_predicates.push(*predicate);
+                        false
+                    } else {
+                        true
+                    }
+                });
+                let lp = self.pushdown_and_continue(lp, acc_predicates, lp_arena, expr_arena, false)?;
+                Ok(self.optional_apply_predicate(lp, local_predicates, lp_arena, expr_arena))
+
+            }
             // Pushed down passed these nodes
-            lp @ Sort { .. } |lp @ FileSink {..} => {
+            lp@ FileSink {..} => {
                 self.pushdown_and_continue(lp, acc_predicates, lp_arena, expr_arena, false)
             }
             lp @ HStack {..} | lp @ Projection {..} | lp @ ExtContext {..} => {
                 self.pushdown_and_continue(lp, acc_predicates, lp_arena, expr_arena, true)
             }
             // NOT Pushed down passed these nodes
             // predicates influence slice sizes
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/predicate_pushdown/rename.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/predicate_pushdown/rename.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/predicate_pushdown/utils.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/predicate_pushdown/utils.rs`

 * *Files 2% similar despite different names*

```diff
@@ -72,14 +72,35 @@
         }
         Some(new_predicate)
     } else {
         None
     }
 }
 
+fn shifts_elements(node: Node, expr_arena: &Arena<AExpr>) -> bool {
+    let matches = |e: &AExpr| {
+        matches!(
+            e,
+            AExpr::Function {
+                function: FunctionExpr::Shift(_) | FunctionExpr::ShiftAndFill { .. },
+                ..
+            }
+        )
+    };
+    has_aexpr(node, expr_arena, matches)
+}
+
+pub(super) fn predicate_is_sort_boundary(node: Node, expr_arena: &Arena<AExpr>) -> bool {
+    let matches = |e: &AExpr| match e {
+        AExpr::Window { function, .. } => shifts_elements(*function, expr_arena),
+        _ => false,
+    };
+    has_aexpr(node, expr_arena, matches)
+}
+
 // this checks if a predicate from a node upstream can pass
 // the predicate in this filter
 // Cases where this cannot be the case:
 //
 // .filter(a > 1)           # filter 2
 ///.filter(a == min(a))     # filter 1
 ///
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/functions/melt.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/functions/melt.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/functions/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/functions/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/generic.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/generic.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/groupby.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/groupby.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/hstack.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/hstack.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/joins.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/joins.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/projection.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/projection.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/rename.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/rename.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/semi_anti_join.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/projection_pushdown/semi_anti_join.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/simplify_expr.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/simplify_expr.rs`

 * *Files 3% similar despite different names*

```diff
@@ -454,15 +454,15 @@
             AExpr::BinaryExpr { left, op, right } => {
                 let left_aexpr = expr_arena.get(*left);
                 let right_aexpr = expr_arena.get(*right);
 
                 // lit(left) + lit(right) => lit(left + right)
                 #[allow(clippy::manual_map)]
                 let out = match op {
-                    Operator::Plus => {
+                    Plus => {
                         match eval_binary_same_type!(left_aexpr, +, right_aexpr) {
                             Some(new) => Some(new),
                             None => {
                                 // try to replace addition of string columns with `concat_str`
                                 #[cfg(all(feature = "strings", feature = "concat_str"))]
                                 {
                                     string_addition_to_linear_concat(
@@ -478,18 +478,18 @@
                                 #[cfg(not(all(feature = "strings", feature = "concat_str")))]
                                 {
                                     None
                                 }
                             }
                         }
                     }
-                    Operator::Minus => eval_binary_same_type!(left_aexpr, -, right_aexpr),
-                    Operator::Multiply => eval_binary_same_type!(left_aexpr, *, right_aexpr),
-                    Operator::Divide => eval_binary_same_type!(left_aexpr, /, right_aexpr),
-                    Operator::TrueDivide => {
+                    Minus => eval_binary_same_type!(left_aexpr, -, right_aexpr),
+                    Multiply => eval_binary_same_type!(left_aexpr, *, right_aexpr),
+                    Divide => eval_binary_same_type!(left_aexpr, /, right_aexpr),
+                    TrueDivide => {
                         if let (AExpr::Literal(lit_left), AExpr::Literal(lit_right)) =
                             (left_aexpr, right_aexpr)
                         {
                             match (lit_left, lit_right) {
                                 (LiteralValue::Float32(x), LiteralValue::Float32(y)) => {
                                     Some(AExpr::Literal(LiteralValue::Float32(x / y)))
                                 }
@@ -526,25 +526,25 @@
                                 ),
                                 _ => None,
                             }
                         } else {
                             None
                         }
                     }
-                    Operator::FloorDivide => None,
-                    Operator::Modulus => eval_binary_same_type!(left_aexpr, %, right_aexpr),
-                    Operator::Lt => eval_binary_bool_type!(left_aexpr, <, right_aexpr),
-                    Operator::Gt => eval_binary_bool_type!(left_aexpr, >, right_aexpr),
-                    Operator::Eq => eval_binary_bool_type!(left_aexpr, ==, right_aexpr),
-                    Operator::NotEq => eval_binary_bool_type!(left_aexpr, !=, right_aexpr),
-                    Operator::GtEq => eval_binary_bool_type!(left_aexpr, >=, right_aexpr),
-                    Operator::LtEq => eval_binary_bool_type!(left_aexpr, <=, right_aexpr),
-                    Operator::And => eval_bitwise(left_aexpr, right_aexpr, |l, r| l & r),
-                    Operator::Or => eval_bitwise(left_aexpr, right_aexpr, |l, r| l | r),
-                    Operator::Xor => eval_bitwise(left_aexpr, right_aexpr, |l, r| l ^ r),
+                    Modulus => eval_binary_same_type!(left_aexpr, %, right_aexpr),
+                    Lt => eval_binary_bool_type!(left_aexpr, <, right_aexpr),
+                    Gt => eval_binary_bool_type!(left_aexpr, >, right_aexpr),
+                    Eq | EqValidity => eval_binary_bool_type!(left_aexpr, ==, right_aexpr),
+                    NotEq | NotEqValidity => eval_binary_bool_type!(left_aexpr, !=, right_aexpr),
+                    GtEq => eval_binary_bool_type!(left_aexpr, >=, right_aexpr),
+                    LtEq => eval_binary_bool_type!(left_aexpr, <=, right_aexpr),
+                    And => eval_bitwise(left_aexpr, right_aexpr, |l, r| l & r),
+                    Or => eval_bitwise(left_aexpr, right_aexpr, |l, r| l | r),
+                    Xor => eval_bitwise(left_aexpr, right_aexpr, |l, r| l ^ r),
+                    FloorDivide => None,
                 };
                 if out.is_some() {
                     return Ok(out);
                 }
 
                 // Null propagation.
                 let left_is_null = matches!(left_aexpr, AExpr::Literal(LiteralValue::Null));
@@ -664,15 +664,15 @@
         Ok(out)
     }
 }
 
 fn inline_cast(input: &AExpr, dtype: &DataType) -> Option<AExpr> {
     match (input, dtype) {
         #[cfg(feature = "dtype-duration")]
-        (AExpr::Literal(lv), _) => {
+        (AExpr::Literal(lv), _) if !matches!(dtype, DataType::Unknown) => {
             let av = lv.to_anyvalue()?;
             let out = av.cast(dtype).ok()?;
             let lv: LiteralValue = out.try_into().ok()?;
             Some(AExpr::Literal(lv))
         }
         _ => None,
     }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/slice_pushdown_expr.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/slice_pushdown_expr.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/slice_pushdown_lp.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/slice_pushdown_lp.rs`

 * *Files 1% similar despite different names*

```diff
@@ -201,17 +201,15 @@
                     options,
                     predicate,
                 };
                 Ok(lp)
             }
 
             (Union {inputs, mut options }, Some(state)) => {
-                options.slice = true;
-                options.slice_offset = state.offset;
-                options.slice_len = state.len;
+                options.slice = Some((state.offset, state.len as usize));
                 Ok(Union {inputs, options})
             },
             (Join {
                 input_left,
                 input_right,
                 schema,
                 left_on,
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/stack_opt.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/stack_opt.rs`

 * *Files 20% similar despite different names*

```diff
@@ -17,14 +17,15 @@
     ) -> PolarsResult<Node> {
         let mut changed = true;
 
         let mut plans = Vec::with_capacity(32);
 
         // nodes of expressions and lp node from which the expressions are a member of
         let mut exprs = Vec::with_capacity(32);
+        let mut scratch = vec![];
 
         // run loop until reaching fixed point
         while changed {
             // recurse into sub plans and expressions and apply rules
             changed = false;
             plans.push(lp_top);
             while let Some(current_node) = plans.pop() {
@@ -36,35 +37,55 @@
                         changed = true;
                     }
                 }
 
                 let plan = lp_arena.get(current_node);
 
                 // traverse subplans and expressions and add to the stack
-                plan.copy_exprs(&mut exprs);
+                plan.copy_exprs(&mut scratch);
                 plan.copy_inputs(&mut plans);
 
+                // first do a single pass to ensure we process
+                // from leaves to root.
+                // this ensures for instance
+                // that we first do constant folding on operands
+                // before we decide that multiple binary expression
+                // can be replaced with a fused operator
+                while let Some(expr_node) = scratch.pop() {
+                    exprs.push(expr_node);
+                    // traverse all subexpressions and add to the stack
+                    let expr = unsafe { expr_arena.get_unchecked(expr_node) };
+                    expr.nodes(&mut exprs);
+                }
+
                 // process the expressions on the stack and apply optimizations.
                 while let Some(current_expr_node) = exprs.pop() {
+                    {
+                        let expr = unsafe { expr_arena.get_unchecked(current_expr_node) };
+                        // don't apply rules to `col`, `lit` etc.
+                        if expr.is_leaf() {
+                            continue;
+                        }
+                    }
                     for rule in rules.iter() {
                         // keep iterating over same rule
                         while let Some(x) = rule.optimize_expr(
                             expr_arena,
                             current_expr_node,
                             lp_arena,
                             current_node,
                         )? {
                             expr_arena.replace(current_expr_node, x);
                             changed = true;
                         }
                     }
 
+                    let expr = unsafe { expr_arena.get_unchecked(current_expr_node) };
                     // traverse subexpressions and add to the stack
-                    let expr = expr_arena.get(current_expr_node);
-                    expr.nodes(&mut exprs);
+                    expr.nodes(&mut exprs)
                 }
             }
         }
         Ok(lp_top)
     }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/type_coercion/binary.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/type_coercion/binary.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/optimizer/type_coercion/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/optimizer/type_coercion/mod.rs`

 * *Files 4% similar despite different names*

```diff
@@ -442,15 +442,23 @@
                         unpack!(get_aexpr_and_type(expr_arena, *other, &input_schema));
 
                     // early return until Unknown is set
                     if matches!(type_other, DataType::Unknown) {
                         return Ok(None);
                     }
                     let new_st = unpack!(get_supertype(&super_type, &type_other));
-                    super_type = modify_supertype(new_st, self_ae, other, &type_self, &type_other)
+                    if input.len() == 2 {
+                        // modify_supertype is a bit more conservative of casting columns
+                        // to literals
+                        super_type =
+                            modify_supertype(new_st, self_ae, other, &type_self, &type_other)
+                    } else {
+                        // when dealing with more than 1 argument, we simply find the supertypes
+                        super_type = new_st
+                    }
                 }
                 // only cast if the type is not already the super type.
                 // this can prevent an expensive flattening and subsequent aggregation
                 // in a groupby context. To be able to cast the groups need to be
                 // flattened
                 let new_node_self = if type_self != super_type {
                     expr_arena.add(AExpr::Cast {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/options.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/options.rs`

 * *Files 17% similar despite different names*

```diff
@@ -117,17 +117,15 @@
         }
     }
 }
 
 #[derive(Clone, Debug, Copy, Default, Eq, PartialEq)]
 #[cfg_attr(feature = "serde", derive(Serialize, Deserialize))]
 pub struct UnionOptions {
-    pub slice: bool,
-    pub slice_offset: i64,
-    pub slice_len: IdxSize,
+    pub slice: Option<(i64, usize)>,
     pub parallel: bool,
     // known row_output, estimated row output
     pub rows: (Option<usize>, usize),
     pub from_partitioned_ds: bool,
     pub flattened_by_opt: bool,
 }
 
@@ -169,22 +167,14 @@
     // e.g. [g1, g1, g2] -> list([g1, g1, g2])
     ApplyList,
     // do not collect before apply
     // e.g. [g1, g1, g2] -> [g1, g1, g2]
     ApplyFlat,
 }
 
-#[derive(Copy, Clone, Debug, PartialEq, Eq)]
-#[cfg_attr(feature = "serde", derive(Serialize, Deserialize))]
-pub struct WindowOptions {
-    /// Explode the aggregated list and just do a hstack instead of a join
-    /// this requires the groups to be sorted to make any sense
-    pub explode: bool,
-}
-
 #[derive(Clone, Copy, PartialEq, Eq, Debug)]
 #[cfg_attr(feature = "serde", derive(Serialize, Deserialize))]
 pub struct FunctionOptions {
     /// Collect groups to a list and apply the function over the groups.
     /// This can be important in aggregation context.
     pub collect_groups: ApplyOptions,
     /// There can be two ways of expanding wildcards:
@@ -221,14 +211,16 @@
     // if the expression and its inputs should be cast to supertypes
     pub cast_to_supertypes: bool,
     // apply physical expression may rename the output of this function
     pub allow_rename: bool,
     // if set, then the `Series` passed to the function in the groupby operation
     // will ensure the name is set. This is an extra heap allocation per group.
     pub pass_name_to_apply: bool,
+    // For example a `unique` or a `slice`
+    pub changes_length: bool,
 }
 
 impl FunctionOptions {
     /// Any function that is sensitive to the number of elements in a group
     /// - Aggregations
     /// - Sorts
     /// - Counts
@@ -243,14 +235,15 @@
             collect_groups: ApplyOptions::ApplyGroups,
             input_wildcard_expansion: false,
             auto_explode: false,
             fmt_str: "",
             cast_to_supertypes: false,
             allow_rename: false,
             pass_name_to_apply: false,
+            changes_length: false,
         }
     }
 }
 
 #[derive(Clone, Copy, PartialEq, Eq, Debug)]
 pub struct LogicalPlanUdfOptions {
     ///  allow predicate pushdown optimizations
@@ -301,19 +294,16 @@
 #[cfg_attr(feature = "serde", derive(Serialize, Deserialize))]
 #[derive(Clone, Debug)]
 pub struct FileSinkOptions {
     pub path: Arc<PathBuf>,
     pub file_type: FileType,
 }
 
-#[cfg(any(feature = "parquet", feature = "ipc"))]
 #[cfg_attr(feature = "serde", derive(Serialize, Deserialize))]
 #[derive(Clone, Debug)]
 pub enum FileType {
     #[cfg(feature = "parquet")]
     Parquet(ParquetWriteOptions),
     #[cfg(feature = "ipc")]
     Ipc(IpcWriterOptions),
+    Memory,
 }
-
-#[cfg(not(any(feature = "parquet", feature = "ipc")))]
-pub type FileType = ();
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/projection.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/projection.rs`

 * *Files 2% similar despite different names*

```diff
@@ -52,19 +52,19 @@
 
 /// Take an expression with a root: col("*") and copies that expression for all columns in the schema,
 /// with the exclusion of the `names` in the exclude expression.
 /// The resulting expressions are written to result.
 fn replace_wildcard(
     expr: &Expr,
     result: &mut Vec<Expr>,
-    exclude: &[Arc<str>],
+    exclude: &PlHashSet<Arc<str>>,
     schema: &Schema,
 ) -> PolarsResult<()> {
     for name in schema.iter_names() {
-        if !exclude.iter().any(|excluded| &**excluded == name) {
+        if !exclude.contains(name.as_str()) {
             let new_expr = replace_wildcard_with_column(expr.clone(), Arc::from(name.as_str()));
             let new_expr = rewrite_special_aliases(new_expr)?;
             result.push(new_expr)
         }
     }
     Ok(())
 }
@@ -197,35 +197,39 @@
 
 /// replace `DtypeColumn` with `col("foo")..col("bar")`
 fn expand_dtypes(
     expr: &Expr,
     result: &mut Vec<Expr>,
     schema: &Schema,
     dtypes: &[DataType],
-    exclude: &[Arc<str>],
+    exclude: &PlHashSet<Arc<str>>,
 ) -> PolarsResult<()> {
     // note: we loop over the schema to guarantee that we return a stable
     // field-order, irrespective of which dtypes are filtered against
-    for field in schema.iter_fields().filter(|f| dtypes.contains(&f.dtype)) {
+    for field in schema
+        .iter_fields()
+        .filter(|f| (dtypes.contains(&f.dtype) && !exclude.contains(f.name().as_str())))
+    {
         let name = field.name();
-        if exclude.iter().any(|excl| excl.as_ref() == name.as_str()) {
-            continue; // skip excluded names
-        }
         let new_expr = expr.clone();
         let new_expr = replace_dtype_with_column(new_expr, Arc::from(name.as_str()));
         let new_expr = rewrite_special_aliases(new_expr)?;
         result.push(new_expr)
     }
     Ok(())
 }
 
 // schema is not used if regex not activated
 #[allow(unused_variables)]
-fn prepare_excluded(expr: &Expr, schema: &Schema, keys: &[Expr]) -> PolarsResult<Vec<Arc<str>>> {
-    let mut exclude = vec![];
+fn prepare_excluded(
+    expr: &Expr,
+    schema: &Schema,
+    keys: &[Expr],
+) -> PolarsResult<PlHashSet<Arc<str>>> {
+    let mut exclude = PlHashSet::new();
     for e in expr {
         if let Expr::Exclude(_, to_exclude) = e {
             #[cfg(feature = "regex")]
             {
                 // instead of matching the names for regex patterns
                 // and expanding the matches in the schema we
                 // reuse the `replace_regex` function. This is a bit
@@ -235,52 +239,54 @@
                     match to_exclude_single {
                         Excluded::Name(name) => {
                             let e = Expr::Column(name.clone());
                             replace_regex(&e, &mut buf, schema)?;
                             // we cannot loop because of bchck
                             while let Some(col) = buf.pop() {
                                 if let Expr::Column(name) = col {
-                                    exclude.push(name)
+                                    exclude.insert(name);
                                 }
                             }
                         }
                         Excluded::Dtype(dt) => {
                             for fld in schema.iter_fields() {
                                 if fld.data_type() == dt {
-                                    exclude.push(Arc::from(fld.name().as_ref()))
+                                    exclude.insert(Arc::from(fld.name().as_ref()));
                                 }
                             }
                         }
                     }
                 }
             }
 
             #[cfg(not(feature = "regex"))]
             {
                 for to_exclude_single in to_exclude {
                     match to_exclude_single {
-                        Excluded::Name(name) => exclude.push(name.clone()),
+                        Excluded::Name(name) => {
+                            exclude.insert(name.clone());
+                        }
                         Excluded::Dtype(dt) => {
                             for (name, dtype) in schema.iter() {
                                 if matches!(dtype, dt) {
-                                    exclude.push(Arc::from(name.as_str()))
+                                    exclude.insert(Arc::from(name.as_str()));
                                 }
                             }
                         }
                     }
                 }
             }
         }
     }
     for mut expr in keys.iter() {
         // Allow a number of aliases of a column expression, still exclude column from aggregation
         loop {
             match expr {
                 Expr::Column(name) => {
-                    exclude.push(name.clone());
+                    exclude.insert(name.clone());
                     break;
                 }
                 Expr::Alias(e, _) => {
                     expr = e;
                 }
                 _ => {
                     break;
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/logical_plan/schema.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/logical_plan/schema.rs`

 * *Files 0% similar despite different names*

```diff
@@ -108,16 +108,16 @@
                 inputs,
                 mut options,
             } = lp_arena.take(root)
             {
                 let mut sum_output = (None, 0);
                 for input in &inputs {
                     let mut out = set_estimated_row_counts(*input, lp_arena, expr_arena, 0);
-                    if options.slice {
-                        apply_slice(&mut out, Some((0, options.slice_len as usize)))
+                    if let Some((_offset, len)) = options.slice {
+                        apply_slice(&mut out, Some((0, len)))
                     }
                     // todo! deal with known as well
                     let out = estimate_sizes(out.0, out.1, out.2);
                     sum_output.1 += out.1;
                 }
                 options.rows = sum_output;
                 lp_arena.replace(root, Union { inputs, options });
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/prelude.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/prelude.rs`

 * *Files 13% similar despite different names*

```diff
@@ -1,20 +1,23 @@
 pub(crate) use polars_ops::prelude::*;
-#[cfg(feature = "rolling_window")]
-pub(crate) use polars_time::chunkedarray::{RollingOptions, RollingOptionsImpl};
 #[cfg(feature = "temporal")]
 pub(crate) use polars_time::in_nanoseconds_window;
 #[cfg(any(
     feature = "temporal",
     feature = "dtype-duration",
     feature = "dtype-date",
     feature = "dtype-date",
     feature = "dtype-time"
 ))]
 pub(crate) use polars_time::prelude::*;
+#[cfg(feature = "rolling_window")]
+pub(crate) use polars_time::{
+    chunkedarray::{RollingOptions, RollingOptionsImpl},
+    Duration,
+};
 pub use polars_utils::arena::{Arena, Node};
 
 pub use crate::dsl::*;
 pub(crate) use crate::logical_plan::alp::*;
 pub(crate) use crate::logical_plan::conversion::*;
 #[cfg(feature = "debugging")]
 pub use crate::logical_plan::debug::*;
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-plan/src/utils.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-plan/src/utils.rs`

 * *Files 1% similar despite different names*

```diff
@@ -132,15 +132,15 @@
 pub fn has_null(current_expr: &Expr) -> bool {
     has_expr(current_expr, |e| {
         matches!(e, Expr::Literal(LiteralValue::Null))
     })
 }
 
 /// output name of expr
-pub(crate) fn expr_output_name(expr: &Expr) -> PolarsResult<Arc<str>> {
+pub fn expr_output_name(expr: &Expr) -> PolarsResult<Arc<str>> {
     for e in expr {
         match e {
             // don't follow the partition by branch
             Expr::Window { function, .. } => return expr_output_name(function),
             Expr::Column(name) => return Ok(name.clone()),
             Expr::Alias(_, name) => return Ok(name.clone()),
             Expr::KeepName(_) | Expr::Wildcard | Expr::RenameAlias { .. } => polars_bail!(
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/LICENSE` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/LICENSE`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/operators/filter.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/operators/filter.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/operators/function.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/operators/function.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/operators/placeholder.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/operators/placeholder.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/operators/projection.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/operators/projection.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/operators/reproject.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/operators/reproject.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/file_sink.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/file_sink.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/convert.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/convert.rs`

 * *Files 14% similar despite different names*

```diff
@@ -20,16 +20,18 @@
 use crate::executors::sinks::groupby::aggregates::{AggregateFunction, SumAgg};
 use crate::expressions::PhysicalPipedExpr;
 use crate::operators::DataChunk;
 
 struct Count {}
 
 impl PhysicalPipedExpr for Count {
-    fn evaluate(&self, _chunk: &DataChunk, _lazy_state: &dyn Any) -> PolarsResult<Series> {
-        Ok(Series::new_empty("", &IDX_DTYPE))
+    fn evaluate(&self, chunk: &DataChunk, _lazy_state: &dyn Any) -> PolarsResult<Series> {
+        // the length must match the chunks as the operators expect that
+        // so we fill a null series.
+        Ok(Series::new_null("", chunk.data.height()))
     }
 
     fn field(&self, _input_schema: &Schema) -> PolarsResult<Field> {
         todo!()
     }
 
     fn expression(&self) -> Expr {
@@ -100,26 +102,31 @@
             _ => false,
         }
     } else {
         false
     }
 }
 
+/// # Returns:
+///     - input_dtype: dtype that goes into the agg expression
+///     - physical expr: physical expression that produces the input of the aggregation
+///     - aggregation function: the aggregation function
 pub(crate) fn convert_to_hash_agg<F>(
     node: Node,
     expr_arena: &Arena<AExpr>,
     schema: &SchemaRef,
     to_physical: &F,
-) -> (Arc<dyn PhysicalPipedExpr>, AggregateFunction)
+) -> (DataType, Arc<dyn PhysicalPipedExpr>, AggregateFunction)
 where
     F: Fn(Node, &Arena<AExpr>, Option<&SchemaRef>) -> PolarsResult<Arc<dyn PhysicalPipedExpr>>,
 {
     match expr_arena.get(node) {
         AExpr::Alias(input, _) => convert_to_hash_agg(*input, expr_arena, schema, to_physical),
         AExpr::Count => (
+            IDX_DTYPE,
             Arc::new(Count {}),
             AggregateFunction::Count(CountAgg::new()),
         ),
         AExpr::Agg(agg) => match agg {
             AAggExpr::Min { input, .. } => {
                 let phys_expr = to_physical(*input, expr_arena, Some(schema)).unwrap();
                 let logical_dtype = phys_expr.field(schema).unwrap().dtype;
@@ -133,15 +140,15 @@
                     DataType::UInt16 => AggregateFunction::MinMaxU16(new_min()),
                     DataType::UInt32 => AggregateFunction::MinMaxU32(new_min()),
                     DataType::UInt64 => AggregateFunction::MinMaxU64(new_min()),
                     DataType::Float32 => AggregateFunction::MinMaxF32(new_min()),
                     DataType::Float64 => AggregateFunction::MinMaxF64(new_min()),
                     dt => panic!("{dt} unexpected"),
                 };
-                (phys_expr, agg_fn)
+                (logical_dtype, phys_expr, agg_fn)
             }
             AAggExpr::Max { input, .. } => {
                 let phys_expr = to_physical(*input, expr_arena, Some(schema)).unwrap();
                 let logical_dtype = phys_expr.field(schema).unwrap().dtype;
 
                 let agg_fn = match logical_dtype.to_physical() {
                     DataType::Int8 => AggregateFunction::MinMaxI8(new_max()),
@@ -152,23 +159,24 @@
                     DataType::UInt16 => AggregateFunction::MinMaxU16(new_max()),
                     DataType::UInt32 => AggregateFunction::MinMaxU32(new_max()),
                     DataType::UInt64 => AggregateFunction::MinMaxU64(new_max()),
                     DataType::Float32 => AggregateFunction::MinMaxF32(new_max()),
                     DataType::Float64 => AggregateFunction::MinMaxF64(new_max()),
                     dt => panic!("{dt} unexpected"),
                 };
-                (phys_expr, agg_fn)
+                (logical_dtype, phys_expr, agg_fn)
             }
             AAggExpr::Sum(input) => {
                 let phys_expr = to_physical(*input, expr_arena, Some(schema)).unwrap();
                 let logical_dtype = phys_expr.field(schema).unwrap().dtype;
 
                 #[cfg(feature = "dtype-categorical")]
                 if matches!(logical_dtype, DataType::Categorical(_)) {
                     return (
+                        logical_dtype.clone(),
                         phys_expr,
                         AggregateFunction::Null(NullAgg::new(logical_dtype)),
                     );
                 }
 
                 let agg_fn = match logical_dtype.to_physical() {
                     // Boolean is aggregated as the IDX type.
@@ -189,53 +197,61 @@
                     DataType::UInt64 => AggregateFunction::SumU64(SumAgg::<u64>::new()),
                     DataType::Int32 => AggregateFunction::SumI32(SumAgg::<i32>::new()),
                     DataType::Int64 => AggregateFunction::SumI64(SumAgg::<i64>::new()),
                     DataType::Float32 => AggregateFunction::SumF32(SumAgg::<f32>::new()),
                     DataType::Float64 => AggregateFunction::SumF64(SumAgg::<f64>::new()),
                     dt => AggregateFunction::Null(NullAgg::new(dt)),
                 };
-                (phys_expr, agg_fn)
+                (logical_dtype, phys_expr, agg_fn)
             }
             AAggExpr::Mean(input) => {
                 let phys_expr = to_physical(*input, expr_arena, Some(schema)).unwrap();
 
                 let logical_dtype = phys_expr.field(schema).unwrap().dtype;
                 #[cfg(feature = "dtype-categorical")]
                 if matches!(logical_dtype, DataType::Categorical(_)) {
                     return (
+                        logical_dtype.clone(),
                         phys_expr,
                         AggregateFunction::Null(NullAgg::new(logical_dtype)),
                     );
                 }
                 let agg_fn = match logical_dtype.to_physical() {
                     dt if dt.is_integer() => AggregateFunction::MeanF64(MeanAgg::<f64>::new()),
                     DataType::Float32 => AggregateFunction::MeanF32(MeanAgg::<f32>::new()),
                     DataType::Float64 => AggregateFunction::MeanF64(MeanAgg::<f64>::new()),
                     dt => AggregateFunction::Null(NullAgg::new(dt)),
                 };
-                (phys_expr, agg_fn)
+                (logical_dtype, phys_expr, agg_fn)
             }
             AAggExpr::First(input) => {
                 let phys_expr = to_physical(*input, expr_arena, Some(schema)).unwrap();
-                let dtype = phys_expr.field(schema).unwrap().dtype;
+                let logical_dtype = phys_expr.field(schema).unwrap().dtype;
                 (
+                    logical_dtype.clone(),
                     phys_expr,
-                    AggregateFunction::First(FirstAgg::new(dtype.to_physical())),
+                    AggregateFunction::First(FirstAgg::new(logical_dtype.to_physical())),
                 )
             }
             AAggExpr::Last(input) => {
                 let phys_expr = to_physical(*input, expr_arena, Some(schema)).unwrap();
-                let dtype = phys_expr.field(schema).unwrap().dtype;
+                let logical_dtype = phys_expr.field(schema).unwrap().dtype;
                 (
+                    logical_dtype.clone(),
                     phys_expr,
-                    AggregateFunction::Last(LastAgg::new(dtype.to_physical())),
+                    AggregateFunction::Last(LastAgg::new(logical_dtype.to_physical())),
                 )
             }
             AAggExpr::Count(input) => {
                 let phys_expr = to_physical(*input, expr_arena, Some(schema)).unwrap();
-                (phys_expr, AggregateFunction::Count(CountAgg::new()))
+                let logical_dtype = phys_expr.field(schema).unwrap().dtype;
+                (
+                    logical_dtype,
+                    phys_expr,
+                    AggregateFunction::Count(CountAgg::new()),
+                )
             }
             agg => panic!("{agg:?} not yet implemented."),
         },
         _ => todo!(),
     }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/count.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/count.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/first.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/first.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/interface.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/interface.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/last.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/last.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/mean.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/mean.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/min_max.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/min_max.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/null.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/null.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/sum.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/aggregates/sum.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/eval.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/eval.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/global.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/global.rs`

 * *Files 13% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 use std::sync::Mutex;
 
 use polars_core::utils::accumulate_dataframes_vertical_unchecked;
 use polars_core::POOL;
 use rayon::prelude::*;
 
 use super::*;
-use crate::pipeline::PARTITION_SIZE;
+use crate::pipeline::{FORCE_OOC, PARTITION_SIZE};
 
 struct SpillPartitions {
     // outer vec: partitions (factor of 2)
     partitions: PartitionVec<Mutex<LinkedList<SpillPayload>>>,
 }
 
 impl SpillPartitions {
@@ -55,37 +55,47 @@
     }
 }
 
 pub(super) struct GlobalTable {
     inner_maps: PartitionVec<Mutex<AggHashTable<false>>>,
     spill_partitions: SpillPartitions,
     early_merge_counter: Arc<AtomicU16>,
+    // IO is expensive so we only spill if we have `N` payloads to dump.
+    spill_partition_ob_size: usize,
 }
 
 impl GlobalTable {
     pub(super) fn new(
         agg_constructors: Arc<[AggregateFunction]>,
         key_dtypes: &[DataType],
         output_schema: SchemaRef,
     ) -> Self {
         let spill_partitions = SpillPartitions::new();
+
+        let spill_partition_ob_size = if std::env::var(FORCE_OOC).is_ok() {
+            1
+        } else {
+            64
+        };
+
         let mut inner_maps = Vec::with_capacity(PARTITION_SIZE);
         inner_maps.resize_with(PARTITION_SIZE, || {
             Mutex::new(AggHashTable::new(
                 agg_constructors.clone(),
                 key_dtypes,
                 output_schema.clone(),
                 None,
             ))
         });
 
         Self {
             inner_maps,
             spill_partitions,
             early_merge_counter: Default::default(),
+            spill_partition_ob_size,
         }
     }
 
     #[inline]
     pub(super) fn spill(&self, partition: usize, payload: SpillPayload) {
         self.spill_partitions.insert(partition, payload);
     }
@@ -101,67 +111,95 @@
         self.spill_partitions.spill_schema()
     }
 
     pub(super) fn get_ooc_dump(&self) -> Option<(usize, DataFrame)> {
         // round robin a partition to dump
         let partition =
             self.early_merge_counter.fetch_add(1, Ordering::Relaxed) as usize % PARTITION_SIZE;
+
         // IO is expensive so we only spill if we have `N` payloads to dump.
-        let bucket = self.spill_partitions.drain_partition(partition, 64)?;
+        let bucket = self
+            .spill_partitions
+            .drain_partition(partition, self.spill_partition_ob_size)?;
         Some((
             partition,
             accumulate_dataframes_vertical_unchecked(bucket.into_iter().map(|pl| pl.into_df())),
         ))
     }
 
+    fn process_partition_impl(
+        &self,
+        hash_map: &mut AggHashTable<false>,
+        hashes: &[u64],
+        chunk_indexes: &[IdxSize],
+        keys: &[Series],
+        agg_cols: &[Series],
+    ) {
+        debug_assert_eq!(hashes.len(), chunk_indexes.len());
+        debug_assert_eq!(hashes.len(), keys[0].len());
+
+        let mut keys_iters = keys.iter().map(|s| s.phys_iter()).collect::<Vec<_>>();
+        let mut agg_cols_iters = agg_cols.iter().map(|s| s.phys_iter()).collect::<Vec<_>>();
+
+        // amortize loop counter
+        for i in 0..hashes.len() {
+            unsafe {
+                let hash = *hashes.get_unchecked(i);
+                let chunk_index = *chunk_indexes.get_unchecked(i);
+
+                // safety: keys_iters and cols_iters are not depleted
+                let out = hash_map.insert(hash, &mut keys_iters, &mut agg_cols_iters, chunk_index);
+                // should never overflow
+                debug_assert!(out.is_none());
+            }
+        }
+    }
+
+    pub(super) fn process_partition_from_dumped(&self, partition: usize, spilled: &DataFrame) {
+        let mut hash_map = self.inner_maps[partition].lock().unwrap();
+        let (hashes, chunk_indexes, keys_and_aggs) = SpillPayload::spilled_to_columns(spilled);
+        let keys = &keys_and_aggs[..hash_map.num_keys];
+        let aggs = &keys_and_aggs[hash_map.num_keys..];
+        self.process_partition_impl(&mut hash_map, hashes, chunk_indexes, keys, aggs);
+    }
+
     fn process_partition(&self, partition: usize) {
         if let Some(bucket) = self.spill_partitions.drain_partition(partition, 0) {
             let mut hash_map = self.inner_maps[partition].lock().unwrap();
 
             for payload in bucket {
                 let hashes = payload.hashes();
                 let keys = payload.keys();
                 let chunk_indexes = payload.chunk_index();
                 let agg_cols = payload.cols();
-                debug_assert_eq!(hashes.len(), chunk_indexes.len());
-                debug_assert_eq!(hashes.len(), keys[0].len());
-
-                let mut keys_iters = keys.iter().map(|s| s.phys_iter()).collect::<Vec<_>>();
-                let mut agg_cols_iters = agg_cols.iter().map(|s| s.phys_iter()).collect::<Vec<_>>();
-
-                // amortize loop counter
-                for i in 0..hashes.len() {
-                    unsafe {
-                        let hash = *hashes.get_unchecked(i);
-                        let chunk_index = *chunk_indexes.get_unchecked(i);
-
-                        // safety: keys_iters and cols_iters are not depleted
-                        let out = hash_map.insert(
-                            hash,
-                            &mut keys_iters,
-                            &mut agg_cols_iters,
-                            chunk_index,
-                        );
-                        // should never overflow
-                        debug_assert!(out.is_none());
-                    }
-                }
+                self.process_partition_impl(&mut hash_map, hashes, chunk_indexes, keys, agg_cols);
             }
         }
     }
 
     pub(super) fn merge_local_map(&self, finalized_local_map: &mut AggHashTable<true>) {
         // TODO! maybe parallelize?
         // needs unsafe, first benchmark.
         for (partition_i, pt_map) in self.inner_maps.iter().enumerate() {
             let mut pt_map = pt_map.lock().unwrap();
             pt_map.combine_on_partition(partition_i, finalized_local_map)
         }
     }
 
+    pub(super) fn finalize_partition(
+        &self,
+        partition: usize,
+        slice: &mut Option<(i64, usize)>,
+    ) -> DataFrame {
+        // ensure all spilled partitions are processed
+        self.process_partition(partition);
+        let mut hash_map = self.inner_maps[partition].lock().unwrap();
+        hash_map.finalize(slice)
+    }
+
     // only should be called if all state is in-memory
     pub(super) fn finalize(&self, slice: &mut Option<(i64, usize)>) -> Vec<DataFrame> {
         if slice.is_none() {
             POOL.install(|| {
                 (0..PARTITION_SIZE)
                     .into_par_iter()
                     .map(|part_i| {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/hash_table.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/hash_table.rs`

 * *Files 1% similar despite different names*

```diff
@@ -98,15 +98,21 @@
         hash: u64,
         keys: &mut [SeriesPhysIter],
         agg_iters: &mut [SeriesPhysIter],
         chunk_index: IdxSize,
     ) -> Option<&[AnyValue]> {
         // safety: no references
         let keys_scratch = unsafe { &mut *self.keys_scratch.get() };
-        keys_scratch.clear();
+
+        unsafe {
+            // safety:
+            // this scratch is set with borrowed anyvalues, so we don't have to drop
+            // them as they only borrow data
+            keys_scratch.set_len(0);
+        }
         for key in keys {
             unsafe {
                 // safety: this function should never be called if iterator is depleted
                 let key = key.next().unwrap_unchecked_release();
                 // safety: the static is temporary, we will never access them outside of this function
                 let key = std::mem::transmute::<AnyValue<'_>, AnyValue<'static>>(key);
                 // safety: we amortized n_keys
@@ -290,18 +296,17 @@
             .output_schema
             .iter_dtypes()
             .take(self.num_keys)
             .map(|dtype| AnyValueBufferTrusted::new(&dtype.to_physical(), take_len))
             .collect::<Vec<_>>();
 
         let mut agg_builders = self
-            .output_schema
-            .iter_dtypes()
-            .skip(self.num_keys)
-            .map(|dtype| AnyValueBufferTrusted::new(dtype, take_len))
+            .agg_constructors
+            .iter()
+            .map(|ac| AnyValueBufferTrusted::new(&ac.dtype(), take_len))
             .collect::<Vec<_>>();
         let num_aggs = self.agg_constructors.len();
 
         inner_map
             .into_iter()
             .skip(skip_len)
             .take(take_len)
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/mod.rs`

 * *Files 8% similar despite different names*

```diff
@@ -1,16 +1,18 @@
 mod eval;
 mod global;
 mod hash_table;
 mod ooc_state;
 mod sink;
+mod source;
 mod thread_local;
 
 use std::any::Any;
 use std::hash::{Hash, Hasher};
+use std::sync::Mutex;
 
 use eval::Eval;
 use hash_table::AggHashTable;
 use hashbrown::hash_map::{RawEntryMut, RawVacantEntryMut};
 use polars_core::frame::row::AnyValueBufferTrusted;
 use polars_core::series::SeriesPhysIter;
 use polars_core::IdBuildHasher;
@@ -18,17 +20,19 @@
 use polars_utils::slice::GetSaferUnchecked;
 use polars_utils::unwrap::UnwrapUncheckedRelease;
 pub(crate) use sink::GenericGroupby2;
 use thread_local::ThreadLocalTable;
 
 use super::*;
 use crate::executors::sinks::groupby::aggregates::{AggregateFn, AggregateFunction};
+use crate::executors::sinks::io::IOThread;
 use crate::operators::{DataChunk, FinalizedSink, PExecutionContext, Sink, SinkResult};
 
 type PartitionVec<T> = Vec<T>;
+type IOThreadRef = Arc<Mutex<Option<IOThread>>>;
 
 #[derive(Clone)]
 struct SpillPayload {
     hashes: Vec<u64>,
     chunk_idx: Vec<IdxSize>,
     keys_and_aggs: Vec<Series>,
     num_keys: usize,
@@ -62,24 +66,34 @@
             schema.with_column(s.name().into(), s.dtype().clone());
         }
         schema
     }
 
     fn into_df(self) -> DataFrame {
         debug_assert_eq!(self.hashes.len(), self.chunk_idx.len());
-        debug_assert_eq!(self.hashes.len(), self.keys_and_aggs.len());
+        debug_assert_eq!(self.hashes.len(), self.keys_and_aggs[0].len());
 
         let hashes = UInt64Chunked::from_vec(HASH_COL, self.hashes).into_series();
         let chunk_idx = IdxCa::from_vec(INDEX_COL, self.chunk_idx).into_series();
         let mut cols = Vec::with_capacity(self.keys_and_aggs.len() + 2);
         cols.push(hashes);
         cols.push(chunk_idx);
         cols.extend(self.keys_and_aggs);
         DataFrame::new_no_checks(cols)
     }
+
+    fn spilled_to_columns(spilled: &DataFrame) -> (&[u64], &[IdxSize], &[Series]) {
+        let cols = spilled.get_columns();
+        let hashes = cols[0].u64().unwrap();
+        let hashes = hashes.cont_slice().unwrap();
+        let chunk_indexes = cols[1].idx().unwrap();
+        let chunk_indexes = chunk_indexes.cont_slice().unwrap();
+        let keys_and_aggs = &cols[2..];
+        (hashes, chunk_indexes, keys_and_aggs)
+    }
 }
 
 // This is the hash and the Index offset in the linear buffer
 #[derive(Copy, Clone)]
 pub(super) struct Key {
     pub(super) hash: u64,
     pub(super) idx: IdxSize,
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/ooc_state.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/ooc_state.rs`

 * *Files 11% similar despite different names*

```diff
@@ -1,36 +1,42 @@
-use std::sync::Mutex;
-
 use polars_core::config::verbose;
 
 use super::*;
 use crate::executors::sinks::io::IOThread;
 use crate::executors::sinks::memory::MemTracker;
-use crate::pipeline::morsels_per_sink;
+use crate::pipeline::{morsels_per_sink, FORCE_OOC};
 
 #[derive(Clone)]
 pub(super) struct OocState {
     // OOC
     // Stores available memory in the system at the start of this sink.
     // and stores the memory used by this this sink.
     mem_track: MemTracker,
     // sort in-memory or out-of-core
     pub(super) ooc: bool,
     // when ooc, we write to disk using an IO thread
-    pub(super) io_thread: Arc<Mutex<Option<IOThread>>>,
+    pub(super) io_thread: IOThreadRef,
     count: u16,
+    to_disk_threshold: f64,
 }
 
 impl Default for OocState {
     fn default() -> Self {
+        let to_disk_threshold = if std::env::var(FORCE_OOC).is_ok() {
+            1.0
+        } else {
+            TO_DISK_THRESHOLD
+        };
+
         Self {
             mem_track: MemTracker::new(morsels_per_sink()),
             ooc: false,
             io_thread: Default::default(),
             count: 0,
+            to_disk_threshold,
         }
     }
 }
 
 // If this is reached we early merge the overflow buckets
 // to free up memory
 const EARLY_MERGE_THRESHOLD: f64 = 0.5;
@@ -41,43 +47,45 @@
 pub(super) enum SpillAction {
     EarlyMerge,
     Dump,
     None,
 }
 
 impl OocState {
-    fn init_ooc(&mut self, spill_schema: &dyn Fn() -> Option<Schema>) -> PolarsResult<()> {
+    fn init_ooc(&mut self, spill_schema: Schema) -> PolarsResult<()> {
         if verbose() {
             eprintln!("OOC groupby started");
         }
         self.ooc = true;
 
         // start IO thread
         let mut iot = self.io_thread.lock().unwrap();
         if iot.is_none() {
-            if let Some(schema) = spill_schema() {
-                *iot = Some(IOThread::try_new(Arc::new(schema), "groupby")?)
-            }
+            *iot = Some(IOThread::try_new(Arc::new(spill_schema), "groupby").unwrap());
         }
         Ok(())
     }
 
     pub(super) fn check_memory_usage(
         &mut self,
         spill_schema: &dyn Fn() -> Option<Schema>,
     ) -> PolarsResult<SpillAction> {
         if self.ooc {
             return Ok(SpillAction::Dump);
         }
         let free_frac = self.mem_track.free_memory_fraction_since_start();
         self.count += 1;
 
-        if free_frac < TO_DISK_THRESHOLD {
-            self.init_ooc(spill_schema)?;
-            Ok(SpillAction::Dump)
+        if free_frac < self.to_disk_threshold {
+            if let Some(schema) = spill_schema() {
+                self.init_ooc(schema)?;
+                Ok(SpillAction::Dump)
+            } else {
+                Ok(SpillAction::None)
+            }
         } else if free_frac < EARLY_MERGE_THRESHOLD
         // clean up some spills
          || (self.count % 512) == 0
         {
             Ok(SpillAction::EarlyMerge)
         } else {
             Ok(SpillAction::None)
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/sink.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/sink.rs`

 * *Files 9% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 use std::cell::UnsafeCell;
 
 use polars_core::utils::accumulate_dataframes_vertical_unchecked;
 
 use super::*;
 use crate::executors::sinks::groupby::generic::global::GlobalTable;
 use crate::executors::sinks::groupby::generic::ooc_state::{OocState, SpillAction};
+use crate::executors::sinks::groupby::generic::source::GroupBySource;
 use crate::executors::sources::DataFrameSource;
 use crate::expressions::PhysicalPipedExpr;
 
 pub(crate) struct GenericGroupby2 {
     thread_local_table: UnsafeCell<ThreadLocalTable>,
     global_table: Arc<GlobalTable>,
     eval: Eval,
@@ -18,34 +19,38 @@
 
 impl GenericGroupby2 {
     pub(crate) fn new(
         key_columns: Arc<Vec<Arc<dyn PhysicalPipedExpr>>>,
         aggregation_columns: Arc<Vec<Arc<dyn PhysicalPipedExpr>>>,
         agg_constructors: Arc<[AggregateFunction]>,
         output_schema: SchemaRef,
+        agg_input_dtypes: Vec<DataType>,
         slice: Option<(i64, usize)>,
     ) -> Self {
         let key_dtypes: Arc<[DataType]> = Arc::from(
             output_schema
                 .iter_dtypes()
                 .take(key_columns.len())
                 .cloned()
                 .collect::<Vec<_>>(),
         );
 
+        let agg_dtypes: Arc<[DataType]> = Arc::from(agg_input_dtypes);
+
         let global_map = GlobalTable::new(
             agg_constructors.clone(),
             key_dtypes.as_ref(),
             output_schema.clone(),
         );
 
         Self {
             thread_local_table: UnsafeCell::new(ThreadLocalTable::new(
                 agg_constructors,
                 key_dtypes,
+                agg_dtypes,
                 output_schema,
             )),
             global_table: Arc::new(global_map),
             eval: Eval::new(key_columns, aggregation_columns),
             slice,
             ooc_state: Default::default(),
         }
@@ -69,15 +74,14 @@
             // safety: the mutable borrows are not aliasing
             let table = &mut *self.thread_local_table.get();
 
             for hash in self.eval.hashes() {
                 if let Some((partition, spill_payload)) =
                     table.insert(*hash, &mut keys, &mut aggs, chunk_idx)
                 {
-                    // append payload to global spills
                     self.global_table.spill(partition, spill_payload)
                 }
             }
         }
 
         // clear memory
         unsafe {
@@ -153,15 +157,19 @@
 
                 let out = self.global_table.finalize(&mut self.slice);
                 let src = DataFrameSource::from_df(accumulate_dataframes_vertical_unchecked(out));
                 Ok(FinalizedSink::Source(Box::new(src)))
             }
             // create an ooc source
             else {
-                todo!()
+                Ok(FinalizedSink::Source(Box::new(GroupBySource::new(
+                    &self.ooc_state.io_thread,
+                    self.slice,
+                    self.global_table.clone(),
+                )?)))
             }
         }
     }
 
     fn as_any(&mut self) -> &mut dyn Any {
         self
     }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/thread_local.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/generic/thread_local.rs`

 * *Files 3% similar despite different names*

```diff
@@ -13,31 +13,33 @@
     num_keys: usize,
     spilled: bool,
     // this only fills during the reduce phase IFF
     // there are spilled tuples
     finished_payloads: PartitionVec<Vec<SpillPayload>>,
     keys_dtypes: Arc<[DataType]>,
     agg_dtypes: Arc<[DataType]>,
+    output_schema: SchemaRef,
 }
 
 impl SpillPartitions {
-    fn new(keys: Arc<[DataType]>, aggs: Arc<[DataType]>) -> Self {
+    fn new(keys: Arc<[DataType]>, aggs: Arc<[DataType]>, output_schema: SchemaRef) -> Self {
         let hash_partitioned = vec![];
         let chunk_index_partitioned = vec![];
 
-        // construct via split so that preallocation succeeds
+        // construct via split so that pre-allocation succeeds
         Self {
             keys_aggs_partitioned: vec![],
             hash_partitioned,
             chunk_index_partitioned,
             num_keys: keys.as_ref().len(),
             spilled: false,
             finished_payloads: vec![],
             keys_dtypes: keys,
             agg_dtypes: aggs,
+            output_schema,
         }
         .split()
     }
 
     fn split(&self) -> Self {
         let n_columns = self.keys_dtypes.as_ref().len() + self.agg_dtypes.as_ref().len();
 
@@ -64,14 +66,15 @@
             hash_partitioned,
             chunk_index_partitioned,
             num_keys: self.num_keys,
             spilled: false,
             finished_payloads: vec![],
             keys_dtypes: self.keys_dtypes.clone(),
             agg_dtypes: self.agg_dtypes.clone(),
+            output_schema: self.output_schema.clone(),
         }
     }
 }
 
 impl SpillPartitions {
     fn pre_alloc(&mut self) {
         if !self.spilled {
@@ -141,15 +144,23 @@
                 std::mem::swap(&mut new_chunk_indexes, chunk_indexes);
 
                 Some((
                     partition,
                     SpillPayload {
                         hashes: new_hashes,
                         chunk_idx: new_chunk_indexes,
-                        keys_and_aggs: keys_aggs.iter_mut().map(|b| b.reset(OB_SIZE)).collect(),
+                        keys_and_aggs: keys_aggs
+                            .iter_mut()
+                            .zip(self.output_schema.iter_names())
+                            .map(|(b, name)| {
+                                let mut s = b.reset(OB_SIZE);
+                                s.rename(name);
+                                s
+                            })
+                            .collect(),
                         num_keys: self.num_keys,
                     },
                 ))
             } else {
                 None
             }
         }
@@ -232,23 +243,19 @@
     spill_partitions: SpillPartitions,
 }
 
 impl ThreadLocalTable {
     pub(super) fn new(
         agg_constructors: Arc<[AggregateFunction]>,
         key_dtypes: Arc<[DataType]>,
+        agg_dtypes: Arc<[DataType]>,
         output_schema: SchemaRef,
     ) -> Self {
-        let agg_dtypes: Arc<[DataType]> = Arc::from(
-            agg_constructors
-                .iter()
-                .map(|agg| agg.dtype())
-                .collect::<Vec<_>>(),
-        );
-        let spill_partitions = SpillPartitions::new(key_dtypes.clone(), agg_dtypes);
+        let spill_partitions =
+            SpillPartitions::new(key_dtypes.clone(), agg_dtypes, output_schema.clone());
 
         Self {
             inner_map: AggHashTable::new(
                 agg_constructors,
                 key_dtypes.as_ref(),
                 output_schema,
                 Some(256),
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/mod.rs`

 * *Files 6% similar despite different names*

```diff
@@ -9,16 +9,14 @@
 pub(crate) use generic::GenericGroupby2;
 use polars_core::prelude::*;
 #[cfg(feature = "dtype-categorical")]
 use polars_core::using_string_cache;
 pub(crate) use primitive::*;
 pub(crate) use string::*;
 
-const MEMORY_FRACTION_THRESHOLD: f64 = 0.3;
-
 pub(super) fn physical_agg_to_logical(cols: &mut [Series], output_schema: &Schema) {
     for (s, (name, dtype)) in cols.iter_mut().zip(output_schema.iter()) {
         if s.name() != name {
             s.rename(name);
         }
         match dtype {
             #[cfg(feature = "dtype-categorical")]
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/ooc.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/ooc.rs`

 * *Files 3% similar despite different names*

```diff
@@ -81,15 +81,15 @@
                     })
                     .collect::<PolarsResult<Vec<_>>>()?;
 
                 // create a pipeline with a the files as sources and the groupby as sink
                 let mut pipe =
                     PipeLine::new_simple(sources, vec![], self.groupby_sink.split(0), verbose());
 
-                match pipe.run_pipeline(context)? {
+                match pipe.run_pipeline(context, Default::default())?.unwrap() {
                     FinalizedSink::Finished(mut df) => {
                         if let Some(slice) = &mut self.slice {
                             let height = df.height();
                             if slice.0 >= height {
                                 slice.0 -= height;
                                 return self.get_batches(context);
                             } else {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/primitive/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/primitive/mod.rs`

 * *Files 2% similar despite different names*

```diff
@@ -24,15 +24,14 @@
 use crate::executors::sinks::groupby::string::{apply_aggregate, write_agg_idx};
 use crate::executors::sinks::groupby::utils::{compute_slices, finalize_groupby};
 use crate::executors::sinks::io::IOThread;
 use crate::executors::sinks::utils::load_vec;
 use crate::executors::sinks::HASHMAP_INIT_SIZE;
 use crate::expressions::PhysicalPipedExpr;
 use crate::operators::{DataChunk, FinalizedSink, PExecutionContext, Sink, SinkResult};
-use crate::pipeline::FORCE_OOC_GROUPBY;
 
 // hash + value
 #[derive(Eq, Copy, Clone)]
 struct Key<T: Copy> {
     hash: u64,
     value: T,
 }
@@ -88,24 +87,24 @@
         key: Arc<dyn PhysicalPipedExpr>,
         aggregation_columns: Arc<Vec<Arc<dyn PhysicalPipedExpr>>>,
         agg_fns: Vec<AggregateFunction>,
         input_schema: SchemaRef,
         output_schema: SchemaRef,
         slice: Option<(i64, usize)>,
     ) -> Self {
-        let ooc = std::env::var(FORCE_OOC_GROUPBY).is_ok();
+        // this ooc is broken fix later
         Self::new_inner(
             key,
             aggregation_columns,
             agg_fns,
             input_schema,
             output_schema,
             slice,
             None,
-            ooc,
+            false,
         )
     }
 
     #[allow(clippy::too_many_arguments)]
     pub(crate) fn new_inner(
         key: Arc<dyn PhysicalPipedExpr>,
         aggregation_columns: Arc<Vec<Arc<dyn PhysicalPipedExpr>>>,
@@ -348,15 +347,15 @@
             return self.sink_ooc(context, chunk);
         }
         let s = self.prepare_key_and_aggregation_series(context, &chunk)?;
         // cow -> &series -> &dyn series_trait -> &chunkedarray
         let ca: &ChunkedArray<K> = s.as_ref().as_ref();
 
         // sorted fast path
-        if matches!(ca.is_sorted_flag2(), IsSorted::Ascending) && ca.null_count() == 0 {
+        if matches!(ca.is_sorted_flag(), IsSorted::Ascending) && ca.null_count() == 0 {
             return self.sink_sorted(ca, chunk);
         }
 
         s.vec_hash(self.hb.clone(), &mut self.hashes).unwrap();
 
         // this reuses the hashes buffer as [u64] as idx buffer as [idxsize]
         // write the hashes to self.hashes buffer
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/string.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/string.rs`

 * *Files 2% similar despite different names*

```diff
@@ -21,15 +21,14 @@
 use crate::executors::sinks::groupby::primitive::apply_aggregation;
 use crate::executors::sinks::groupby::utils::{compute_slices, finalize_groupby};
 use crate::executors::sinks::io::IOThread;
 use crate::executors::sinks::utils::load_vec;
 use crate::executors::sinks::HASHMAP_INIT_SIZE;
 use crate::expressions::PhysicalPipedExpr;
 use crate::operators::{DataChunk, FinalizedSink, PExecutionContext, Sink, SinkResult};
-use crate::pipeline::FORCE_OOC_GROUPBY;
 
 // we store a hashmap per partition (partitioned by hash)
 // the hashmap contains indexes as keys and as values
 // those indexes point into the keys buffer and the values buffer
 // the keys buffer are buffers of AnyValue per partition
 // and the values are buffer of Aggregation functions per partition
 pub struct Utf8GroupbySink {
@@ -68,24 +67,23 @@
         key_column: Arc<dyn PhysicalPipedExpr>,
         aggregation_columns: Arc<Vec<Arc<dyn PhysicalPipedExpr>>>,
         agg_fns: Vec<AggregateFunction>,
         input_schema: SchemaRef,
         output_schema: SchemaRef,
         slice: Option<(i64, usize)>,
     ) -> Self {
-        let ooc = std::env::var(FORCE_OOC_GROUPBY).is_ok();
         Self::new_inner(
             key_column,
             aggregation_columns,
             agg_fns,
             input_schema,
             output_schema,
             slice,
             None,
-            ooc,
+            false,
         )
     }
 
     #[allow(clippy::too_many_arguments)]
     fn new_inner(
         key_column: Arc<dyn PhysicalPipedExpr>,
         aggregation_columns: Arc<Vec<Arc<dyn PhysicalPipedExpr>>>,
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/groupby/utils.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/groupby/utils.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/io.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/io.rs`

 * *Files 9% similar despite different names*

```diff
@@ -20,27 +20,29 @@
 pub(crate) struct IOThread {
     sender: Sender<Payload>,
     _lockfile: Arc<LockFile>,
     pub(in crate::executors::sinks) dir: PathBuf,
     pub(in crate::executors::sinks) sent: Arc<AtomicUsize>,
     pub(in crate::executors::sinks) total: Arc<AtomicUsize>,
     pub(in crate::executors::sinks) thread_local_count: Arc<AtomicUsize>,
+    schema: SchemaRef,
 }
 
 fn get_lockfile_path(dir: &Path) -> PathBuf {
     let mut lockfile_path = dir.to_path_buf();
     lockfile_path.push(".lock");
     lockfile_path
 }
 
 /// Starts a new thread that will clean up operations of directories that don't
 /// have a lockfile (opened with 'w' permissions).
 fn gc_thread(operation_name: &'static str) {
     let _ = std::thread::spawn(move || {
-        let dir = resolve_homedir(Path::new(&format!("~/.polars/{operation_name}/")));
+        let mut dir = std::env::temp_dir();
+        dir.push(&format!("polars/{operation_name}"));
 
         // if the directory does not exist, there is nothing to clean
         let rd = match std::fs::read_dir(&dir) {
             Ok(rd) => rd,
             _ => panic!("cannot find {:?}", dir),
         };
 
@@ -79,15 +81,16 @@
         operation_name: &'static str,
     ) -> PolarsResult<Self> {
         let uuid = SystemTime::now()
             .duration_since(UNIX_EPOCH)
             .unwrap()
             .as_nanos();
 
-        let dir = resolve_homedir(Path::new(&format!("~/.polars/{operation_name}/{uuid}")));
+        let mut dir = std::env::temp_dir();
+        dir.push(&format!("polars/{operation_name}/{uuid}"));
         std::fs::create_dir_all(&dir)?;
 
         // make sure we create lockfile before we GC
         let lockfile_path = get_lockfile_path(&dir);
         let lockfile = Arc::new(LockFile::new(lockfile_path)?);
 
         // start a thread that will clean up old dumps.
@@ -100,15 +103,17 @@
         let sent: Arc<AtomicUsize> = Default::default();
         let total: Arc<AtomicUsize> = Default::default();
         let thread_local_count: Arc<AtomicUsize> = Default::default();
 
         let dir2 = dir.clone();
         let total2 = total.clone();
         let lockfile2 = lockfile.clone();
+        let schema2 = schema.clone();
         std::thread::spawn(move || {
+            let schema = schema2;
             // this moves the lockfile in the thread
             // we keep one in the thread and one in the `IoThread` struct
             let _keep_hold_on_lockfile = lockfile2;
 
             let mut count = 0usize;
 
             // We accept 2 cases. E.g.
@@ -121,26 +126,26 @@
                     for (part, df) in partitions.into_no_null_iter().zip(iter) {
                         let mut path = dir2.clone();
                         path.push(format!("{part}"));
 
                         let _ = std::fs::create_dir(&path);
                         path.push(format!("{count}.ipc"));
 
-                        let file = std::fs::File::create(path).unwrap();
+                        let file = File::create(path).unwrap();
                         let writer = IpcWriter::new(file);
                         let mut writer = writer.batched(&schema).unwrap();
                         writer.write_batch(&df).unwrap();
                         writer.finish().unwrap();
                         count += 1;
                     }
                 } else {
                     let mut path = dir2.clone();
                     path.push(format!("{count}.ipc"));
 
-                    let file = std::fs::File::create(path).unwrap();
+                    let file = File::create(path).unwrap();
                     let writer = IpcWriter::new(file);
                     let mut writer = writer.batched(&schema).unwrap();
 
                     for df in iter {
                         writer.write_batch(&df).unwrap();
                     }
                     writer.finish().unwrap();
@@ -154,42 +159,63 @@
         Ok(Self {
             sender,
             dir,
             sent,
             total,
             _lockfile: lockfile,
             thread_local_count,
+            schema,
         })
     }
 
     pub(in crate::executors::sinks) fn dump_chunk(&self, mut df: DataFrame) {
         // if IO thread is blocked
         // we write locally on this thread
         if self.sender.is_full() {
             let mut path = self.dir.clone();
             let count = self.thread_local_count.fetch_add(1, Ordering::Relaxed);
             // thread local name we start with an underscore to ensure we don't get
             // duplicates
             path.push(format!("_{count}.ipc"));
 
-            let file = std::fs::File::create(path).unwrap();
+            let file = File::create(path).unwrap();
             let mut writer = IpcWriter::new(file);
             writer.finish(&mut df).unwrap();
         } else {
             let iter = Box::new(std::iter::once(df));
             self.dump_iter(None, iter)
         }
     }
 
     pub(in crate::executors::sinks) fn dump_partition(&self, partition_no: IdxSize, df: DataFrame) {
         let partition = Some(IdxCa::from_vec("", vec![partition_no]));
         let iter = Box::new(std::iter::once(df));
         self.dump_iter(partition, iter)
     }
 
+    pub(in crate::executors::sinks) fn dump_partition_local(
+        &self,
+        partition_no: IdxSize,
+        df: DataFrame,
+    ) {
+        let count = self.thread_local_count.fetch_add(1, Ordering::Relaxed);
+        let mut path = self.dir.clone();
+        path.push(format!("{partition_no}"));
+
+        let _ = std::fs::create_dir(&path);
+        // thread local name we start with an underscore to ensure we don't get
+        // duplicates
+        path.push(format!("_{count}.ipc"));
+        let file = File::create(path).unwrap();
+        let writer = IpcWriter::new(file);
+        let mut writer = writer.batched(&self.schema).unwrap();
+        writer.write_batch(&df).unwrap();
+        writer.finish().unwrap();
+    }
+
     pub(in crate::executors::sinks) fn dump_iter(&self, partition: Option<IdxCa>, iter: DfIter) {
         let add = iter.size_hint().1.unwrap();
         self.sender.send((partition, iter)).unwrap();
         self.sent.fetch_add(add, Ordering::Relaxed);
     }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/joins/cross.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/joins/cross.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/joins/generic_build.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/joins/generic_build.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/joins/inner_left.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/joins/inner_left.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/ordered.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/ordered.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/reproject.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/reproject.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/slice.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/slice.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/sort/ooc.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/groupby_dynamic.rs`

 * *Files 27% similar despite different names*

```diff
@@ -1,110 +1,124 @@
-use std::path::Path;
-
-use polars_core::prelude::*;
-use polars_core::utils::_split_offsets;
-use polars_core::POOL;
-use polars_io::ipc::IpcReader;
-use polars_io::SerReader;
-use polars_ops::prelude::*;
-use rayon::prelude::*;
-
-use crate::executors::sinks::io::{block_thread_until_io_thread_done, DfIter, IOThread};
-use crate::executors::sinks::sort::source::SortSource;
-use crate::operators::FinalizedSink;
-
-pub(super) fn read_df(path: &Path) -> PolarsResult<DataFrame> {
-    let file = std::fs::File::open(path)?;
-    IpcReader::new(file).set_rechunk(false).finish()
+#[cfg(feature = "dynamic_groupby")]
+use polars_core::frame::groupby::GroupBy;
+#[cfg(feature = "dynamic_groupby")]
+use polars_time::DynamicGroupOptions;
+
+use super::*;
+
+#[cfg_attr(not(feature = "dynamic_groupby"), allow(dead_code))]
+pub(crate) struct GroupByDynamicExec {
+    pub(crate) input: Box<dyn Executor>,
+    // we will use this later
+    #[allow(dead_code)]
+    pub(crate) keys: Vec<Arc<dyn PhysicalExpr>>,
+    pub(crate) aggs: Vec<Arc<dyn PhysicalExpr>>,
+    #[cfg(feature = "dynamic_groupby")]
+    pub(crate) options: DynamicGroupOptions,
+    pub(crate) input_schema: SchemaRef,
+    pub(crate) slice: Option<(i64, usize)>,
+    pub(crate) apply: Option<Arc<dyn DataFrameUdf>>,
 }
 
-pub(super) fn sort_ooc(
-    io_thread: &IOThread,
-    partitions: Series,
-    idx: usize,
-    descending: bool,
-    slice: Option<(i64, usize)>,
-) -> PolarsResult<FinalizedSink> {
-    let partitions = partitions.to_physical_repr().into_owned();
-
-    // we collect as I am not sure that if we write to the same directory the
-    // iterator will read those also.
-    // We don't want to merge files we just written to disk
-    let dir = &io_thread.dir;
-    let files = std::fs::read_dir(dir)?.collect::<std::io::Result<Vec<_>>>()?;
-
-    let offsets = _split_offsets(files.len(), POOL.current_num_threads());
-    POOL.install(|| {
-        offsets.par_iter().try_for_each(|(offset, len)| {
-            let files = &files[*offset..*offset + *len];
-
-            for entry in files {
-                let path = entry.path();
-
-                // don't read the lock file
-                if path.ends_with(".lock") {
-                    continue;
-                }
-                let df = read_df(&path)?;
-
-                let sort_col = &df.get_columns()[idx];
-                let assigned_parts = det_partitions(sort_col, &partitions, descending);
-
-                // partition the dataframe into proper buckets
-                let (iter, unique_assigned_parts) = partition_df(df, &assigned_parts)?;
-                io_thread.dump_iter(Some(unique_assigned_parts), iter);
-            }
-            PolarsResult::Ok(())
-        })
-    })?;
-
-    block_thread_until_io_thread_done(io_thread);
-
-    let files = std::fs::read_dir(dir)?
-        .flat_map(|entry| {
-            entry
-                .map(|entry| {
-                    let path = entry.path();
-                    if path.is_dir() {
-                        let dirname = path.file_name().unwrap();
-                        let partition = dirname.to_string_lossy().parse::<u32>().unwrap();
-                        Some((partition, path))
-                    } else {
-                        None
-                    }
-                })
-                .transpose()
-        })
-        .collect::<std::io::Result<Vec<_>>>()?;
+impl GroupByDynamicExec {
+    #[cfg(feature = "dynamic_groupby")]
+    fn execute_impl(
+        &mut self,
+        state: &mut ExecutionState,
+        mut df: DataFrame,
+    ) -> PolarsResult<DataFrame> {
+        df.as_single_chunk_par();
+
+        let keys = self
+            .keys
+            .iter()
+            .map(|e| e.evaluate(&df, state))
+            .collect::<PolarsResult<Vec<_>>>()?;
+
+        let (mut time_key, mut keys, groups) = df.groupby_dynamic(keys, &self.options)?;
+
+        if let Some(f) = &self.apply {
+            let gb = GroupBy::new(&df, vec![], groups, None);
+            let out = gb.apply(move |df| f.call_udf(df))?;
+            return Ok(if let Some((offset, len)) = self.slice {
+                out.slice(offset, len)
+            } else {
+                out
+            });
+        }
 
-    let source = SortSource::new(files, idx, descending, slice);
-    Ok(FinalizedSink::Source(Box::new(source)))
-}
+        let mut groups = &groups;
+        #[allow(unused_assignments)]
+        // it is unused because we only use it to keep the lifetime of sliced_group valid
+        let mut sliced_groups = None;
+
+        if let Some((offset, len)) = self.slice {
+            sliced_groups = Some(groups.slice(offset, len));
+            groups = sliced_groups.as_deref().unwrap();
+
+            time_key = time_key.slice(offset, len);
+
+            // todo! optimize this, we can prevent an agg_first aggregation upstream
+            // the ordering has changed due to the groupby
+            for key in keys.iter_mut() {
+                *key = key.slice(offset, len)
+            }
+        }
 
-fn det_partitions(s: &Series, partitions: &Series, descending: bool) -> IdxCa {
-    let s = s.to_physical_repr();
+        state.expr_cache = Some(Default::default());
+        let agg_columns = POOL.install(|| {
+            self.aggs
+                .par_iter()
+                .map(|expr| {
+                    let agg = expr.evaluate_on_groups(&df, groups, state)?.finalize();
+                    polars_ensure!(agg.len() == groups.len(), agg_len = agg.len(), groups.len());
+                    Ok(agg)
+                })
+                .collect::<PolarsResult<Vec<_>>>()
+        })?;
+        state.expr_cache = None;
+
+        let mut columns = Vec::with_capacity(agg_columns.len() + 1 + keys.len());
+        columns.extend_from_slice(&keys);
+        columns.push(time_key);
+        columns.extend_from_slice(&agg_columns);
 
-    search_sorted(partitions, &s, SearchSortedSide::Any, descending).unwrap()
+        DataFrame::new(columns)
+    }
 }
 
-fn partition_df(df: DataFrame, partitions: &IdxCa) -> PolarsResult<(DfIter, IdxCa)> {
-    let groups = partitions.group_tuples(false, false)?;
-    let partitions = unsafe { partitions.clone().into_series().agg_first(&groups) };
-    let partitions = partitions.idx().unwrap().clone();
-
-    let out = match groups {
-        GroupsProxy::Idx(idx) => {
-            let iter = idx.into_iter().map(move |(_, group)| {
-                // groups are in bounds
-                unsafe { df._take_unchecked_slice(&group, false) }
-            });
-            Box::new(iter) as DfIter
+impl Executor for GroupByDynamicExec {
+    #[cfg(not(feature = "dynamic_groupby"))]
+    fn execute(&mut self, _state: &mut ExecutionState) -> PolarsResult<DataFrame> {
+        panic!("activate feature dynamic_groupby")
+    }
+
+    #[cfg(feature = "dynamic_groupby")]
+    fn execute(&mut self, state: &mut ExecutionState) -> PolarsResult<DataFrame> {
+        #[cfg(debug_assertions)]
+        {
+            if state.verbose() {
+                println!("run GroupbyDynamicExec")
+            }
         }
-        GroupsProxy::Slice { groups, .. } => {
-            let iter = groups
-                .into_iter()
-                .map(move |[first, len]| df.slice(first as i64, len as usize));
-            Box::new(iter) as DfIter
+        let df = self.input.execute(state)?;
+
+        let profile_name = if state.has_node_timer() {
+            let by = self
+                .keys
+                .iter()
+                .map(|s| Ok(s.to_field(&self.input_schema)?.name))
+                .collect::<PolarsResult<Vec<_>>>()?;
+            let name = comma_delimited("groupby_dynamic".to_string(), &by);
+            Cow::Owned(name)
+        } else {
+            Cow::Borrowed("")
+        };
+
+        if state.has_node_timer() {
+            let new_state = state.clone();
+            new_state.record(|| self.execute_impl(state, df), profile_name)
+        } else {
+            self.execute_impl(state, df)
         }
-    };
-    Ok((out, partitions))
+    }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/sort/sink.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/sort/sink.rs`

 * *Files 9% similar despite different names*

```diff
@@ -1,57 +1,63 @@
 use std::any::Any;
-use std::collections::VecDeque;
 use std::sync::{Arc, RwLock};
 
 use polars_core::config::verbose;
 use polars_core::error::PolarsResult;
 use polars_core::frame::DataFrame;
 use polars_core::prelude::{AnyValue, SchemaRef, Series, SortOptions};
 use polars_core::utils::accumulate_dataframes_vertical_unchecked;
 use polars_plan::prelude::SortArguments;
 
 use crate::executors::sinks::io::{block_thread_until_io_thread_done, IOThread};
 use crate::executors::sinks::memory::MemTracker;
 use crate::executors::sinks::sort::ooc::sort_ooc;
 use crate::operators::{DataChunk, FinalizedSink, PExecutionContext, Sink, SinkResult};
-use crate::pipeline::{morsels_per_sink, FORCE_OOC_SORT};
+use crate::pipeline::{morsels_per_sink, FORCE_OOC};
 
 pub struct SortSink {
     schema: SchemaRef,
-    chunks: VecDeque<DataFrame>,
+    chunks: Vec<DataFrame>,
     // Stores available memory in the system at the start of this sink.
     // and stores the memory used by this this sink.
     mem_track: MemTracker,
     // sort in-memory or out-of-core
     ooc: bool,
     // when ooc, we write to disk using an IO thread
     // RwLock as we want to have multiple readers at once.
     io_thread: Arc<RwLock<Option<IOThread>>>,
     // location in the dataframe of the columns to sort by
     sort_idx: usize,
     sort_args: SortArguments,
+    // Statistics
     // sampled values so we can find the distribution.
     dist_sample: Vec<AnyValue<'static>>,
+    // total rows accumulated in current chunk
+    current_chunk_rows: usize,
+    // total bytes of tables in current chunks
+    current_chunks_size: usize,
 }
 
 impl SortSink {
     pub(crate) fn new(sort_idx: usize, sort_args: SortArguments, schema: SchemaRef) -> Self {
         // for testing purposes
-        let ooc = std::env::var(FORCE_OOC_SORT).is_ok();
+        let ooc = std::env::var(FORCE_OOC).is_ok();
         let n_morsels_per_sink = morsels_per_sink();
 
         let mut out = Self {
             schema,
             chunks: Default::default(),
             mem_track: MemTracker::new(n_morsels_per_sink),
             ooc,
             io_thread: Default::default(),
             sort_idx,
             sort_args,
             dist_sample: vec![],
+            current_chunk_rows: 0,
+            current_chunks_size: 0,
         };
         if ooc {
             eprintln!("OOC sort forced");
             out.init_ooc().unwrap();
         }
         out
     }
@@ -67,86 +73,104 @@
         if iot.is_none() {
             *iot = Some(IOThread::try_new(self.schema.clone(), "sort")?)
         }
         Ok(())
     }
 
     fn store_chunk(&mut self, chunk: DataChunk) -> PolarsResult<()> {
+        let chunk_bytes = chunk.data.estimated_size();
         if !self.ooc {
-            let chunk_bytes = chunk.data.estimated_size();
             let used = self.mem_track.fetch_add(chunk_bytes);
             let free = self.mem_track.get_available();
 
             // we need some free memory to be able to sort
             // so we keep 3x the sort data size before we go out of core
             if used * 3 > free {
                 self.init_ooc()?;
+                self.dump(true)?;
             }
-        }
-        self.chunks.push_back(chunk.data);
+        };
+        self.current_chunks_size += chunk_bytes;
+        self.current_chunk_rows += chunk.data.height();
+        self.chunks.push(chunk.data);
         Ok(())
     }
 
-    fn dump(&mut self) -> PolarsResult<()> {
-        // take from the front so that sorted data remains sorted in writing order
-        while let Some(df) = self.chunks.pop_front() {
+    fn dump(&mut self, force: bool) -> PolarsResult<()> {
+        let larger_than_32_mb = self.current_chunks_size > 1 << 25;
+        if (force || larger_than_32_mb || self.current_chunk_rows > 50_000)
+            && !self.chunks.is_empty()
+        {
+            // into a single chunk because multiple file IO's is expensive
+            // and may lead to many smaller files in ooc-sort later, which is exponentially
+            // expensive
+            let df = accumulate_dataframes_vertical_unchecked(self.chunks.drain(..));
             if df.height() > 0 {
                 // safety: we just asserted height > 0
                 let sample = unsafe {
                     let s = &df.get_columns()[self.sort_idx];
                     s.to_physical_repr().get_unchecked(0).into_static().unwrap()
                 };
                 self.dist_sample.push(sample);
 
                 let iot = self.io_thread.read().unwrap();
                 let iot = iot.as_ref().unwrap();
-                iot.dump_chunk(df)
+
+                iot.dump_chunk(df);
+
+                // reset sizes
+                self.current_chunk_rows = 0;
+                self.current_chunks_size = 0;
             }
         }
         Ok(())
     }
 }
 
 impl Sink for SortSink {
     fn sink(&mut self, _context: &PExecutionContext, chunk: DataChunk) -> PolarsResult<SinkResult> {
         self.store_chunk(chunk)?;
 
         if self.ooc {
-            self.dump()?;
+            self.dump(false)?;
         }
         Ok(SinkResult::CanHaveMoreInput)
     }
 
     fn combine(&mut self, other: &mut dyn Sink) {
         let other = other.as_any().downcast_mut::<Self>().unwrap();
         self.chunks.extend(std::mem::take(&mut other.chunks));
         self.ooc |= other.ooc;
         self.dist_sample
             .extend(std::mem::take(&mut other.dist_sample));
 
         if self.ooc {
-            self.dump().unwrap()
+            self.dump(false).unwrap()
         }
     }
 
     fn split(&self, _thread_no: usize) -> Box<dyn Sink> {
         Box::new(Self {
             schema: self.schema.clone(),
             chunks: Default::default(),
             mem_track: self.mem_track.clone(),
             ooc: self.ooc,
             io_thread: self.io_thread.clone(),
             sort_idx: self.sort_idx,
             sort_args: self.sort_args.clone(),
             dist_sample: vec![],
+            current_chunk_rows: 0,
+            current_chunks_size: 0,
         })
     }
 
-    fn finalize(&mut self, _context: &PExecutionContext) -> PolarsResult<FinalizedSink> {
+    fn finalize(&mut self, context: &PExecutionContext) -> PolarsResult<FinalizedSink> {
         if self.ooc {
+            // spill everything
+            self.dump(true).unwrap();
             let lock = self.io_thread.read().unwrap();
             let io_thread = lock.as_ref().unwrap();
 
             let dist = Series::from_any_values("", &self.dist_sample, false).unwrap();
             let dist = dist.sort_with(SortOptions {
                 descending: self.sort_args.descending[0],
                 nulls_last: self.sort_args.nulls_last,
@@ -157,14 +181,15 @@
 
             sort_ooc(
                 io_thread,
                 dist,
                 self.sort_idx,
                 self.sort_args.descending[0],
                 self.sort_args.slice,
+                context.verbose,
             )
         } else {
             let chunks = std::mem::take(&mut self.chunks);
             let df = accumulate_dataframes_vertical_unchecked(chunks);
             let df = sort_accumulated(
                 df,
                 self.sort_idx,
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/sort/sink_multiple.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/sort/sink_multiple.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/sort/source.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/sort/source.rs`

 * *Files 6% similar despite different names*

```diff
@@ -21,15 +21,20 @@
 
 impl SortSource {
     pub(super) fn new(
         mut files: Vec<(u32, PathBuf)>,
         sort_idx: usize,
         descending: bool,
         slice: Option<(i64, usize)>,
+        verbose: bool,
     ) -> Self {
+        if verbose {
+            eprintln!("started sort source phase");
+        }
+
         files.sort_unstable_by_key(|entry| entry.0);
 
         let n_threads = POOL.current_num_threads();
         let files = files.into_iter();
 
         Self {
             files,
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sinks/utils.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sinks/utils.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sources/csv.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sources/csv.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sources/frame.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sources/frame.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sources/ipc_one_shot.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sources/ipc_one_shot.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sources/parquet.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sources/parquet.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sources/reproject.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sources/reproject.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/executors/sources/union.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/executors/sources/union.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/operators/chunks.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/operators/chunks.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/operators/sink.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/operators/sink.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/pipeline/convert.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/pipeline/convert.rs`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,12 @@
+use std::cell::RefCell;
+use std::rc::Rc;
 use std::sync::Arc;
 
+use hashbrown::hash_map::Entry;
 use polars_core::prelude::*;
 use polars_core::with_match_physical_integer_polars_type;
 use polars_plan::prelude::*;
 
 use crate::executors::sinks::groupby::aggregates::convert_to_hash_agg;
 use crate::executors::sinks::groupby::GenericGroupby2;
 use crate::executors::sinks::*;
@@ -118,28 +121,28 @@
     to_physical: &F,
 ) -> PolarsResult<Box<dyn Sink>>
 where
     F: Fn(Node, &Arena<AExpr>, Option<&SchemaRef>) -> PolarsResult<Arc<dyn PhysicalPipedExpr>>,
 {
     use ALogicalPlan::*;
     let out = match lp_arena.get(node) {
-        #[cfg(any(feature = "parquet", feature = "ipc"))]
         FileSink { input, payload } => {
             let path = payload.path.as_ref().as_path();
             let input_schema = lp_arena.get(*input).schema(lp_arena);
             match &payload.file_type {
                 #[cfg(feature = "parquet")]
                 FileType::Parquet(options) => {
                     Box::new(ParquetSink::new(path, *options, input_schema.as_ref())?)
                         as Box<dyn Sink>
                 }
                 #[cfg(feature = "ipc")]
                 FileType::Ipc(options) => {
                     Box::new(IpcSink::new(path, *options, input_schema.as_ref())?) as Box<dyn Sink>
                 }
+                FileType::Memory => Box::new(OrderedSink::new()),
             }
         }
         Join {
             input_left,
             input_right,
             options,
             left_on,
@@ -279,28 +282,31 @@
                 expr_arena,
                 to_physical,
                 Some(&input_schema),
             )?);
 
             let mut aggregation_columns = Vec::with_capacity(aggs.len());
             let mut agg_fns = Vec::with_capacity(aggs.len());
+            let mut input_agg_dtypes = Vec::with_capacity(aggs.len());
 
             for node in &aggs {
-                let (index, agg_fn) =
+                let (input_dtype, index, agg_fn) =
                     convert_to_hash_agg(*node, expr_arena, &input_schema, &to_physical);
                 aggregation_columns.push(index);
-                agg_fns.push(agg_fn)
+                agg_fns.push(agg_fn);
+                input_agg_dtypes.push(input_dtype);
             }
             let aggregation_columns = Arc::new(aggregation_columns);
 
             let groupby_sink = Box::new(GenericGroupby2::new(
                 key_columns,
                 aggregation_columns,
                 Arc::from(agg_fns),
                 output_schema,
+                input_agg_dtypes,
                 options.slice,
             ));
 
             Box::new(ReProjectSink::new(input_schema, groupby_sink))
         }
         Aggregate {
             input,
@@ -316,29 +322,32 @@
                 expr_arena,
                 to_physical,
                 Some(&input_schema),
             )?);
 
             let mut aggregation_columns = Vec::with_capacity(aggs.len());
             let mut agg_fns = Vec::with_capacity(aggs.len());
+            let mut input_agg_dtypes = Vec::with_capacity(aggs.len());
 
             for node in aggs {
-                let (index, agg_fn) =
+                let (input_dtype, index, agg_fn) =
                     convert_to_hash_agg(*node, expr_arena, &input_schema, &to_physical);
                 aggregation_columns.push(index);
-                agg_fns.push(agg_fn)
+                agg_fns.push(agg_fn);
+                input_agg_dtypes.push(input_dtype);
             }
             let aggregation_columns = Arc::new(aggregation_columns);
 
             if std::env::var("POLARS_STREAMING_GB2").as_deref() == Ok("1") {
                 Box::new(GenericGroupby2::new(
                     key_columns,
                     aggregation_columns,
                     Arc::from(agg_fns),
                     output_schema.clone(),
+                    input_agg_dtypes,
                     options.slice,
                 ))
             } else {
                 match (
                     output_schema.get_at_index(0).unwrap().1.to_physical(),
                     keys.len(),
                 ) {
@@ -363,14 +372,15 @@
                         options.slice,
                     )) as Box<dyn Sink>,
                     _ => Box::new(GenericGroupby2::new(
                         key_columns,
                         aggregation_columns,
                         Arc::from(agg_fns),
                         output_schema.clone(),
+                        input_agg_dtypes,
                         options.slice,
                     )),
                 }
             }
         }
         lp => {
             panic!("{lp:?} not implemented")
@@ -426,32 +436,37 @@
             };
             Box::new(op) as Box<dyn Operator>
         }
         MapFunction { function, .. } => {
             let op = operators::FunctionOperator::new(function.clone());
             Box::new(op) as Box<dyn Operator>
         }
+        Union { .. } => {
+            let op = operators::Pass::new("union");
+            Box::new(op) as Box<dyn Operator>
+        }
 
         lp => {
             panic!("operator {lp:?} not (yet) supported")
         }
     };
     Ok(op)
 }
 
 #[allow(clippy::too_many_arguments)]
 pub fn create_pipeline<F>(
     sources: &[Node],
     operators: Vec<Box<dyn Operator>>,
     operator_nodes: Vec<Node>,
-    sink_nodes: Vec<(usize, Node)>,
+    sink_nodes: Vec<(usize, Node, Rc<RefCell<u32>>)>,
     lp_arena: &mut Arena<ALogicalPlan>,
     expr_arena: &mut Arena<AExpr>,
     to_physical: F,
     verbose: bool,
+    sink_cache: &mut PlHashMap<usize, Box<dyn Sink>>,
 ) -> PolarsResult<PipeLine>
 where
     F: Fn(Node, &Arena<AExpr>, Option<&SchemaRef>) -> PolarsResult<Arc<dyn PhysicalPipedExpr>>,
 {
     use ALogicalPlan::*;
 
     let mut source_objects = Vec::with_capacity(sources.len());
@@ -511,38 +526,36 @@
         source_objects.push(src)
     }
 
     // this offset is because the source might have inserted operators
     let operator_offset = operator_objects.len();
     operator_objects.extend(operators);
 
-    let mut sink_nodes = sink_nodes
+    let sink_nodes = sink_nodes
         .into_iter()
-        .map(|(offset, node)| {
-            Ok((
-                offset + operator_offset,
-                node,
-                get_sink(node, lp_arena, expr_arena, &to_physical)?,
-            ))
+        .map(|(offset, node, shared_count)| {
+            // ensure that shared sinks are really shared
+            // to achieve this we store/fetch them in a cache
+            let sink = if *shared_count.borrow() == 1 {
+                get_sink(node, lp_arena, expr_arena, &to_physical)?
+            } else {
+                match sink_cache.entry(node.0) {
+                    Entry::Vacant(entry) => {
+                        let sink = get_sink(node, lp_arena, expr_arena, &to_physical)?;
+                        entry.insert(sink.split(0));
+                        sink
+                    }
+                    Entry::Occupied(entry) => entry.get().split(0),
+                }
+            };
+
+            Ok((offset + operator_offset, node, sink, shared_count))
         })
         .collect::<PolarsResult<Vec<_>>>()?;
 
-    if sink_nodes.is_empty() ||
-        // if this evaluates true
-        // then there are still operators after the last sink
-        // so we add a final sink to make sure the latest operators run
-        sink_nodes[sink_nodes.len() - 1].0 < operator_nodes.len()
-    {
-        sink_nodes.push((
-            operator_objects.len(),
-            Node::default(),
-            Box::new(OrderedSink::new()),
-        ));
-    }
-
     Ok(PipeLine::new(
         source_objects,
         operator_objects,
         operator_nodes,
         sink_nodes,
         operator_offset,
         verbose,
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-pipe/src/pipeline/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-pipe/src/pipeline/mod.rs`

 * *Files 6% similar despite different names*

```diff
@@ -14,16 +14,15 @@
 }
 
 // Number of OOC partitions.
 // proxy for RAM size multiplier
 pub(crate) const PARTITION_SIZE: usize = 64;
 
 // env vars
-pub(crate) static FORCE_OOC_GROUPBY: &str = "POLARS_FORCE_OOC_GROUPBY";
-pub(crate) static FORCE_OOC_SORT: &str = "POLARS_FORCE_OOC_SORT";
+pub(crate) static FORCE_OOC: &str = "POLARS_FORCE_OOC";
 
 /// ideal chunk size we strive to have
 /// scale the chunk size depending on the number of
 /// columns. With 10 columns we use a chunk size of 40_000
 pub(crate) fn determine_chunk_size(n_cols: usize, n_threads: usize) -> PolarsResult<usize> {
     if let Ok(val) = std::env::var("POLARS_STREAMING_CHUNK_SIZE") {
         val.parse().map_err(
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-row/Cargo.toml` & `polars_lts_cpu-0.18.0/local_dependencies/polars-row/Cargo.toml`

 * *Files 4% similar despite different names*

```diff
@@ -1,28 +1,28 @@
 [package]
 name = "polars-row"
-version = "0.28.0"
+version= "0.30.0"
 edition = "2021"
 license = "MIT"
 repository = "https://github.com/pola-rs/polars"
 description = "Row encodings for the Polars DataFrame library"
 
 # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html
 
 [dependencies]
-polars-error = { version = "0.28.0", path = "../polars-error" }
-polars-utils = { version = "0.28.0", path = "../polars-utils" }
+polars-error = { version = "0.30.0", path = "../polars-error" }
+polars-utils = { version = "0.30.0", path = "../polars-utils" }
 
 [dependencies.arrow]
 package = "arrow2"
 # git = "https://github.com/jorgecarleitao/arrow2"
-git = "https://github.com/ritchie46/arrow2"
+# git = "https://github.com/ritchie46/arrow2"
 # rev = "1491c6e8f4fd100f53c358e4f3ef1536d9e75090"
 # path = "../arrow2"
-branch = "polars_2023-04-20"
+# branch = "polars_2023-05-25"
 version = "0.17"
 default-features = false
 features = [
   "compute_aggregate",
   "compute_arithmetics",
   "compute_boolean",
   "compute_boolean_kleene",
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-row/LICENSE` & `polars_lts_cpu-0.18.0/local_dependencies/polars-utils/LICENSE`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-row/src/encode.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-row/src/encode.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-row/src/encodings/fixed.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-row/src/encodings/fixed.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-row/src/encodings/variable.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-row/src/encodings/variable.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-row/src/lib.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-row/src/lib.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-row/src/row.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-row/src/row.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-row/src/utils.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-row/src/utils.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/Cargo.toml` & `polars_lts_cpu-0.18.0/local_dependencies/polars/Cargo.toml`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 [package]
 name = "polars"
-version= "0.28.0"
+version= "0.30.0"
 authors = ["ritchie46 <ritchie46@gmail.com>"]
 edition = "2021"
 keywords = ["dataframe", "query-engine", "arrow"]
 license = "MIT"
 readme = "../README.md"
 repository = "https://github.com/pola-rs/polars"
 description = "DataFrame Library based on Apache Arrow"
@@ -39,15 +39,15 @@
 # commented out until UB is fixed
 # parallel = ["polars-core/parallel"]
 
 # extra utilities for Utf8Chunked
 strings = ["polars-core/strings", "polars-lazy/strings", "polars-ops/strings"]
 
 # support for ObjectChunked<T> (downcastable Series of any type)
-object = ["polars-core/object", "polars-lazy/object"]
+object = ["polars-core/object", "polars-lazy/object", "polars-io/object"]
 
 # support for arrows json parsing
 json = ["polars-io", "polars-io/json", "polars-lazy/json", "polars-sql/json"]
 
 # support for arrows ipc file parsing
 ipc = ["polars-io", "polars-io/ipc", "polars-lazy/ipc", "polars-sql/ipc"]
 
@@ -66,14 +66,15 @@
   "chunked_ids",
   "dtype-u8",
   "dtype-u16",
   "dtype-struct",
   "cse",
   "polars-ops/performant",
   "streaming",
+  "fused",
 ]
 
 # Dataframe formatting.
 fmt = ["polars-core/fmt"]
 fmt_no_tty = ["polars-core/fmt_no_tty"]
 
 # sort by multiple columns
@@ -87,15 +88,15 @@
 checked_arithmetic = ["polars-core/checked_arithmetic"]
 repeat_by = ["polars-core/repeat_by", "polars-lazy/repeat_by"]
 is_first = ["polars-lazy/is_first", "polars-ops/is_first"]
 is_unique = ["polars-lazy/is_unique", "polars-ops/is_unique"]
 is_last = ["polars-core/is_last"]
 asof_join = ["polars-core/asof_join", "polars-lazy/asof_join", "polars-ops/asof_join"]
 cross_join = ["polars-core/cross_join", "polars-lazy/cross_join", "polars-ops/cross_join"]
-dot_product = ["polars-core/dot_product", "polars-lazy/dot_product"]
+dot_product = ["polars-core/dot_product"]
 concat_str = ["polars-core/concat_str", "polars-lazy/concat_str"]
 row_hash = ["polars-core/row_hash", "polars-lazy/row_hash"]
 reinterpret = ["polars-core/reinterpret"]
 decompress = ["polars-io/decompress"]
 decompress-fast = ["polars-io/decompress-fast"]
 mode = ["polars-core/mode", "polars-lazy/mode"]
 take_opt_iter = ["polars-core/take_opt_iter"]
@@ -147,14 +148,15 @@
 pivot = ["polars-lazy/pivot"]
 top_k = ["polars-lazy/top_k"]
 algo = ["polars-algo"]
 cse = ["polars-lazy/cse"]
 propagate_nans = ["polars-lazy/propagate_nans"]
 coalesce = ["polars-lazy/coalesce"]
 streaming = ["polars-lazy/streaming"]
+fused = ["polars-ops/fused", "polars-lazy/fused"]
 
 test = [
   "lazy",
   "private",
   "rolling_window",
   "rank",
   "round_series",
@@ -175,14 +177,15 @@
 
 # all opt-in datatypes
 dtype-full = [
   "dtype-date",
   "dtype-datetime",
   "dtype-duration",
   "dtype-time",
+  "dtype-array",
   "dtype-i8",
   "dtype-i16",
   "dtype-decimal",
   "dtype-u8",
   "dtype-u16",
   "dtype-categorical",
   "dtype-struct",
@@ -215,14 +218,19 @@
   "polars-core/dtype-duration",
   "polars-lazy/dtype-duration",
   "polars-time/dtype-duration",
   "polars-core/dtype-duration",
   "polars-ops/dtype-duration",
 ]
 dtype-time = ["polars-core/dtype-time", "polars-io/dtype-time", "polars-time/dtype-time", "polars-ops/dtype-time"]
+dtype-array = [
+  "polars-core/dtype-array",
+  "polars-lazy/dtype-array",
+  "polars-ops/dtype-array",
+]
 dtype-i8 = ["polars-core/dtype-i8", "polars-lazy/dtype-i8", "polars-ops/dtype-i8"]
 dtype-i16 = ["polars-core/dtype-i16", "polars-lazy/dtype-i16", "polars-ops/dtype-i16"]
 dtype-decimal = ["polars-core/dtype-decimal", "polars-lazy/dtype-decimal", "polars-ops/dtype-decimal"]
 dtype-u8 = ["polars-core/dtype-u8", "polars-lazy/dtype-u8", "polars-ops/dtype-u8"]
 dtype-u16 = ["polars-core/dtype-u16", "polars-lazy/dtype-u16", "polars-ops/dtype-u16"]
 dtype-categorical = [
   "polars-core/dtype-categorical",
@@ -290,21 +298,21 @@
 ]
 
 bench = [
   "lazy",
 ]
 
 [dependencies]
-polars-algo = { version = "0.28.0", path = "../polars-algo", optional = true }
-polars-core = { version = "0.28.0", path = "../polars-core", features = ["docs", "private"], default-features = false }
-polars-io = { version = "0.28.0", path = "../polars-io", features = ["private"], default-features = false, optional = true }
-polars-lazy = { version = "0.28.0", path = "../polars-lazy", features = ["private"], default-features = false, optional = true }
-polars-ops = { version = "0.28.0", path = "../polars-ops" }
-polars-sql = { version = "0.28.0", path = "../polars-sql", default-features = false, optional = true }
-polars-time = { version = "0.28.0", path = "../polars-time", default-features = false, optional = true }
+polars-algo = { version = "0.30.0", path = "../polars-algo", optional = true }
+polars-core = { version = "0.30.0", path = "../polars-core", features = ["docs", "private"], default-features = false }
+polars-io = { version = "0.30.0", path = "../polars-io", features = ["private"], default-features = false, optional = true }
+polars-lazy = { version = "0.30.0", path = "../polars-lazy", features = ["private"], default-features = false, optional = true }
+polars-ops = { version = "0.30.0", path = "../polars-ops" }
+polars-sql = { version = "0.30.0", path = "../polars-sql", default-features = false, optional = true }
+polars-time = { version = "0.30.0", path = "../polars-time", default-features = false, optional = true }
 
 # enable js feature for getrandom to work in wasm
 [target.'cfg(target_family = "wasm")'.dependencies.getrandom]
 version = "0.2"
 features = ["js"]
 
 [dev-dependencies]
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/LICENSE` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/LICENSE`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/Makefile` & `polars_lts_cpu-0.18.0/local_dependencies/polars/Makefile`

 * *Files 18% similar despite different names*

```diff
@@ -17,28 +17,30 @@
 		-p polars-core \
 		-p polars-io \
 		-p polars-lazy \
 		-p polars-arrow \
 		-p polars-time \
 		-p polars-error \
 		-p polars-ops \
-		-p polars-sql
+		-p polars-sql \
+		-p polars-json
 
 clippy:
 	cargo clippy --all-features \
 		-p polars-core \
 		-p polars-io \
 		-p polars-lazy \
 		-p polars-arrow \
 		-p polars-utils \
 		-p polars-ops \
 		-p polars-error \
 		-p polars-row \
 		-p polars-time \
-		-p polars-sql
+		-p polars-sql \
+		-p polars-json
 
 clippy-default:
 	cargo clippy
 
 test:  ## Run tests
 	POLARS_NO_STREAMING_GROUPBY=1 POLARS_MAX_THREADS=4 cargo t -p polars-core test_4_threads
 	cargo test --all-features \
@@ -97,39 +99,28 @@
 	cargo doc -p polars-ops
 	cargo doc --all-features -p polars-io
 	cargo doc --all-features -p polars-lazy
 	cargo doc --features=docs-selection -p polars
 	cargo doc --all-features -p polars-sql
 
 publish:
-#	cargo publish --allow-dirty -p polars-error
-#	sleep 20
-#	cargo publish --allow-dirty -p polars-utils
-#	sleep 20
-#	cargo publish --allow-dirty -p polars-row
-#	sleep 20
+	cargo publish --allow-dirty -p polars-error
+	cargo publish --allow-dirty -p polars-utils
+	cargo publish --allow-dirty -p polars-row
 	cargo publish --allow-dirty -p polars-arrow
-	sleep 20
+	cargo publish --allow-dirty -p polars-json
 	cargo publish --allow-dirty -p polars-core
-	sleep 20
 	cargo publish --allow-dirty -p polars-ops
-	sleep 20
 	cargo publish --allow-dirty -p polars-time
-	sleep 20
 	cargo publish --allow-dirty -p polars-io
-	sleep 20
 	cargo publish --allow-dirty -p polars-plan
-	sleep 20
 	cargo publish --allow-dirty -p polars-pipe
-	sleep 20
 	cargo publish --allow-dirty -p polars-lazy
-	sleep 20
 	cargo publish --allow-dirty -p polars-algo
-	sleep 20
 	cargo publish --allow-dirty -p polars-sql
-	sleep 20
 	cargo publish --allow-dirty -p polars
+	cd .. && cargo publish --allow-dirty -p polars-cli
 
 .PHONY: help
 help:  ## Display this help screen
 	@echo -e "\033[1mAvailable commands:\033[0m\n"
 	@grep -E '^[a-z.A-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | awk 'BEGIN {FS = ":.*?## "}; {printf "  \033[36m%-18s\033[0m %s\n", $$1, $$2}' | sort
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/src/docs/eager.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/src/docs/eager.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/src/docs/lazy.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/src/docs/lazy.rs`

 * *Files 1% similar despite different names*

```diff
@@ -102,15 +102,15 @@
 //!
 //! # Ok(())
 //! # }
 //! ```
 //!
 //! ## Groupby
 //!
-//! This example is from the polars [user guide](https://pola-rs.github.io/polars-book/user-guide/howcani/df/groupby.html).
+//! This example is from the polars [user guide](https://pola-rs.github.io/polars-book/user-guide/concepts/contexts/#groupby-aggregation).
 //!
 //! ```
 //! use polars::prelude::*;
 //! # fn example() -> PolarsResult<()> {
 //!
 //!  let df = LazyCsvReader::new("reddit.csv")
 //!     .has_header(true)
@@ -250,15 +250,15 @@
 //!
 //! fn apply_multiples(lf: LazyFrame) -> PolarsResult<DataFrame> {
 //!     df![
 //!         "a" => [1.0, 2.0, 3.0],
 //!         "b" => [3.0, 5.1, 0.3]
 //!     ]?
 //!     .lazy()
-//!     .select([concat_lst(["col_a", "col_b"]).map(
+//!     .select([concat_list(["col_a", "col_b"]).map(
 //!         |s| {
 //!             let ca = s.struct_()?;
 //!
 //!             let b = ca.field_by_name("col_a")?;
 //!             let a = ca.field_by_name("col_b")?;
 //!             let a = a.f32()?;
 //!             let b = b.f32()?;
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/src/docs/performance.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/src/docs/performance.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/src/lib.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/src/lib.rs`

 * *Files 1% similar despite different names*

```diff
@@ -129,15 +129,15 @@
 //! # Ok(())
 //! # }
 //! ```
 //! All expressions are ran in parallel, meaning that separate polars expressions are embarrassingly parallel.
 //! (Note that within an expression there may be more parallelization going on).
 //!
 //! Understanding polars expressions is most important when starting with the polars library. Read more
-//! about them in the [User Guide](https://pola-rs.github.io/polars-book/user-guide/dsl/intro.html).
+//! about them in the [User Guide](https://pola-rs.github.io/polars-book/user-guide/concepts/expressions).
 //! Though the examples given there are in python. The expressions API is almost identical and the
 //! the read should certainly be valuable to rust users as well.
 //!
 //! ### Eager
 //! Read more in the pages of the following data structures /traits.
 //!
 //! * [DataFrame struct](crate::frame::DataFrame)
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/core/date_like.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/core/date_like.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/core/groupby.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/core/groupby.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/core/joins.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/core/joins.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/core/list.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/core/list.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/core/pivot.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/core/pivot.rs`

 * *Files 2% similar despite different names*

```diff
@@ -20,15 +20,15 @@
         "1972-09-27" => [first, 3, 2, 2]
     ]?;
     assert!(out.frame_equal_missing(&expected));
 
     let mut out = pivot_stable(&df, ["C"], ["B"], ["A"], true, Some(PivotAgg::First), None)?;
     out.try_apply("1", |s| {
         let ca = s.date()?;
-        Ok(ca.strftime("%Y-%d-%m"))
+        Ok(ca.to_string("%Y-%d-%m"))
     })?;
 
     let expected = df![
         "B" => [8i32, 2, 3, 6],
         "1" => ["1972-27-09", "1972-27-09", "1972-27-09", "1972-27-09"]
     ]?;
     assert!(out.frame_equal_missing(&expected));
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/core/random.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/core/random.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/core/rolling_window.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/core/rolling_window.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/core/series.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/core/series.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/io/csv.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/io/csv.rs`

 * *Files 1% similar despite different names*

```diff
@@ -337,14 +337,29 @@
  "#;
 
     let file = Cursor::new(s);
     let _df = CsvReader::new(file).has_header(true).finish().unwrap();
 }
 
 #[test]
+fn test_new_line_escape_on_header() {
+    let s = r#""length","header with
+new line character","width"
+5.1,3.5,1.4
+"#;
+    let file: Cursor<&str> = Cursor::new(s);
+    let df: DataFrame = CsvReader::new(file).has_header(true).finish().unwrap();
+    assert_eq!(df.shape(), (1, 3));
+    assert_eq!(
+        df.get_column_names(),
+        &["length", "header with\nnew line character", "width"]
+    );
+}
+
+#[test]
 fn test_quoted_numeric() {
     // CSV fields may be quoted
     let s = r#""foo","bar"
 "4.9","3"
 "1.4","2"
 "#;
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/io/ipc_stream.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/io/ipc_stream.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/io/json.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/io/json.rs`

 * *Files 0% similar despite different names*

```diff
@@ -109,15 +109,16 @@
     assert_eq!("a", df.get_columns()[0].name());
     assert_eq!("d", df.get_columns()[3].name());
     assert_eq!((12, 4), df.shape());
 }
 
 #[test]
 fn read_ndjson_with_trailing_newline() {
-    let data = r#"{"Column1":"Value1"}\n"#;
+    let data = r#"{"Column1":"Value1"}
+"#;
 
     let file = Cursor::new(data);
     let df = JsonReader::new(file)
         .with_json_format(JsonFormat::JsonLines)
         .finish()
         .unwrap();
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/joins.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/joins.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/aggregation.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/aggregation.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/cse.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/cse.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/expressions/apply.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/expressions/apply.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/expressions/arity.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/expressions/arity.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/expressions/expand.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/expressions/expand.rs`

 * *Files 12% similar despite different names*

```diff
@@ -27,15 +27,15 @@
         "dt1" => date_range.clone(),
         "dt2" => date_range,
     ]?
     .lazy()
     .with_column(
         dtype_col(&DataType::Datetime(TimeUnit::Milliseconds, None))
             .dt()
-            .strftime("%m/%d/%Y"),
+            .to_string("%m/%d/%Y"),
     )
     .limit(3)
     .collect()?;
 
     let expected = df![
         "dt1" => ["01/01/2020", "01/08/2020", "01/15/2020"],
         "dt2" => ["01/01/2020", "01/08/2020", "01/15/2020"],
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/expressions/filter.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/expressions/filter.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/expressions/slice.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/expressions/slice.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/expressions/window.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/expressions/window.rs`

 * *Files 7% similar despite different names*

```diff
@@ -43,30 +43,34 @@
 
     // a ternary expression with a final list aggregation
     let out1 = df
         .clone()
         .lazy()
         .select([
             col("fruits"),
-            col("B")
-                .shift_and_fill(-1, lit(-1))
-                .implode()
-                .over([col("fruits")]),
+            col("B").shift_and_fill(-1, lit(-1)).over_with_options(
+                [col("fruits")],
+                WindowOptions {
+                    mapping: WindowMapping::Join,
+                },
+            ),
         ])
         .collect()?;
 
     // same expression, no final list aggregation
     let out2 = df
         .lazy()
         .select([
             col("fruits"),
-            col("B")
-                .shift_and_fill(-1, lit(-1))
-                .implode()
-                .over([col("fruits")]),
+            col("B").shift_and_fill(-1, lit(-1)).over_with_options(
+                [col("fruits")],
+                WindowOptions {
+                    mapping: WindowMapping::Join,
+                },
+            ),
         ])
         .collect()?;
 
     assert!(out1.frame_equal(&out2));
 
     Ok(())
 }
@@ -79,17 +83,20 @@
         .clone()
         .lazy()
         .sort("fruits", Default::default())
         .select([
             col("fruits"),
             col("B")
                 .shift(1)
-                .implode()
-                .over([col("fruits")])
-                .explode()
+                .over_with_options(
+                    [col("fruits")],
+                    WindowOptions {
+                        mapping: WindowMapping::Explode,
+                    },
+                )
                 .alias("shifted"),
         ])
         .collect()?;
 
     assert_eq!(
         Vec::from(out.column("shifted")?.i32()?),
         &[None, Some(3), None, Some(5), Some(4)]
@@ -100,17 +107,20 @@
     let out = df
         .lazy()
         .sort("fruits", Default::default())
         .select([
             col("fruits"),
             col("B")
                 .shift_and_fill(1, lit(-1.0f32))
-                .implode()
-                .over([col("fruits")])
-                .explode()
+                .over_with_options(
+                    [col("fruits")],
+                    WindowOptions {
+                        mapping: WindowMapping::Explode,
+                    },
+                )
                 .alias("shifted"),
         ])
         .collect()?;
 
     // even though we fill with f32, cast i32 -> f32 can overflow so the result is f64
     assert_eq!(
         Vec::from(out.column("shifted")?.f64()?),
@@ -171,16 +181,20 @@
         "chars" => ["a", "a", "b"]
     ]?;
 
     let out = df
         .lazy()
         .select([repeat(1, count())
             .cumsum(false)
-            .implode()
-            .over([col("chars")])
+            .over_with_options(
+                [col("chars")],
+                WindowOptions {
+                    mapping: WindowMapping::Join,
+                },
+            )
             .alias("foo")])
         .collect()?;
 
     let out = out.column("foo")?;
     assert!(matches!(out.dtype(), DataType::List(_)));
     let flat = out.explode()?;
     let flat = flat.i32()?;
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/folds.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/folds.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/functions.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/functions.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/groupby.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/groupby.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/predicate_queries.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/predicate_queries.rs`

 * *Files 4% similar despite different names*

```diff
@@ -35,15 +35,23 @@
     let with_not_true = df
         .clone()
         .lazy()
         .filter(not(filter.clone()))
         .with_predicate_pushdown(false)
         .with_projection_pushdown(false)
         .collect()?;
+    let with_null = df
+        .clone()
+        .lazy()
+        .filter(col("a").is_null())
+        .with_predicate_pushdown(false)
+        .with_projection_pushdown(false)
+        .collect()?;
     let res = with_true.vstack(&with_not_true)?;
+    let res = res.vstack(&with_null)?;
     assert!(res.frame_equal_missing(&df));
     Ok(())
 }
 
 fn create_n_filters(col_name: &str, num_filters: usize) -> Vec<Expr> {
     (0..num_filters)
         .into_iter()
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/projection_queries.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/projection_queries.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/lazy/queries.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/lazy/queries.rs`

 * *Files 5% similar despite different names*

```diff
@@ -39,21 +39,23 @@
         "a" => [1, 2, 3, 4, 5],
         "b" => [1, 2, 3, 4, 5],
     ]?;
 
     let out = df
         .clone()
         .lazy()
+        .with_column(col("a").set_sorted_flag(IsSorted::Ascending))
         .groupby_rolling(
+            col("a"),
             [],
             RollingGroupOptions {
-                index_column: "a".into(),
                 period: Duration::parse("2i"),
                 offset: Duration::parse("0i"),
                 closed_window: ClosedWindow::Left,
+                ..Default::default()
             },
         )
         .agg([col("b").sum().alias("sum")])
         .select([col("a"), col("sum")])
         .collect()?;
 
     assert_eq!(
@@ -62,18 +64,19 @@
             .into_no_null_iter()
             .collect::<Vec<_>>(),
         &[3, 5, 7, 9, 5]
     );
 
     let out = df
         .lazy()
+        .with_column(col("a").set_sorted_flag(IsSorted::Ascending))
         .groupby_dynamic(
+            col("a"),
             [],
             DynamicGroupOptions {
-                index_column: "a".into(),
                 every: Duration::parse("2i"),
                 period: Duration::parse("2i"),
                 offset: Duration::parse("0i"),
                 truncate: false,
                 include_boundaries: false,
                 closed_window: ClosedWindow::Left,
                 ..Default::default()
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars/tests/it/schema.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars/tests/it/schema.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/Cargo.toml` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/Cargo.toml`

 * *Files 7% similar despite different names*

```diff
@@ -1,45 +1,48 @@
 [package]
 name = "polars-arrow"
-version= "0.28.0"
+version= "0.30.0"
 authors = ["ritchie46 <ritchie46@gmail.com>"]
 edition = "2021"
 license = "MIT"
 description = "Arrow interfaces for Polars DataFrame library"
 
 # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html
 
 [dependencies]
+atoi = { version = "2.0.0", optional = true }
 chrono = { version = "0.4", default-features = false, features = ["std"], optional = true }
 chrono-tz = { version = "0.8", optional = true }
 hashbrown= { version = "0.13.1", features = ["rayon", "ahash"] }
 multiversion= "0.7"
 num-traits= "0.2"
-polars-error = { version = "0.28.0", path = "../polars-error" }
+polars-error = { version = "0.30.0", path = "../polars-error" }
 serde = { version = "1", features = ["derive"], optional = true }
 thiserror= "^1"
 
 [features]
+dtype-decimal = ["atoi"]
+dtype-array = []
 nightly = ["hashbrown/nightly"]
 strings = []
 compute = ["arrow/compute_cast"]
 temporal = ["arrow/compute_temporal"]
 bigidx = []
 performant = []
 like = ["arrow/compute_like"]
 timezones = ["chrono-tz", "chrono"]
 simd = []
 
 [dependencies.arrow]
 package = "arrow2"
 # git = "https://github.com/jorgecarleitao/arrow2"
-git = "https://github.com/ritchie46/arrow2"
+# git = "https://github.com/ritchie46/arrow2"
 # rev = "1491c6e8f4fd100f53c358e4f3ef1536d9e75090"
 # path = "../arrow2"
-branch = "polars_2023-04-20"
+# branch = "polars_2023-05-25"
 version = "0.17"
 default-features = false
 features = [
   "compute_aggregate",
   "compute_arithmetics",
   "compute_boolean",
   "compute_boolean_kleene",
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/LICENSE` & `polars_lts_cpu-0.18.0/local_dependencies/polars-core/LICENSE`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/array/default_arrays.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/array/default_arrays.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/array/get.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/array/get.rs`

 * *Files 9% similar despite different names*

```diff
@@ -1,8 +1,10 @@
-use arrow::array::{Array, BinaryArray, BooleanArray, ListArray, PrimitiveArray, Utf8Array};
+use arrow::array::{
+    Array, BinaryArray, BooleanArray, FixedSizeListArray, ListArray, PrimitiveArray, Utf8Array,
+};
 use arrow::types::NativeType;
 
 use crate::is_valid::IsValid;
 
 pub trait ArrowGetItem {
     type Item;
 
@@ -109,14 +111,38 @@
     type Item = Box<dyn Array>;
 
     #[inline]
     fn get(&self, item: usize) -> Option<Self::Item> {
         debug_assert!(item < self.len());
         if item >= self.len() {
             None
+        } else {
+            unsafe { self.get_unchecked(item) }
+        }
+    }
+
+    #[inline]
+    unsafe fn get_unchecked(&self, item: usize) -> Option<Self::Item> {
+        debug_assert!(item < self.len());
+        if self.is_null_unchecked(item) {
+            None
+        } else {
+            Some(self.value_unchecked(item))
+        }
+    }
+}
+
+impl ArrowGetItem for FixedSizeListArray {
+    type Item = Box<dyn Array>;
+
+    #[inline]
+    fn get(&self, item: usize) -> Option<Self::Item> {
+        debug_assert!(item < self.len());
+        if item >= self.len() {
+            None
         } else {
             unsafe { self.get_unchecked(item) }
         }
     }
 
     #[inline]
     unsafe fn get_unchecked(&self, item: usize) -> Option<Self::Item> {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/array/list.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/array/list.rs`

 * *Files 7% similar despite different names*

```diff
@@ -60,22 +60,31 @@
             self.size += arr.len() as i64;
             self.arrays.push(arr.as_ref());
         }
         self.offsets.push(self.size);
         self.update_validity()
     }
 
+    #[inline]
     pub fn push_null(&mut self) {
         self.offsets.push(self.last_offset());
         match &mut self.validity {
             Some(validity) => validity.push(false),
             None => self.init_validity(),
         }
     }
 
+    #[inline]
+    pub fn push_opt(&mut self, arr: Option<&'a dyn Array>) {
+        match arr {
+            None => self.push_null(),
+            Some(arr) => self.push(arr),
+        }
+    }
+
     pub fn push_empty(&mut self) {
         self.offsets.push(self.last_offset());
         self.update_validity()
     }
 
     fn init_validity(&mut self) {
         let len = self.offsets.len() - 1;
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/array/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/array/mod.rs`

 * *Files 3% similar despite different names*

```diff
@@ -1,17 +1,21 @@
-use arrow::array::{Array, BinaryArray, BooleanArray, ListArray, PrimitiveArray, Utf8Array};
+use arrow::array::{
+    Array, BinaryArray, BooleanArray, FixedSizeListArray, ListArray, PrimitiveArray, Utf8Array,
+};
 use arrow::bitmap::MutableBitmap;
 use arrow::datatypes::DataType;
 use arrow::offset::Offsets;
 use arrow::types::NativeType;
 
 use crate::prelude::*;
 use crate::utils::CustomIterTools;
 
 pub mod default_arrays;
+#[cfg(feature = "dtype-array")]
+pub mod fixed_size_list;
 mod get;
 pub mod list;
 pub mod null;
 pub mod slice;
 pub mod utf8;
 
 pub use get::ArrowGetItem;
@@ -25,14 +29,20 @@
 
 impl ValueSize for ListArray<i64> {
     fn get_values_size(&self) -> usize {
         self.values().len()
     }
 }
 
+impl ValueSize for FixedSizeListArray {
+    fn get_values_size(&self) -> usize {
+        self.values().len()
+    }
+}
+
 impl ValueSize for Utf8Array<i64> {
     fn get_values_size(&self) -> usize {
         self.values().len()
     }
 }
 
 impl ValueSize for BinaryArray<i64> {
@@ -45,14 +55,19 @@
     fn get_values_size(&self) -> usize {
         match self.data_type() {
             DataType::LargeUtf8 => self
                 .as_any()
                 .downcast_ref::<Utf8Array<i64>>()
                 .unwrap()
                 .get_values_size(),
+            DataType::FixedSizeList(_, _) => self
+                .as_any()
+                .downcast_ref::<FixedSizeListArray>()
+                .unwrap()
+                .get_values_size(),
             DataType::LargeList(_) => self
                 .as_any()
                 .downcast_ref::<ListArray<i64>>()
                 .unwrap()
                 .get_values_size(),
             DataType::LargeBinary => self
                 .as_any()
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/array/null.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/array/null.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/array/slice.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/array/slice.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/array/utf8.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/array/utf8.rs`

 * *Files 12% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-use arrow::array::Utf8Array;
+use arrow::array::{BinaryArray, Utf8Array};
 use arrow::datatypes::DataType;
 use arrow::offset::Offsets;
 
 use crate::trusted_len::PushUnchecked;
 
 #[inline]
 unsafe fn extend_from_trusted_len_values_iter<I, P>(
@@ -65,7 +65,21 @@
         unsafe {
             Utf8Array::new_unchecked(DataType::LargeUtf8, offsets.into(), values.into(), None)
         }
     }
 }
 
 impl Utf8FromIter for Utf8Array<i64> {}
+
+pub trait BinaryFromIter {
+    #[inline]
+    fn from_values_iter<I, S>(iter: I, len: usize, value_cap: usize) -> BinaryArray<i64>
+    where
+        S: AsRef<[u8]>,
+        I: Iterator<Item = S>,
+    {
+        let (offsets, values) = unsafe { fill_offsets_and_values(iter, value_cap, len) };
+        BinaryArray::new(DataType::LargeBinary, offsets.into(), values.into(), None)
+    }
+}
+
+impl BinaryFromIter for BinaryArray<i64> {}
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/bit_util.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/bit_util.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/bitmap/mutable.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/bitmap/mutable.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/compute/take/boolean.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/compute/take/boolean.rs`

 * *Files 5% similar despite different names*

```diff
@@ -1,46 +1,39 @@
 use arrow::array::{Array, BooleanArray, PrimitiveArray};
 use arrow::bitmap::{Bitmap, MutableBitmap};
 
+use super::bitmap::take_bitmap_unchecked;
 use crate::index::IdxSize;
 
-unsafe fn take_values(values: &Bitmap, indices: &[IdxSize]) -> Bitmap {
-    let values = indices.iter().map(|&index| {
-        debug_assert!((index as usize) < values.len());
-        values.get_bit_unchecked(index as usize)
-    });
-    Bitmap::from_trusted_len_iter(values)
-}
-
 // take implementation when neither values nor indices contain nulls
 unsafe fn take_no_validity(values: &Bitmap, indices: &[IdxSize]) -> (Bitmap, Option<Bitmap>) {
-    (take_values(values, indices), None)
+    (take_bitmap_unchecked(values, indices), None)
 }
 
 // take implementation when only values contain nulls
 unsafe fn take_values_validity(
     values: &BooleanArray,
     indices: &[IdxSize],
 ) -> (Bitmap, Option<Bitmap>) {
     let validity_values = values.validity().unwrap();
-    let validity = take_values(validity_values, indices);
+    let validity = take_bitmap_unchecked(validity_values, indices);
 
     let values_values = values.values();
-    let buffer = take_values(values_values, indices);
+    let buffer = take_bitmap_unchecked(values_values, indices);
 
     (buffer, validity.into())
 }
 
 // take implementation when only indices contain nulls
 unsafe fn take_indices_validity(
     values: &Bitmap,
     indices: &PrimitiveArray<IdxSize>,
 ) -> (Bitmap, Option<Bitmap>) {
     // simply take all and copy the bitmap
-    let buffer = take_values(values, indices.values());
+    let buffer = take_bitmap_unchecked(values, indices.values());
 
     (buffer, indices.validity().cloned())
 }
 
 // take implementation when both values and indices contain nulls
 unsafe fn take_values_indices_validity(
     values: &BooleanArray,
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/compute/take/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/compute/take/mod.rs`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,11 @@
+pub mod bitmap;
 mod boolean;
+#[cfg(feature = "dtype-array")]
+mod fixed_size_list;
 
 use arrow::array::*;
 use arrow::bitmap::MutableBitmap;
 use arrow::buffer::Buffer;
 use arrow::datatypes::{DataType, PhysicalType};
 use arrow::offset::Offsets;
 use arrow::types::NativeType;
@@ -11,14 +14,17 @@
 use crate::prelude::*;
 use crate::trusted_len::{PushUnchecked, TrustedLen};
 use crate::utils::{with_match_primitive_type, CustomIterTools};
 
 /// # Safety
 /// Does not do bounds checks
 pub unsafe fn take_unchecked(arr: &dyn Array, idx: &IdxArr) -> ArrayRef {
+    if idx.null_count() == idx.len() {
+        return new_null_array(arr.data_type().clone(), idx.len());
+    }
     use PhysicalType::*;
     match arr.data_type().to_physical_type() {
         Primitive(primitive) => with_match_primitive_type!(primitive, |$T| {
             let arr: &PrimitiveArray<$T> = arr.as_any().downcast_ref().unwrap();
             if arr.null_count() > 0 {
                 take_primitive_unchecked::<$T>(arr, idx)
             } else {
@@ -29,14 +35,19 @@
             let arr = arr.as_any().downcast_ref().unwrap();
             take_utf8_unchecked(arr, idx)
         }
         Boolean => {
             let arr = arr.as_any().downcast_ref().unwrap();
             Box::new(boolean::take_unchecked(arr, idx))
         }
+        #[cfg(feature = "dtype-array")]
+        FixedSizeList => {
+            let arr = arr.as_any().downcast_ref().unwrap();
+            Box::new(fixed_size_list::take_unchecked(arr, idx))
+        }
         // TODO! implement proper unchecked version
         #[cfg(feature = "compute")]
         _ => {
             use arrow::compute::take::take;
             take(arr, idx).unwrap()
         }
         #[cfg(not(feature = "compute"))]
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/conversion.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/conversion.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/data_types.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/data_types.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/floats/ord.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/floats/ord.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/index.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/index.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/is_valid.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/is_valid.rs`

 * *Files 20% similar despite different names*

```diff
@@ -1,8 +1,10 @@
-use arrow::array::{Array, BinaryArray, BooleanArray, ListArray, PrimitiveArray, Utf8Array};
+use arrow::array::{
+    Array, BinaryArray, BooleanArray, FixedSizeListArray, ListArray, PrimitiveArray, Utf8Array,
+};
 use arrow::types::NativeType;
 
 pub trait IsValid {
     /// # Safety
     /// no bound checks
     unsafe fn is_valid_unchecked(&self, i: usize) -> bool;
 
@@ -14,14 +16,15 @@
 pub trait ArrowArray: Array {}
 
 impl ArrowArray for BinaryArray<i64> {}
 impl ArrowArray for Utf8Array<i64> {}
 impl<T: NativeType> ArrowArray for PrimitiveArray<T> {}
 impl ArrowArray for BooleanArray {}
 impl ArrowArray for ListArray<i64> {}
+impl ArrowArray for FixedSizeListArray {}
 
 impl<A: ArrowArray> IsValid for A {
     unsafe fn is_valid_unchecked(&self, i: usize) -> bool {
         if let Some(b) = self.validity() {
             b.get_bit_unchecked(i)
         } else {
             true
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/agg_mean.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/agg_mean.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/concatenate.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/concatenate.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/ewm/average.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/ewm/average.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/ewm/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/ewm/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/ewm/variance.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/ewm/variance.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/float.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/float.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/list.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/list.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/list_bytes_iter.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/list_bytes_iter.rs`

 * *Files 10% similar despite different names*

```diff
@@ -14,15 +14,15 @@
     let mut start = offsets[0] as usize;
     offsets[1..].iter().enumerate().map(move |(i, end)| {
         let end = *end as usize;
         let out = values.get_unchecked(start..end);
         start = end;
 
         let data = out.as_ptr() as *const u8;
-        let out = std::slice::from_raw_parts(data, std::mem::size_of::<T>() * out.len());
+        let out = std::slice::from_raw_parts(data, std::mem::size_of_val(out));
         match validity {
             None => Some(out),
             Some(validity) => {
                 if validity.get_bit_unchecked(i) {
                     Some(out)
                 } else {
                     None
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/mod.rs`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 use std::iter::Enumerate;
 
 use arrow::array::BooleanArray;
 use arrow::bitmap::utils::BitChunks;
 #[cfg(feature = "simd")]
 pub mod agg_mean;
+#[cfg(feature = "dtype-array")]
+pub mod comparison;
 pub mod concatenate;
 pub mod ewm;
 pub mod float;
 pub mod list;
 pub mod list_bytes_iter;
 pub mod rolling;
 pub mod set;
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/no_nulls/mean.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/no_nulls/mean.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/no_nulls/min_max.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/no_nulls/min_max.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/no_nulls/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/no_nulls/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/no_nulls/quantile.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/no_nulls/quantile.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/no_nulls/sum.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/no_nulls/sum.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/no_nulls/variance.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/no_nulls/variance.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/nulls/mean.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/nulls/mean.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/nulls/min_max.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/nulls/min_max.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/nulls/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/nulls/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/nulls/quantile.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/nulls/quantile.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/nulls/sum.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/nulls/sum.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/nulls/variance.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/nulls/variance.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/rolling/window.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/rolling/window.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/set.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/set.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/sort_partition.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/sort_partition.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/sorted_join/inner.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/sorted_join/inner.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/sorted_join/left.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/sorted_join/left.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/string.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/string.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/take_agg/boolean.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/take_agg/boolean.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/take_agg/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/take_agg/mod.rs`

 * *Files 2% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 pub use var::*;
 
 use crate::array::PolarsArray;
 use crate::index::IdxSize;
 
 /// Take kernel for single chunk without nulls and an iterator as index.
 /// # Safety
-/// caller must enure iterators indexes are in bounds
+/// caller must ensure iterators indexes are in bounds
 #[inline]
 pub unsafe fn take_agg_no_null_primitive_iter_unchecked<
     T: NativeType + ToPrimitive,
     TOut: NumCast + NativeType,
     I: IntoIterator<Item = usize>,
     F: Fn(TOut, TOut) -> TOut,
 >(
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/kernels/take_agg/var.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/kernels/take_agg/var.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/slice.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/slice.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/trusted_len/boolean.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/trusted_len/boolean.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/trusted_len/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/trusted_len/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/trusted_len/push_unchecked.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/trusted_len/push_unchecked.rs`

 * *Files 6% similar despite different names*

```diff
@@ -3,20 +3,14 @@
 pub trait PushUnchecked<T> {
     /// Will push an item and not check if there is enough capacity
     ///
     /// # Safety
     /// Caller must ensure the array has enough capacity to hold `T`.
     unsafe fn push_unchecked(&mut self, value: T);
 
-    /// Will push an item and not check if there is enough capacity nor update the array's length
-    /// # Safety
-    /// Caller must ensure the array has enough capacity to hold `T`.
-    /// Caller must update the length when its done updating the vector.
-    unsafe fn push_unchecked_no_len_set(&mut self, value: T);
-
     /// Extend the array with an iterator who's length can be trusted
     fn extend_trusted_len<I: IntoIterator<Item = T, IntoIter = J>, J: TrustedLen>(
         &mut self,
         iter: I,
     ) {
         unsafe { self.extend_trusted_len_unchecked(iter) }
     }
@@ -44,32 +38,27 @@
         debug_assert!(self.capacity() > self.len());
         let end = self.as_mut_ptr().add(self.len());
         std::ptr::write(end, value);
         self.set_len(self.len() + 1);
     }
 
     #[inline]
-    unsafe fn push_unchecked_no_len_set(&mut self, value: T) {
-        let end = self.as_mut_ptr().add(self.len());
-        std::ptr::write(end, value);
-    }
-
-    #[inline]
     unsafe fn extend_trusted_len_unchecked<I: IntoIterator<Item = T>>(&mut self, iter: I) {
         let iter = iter.into_iter();
         let upper = iter.size_hint().1.expect("must have an upper bound");
         self.reserve(upper);
 
         let mut dst = self.as_mut_ptr().add(self.len());
         for value in iter {
             std::ptr::write(dst, value);
             dst = dst.add(1)
         }
         self.set_len(self.len() + upper)
     }
 
+    #[inline]
     unsafe fn from_trusted_len_iter_unchecked<I: IntoIterator<Item = T>>(iter: I) -> Self {
         let mut v = vec![];
         v.extend_trusted_len_unchecked(iter);
         v
     }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-arrow/src/utils.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-arrow/src/utils.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-utils/LICENSE` & `polars_lts_cpu-0.18.0/local_dependencies/polars-algo/LICENSE`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/arena.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/arena.rs`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 use crate::error::*;
+use crate::slice::GetSaferUnchecked;
 
 unsafe fn index_of_unchecked<T>(slice: &[T], item: &T) -> usize {
     (item as *const _ as usize - slice.as_ptr() as usize) / std::mem::size_of::<T>()
 }
 
 fn index_of<T>(slice: &[T], item: &T) -> Option<usize> {
     debug_assert!(std::mem::size_of::<T>() > 0);
@@ -77,14 +78,21 @@
 
     #[inline]
     pub fn get(&self, idx: Node) -> &T {
         self.items.get(idx.0).unwrap()
     }
 
     #[inline]
+    /// # Safety
+    /// Doesn't do any bound checks
+    pub unsafe fn get_unchecked(&self, idx: Node) -> &T {
+        self.items.get_unchecked_release(idx.0)
+    }
+
+    #[inline]
     pub fn get_mut(&mut self, idx: Node) -> &mut T {
         self.items.get_mut(idx.0).unwrap()
     }
 
     #[inline]
     pub fn replace(&mut self, idx: Node, val: T) {
         let x = self.get_mut(idx);
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/atomic.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/atomic.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/cell.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/cell.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/contention_pool.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/contention_pool.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/functions.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/functions.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/hash.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/hash.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/iter/enumerate_idx.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/iter/enumerate_idx.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/lib.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/lib.rs`

 * *Files 16% similar despite different names*

```diff
@@ -20,12 +20,13 @@
 pub use hash::HashSingle;
 
 #[cfg(not(feature = "bigidx"))]
 pub type IdxSize = u32;
 #[cfg(feature = "bigidx")]
 pub type IdxSize = u64;
 
+pub mod aliases;
 pub mod fmt;
 pub mod iter;
 pub mod macros;
 #[cfg(target_family = "wasm")]
 pub mod wasm;
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/macros.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/macros.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/slice.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/slice.rs`

 * *Files 15% similar despite different names*

```diff
@@ -14,14 +14,37 @@
 
     fn max_value(&self) -> Option<&T> {
         self.iter()
             .max_by(|a, b| a.partial_cmp(b).unwrap_or(Ordering::Equal))
     }
 }
 
+pub trait SortedSlice<T> {
+    fn is_sorted_ascending(&self) -> bool;
+}
+
+impl<T: PartialOrd + Copy> SortedSlice<T> for [T] {
+    fn is_sorted_ascending(&self) -> bool {
+        if self.is_empty() {
+            true
+        } else {
+            let mut previous = self[0];
+            let mut sorted = true;
+
+            // don't early stop or branch
+            // so it autovectorizes
+            for &v in &self[1..] {
+                sorted &= previous <= v;
+                previous = v;
+            }
+            sorted
+        }
+    }
+}
+
 pub trait GetSaferUnchecked<T> {
     /// # Safety
     ///
     /// Calling this method with an out-of-bounds index is *[undefined behavior]*
     /// even if the resulting reference is not used.
     unsafe fn get_unchecked_release<I>(&self, index: I) -> &<I as SliceIndex<[T]>>::Output
     where
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/sort.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/sort.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/sync.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/sync.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/unwrap.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/unwrap.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-utils/src/wasm.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-utils/src/wasm.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/Cargo.toml` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/Cargo.toml`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 [package]
 name = "polars-lazy"
-version= "0.28.0"
+version= "0.30.0"
 authors = ["ritchie46 <ritchie46@gmail.com>"]
 edition = "2021"
 license = "MIT"
 repository = "https://github.com/pola-rs/polars"
 description = "Lazy query engine for the Polars DataFrame library"
 
 # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html
@@ -13,22 +13,23 @@
 serde_json = "1"
 
 [dependencies]
 ahash= "0.8"
 bitflags= "1.3"
 glob = "0.3"
 once_cell = "1"
-polars-arrow = { version = "0.28.0", path = "../polars-arrow" }
-polars-core = { version = "0.28.0", path = "../polars-core", features = ["lazy", "private", "zip_with", "random"], default-features = false }
-polars-io = { version = "0.28.0", path = "../polars-io", features = ["lazy", "csv", "private"], default-features = false }
-polars-ops = { version = "0.28.0", path = "../polars-ops", default-features = false }
-polars-pipe = { version = "0.28.0", path = "../polars-pipe", optional = true }
-polars-plan = { version = "0.28.0", path = "../polars-plan" }
-polars-time = { version = "0.28.0", path = "../polars-time", optional = true }
-polars-utils = { version = "0.28.0", path = "../polars-utils" }
+polars-arrow = { version = "0.30.0", path = "../polars-arrow" }
+polars-core = { version = "0.30.0", path = "../polars-core", features = ["lazy", "private", "zip_with", "random"], default-features = false }
+polars-io = { version = "0.30.0", path = "../polars-io", features = ["lazy", "csv", "private"], default-features = false }
+polars-json = { version = "0.30.0", path = "../polars-json", optional = true }
+polars-ops = { version = "0.30.0", path = "../polars-ops", default-features = false }
+polars-pipe = { version = "0.30.0", path = "../polars-pipe", optional = true }
+polars-plan = { version = "0.30.0", path = "../polars-plan" }
+polars-time = { version = "0.30.0", path = "../polars-time", optional = true }
+polars-utils = { version = "0.30.0", path = "../polars-utils" }
 pyo3 = { version = "0.18", optional = true }
 rayon= "1.6"
 smartstring= { version = "1" }
 
 [features]
 nightly = ["polars-core/nightly", "polars-pipe/nightly"]
 compile = ["polars-plan/compile"]
@@ -38,15 +39,15 @@
 async = [
   "polars-plan/async",
   "polars-io/cloud",
   "polars-pipe/async",
   "streaming",
 ]
 ipc = ["polars-io/ipc", "polars-plan/ipc", "polars-pipe/ipc"]
-json = ["polars-io/json", "polars-plan/json"]
+json = ["polars-io/json", "polars-plan/json", "polars-json"]
 csv = ["polars-io/csv", "polars-plan/csv", "polars-pipe/csv"]
 temporal = ["dtype-datetime", "dtype-date", "dtype-time", "dtype-duration", "polars-plan/temporal"]
 # debugging purposes
 fmt = ["polars-core/fmt", "polars-plan/fmt"]
 strings = ["polars-plan/strings"]
 future = []
 dtype-u8 = ["polars-plan/dtype-u8", "polars-pipe/dtype-u8"]
@@ -54,14 +55,15 @@
 dtype-i8 = ["polars-plan/dtype-i8", "polars-pipe/dtype-i8"]
 dtype-i16 = ["polars-plan/dtype-i16", "polars-pipe/dtype-i16"]
 dtype-decimal = ["polars-plan/dtype-decimal", "polars-pipe/dtype-decimal"]
 dtype-date = ["polars-plan/dtype-date", "polars-time/dtype-date", "temporal"]
 dtype-datetime = ["polars-plan/dtype-datetime", "polars-time/dtype-datetime", "temporal"]
 dtype-duration = ["polars-plan/dtype-duration", "polars-time/dtype-duration", "temporal"]
 dtype-time = ["polars-core/dtype-time", "temporal"]
+dtype-array = ["polars-plan/dtype-array", "polars-pipe/dtype-array"]
 dtype-categorical = ["polars-plan/dtype-categorical", "polars-pipe/dtype-categorical"]
 dtype-struct = ["polars-plan/dtype-struct"]
 object = ["polars-plan/object"]
 date_offset = ["polars-plan/date_offset"]
 trigonometry = ["polars-plan/trigonometry"]
 sign = ["polars-plan/sign"]
 timezones = ["polars-plan/timezones"]
@@ -73,17 +75,16 @@
 # operations
 approx_unique = ["polars-plan/approx_unique"]
 is_in = ["polars-plan/is_in"]
 repeat_by = ["polars-plan/repeat_by"]
 round_series = ["polars-plan/round_series", "polars-ops/round_series"]
 is_first = ["polars-plan/is_first"]
 is_unique = ["polars-plan/is_unique"]
-cross_join = ["polars-plan/cross_join", "polars-pipe/cross_join"]
+cross_join = ["polars-plan/cross_join", "polars-pipe/cross_join", "polars-ops/cross_join"]
 asof_join = ["polars-plan/asof_join", "polars-time"]
-dot_product = ["polars-plan/dot_product"]
 concat_str = ["polars-plan/concat_str"]
 arange = ["polars-plan/arange"]
 mode = ["polars-plan/mode"]
 cum_agg = ["polars-plan/cum_agg"]
 interpolate = ["polars-plan/interpolate"]
 rolling_window = [
   "polars-plan/rolling_window",
@@ -124,14 +125,15 @@
   "polars-plan/serde",
   "polars-arrow/serde",
   "polars-core/serde-lazy",
   "polars-time/serde",
   "polars-io/serde",
   "polars-ops/serde",
 ]
+fused = ["polars-plan/fused", "polars-ops/fused"]
 
 binary_encoding = ["polars-plan/binary_encoding"]
 
 # no guarantees whatsoever
 private = ["polars-plan/private"]
 
 bigidx = ["polars-plan/bigidx"]
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/LICENSE` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/LICENSE`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/dot.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/dot.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/dsl/eval.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/dsl/eval.rs`

 * *Files 2% similar despite different names*

```diff
@@ -52,15 +52,21 @@
 
             // ensure we get the new schema
             let output_field = eval_field_to_dtype(s.field().as_ref(), &expr, false);
 
             let expr = expr.clone();
             let mut arena = Arena::with_capacity(10);
             let aexpr = to_aexpr(expr, &mut arena);
-            let phys_expr = create_physical_expr(aexpr, Context::Default, &arena, None)?;
+            let phys_expr = create_physical_expr(
+                aexpr,
+                Context::Default,
+                &arena,
+                None,
+                &mut Default::default(),
+            )?;
 
             let state = ExecutionState::new();
 
             let finish = |out: Series| {
                 polars_ensure!(
                     out.len() <= 1,
                     ComputeError:
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/dsl/functions.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/dsl/functions.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/dsl/list.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/dsl/list.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/dsl/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/dsl/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/frame/anonymous_scan.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/frame/anonymous_scan.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/frame/csv.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/frame/csv.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/frame/file_list_reader.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/frame/file_list_reader.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/frame/ipc.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/frame/ipc.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/frame/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/frame/mod.rs`

 * *Files 2% similar despite different names*

```diff
@@ -169,33 +169,42 @@
 
     /// Allow (partial) streaming engine
     pub fn with_streaming(mut self, toggle: bool) -> Self {
         self.opt_state.streaming = toggle;
         self
     }
 
-    /// Describe the logical plan.
+    /// Explain the naive logical plan.
     pub fn describe_plan(&self) -> String {
         self.logical_plan.describe()
     }
 
-    /// Describe the optimized logical plan.
+    /// Explain the optimized logical plan.
     pub fn describe_optimized_plan(&self) -> PolarsResult<String> {
         let mut expr_arena = Arena::with_capacity(64);
         let mut lp_arena = Arena::with_capacity(64);
         let lp_top = self.clone().optimize_with_scratch(
             &mut lp_arena,
             &mut expr_arena,
             &mut vec![],
             true,
         )?;
         let logical_plan = node_to_lp(lp_top, &expr_arena, &mut lp_arena);
         Ok(logical_plan.describe())
     }
 
+    /// Explain the logical plan.
+    pub fn explain(&self, optimized: bool) -> PolarsResult<String> {
+        if optimized {
+            self.describe_optimized_plan()
+        } else {
+            Ok(self.describe_plan())
+        }
+    }
+
     /// Add a sort operation to the logical plan.
     ///
     /// # Example
     ///
     /// ```rust
     /// use polars_core::prelude::*;
     /// use polars_lazy::prelude::*;
@@ -725,17 +734,26 @@
     ///
     /// Different from a [`dynamic_groupby`] the windows are now determined by the
     /// individual values and are not of constant intervals. For constant intervals use
     /// *groupby_dynamic*
     #[cfg(feature = "dynamic_groupby")]
     pub fn groupby_rolling<E: AsRef<[Expr]>>(
         self,
+        index_column: Expr,
         by: E,
-        options: RollingGroupOptions,
+        mut options: RollingGroupOptions,
     ) -> LazyGroupBy {
+        if let Expr::Column(name) = index_column {
+            options.index_column = name.as_ref().into();
+        } else {
+            let name = expr_output_name(&index_column).unwrap();
+            return self
+                .with_column(index_column)
+                .groupby_rolling(Expr::Column(name), by, options);
+        }
         let opt_state = self.get_opt_state();
         LazyGroupBy {
             logical_plan: self.logical_plan,
             opt_state,
             keys: by.as_ref().to_vec(),
             maintain_order: true,
             dynamic_options: None,
@@ -757,17 +775,26 @@
     /// - offset: offset of the window
     ///
     /// The `by` argument should be empty `[]` if you don't want to combine this
     /// with a ordinary groupby on these keys.
     #[cfg(feature = "dynamic_groupby")]
     pub fn groupby_dynamic<E: AsRef<[Expr]>>(
         self,
+        index_column: Expr,
         by: E,
-        options: DynamicGroupOptions,
+        mut options: DynamicGroupOptions,
     ) -> LazyGroupBy {
+        if let Expr::Column(name) = index_column {
+            options.index_column = name.as_ref().into();
+        } else {
+            let name = expr_output_name(&index_column).unwrap();
+            return self
+                .with_column(index_column)
+                .groupby_dynamic(Expr::Column(name), by, options);
+        }
         let opt_state = self.get_opt_state();
         LazyGroupBy {
             logical_plan: self.logical_plan,
             opt_state,
             keys: by.as_ref().to_vec(),
             maintain_order: true,
             dynamic_options: Some(options),
@@ -1002,14 +1029,19 @@
             .map(|e| e.clone().into())
             .collect::<Vec<_>>();
         let opt_state = self.get_opt_state();
         let lp = self.get_plan_builder().explode(columns).build();
         Self::from_logical_plan(lp, opt_state)
     }
 
+    /// Aggregate all the columns as the sum of their null value count.
+    pub fn null_count(self) -> LazyFrame {
+        self.select_local(vec![col("*").null_count()])
+    }
+
     /// Keep unique rows and maintain order
     pub fn unique_stable(
         self,
         subset: Option<Vec<String>>,
         keep_strategy: UniqueKeepStrategy,
     ) -> LazyFrame {
         let opt_state = self.get_opt_state();
@@ -1309,16 +1341,16 @@
     /// this as materializing the `DataFrame` is very expensive.
     pub fn apply<F>(self, f: F, schema: SchemaRef) -> LazyFrame
     where
         F: 'static + Fn(DataFrame) -> PolarsResult<DataFrame> + Send + Sync,
     {
         #[cfg(feature = "dynamic_groupby")]
         let options = GroupbyOptions {
-            dynamic: None,
-            rolling: None,
+            dynamic: self.dynamic_options,
+            rolling: self.rolling_options,
             slice: None,
         };
 
         #[cfg(not(feature = "dynamic_groupby"))]
         let options = GroupbyOptions { slice: None };
 
         let lp = LogicalPlan::Aggregate {
@@ -1393,16 +1425,16 @@
     /// Allow parallel table evaluation.
     pub fn allow_parallel(mut self, allow: bool) -> Self {
         self.allow_parallel = allow;
         self
     }
 
     /// Force parallel table evaluation.
-    pub fn force_parallel(mut self, allow: bool) -> Self {
-        self.allow_parallel = allow;
+    pub fn force_parallel(mut self, force: bool) -> Self {
+        self.force_parallel = force;
         self
     }
 
     /// Suffix to add duplicate column names in join.
     /// Defaults to `"_right"`.
     pub fn suffix<S: AsRef<str>>(mut self, suffix: S) -> Self {
         self.suffix = Some(suffix.as_ref().to_string());
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/frame/ndjson.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/frame/ndjson.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/frame/parquet.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/frame/parquet.rs`

 * *Files 19% similar despite different names*

```diff
@@ -102,34 +102,11 @@
     fn row_count(&self) -> Option<&RowCount> {
         self.args.row_count.as_ref()
     }
 }
 
 impl LazyFrame {
     /// Create a LazyFrame directly from a parquet scan.
-    #[deprecated(note = "please use `concat_lf` instead")]
-    pub fn scan_parquet_files<P: AsRef<Path>>(
-        paths: Vec<P>,
-        args: ScanArgsParquet,
-    ) -> PolarsResult<Self> {
-        let reader = LazyParquetReader::new(
-            paths.first().expect("got no files").as_ref().to_owned(),
-            args,
-        );
-        let lfs = paths
-            .iter()
-            .map(|p| {
-                reader
-                    .clone()
-                    .with_path(p.as_ref().to_owned())
-                    .finish_no_glob()
-            })
-            .collect::<PolarsResult<Vec<_>>>()?;
-
-        reader.concat_impl(lfs)
-    }
-
-    /// Create a LazyFrame directly from a parquet scan.
     pub fn scan_parquet(path: impl AsRef<Path>, args: ScanArgsParquet) -> PolarsResult<Self> {
         LazyParquetReader::new(path.as_ref().to_owned(), args).finish()
     }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/frame/pivot.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/frame/pivot.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/lib.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/lib.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/cache.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/cache.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/executor.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/executor.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/ext_context.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/ext_context.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/filter.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/filter.rs`

 * *Files 23% similar despite different names*

```diff
@@ -1,29 +1,43 @@
 use super::*;
 
 pub struct FilterExec {
     pub(crate) predicate: Arc<dyn PhysicalExpr>,
     pub(crate) input: Box<dyn Executor>,
+    // if the predicate contains a window function
+    has_window: bool,
 }
 
 impl FilterExec {
-    pub fn new(predicate: Arc<dyn PhysicalExpr>, input: Box<dyn Executor>) -> Self {
-        Self { predicate, input }
+    pub fn new(
+        predicate: Arc<dyn PhysicalExpr>,
+        input: Box<dyn Executor>,
+        has_window: bool,
+    ) -> Self {
+        Self {
+            predicate,
+            input,
+            has_window,
+        }
     }
 }
 
 impl Executor for FilterExec {
     fn execute(&mut self, state: &mut ExecutionState) -> PolarsResult<DataFrame> {
         #[cfg(debug_assertions)]
         {
             if state.verbose() {
                 println!("run FilterExec")
             }
         }
         let df = self.input.execute(state)?;
+
+        if self.has_window {
+            state.insert_has_window_function_flag()
+        }
         let s = self.predicate.evaluate(&df, state)?;
         let mask = s.bool().map_err(|_| {
             polars_err!(
                 ComputeError: "filter predicate must be of type `Boolean`, got `{}`", s.dtype()
             )
         })?;
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/groupby.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/groupby.rs`

 * *Files 4% similar despite different names*

```diff
@@ -58,14 +58,15 @@
     let mut sliced_groups = None;
 
     if let Some((offset, len)) = slice {
         sliced_groups = Some(groups.slice(offset, len));
         groups = sliced_groups.as_deref().unwrap();
     }
 
+    state.expr_cache = Some(Default::default());
     let (mut columns, agg_columns) = POOL.install(|| {
         let get_columns = || gb.keys_sliced(slice);
 
         let get_agg = || {
             aggs.par_iter()
                 .map(|expr| {
                     let agg = expr.evaluate_on_groups(&df, groups, state)?.finalize();
@@ -74,14 +75,15 @@
                 })
                 .collect::<PolarsResult<Vec<_>>>()
         };
 
         rayon::join(get_columns, get_agg)
     });
     let agg_columns = agg_columns?;
+    state.expr_cache = None;
 
     columns.extend_from_slice(&agg_columns);
     DataFrame::new(columns)
 }
 
 impl GroupByExec {
     fn execute_impl(
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/groupby_partitioned.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/groupby_partitioned.rs`

 * *Files 1% similar despite different names*

```diff
@@ -287,16 +287,18 @@
                     state,
                     self.maintain_order,
                     self.slice,
                 );
             }
 
             #[cfg(feature = "streaming")]
-            if let Some(out) = self.run_streaming(state, original_df.clone()) {
-                return out;
+            if !self.maintain_order {
+                if let Some(out) = self.run_streaming(state, original_df.clone()) {
+                    return out;
+                }
             }
 
             if state.verbose() {
                 eprintln!("run PARTITIONED HASH AGGREGATION")
             }
 
             // Run the partitioned aggregations
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/groupby_rolling.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/groupby_rolling.rs`

 * *Files 8% similar despite different names*

```diff
@@ -1,21 +1,24 @@
 #[cfg(feature = "dynamic_groupby")]
+use polars_core::frame::groupby::GroupBy;
+#[cfg(feature = "dynamic_groupby")]
 use polars_time::RollingGroupOptions;
 
 use super::*;
 
 #[cfg_attr(not(feature = "dynamic_groupby"), allow(dead_code))]
 pub(crate) struct GroupByRollingExec {
     pub(crate) input: Box<dyn Executor>,
     pub(crate) keys: Vec<Arc<dyn PhysicalExpr>>,
     pub(crate) aggs: Vec<Arc<dyn PhysicalExpr>>,
     #[cfg(feature = "dynamic_groupby")]
     pub(crate) options: RollingGroupOptions,
     pub(crate) input_schema: SchemaRef,
     pub(crate) slice: Option<(i64, usize)>,
+    pub(crate) apply: Option<Arc<dyn DataFrameUdf>>,
 }
 
 impl GroupByRollingExec {
     #[cfg(feature = "dynamic_groupby")]
     fn execute_impl(
         &mut self,
         state: &mut ExecutionState,
@@ -27,14 +30,24 @@
             .keys
             .iter()
             .map(|e| e.evaluate(&df, state))
             .collect::<PolarsResult<Vec<_>>>()?;
 
         let (mut time_key, mut keys, groups) = df.groupby_rolling(keys, &self.options)?;
 
+        if let Some(f) = &self.apply {
+            let gb = GroupBy::new(&df, vec![], groups, None);
+            let out = gb.apply(move |df| f.call_udf(df))?;
+            return Ok(if let Some((offset, len)) = self.slice {
+                out.slice(offset, len)
+            } else {
+                out
+            });
+        }
+
         let mut groups = &groups;
         #[allow(unused_assignments)]
         // it is unused because we only use it to keep the lifetime of sliced_group valid
         let mut sliced_groups = None;
 
         if let Some((offset, len)) = self.slice {
             sliced_groups = Some(groups.slice(offset, len));
@@ -63,24 +76,26 @@
                             *key = key.take_iter_unchecked(iter);
                         }
                     }
                 }
             }
         };
 
+        state.expr_cache = Some(Default::default());
         let agg_columns = POOL.install(|| {
             self.aggs
                 .par_iter()
                 .map(|expr| {
                     let agg = expr.evaluate_on_groups(&df, groups, state)?.aggregated();
                     polars_ensure!(agg.len() == groups.len(), agg_len = agg.len(), groups.len());
                     Ok(agg)
                 })
                 .collect::<PolarsResult<Vec<_>>>()
         })?;
+        state.expr_cache = None;
 
         let mut columns = Vec::with_capacity(agg_columns.len() + 1 + keys.len());
         columns.extend_from_slice(&keys);
         columns.push(time_key);
         columns.extend_from_slice(&agg_columns);
 
         DataFrame::new(columns)
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/join.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/join.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/mod.rs`

 * *Files 8% similar despite different names*

```diff
@@ -59,39 +59,32 @@
     //   so they are cached and run sequential
 
     // the partitioning messes with column order, so we also store the idx
     // and use those to restore the original projection order
     #[allow(clippy::type_complexity)]
     // String: partition_name,
     // u32: index,
-    // bool: flatten (we must run those first because they need a sorted group tuples.
-    //       if we cache the group tuples we must ensure we cast the sorted ones.
-    let mut windows: Vec<(String, Vec<(u32, bool, Arc<dyn PhysicalExpr>)>)> = vec![];
+    let mut windows: Vec<(String, Vec<(u32, Arc<dyn PhysicalExpr>)>)> = vec![];
     let mut other = Vec::with_capacity(exprs.len());
 
     // first we partition the window function by the values they group over.
     // the groupby values should be cached
     let mut index = 0u32;
     exprs.iter().for_each(|phys| {
         index += 1;
         let e = phys.as_expression().unwrap();
 
         let mut is_window = false;
         for e in e.into_iter() {
-            if let Expr::Window {
-                partition_by,
-                options,
-                ..
-            } = e
-            {
+            if let Expr::Window { partition_by, .. } = e {
                 let groupby = format!("{:?}", partition_by.as_slice());
                 if let Some(tpl) = windows.iter_mut().find(|tpl| tpl.0 == groupby) {
-                    tpl.1.push((index, options.explode, phys.clone()))
+                    tpl.1.push((index, phys.clone()))
                 } else {
-                    windows.push((groupby, vec![(index, options.explode, phys.clone())]))
+                    windows.push((groupby, vec![(index, phys.clone())]))
                 }
                 is_window = true;
                 break;
             }
         }
         if !is_window {
             other.push((index, phys))
@@ -101,32 +94,28 @@
     let mut selected_columns = POOL.install(|| {
         other
             .par_iter()
             .map(|(idx, expr)| expr.evaluate(df, state).map(|s| (*idx, s)))
             .collect::<PolarsResult<Vec<_>>>()
     })?;
 
-    for mut partition in windows {
+    for partition in windows {
         // clear the cache for every partitioned group
         let mut state = state.split();
+        // inform the expression it has window functions.
+        state.insert_has_window_function_flag();
 
         // don't bother caching if we only have a single window function in this partition
         if partition.1.len() == 1 {
             state.remove_cache_window_flag();
         } else {
             state.insert_cache_window_flag();
         }
 
-        partition.1.sort_unstable_by_key(|(_idx, explode, _)| {
-            // negate as `false` will be first and we want the exploded
-            // e.g. the sorted groups cd to be the first to fill the cache.
-            !explode
-        });
-
-        for (index, _, e) in partition.1 {
+        for (index, e) in partition.1 {
             if e.as_expression()
                 .unwrap()
                 .into_iter()
                 .filter(|e| matches!(e, Expr::Window { .. }))
                 .count()
                 == 1
             {
@@ -150,25 +139,28 @@
 
 pub(crate) fn evaluate_physical_expressions(
     df: &DataFrame,
     exprs: &[Arc<dyn PhysicalExpr>],
     state: &mut ExecutionState,
     has_windows: bool,
 ) -> PolarsResult<DataFrame> {
+    state.expr_cache = Some(Default::default());
     let zero_length = df.height() == 0;
     let selected_columns = if has_windows {
         execute_projection_cached_window_fns(df, exprs, state)?
     } else {
         POOL.install(|| {
             exprs
                 .par_iter()
                 .map(|expr| expr.evaluate(df, state))
                 .collect::<PolarsResult<_>>()
         })?
     };
+    state.clear_window_expr_cache();
+    state.expr_cache = None;
 
     check_expand_literals(selected_columns, zero_length)
 }
 
 fn check_expand_literals(
     mut selected_columns: Vec<Series>,
     zero_length: bool,
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/projection.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/projection.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/python_scan.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/python_scan.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/scan/csv.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/scan/csv.rs`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,9 @@
 use std::path::PathBuf;
 
-use polars_io::predicates::PhysicalIoExpr;
-
 use super::*;
 
 pub struct CsvExec {
     pub path: PathBuf,
     pub schema: SchemaRef,
     pub options: CsvParserOptions,
     pub predicate: Option<Arc<dyn PhysicalExpr>>,
@@ -20,18 +18,15 @@
             columns
         });
 
         if projected_len == 0 {
             with_columns = None;
         }
         let n_rows = _set_n_rows_for_scan(self.options.n_rows);
-        let predicate = self
-            .predicate
-            .clone()
-            .map(|expr| Arc::new(PhysicalIoHelper { expr }) as Arc<dyn PhysicalIoExpr>);
+        let predicate = self.predicate.clone().map(phys_expr_to_io_expr);
 
         CsvReader::from_path(&self.path)
             .unwrap()
             .has_header(self.options.has_header)
             .with_dtypes(Some(self.schema.clone()))
             .with_delimiter(self.options.delimiter)
             .with_ignore_errors(self.options.ignore_errors)
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/scan/ipc.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/scan/ipc.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/scan/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/scan/mod.rs`

 * *Files 8% similar despite different names*

```diff
@@ -19,14 +19,15 @@
 use polars_io::predicates::PhysicalIoExpr;
 use polars_io::prelude::*;
 use polars_plan::global::_set_n_rows_for_scan;
 #[cfg(any(feature = "parquet", feature = "csv", feature = "ipc", feature = "cse"))]
 use polars_plan::logical_plan::FileFingerPrint;
 
 use super::*;
+use crate::physical_plan::expressions::phys_expr_to_io_expr;
 use crate::prelude::*;
 
 #[cfg(any(feature = "ipc", feature = "parquet"))]
 type Projection = Option<Vec<usize>>;
 #[cfg(any(feature = "ipc", feature = "parquet"))]
 type StopNRows = Option<usize>;
 #[cfg(any(feature = "ipc", feature = "parquet"))]
@@ -49,40 +50,42 @@
         with_columns
             .iter()
             .map(|name| schema.index_of(name).unwrap())
             .collect()
     });
 
     let n_rows = _set_n_rows_for_scan(n_rows);
-    let predicate = predicate
-        .clone()
-        .map(|expr| Arc::new(PhysicalIoHelper { expr }) as Arc<dyn PhysicalIoExpr>);
+    let predicate = predicate.clone().map(phys_expr_to_io_expr);
 
     (file, projection, n_rows, predicate)
 }
 
 /// Producer of an in memory DataFrame
 pub struct DataFrameExec {
     pub(crate) df: Arc<DataFrame>,
     pub(crate) selection: Option<Arc<dyn PhysicalExpr>>,
     pub(crate) projection: Option<Arc<Vec<String>>>,
+    pub(crate) predicate_has_windows: bool,
 }
 
 impl Executor for DataFrameExec {
     fn execute(&mut self, state: &mut ExecutionState) -> PolarsResult<DataFrame> {
         let df = mem::take(&mut self.df);
         let mut df = Arc::try_unwrap(df).unwrap_or_else(|df| (*df).clone());
 
         // projection should be before selection as those are free
         // TODO: this is only the case if we don't create new columns
         if let Some(projection) = &self.projection {
             df = df.select(projection.as_ref())?;
         }
 
         if let Some(selection) = &self.selection {
+            if self.predicate_has_windows {
+                state.insert_has_window_function_flag()
+            }
             let s = selection.evaluate(&df, state)?;
             let mask = s.bool().map_err(
                 |_| polars_err!(ComputeError: "filter predicate was not of type boolean"),
             )?;
             df = df.filter(mask)?;
         }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/scan/ndjson.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/scan/ndjson.rs`

 * *Files 9% similar despite different names*

```diff
@@ -20,15 +20,15 @@
     }
 
     fn schema(&self, infer_schema_length: Option<usize>) -> PolarsResult<Schema> {
         let f = std::fs::File::open(&self.path)?;
         let mut reader = std::io::BufReader::new(f);
 
         let data_type =
-            arrow_ndjson::read::infer(&mut reader, infer_schema_length).map_err(to_compute_err)?;
+            polars_json::ndjson::infer(&mut reader, infer_schema_length).map_err(to_compute_err)?;
         let schema = Schema::from_iter(StructArray::get_fields(&data_type));
 
         Ok(schema)
     }
     fn allows_projection_pushdown(&self) -> bool {
         true
     }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/scan/parquet.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/scan/parquet.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/slice.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/slice.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/sort.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/sort.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/stack.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/stack.rs`

 * *Files 20% similar despite different names*

```diff
@@ -9,27 +9,29 @@
 
 impl StackExec {
     fn execute_impl(
         &mut self,
         state: &mut ExecutionState,
         mut df: DataFrame,
     ) -> PolarsResult<DataFrame> {
+        state.expr_cache = Some(Default::default());
         let res = if self.has_windows {
             // we have a different run here
             // to ensure the window functions run sequential and share caches
             execute_projection_cached_window_fns(&df, &self.expr, state)?
         } else {
             POOL.install(|| {
                 self.expr
                     .par_iter()
                     .map(|expr| expr.evaluate(&df, state))
                     .collect::<PolarsResult<Vec<_>>>()
             })?
         };
-        state.clear_expr_cache();
+        state.clear_window_expr_cache();
+        state.expr_cache = None;
 
         let schema = &*self.input_schema;
         df._add_columns(res, schema)?;
 
         Ok(df)
     }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/udf.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/udf.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/union.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/union.rs`

 * *Files 20% similar despite different names*

```diff
@@ -18,61 +18,69 @@
         }
         // keep scans thread local if 'fetch' is used.
         if _is_fetch_query() {
             self.options.parallel = false;
         }
         let mut inputs = std::mem::take(&mut self.inputs);
 
-        let sliced_path = self.options.slice && self.options.slice_offset >= 0;
+        let sliced_path = if let Some((offset, _)) = self.options.slice {
+            offset >= 0
+        } else {
+            false
+        };
 
         if !self.options.parallel || sliced_path {
             if state.verbose() {
                 if !self.options.parallel {
                     println!("UNION: `parallel=false` union is run sequentially")
                 } else {
                     println!("UNION: `slice is set` union is run sequentially")
                 }
             }
 
-            let mut offset = self.options.slice_offset as usize;
-            let mut len = self.options.slice_len as usize;
-            let dfs = inputs
-                .into_iter()
-                .enumerate()
-                .map(|(idx, mut input)| {
-                    let mut state = state.split();
-                    state.branch_idx += idx;
-                    let df = input.execute(&mut state)?;
-
-                    if !sliced_path {
-                        return Ok(Some(df));
-                    }
+            let (slice_offset, mut slice_len) = self.options.slice.unwrap_or((0, usize::MAX));
+            let mut slice_offset = slice_offset as usize;
+            let mut dfs = Vec::with_capacity(inputs.len());
+
+            for (idx, mut input) in inputs.into_iter().enumerate() {
+                let mut state = state.split();
+                state.branch_idx += idx;
+
+                let df = input.execute(&mut state)?;
+
+                if !sliced_path {
+                    dfs.push(df);
+                    continue;
+                }
 
-                    Ok(if offset > df.height() {
-                        offset -= df.height();
-                        None
-                    } else if offset + len > df.height() {
-                        len -= df.height() - offset;
-                        if offset == 0 {
-                            Some(df)
-                        } else {
-                            let out = Some(df.slice(offset as i64, usize::MAX));
-                            offset = 0;
-                            out
-                        }
+                let height = df.height();
+                // this part can be skipped as we haven't reached the offset yet
+                // TODO!: don't read the file yet!
+                if slice_offset > height {
+                    slice_offset -= height;
+                }
+                // applying the slice
+                // continue iteration
+                else if slice_offset + slice_len > height {
+                    slice_len -= height - slice_offset;
+                    if slice_offset == 0 {
+                        dfs.push(df);
                     } else {
-                        let out = Some(df.slice(offset as i64, len));
-                        len = 0;
-                        offset = 0;
-                        out
-                    })
-                })
-                .collect::<PolarsResult<Vec<_>>>()?;
+                        dfs.push(df.slice(slice_offset as i64, usize::MAX));
+                        slice_offset = 0;
+                    }
+                }
+                // we finished the slice
+                else {
+                    dfs.push(df.slice(slice_offset as i64, slice_len));
+                    break;
+                }
+            }
 
-            concat_df(dfs.iter().flatten())
+            concat_df(&dfs)
         } else {
             if state.verbose() {
                 println!("UNION: union is run in parallel")
             }
 
             // we don't use par_iter directly because the LP may also start threads for every LP (for instance scan_csv)
             // this might then lead to a rayon SO. So we take a multitude of the threads to keep work stealing
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/executors/unique.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/executors/unique.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/exotic.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/exotic.rs`

 * *Files 2% similar despite different names*

```diff
@@ -35,9 +35,9 @@
         .without_optimizations()
         .with_simplify_expr(true)
         .select([expr.clone()]);
     let optimized = lf.optimize(&mut lp_arena, &mut expr_arena).unwrap();
     let lp = lp_arena.get(optimized);
     let aexpr = lp.get_exprs().pop().unwrap();
 
-    create_physical_expr(aexpr, ctxt, &expr_arena, None)
+    create_physical_expr(aexpr, ctxt, &expr_arena, None, &mut Default::default())
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/aggregation.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/aggregation.rs`

 * *Files 12% similar despite different names*

```diff
@@ -15,21 +15,23 @@
 use crate::physical_plan::state::ExecutionState;
 use crate::physical_plan::PartitionedAggregation;
 use crate::prelude::*;
 
 pub(crate) struct AggregationExpr {
     pub(crate) input: Arc<dyn PhysicalExpr>,
     pub(crate) agg_type: GroupByMethod,
+    field: Option<Field>,
 }
 
 impl AggregationExpr {
-    pub fn new(expr: Arc<dyn PhysicalExpr>, agg_type: GroupByMethod) -> Self {
+    pub fn new(expr: Arc<dyn PhysicalExpr>, agg_type: GroupByMethod, field: Option<Field>) -> Self {
         Self {
             input: expr,
             agg_type,
+            field,
         }
     }
 }
 
 impl PhysicalExpr for AggregationExpr {
     fn as_expression(&self) -> Option<&Expr> {
         None
@@ -62,42 +64,46 @@
                             polars_bail!(ComputeError: "cannot aggregate as {}, the column is already aggregated");
                         }
                     },
                     _ => ()
                 }
             }
         }
-
         // Safety:
         // groups must always be in bounds.
         let out = unsafe {
             match self.agg_type {
                 GroupByMethod::Min => {
                     check_null_prop!();
-                    let agg_s = ac.flat_naive().into_owned().agg_min(ac.groups());
+                    let (s, groups) = ac.get_final_aggregation();
+                    let agg_s = s.agg_min(&groups);
                     rename_series(agg_s, &keep_name)
                 }
                 GroupByMethod::Max => {
                     check_null_prop!();
-                    let agg_s = ac.flat_naive().into_owned().agg_max(ac.groups());
+                    let (s, groups) = ac.get_final_aggregation();
+                    let agg_s = s.agg_max(&groups);
                     rename_series(agg_s, &keep_name)
                 }
                 GroupByMethod::Median => {
                     check_null_prop!();
-                    let agg_s = ac.flat_naive().into_owned().agg_median(ac.groups());
+                    let (s, groups) = ac.get_final_aggregation();
+                    let agg_s = s.agg_median(&groups);
                     rename_series(agg_s, &keep_name)
                 }
                 GroupByMethod::Mean => {
                     check_null_prop!();
-                    let agg_s = ac.flat_naive().into_owned().agg_mean(ac.groups());
+                    let (s, groups) = ac.get_final_aggregation();
+                    let agg_s = s.agg_mean(&groups);
                     rename_series(agg_s, &keep_name)
                 }
                 GroupByMethod::Sum => {
                     check_null_prop!();
-                    let agg_s = ac.flat_naive().into_owned().agg_sum(ac.groups());
+                    let (s, groups) = ac.get_final_aggregation();
+                    let agg_s = s.agg_sum(&groups);
                     rename_series(agg_s, &keep_name)
                 }
                 GroupByMethod::Count => {
                     // a few fast paths that prevent materializing new groups
                     match ac.update_groups {
                         UpdateGroups::WithSeriesLen => {
                             let list = ac
@@ -152,99 +158,95 @@
                             ca.rename(&keep_name);
                             ca.into_series()
                         }
                     }
                 }
                 GroupByMethod::First => {
                     check_null_prop!();
-                    let mut agg_s = ac.flat_naive().into_owned().agg_first(ac.groups());
-                    agg_s.rename(&keep_name);
-                    agg_s
+                    let (s, groups) = ac.get_final_aggregation();
+                    let agg_s = s.agg_first(&groups);
+                    rename_series(agg_s, &keep_name)
                 }
                 GroupByMethod::Last => {
                     check_null_prop!();
-                    let mut agg_s = ac.flat_naive().into_owned().agg_last(ac.groups());
-                    agg_s.rename(&keep_name);
-                    agg_s
+                    let (s, groups) = ac.get_final_aggregation();
+                    let agg_s = s.agg_last(&groups);
+                    rename_series(agg_s, &keep_name)
                 }
                 GroupByMethod::NUnique => {
                     check_null_prop!();
-                    let agg_s = ac.flat_naive().into_owned().agg_n_unique(ac.groups());
+                    let (s, groups) = ac.get_final_aggregation();
+                    let agg_s = s.agg_n_unique(&groups);
                     rename_series(agg_s, &keep_name)
                 }
                 GroupByMethod::Implode => {
-                    if state.unset_finalize_window_as_list() {
-                        let agg = ac.aggregated();
-                        rename_series(agg, &keep_name)
-                    } else {
-                        // if the aggregation is already
-                        // in an aggregate flat state for instance by
-                        // a mean aggregation, we simply convert to list
-                        //
-                        // if it is not, we traverse the groups and create
-                        // a list per group.
-                        let s = match ac.agg_state() {
-                            // mean agg:
-                            // -> f64 -> list<f64>
-                            AggState::AggregatedFlat(s) => s.reshape(&[-1, 1]).unwrap(),
-                            _ => {
-                                let agg = ac.aggregated();
-                                agg.as_list().into_series()
-                            }
-                        };
-                        rename_series(s, &keep_name)
-                    }
+                    // if the aggregation is already
+                    // in an aggregate flat state for instance by
+                    // a mean aggregation, we simply convert to list
+                    //
+                    // if it is not, we traverse the groups and create
+                    // a list per group.
+                    let s = match ac.agg_state() {
+                        // mean agg:
+                        // -> f64 -> list<f64>
+                        AggState::AggregatedFlat(s) => s.reshape(&[-1, 1]).unwrap(),
+                        _ => {
+                            let agg = ac.aggregated();
+                            agg.as_list().into_series()
+                        }
+                    };
+                    rename_series(s, &keep_name)
                 }
                 GroupByMethod::Groups => {
                     let mut column: ListChunked = ac.groups().as_list_chunked();
                     column.rename(&keep_name);
                     column.into_series()
                 }
                 GroupByMethod::Std(ddof) => {
                     check_null_prop!();
-                    let agg_s = ac.flat_naive().into_owned().agg_std(ac.groups(), ddof);
+                    let (s, groups) = ac.get_final_aggregation();
+                    let agg_s = s.agg_std(&groups, ddof);
                     rename_series(agg_s, &keep_name)
                 }
                 GroupByMethod::Var(ddof) => {
                     check_null_prop!();
-                    let agg_s = ac.flat_naive().into_owned().agg_var(ac.groups(), ddof);
+                    let (s, groups) = ac.get_final_aggregation();
+                    let agg_s = s.agg_var(&groups, ddof);
                     rename_series(agg_s, &keep_name)
                 }
                 GroupByMethod::Quantile(_, _) => {
                     // implemented explicitly in AggQuantile struct
                     unimplemented!()
                 }
                 GroupByMethod::NanMin => {
                     #[cfg(feature = "propagate_nans")]
                     {
                         check_null_prop!();
-                        let agg_s = ac.flat_naive().into_owned();
-                        let groups = ac.groups();
-                        let agg_s = if agg_s.dtype().is_float() {
-                            nan_propagating_aggregate::group_agg_nan_min_s(&agg_s, groups)
+                        let (s, groups) = ac.get_final_aggregation();
+                        let agg_s = if s.dtype().is_float() {
+                            nan_propagating_aggregate::group_agg_nan_min_s(&s, &groups)
                         } else {
-                            agg_s.agg_min(groups)
+                            s.agg_min(&groups)
                         };
                         rename_series(agg_s, &keep_name)
                     }
                     #[cfg(not(feature = "propagate_nans"))]
                     {
                         panic!("activate 'propagate_nans' feature")
                     }
                 }
                 GroupByMethod::NanMax => {
                     #[cfg(feature = "propagate_nans")]
                     {
                         check_null_prop!();
-                        let agg_s = ac.flat_naive().into_owned();
-                        let groups = ac.groups();
-                        let agg_s = if agg_s.dtype().is_float() {
-                            nan_propagating_aggregate::group_agg_nan_max_s(&agg_s, groups)
+                        let (s, groups) = ac.get_final_aggregation();
+                        let agg_s = if s.dtype().is_float() {
+                            nan_propagating_aggregate::group_agg_nan_max_s(&s, &groups)
                         } else {
-                            agg_s.agg_max(groups)
+                            s.agg_max(&groups)
                         };
                         rename_series(agg_s, &keep_name)
                     }
                     #[cfg(not(feature = "propagate_nans"))]
                     {
                         panic!("activate 'propagate_nans' feature")
                     }
@@ -252,15 +254,19 @@
             }
         };
 
         Ok(AggregationContext::new(out, Cow::Borrowed(groups), true))
     }
 
     fn to_field(&self, input_schema: &Schema) -> PolarsResult<Field> {
-        self.input.to_field(input_schema)
+        if let Some(field) = self.field.as_ref() {
+            Ok(field.clone())
+        } else {
+            self.input.to_field(input_schema)
+        }
     }
 
     fn as_partitioned_aggregator(&self) -> Option<&dyn PartitionedAggregation> {
         Some(self)
     }
 
     fn is_valid_aggregation(&self) -> bool {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/alias.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/alias.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/apply.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/apply.rs`

 * *Files 2% similar despite different names*

```diff
@@ -20,14 +20,15 @@
     pub function: SpecialEq<Arc<dyn SeriesUdf>>,
     pub expr: Expr,
     pub collect_groups: ApplyOptions,
     pub auto_explode: bool,
     pub allow_rename: bool,
     pub pass_name_to_apply: bool,
     pub input_schema: Option<SchemaRef>,
+    pub allow_threading: bool,
 }
 
 impl ApplyExpr {
     pub(crate) fn new_minimal(
         inputs: Vec<Arc<dyn PhysicalExpr>>,
         function: SpecialEq<Arc<dyn SeriesUdf>>,
         expr: Expr,
@@ -38,30 +39,31 @@
             function,
             expr,
             collect_groups,
             auto_explode: false,
             allow_rename: false,
             pass_name_to_apply: false,
             input_schema: None,
+            allow_threading: true,
         }
     }
 
     #[allow(clippy::ptr_arg)]
     fn prepare_multiple_inputs<'a>(
         &self,
         df: &DataFrame,
         groups: &'a GroupsProxy,
         state: &ExecutionState,
     ) -> PolarsResult<Vec<AggregationContext<'a>>> {
-        POOL.install(|| {
-            self.inputs
-                .par_iter()
-                .map(|e| e.evaluate_on_groups(df, groups, state))
-                .collect()
-        })
+        let f = |e: &Arc<dyn PhysicalExpr>| e.evaluate_on_groups(df, groups, state);
+        if self.allow_threading {
+            POOL.install(|| self.inputs.par_iter().map(f).collect())
+        } else {
+            self.inputs.iter().map(f).collect()
+        }
     }
 
     fn finish_apply_groups<'a>(
         &self,
         mut ac: AggregationContext<'a>,
         ca: ListChunked,
     ) -> PolarsResult<AggregationContext<'a>> {
@@ -117,30 +119,40 @@
             let input = Series::full_null("", 0, &input_dtype);
 
             let output = self.eval_and_flatten(&mut [input])?;
             let ca = ListChunked::full(&name, &output, 0);
             return self.finish_apply_groups(ac, ca);
         }
 
-        let mut ca: ListChunked = POOL.install(|| {
+        let f = |opt_s: Option<Series>| match opt_s {
+            None => Ok(None),
+            Some(mut s) => {
+                if self.pass_name_to_apply {
+                    s.rename(&name);
+                }
+                let mut container = [s];
+                self.function.call_udf(&mut container)
+            }
+        };
+
+        let mut ca: ListChunked = if self.allow_threading {
+            POOL.install(|| {
+                agg.list()
+                    .unwrap()
+                    .par_iter()
+                    .map(f)
+                    .collect::<PolarsResult<_>>()
+            })?
+        } else {
             agg.list()
                 .unwrap()
-                .par_iter()
-                .map(|opt_s| match opt_s {
-                    None => Ok(None),
-                    Some(mut s) => {
-                        if self.pass_name_to_apply {
-                            s.rename(&name);
-                        }
-                        let mut container = [s];
-                        self.function.call_udf(&mut container)
-                    }
-                })
-                .collect::<PolarsResult<_>>()
-        })?;
+                .into_iter()
+                .map(f)
+                .collect::<PolarsResult<_>>()?
+        };
 
         ca.rename(&name);
         self.finish_apply_groups(ac, ca)
     }
 
     /// Apply elementwise e.g. ignore the group/list indices
     fn apply_single_elementwise<'a>(
@@ -221,32 +233,37 @@
     let offset = list_arr.offsets().as_slice();
     (offset[offset.len() - 1] as usize) == list_arr.len()
 }
 
 fn check_map_output_len(input_len: usize, output_len: usize, expr: &Expr) -> PolarsResult<()> {
     polars_ensure!(
         input_len == output_len, expr = expr, InvalidOperation:
-        "output length of `map` must be equal to that of the input length; \
-        consider using `apply` instead"
+        "output length of `map` ({}) must be equal to the input length ({}); \
+        consider using `apply` instead", input_len, output_len
     );
     Ok(())
 }
 
 impl PhysicalExpr for ApplyExpr {
     fn as_expression(&self) -> Option<&Expr> {
         Some(&self.expr)
     }
 
     fn evaluate(&self, df: &DataFrame, state: &ExecutionState) -> PolarsResult<Series> {
-        let mut inputs = POOL.install(|| {
-            self.inputs
-                .par_iter()
-                .map(|e| e.evaluate(df, state))
-                .collect::<PolarsResult<Vec<_>>>()
-        })?;
+        let f = |e: &Arc<dyn PhysicalExpr>| e.evaluate(df, state);
+        let mut inputs = if self.allow_threading {
+            POOL.install(|| {
+                self.inputs
+                    .par_iter()
+                    .map(f)
+                    .collect::<PolarsResult<Vec<_>>>()
+            })
+        } else {
+            self.inputs.iter().map(f).collect::<PolarsResult<Vec<_>>>()
+        }?;
 
         if self.allow_rename {
             return self.eval_and_flatten(&mut inputs);
         }
         let in_name = inputs[0].name().to_string();
         let mut out = self.eval_and_flatten(&mut inputs)?;
         if in_name != out.name() {
@@ -367,15 +384,15 @@
                         ac.groups();
                     }
 
                     ac.flat_naive().into_owned()
                 })
                 .collect::<Vec<_>>();
 
-            let input_len = s[0].len();
+            let input_len = s.iter().map(|s| s.len()).max().unwrap();
             let s = function.call_udf(&mut s)?.unwrap();
             check_map_output_len(input_len, s.len(), expr)?;
 
             // take the first aggregation context that as that is the input series
             let mut ac = acs.swap_remove(0);
             ac.with_series(s, false, None)?;
             Ok(ac)
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/binary.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/binary.rs`

 * *Files 7% similar despite different names*

```diff
@@ -29,32 +29,18 @@
         }
     }
 }
 
 /// Can partially do operations in place.
 fn apply_operator_owned(left: Series, right: Series, op: Operator) -> PolarsResult<Series> {
     match op {
-        Operator::Gt => ChunkCompare::<&Series>::gt(&left, &right).map(|ca| ca.into_series()),
-        Operator::GtEq => ChunkCompare::<&Series>::gt_eq(&left, &right).map(|ca| ca.into_series()),
-        Operator::Lt => ChunkCompare::<&Series>::lt(&left, &right).map(|ca| ca.into_series()),
-        Operator::LtEq => ChunkCompare::<&Series>::lt_eq(&left, &right).map(|ca| ca.into_series()),
-        Operator::Eq => ChunkCompare::<&Series>::equal(&left, &right).map(|ca| ca.into_series()),
-        Operator::NotEq => {
-            ChunkCompare::<&Series>::not_equal(&left, &right).map(|ca| ca.into_series())
-        }
         Operator::Plus => Ok(left + right),
         Operator::Minus => Ok(left - right),
         Operator::Multiply => Ok(left * right),
-        Operator::Divide => Ok(&left / &right),
-        Operator::TrueDivide => apply_operator(&left, &right, op),
-        Operator::FloorDivide => apply_operator(&left, &right, op),
-        Operator::And => left.bitand(&right),
-        Operator::Or => left.bitor(&right),
-        Operator::Xor => left.bitxor(&right),
-        Operator::Modulus => Ok(&left % &right),
+        _ => apply_operator(&left, &right, op),
     }
 }
 
 pub fn apply_operator(left: &Series, right: &Series, op: Operator) -> PolarsResult<Series> {
     use DataType::*;
     match op {
         Operator::Gt => ChunkCompare::<&Series>::gt(left, right).map(|ca| ca.into_series()),
@@ -83,14 +69,16 @@
                 panic!("activate 'round_series' feature")
             }
         }
         Operator::And => left.bitand(right),
         Operator::Or => left.bitor(right),
         Operator::Xor => left.bitxor(right),
         Operator::Modulus => Ok(left % right),
+        Operator::EqValidity => left.equal_missing(right).map(|ca| ca.into_series()),
+        Operator::NotEqValidity => left.not_equal_missing(right).map(|ca| ca.into_series()),
     }
 }
 
 impl BinaryExpr {
     fn apply_elementwise<'a>(
         &self,
         mut ac_l: AggregationContext<'a>,
@@ -158,23 +146,32 @@
 
 impl PhysicalExpr for BinaryExpr {
     fn as_expression(&self) -> Option<&Expr> {
         Some(&self.expr)
     }
 
     fn evaluate(&self, df: &DataFrame, state: &ExecutionState) -> PolarsResult<Series> {
-        let mut state = state.split();
-        // don't cache window functions as they run in parallel
-        state.remove_cache_window_flag();
-        let (lhs, rhs) = POOL.install(|| {
-            rayon::join(
-                || self.left.evaluate(df, &state),
-                || self.right.evaluate(df, &state),
+        // window functions may set a global state that determine their output
+        // state, so we don't let them run in parallel as they race
+        // they also saturate the thread pool by themselves, so that's fine
+        let (lhs, rhs) = if state.has_window() {
+            let mut state = state.split();
+            state.remove_cache_window_flag();
+            (
+                self.left.evaluate(df, &state),
+                self.right.evaluate(df, &state),
             )
-        });
+        } else {
+            POOL.install(|| {
+                rayon::join(
+                    || self.left.evaluate(df, state),
+                    || self.right.evaluate(df, state),
+                )
+            })
+        };
         let lhs = lhs?;
         let rhs = rhs?;
         polars_ensure!(
             lhs.len() == rhs.len() || lhs.len() == 1 || rhs.len() == 1,
             expr = self.expr,
             ComputeError: "cannot evaluate two series of different lengths ({} and {})",
             lhs.len(), rhs.len(),
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/cast.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/cast.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/column.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/column.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/count.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/count.rs`

 * *Files 3% similar despite different names*

```diff
@@ -34,15 +34,15 @@
         let mut ca = groups.group_count();
         ca.rename(COUNT_NAME);
         let s = ca.into_series();
 
         Ok(AggregationContext::new(s, Cow::Borrowed(groups), true))
     }
     fn to_field(&self, _input_schema: &Schema) -> PolarsResult<Field> {
-        Ok(Field::new("count", DataType::UInt32))
+        Ok(Field::new("count", IDX_DTYPE))
     }
 
     fn as_partitioned_aggregator(&self) -> Option<&dyn PartitionedAggregation> {
         Some(self)
     }
 
     fn is_valid_aggregation(&self) -> bool {
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/filter.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/filter.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/group_iter.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/group_iter.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/literal.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/literal.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/mod.rs`

 * *Files 3% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 mod aggregation;
 mod alias;
 mod apply;
 mod binary;
+mod cache;
 mod cast;
 mod column;
 mod count;
 mod filter;
 mod group_iter;
 mod literal;
 mod slice;
@@ -18,14 +19,15 @@
 use std::borrow::Cow;
 use std::fmt::{Display, Formatter};
 
 pub(crate) use aggregation::*;
 pub(crate) use alias::*;
 pub(crate) use apply::*;
 pub(crate) use binary::*;
+pub(crate) use cache::*;
 pub(crate) use cast::*;
 pub(crate) use column::*;
 pub(crate) use count::*;
 pub(crate) use filter::*;
 pub(crate) use literal::*;
 use polars_arrow::utils::CustomIterTools;
 use polars_core::frame::groupby::GroupsProxy;
@@ -433,14 +435,54 @@
         match self.agg_state() {
             Literal(s) => s.len() == 1,
             AggregatedFlat(_) => true,
             _ => false,
         }
     }
 
+    pub(crate) fn get_final_aggregation(mut self) -> (Series, Cow<'a, GroupsProxy>) {
+        let _ = self.groups();
+        let groups = self.groups;
+        match self.state {
+            AggState::NotAggregated(s) => (s, groups),
+            AggState::AggregatedFlat(s) => (s, groups),
+            AggState::Literal(s) => (s, groups),
+            AggState::AggregatedList(s) => {
+                let flattened = s.explode().unwrap();
+                let groups = groups.into_owned();
+                // unroll the possible flattened state
+                // say we have groups with overlapping windows:
+                //
+                // offset, len
+                // 0, 1
+                // 0, 2
+                // 0, 4
+                //
+                // gets aggregation
+                //
+                // [0]
+                // [0, 1],
+                // [0, 1, 2, 3]
+                //
+                // before aggregation the column was
+                // [0, 1, 2, 3]
+                // but explode on this list yields
+                // [0, 0, 1, 0, 1, 2, 3]
+                //
+                // so we unroll the groups as
+                //
+                // [0, 1]
+                // [1, 2]
+                // [3, 4]
+                let groups = groups.unroll();
+                (flattened, Cow::Owned(groups))
+            }
+        }
+    }
+
     /// Get the not-aggregated version of the series.
     /// Note that we call it naive, because if a previous expr
     /// has filtered or sorted this, this information is in the
     /// group tuples not the flattened series.
     pub(crate) fn flat_naive(&self) -> Cow<'_, Series> {
         match &self.state {
             AggState::NotAggregated(s) => Cow::Borrowed(s),
@@ -547,27 +589,45 @@
 }
 
 /// Wrapper struct that allow us to use a PhysicalExpr in polars-io.
 ///
 /// This is used to filter rows during the scan of file.
 pub struct PhysicalIoHelper {
     pub expr: Arc<dyn PhysicalExpr>,
+    pub has_window_function: bool,
 }
 
 impl PhysicalIoExpr for PhysicalIoHelper {
     fn evaluate(&self, df: &DataFrame) -> PolarsResult<Series> {
-        self.expr.evaluate(df, &Default::default())
+        let mut state: ExecutionState = Default::default();
+        if self.has_window_function {
+            state.insert_has_window_function_flag();
+        }
+        self.expr.evaluate(df, &state)
     }
 
     #[cfg(feature = "parquet")]
     fn as_stats_evaluator(&self) -> Option<&dyn polars_io::predicates::StatsEvaluator> {
         self.expr.as_stats_evaluator()
     }
 }
 
+pub(super) fn phys_expr_to_io_expr(expr: Arc<dyn PhysicalExpr>) -> Arc<dyn PhysicalIoExpr> {
+    let has_window_function = if let Some(expr) = expr.as_expression() {
+        expr.into_iter()
+            .any(|expr| matches!(expr, Expr::Window { .. }))
+    } else {
+        false
+    };
+    Arc::new(PhysicalIoHelper {
+        expr,
+        has_window_function,
+    }) as Arc<dyn PhysicalIoExpr>
+}
+
 pub trait PartitionedAggregation: Send + Sync + PhysicalExpr {
     /// This is called in partitioned aggregation.
     /// Partitioned results may differ from aggregation results.
     /// For instance, for a `mean` operation a partitioned result
     /// needs to return the `sum` and the `valid_count` (length - null count).
     ///
     /// A final aggregation can then take the sum of sums and sum of valid_counts
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/slice.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/slice.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/sort.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/sort.rs`

 * *Files 6% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 use std::sync::Arc;
 
 use polars_arrow::utils::CustomIterTools;
 use polars_core::frame::groupby::GroupsProxy;
 use polars_core::prelude::*;
+use polars_core::POOL;
+use rayon::prelude::*;
 
 use crate::physical_plan::state::ExecutionState;
 use crate::prelude::*;
 
 pub struct SortExpr {
     pub(crate) physical_expr: Arc<dyn PhysicalExpr>,
     pub(crate) options: SortOptions,
@@ -70,41 +72,47 @@
                 let ca = s.list().unwrap();
                 let out = ca.lst_sort(self.options);
                 ac.with_series(out.into_series(), true, Some(&self.expr))?;
             }
             _ => {
                 let series = ac.flat_naive().into_owned();
 
-                let groups = match ac.groups().as_ref() {
-                    GroupsProxy::Idx(groups) => {
-                        groups
-                            .iter()
-                            .map(|(first, idx)| {
-                                // Safety:
-                                // Group tuples are always in bounds
-                                let group = unsafe {
-                                    series.take_iter_unchecked(&mut idx.iter().map(|i| *i as usize))
-                                };
-
-                                let sorted_idx = group.arg_sort(self.options);
-                                let new_idx = map_sorted_indices_to_group_idx(&sorted_idx, idx);
+                let mut sort_options = self.options;
+                sort_options.multithreaded = false;
+                let groups = POOL.install(|| {
+                    match ac.groups().as_ref() {
+                        GroupsProxy::Idx(groups) => {
+                            groups
+                                .par_iter()
+                                .map(|(first, idx)| {
+                                    // Safety:
+                                    // Group tuples are always in bounds
+                                    let group = unsafe {
+                                        series.take_iter_unchecked(
+                                            &mut idx.iter().map(|i| *i as usize),
+                                        )
+                                    };
+
+                                    let sorted_idx = group.arg_sort(sort_options);
+                                    let new_idx = map_sorted_indices_to_group_idx(&sorted_idx, idx);
+                                    (new_idx.first().copied().unwrap_or(first), new_idx)
+                                })
+                                .collect()
+                        }
+                        GroupsProxy::Slice { groups, .. } => groups
+                            .par_iter()
+                            .map(|&[first, len]| {
+                                let group = series.slice(first as i64, len as usize);
+                                let sorted_idx = group.arg_sort(sort_options);
+                                let new_idx = map_sorted_indices_to_group_slice(&sorted_idx, first);
                                 (new_idx.first().copied().unwrap_or(first), new_idx)
                             })
-                            .collect()
+                            .collect(),
                     }
-                    GroupsProxy::Slice { groups, .. } => groups
-                        .iter()
-                        .map(|&[first, len]| {
-                            let group = series.slice(first as i64, len as usize);
-                            let sorted_idx = group.arg_sort(self.options);
-                            let new_idx = map_sorted_indices_to_group_slice(&sorted_idx, first);
-                            (new_idx.first().copied().unwrap_or(first), new_idx)
-                        })
-                        .collect(),
-                };
+                });
                 let groups = GroupsProxy::Idx(groups);
                 ac.with_groups(groups);
             }
         }
 
         Ok(ac)
     }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/sortby.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/sortby.rs`

 * *Files 8% similar despite different names*

```diff
@@ -70,15 +70,20 @@
                             #[cfg(feature = "dtype-categorical")]
                             DataType::Categorical(_) => s,
                             _ => s.to_physical_repr().into_owned(),
                         })
                     })
                     .collect::<PolarsResult<Vec<_>>>()?;
 
-                s_sort_by[0].arg_sort_multiple(&s_sort_by[1..], &descending)
+                let options = SortMultipleOptions {
+                    other: s_sort_by[1..].to_vec(),
+                    descending,
+                    multithreaded: true,
+                };
+                s_sort_by[0].arg_sort_multiple(&options)
             };
             POOL.install(|| rayon::join(series_f, sorted_idx_f))
         };
         let (sorted_idx, series) = (sorted_idx?, series?);
         polars_ensure!(
             sorted_idx.len() == series.len(),
             expr = self.expr, ComputeError:
@@ -113,33 +118,36 @@
             let mut ac_sort_by = self.by[0].evaluate_on_groups(df, groups, state)?;
             let sort_by = ac_sort_by.aggregated();
             let mut sort_by = sort_by.list().unwrap().clone();
             let s = ac_in.aggregated();
             let mut s = s.list().unwrap().clone();
 
             let descending = self.descending[0];
-            let mut ca: ListChunked = s
-                .par_iter_indexed()
-                .zip(sort_by.par_iter_indexed())
-                .map(|(opt_s, s_sort_by)| match (opt_s, s_sort_by) {
-                    (Some(s), Some(s_sort_by)) => {
-                        if s.len() != s_sort_by.len() {
-                            invalid.store(true, Ordering::Relaxed);
-                            None
-                        } else {
-                            let idx = s_sort_by.arg_sort(SortOptions {
-                                descending,
-                                ..Default::default()
-                            });
-                            Some(unsafe { s.take_unchecked(&idx).unwrap() })
+            let mut ca: ListChunked = POOL.install(|| {
+                s.par_iter_indexed()
+                    .zip(sort_by.par_iter_indexed())
+                    .map(|(opt_s, s_sort_by)| match (opt_s, s_sort_by) {
+                        (Some(s), Some(s_sort_by)) => {
+                            if s.len() != s_sort_by.len() {
+                                invalid.store(true, Ordering::Relaxed);
+                                None
+                            } else {
+                                let idx = s_sort_by.arg_sort(SortOptions {
+                                    descending,
+                                    // we are already in par iter.
+                                    multithreaded: false,
+                                    ..Default::default()
+                                });
+                                Some(unsafe { s.take_unchecked(&idx).unwrap() })
+                            }
                         }
-                    }
-                    _ => None,
-                })
-                .collect();
+                        _ => None,
+                    })
+                    .collect()
+            });
             ca.rename(s.name());
             let s = ca.into_series();
             ac_in.with_series(s, true, Some(&self.expr))?;
             Ok(ac_in)
         } else {
             let descending = prepare_descending(&self.descending, self.by.len());
 
@@ -152,45 +160,52 @@
                 );
                 let ordered_by_group_operation = matches!(
                     ac_sort_by.update_groups,
                     UpdateGroups::WithSeriesLen | UpdateGroups::WithGroupsLen
                 );
                 let groups = ac_sort_by.groups();
 
-                let groups = groups
-                    .par_iter()
-                    .map(|indicator| {
-                        let new_idx = match indicator {
-                            GroupsIndicator::Idx((_, idx)) => {
-                                // Safety:
-                                // Group tuples are always in bounds
-                                let group = unsafe {
-                                    sort_by_s
-                                        .take_iter_unchecked(&mut idx.iter().map(|i| *i as usize))
-                                };
+                let groups = POOL.install(|| {
+                    groups
+                        .par_iter()
+                        .map(|indicator| {
+                            let new_idx = match indicator {
+                                GroupsIndicator::Idx((_, idx)) => {
+                                    // Safety:
+                                    // Group tuples are always in bounds
+                                    let group = unsafe {
+                                        sort_by_s.take_iter_unchecked(
+                                            &mut idx.iter().map(|i| *i as usize),
+                                        )
+                                    };
+
+                                    let sorted_idx = group.arg_sort(SortOptions {
+                                        descending: descending[0],
+                                        // we are already in par iter.
+                                        multithreaded: false,
+                                        ..Default::default()
+                                    });
+                                    map_sorted_indices_to_group_idx(&sorted_idx, idx)
+                                }
+                                GroupsIndicator::Slice([first, len]) => {
+                                    let group = sort_by_s.slice(first as i64, len as usize);
+                                    let sorted_idx = group.arg_sort(SortOptions {
+                                        descending: descending[0],
+                                        // we are already in par iter.
+                                        multithreaded: false,
+                                        ..Default::default()
+                                    });
+                                    map_sorted_indices_to_group_slice(&sorted_idx, first)
+                                }
+                            };
 
-                                let sorted_idx = group.arg_sort(SortOptions {
-                                    descending: descending[0],
-                                    ..Default::default()
-                                });
-                                map_sorted_indices_to_group_idx(&sorted_idx, idx)
-                            }
-                            GroupsIndicator::Slice([first, len]) => {
-                                let group = sort_by_s.slice(first as i64, len as usize);
-                                let sorted_idx = group.arg_sort(SortOptions {
-                                    descending: descending[0],
-                                    ..Default::default()
-                                });
-                                map_sorted_indices_to_group_slice(&sorted_idx, first)
-                            }
-                        };
-
-                        (new_idx[0], new_idx)
-                    })
-                    .collect();
+                            (new_idx[0], new_idx)
+                        })
+                        .collect()
+                });
 
                 (GroupsProxy::Idx(groups), ordered_by_group_operation)
             } else {
                 let mut ac_sort_by = self
                     .by
                     .iter()
                     .map(|e| e.evaluate_on_groups(df, groups, state))
@@ -216,48 +231,60 @@
 
                 let ordered_by_group_operation = matches!(
                     ac_sort_by[0].update_groups,
                     UpdateGroups::WithSeriesLen | UpdateGroups::WithGroupsLen
                 );
                 let groups = ac_sort_by[0].groups();
 
-                let groups = groups
-                    .par_iter()
-                    .map(|indicator| {
-                        let new_idx = match indicator {
-                            GroupsIndicator::Idx((_first, idx)) => {
-                                // Safety:
-                                // Group tuples are always in bounds
-                                let groups = sort_by_s
-                                    .iter()
-                                    .map(|s| unsafe {
-                                        s.take_iter_unchecked(&mut idx.iter().map(|i| *i as usize))
-                                    })
-                                    .collect::<Vec<_>>();
-
-                                let sorted_idx = groups[0]
-                                    .arg_sort_multiple(&groups[1..], &descending)
-                                    .unwrap();
-                                map_sorted_indices_to_group_idx(&sorted_idx, idx)
-                            }
-                            GroupsIndicator::Slice([first, len]) => {
-                                let groups = sort_by_s
-                                    .iter()
-                                    .map(|s| s.slice(first as i64, len as usize))
-                                    .collect::<Vec<_>>();
-                                let sorted_idx = groups[0]
-                                    .arg_sort_multiple(&groups[1..], &descending)
-                                    .unwrap();
-                                map_sorted_indices_to_group_slice(&sorted_idx, first)
-                            }
-                        };
+                let groups = POOL.install(|| {
+                    groups
+                        .par_iter()
+                        .map(|indicator| {
+                            let new_idx = match indicator {
+                                GroupsIndicator::Idx((_first, idx)) => {
+                                    // Safety:
+                                    // Group tuples are always in bounds
+                                    let groups = sort_by_s
+                                        .iter()
+                                        .map(|s| unsafe {
+                                            s.take_iter_unchecked(
+                                                &mut idx.iter().map(|i| *i as usize),
+                                            )
+                                        })
+                                        .collect::<Vec<_>>();
+
+                                    let options = SortMultipleOptions {
+                                        other: groups[1..].to_vec(),
+                                        descending: descending.clone(),
+                                        multithreaded: false,
+                                    };
+
+                                    let sorted_idx = groups[0].arg_sort_multiple(&options).unwrap();
+                                    map_sorted_indices_to_group_idx(&sorted_idx, idx)
+                                }
+                                GroupsIndicator::Slice([first, len]) => {
+                                    let groups = sort_by_s
+                                        .iter()
+                                        .map(|s| s.slice(first as i64, len as usize))
+                                        .collect::<Vec<_>>();
+
+                                    let options = SortMultipleOptions {
+                                        other: groups[1..].to_vec(),
+                                        descending: descending.clone(),
+                                        multithreaded: false,
+                                    };
+                                    let sorted_idx = groups[0].arg_sort_multiple(&options).unwrap();
+                                    map_sorted_indices_to_group_slice(&sorted_idx, first)
+                                }
+                            };
 
-                        (new_idx[0], new_idx)
-                    })
-                    .collect();
+                            (new_idx[0], new_idx)
+                        })
+                        .collect()
+                });
 
                 (GroupsProxy::Idx(groups), ordered_by_group_operation)
             };
             polars_ensure!(
                 !invalid.load(Ordering::Relaxed), expr = self.expr, ComputeError:
                 "the expression in `sort_by` argument must result in the same length"
             );
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/take.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/take.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/ternary.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/ternary.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/expressions/window.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/expressions/window.rs`

 * *Files 6% similar despite different names*

```diff
@@ -36,16 +36,14 @@
 #[cfg_attr(debug_assertions, derive(Debug))]
 enum MapStrategy {
     // Join by key, this the most expensive
     // for reduced aggregations
     Join,
     // explode now
     Explode,
-    // will be exploded by subsequent `.flatten()` call
-    ExplodeLater,
     // Use an arg_sort to map the values back
     Map,
     Nothing,
 }
 
 impl WindowExpr {
     fn map_list_agg_by_arg_sort(
@@ -317,37 +315,32 @@
         has_arity && agg_col
     }
 
     fn determine_map_strategy(
         &self,
         agg_state: &AggState,
         sorted_keys: bool,
-        explicit_list: bool,
         gb: &GroupBy,
     ) -> PolarsResult<MapStrategy> {
-        match (self.options.explode, explicit_list, agg_state) {
+        match (self.options.mapping, agg_state) {
             // Explode
             // `(col("x").sum() * col("y")).list().over("groups").flatten()`
-            (true, true, _) => Ok(MapStrategy::ExplodeLater),
-            // Explode all the aggregated lists. Maybe add later?
-            (true, false, _) => {
-                polars_bail!(
-                    expr = self.expr, ComputeError:
-                    "this operation is likely not what you want (you may need `.list()`)"
-                );
-            }
-            // explicit list
-            // `(col("x").sum() * col("y")).list().over("groups")`
-            (false, true, _) => Ok(MapStrategy::Join),
+            (WindowMapping::Explode, _) => Ok(MapStrategy::Explode),
+            // // explicit list
+            // // `(col("x").sum() * col("y")).list().over("groups")`
+            // (false, false, _) => Ok(MapStrategy::Join),
             // aggregations
             //`sum("foo").over("groups")`
-            (false, false, AggState::AggregatedFlat(_)) => Ok(MapStrategy::Join),
+            (_, AggState::AggregatedFlat(_)) => Ok(MapStrategy::Join),
             // no explicit aggregations, map over the groups
             //`(col("x").sum() * col("y")).over("groups")`
-            (false, false, AggState::AggregatedList(_)) => {
+            (WindowMapping::Join, AggState::AggregatedList(_)) => Ok(MapStrategy::Join),
+            // no explicit aggregations, map over the groups
+            //`(col("x").sum() * col("y")).over("groups")`
+            (WindowMapping::GroupsToRows, AggState::AggregatedList(_)) => {
                 if sorted_keys {
                     if let GroupsProxy::Idx(g) = gb.get_groups() {
                         debug_assert!(g.is_sorted_flag())
                     }
                     // GroupsProxy::Slice is always sorted
 
                     // Note that group columns must be sorted for this to make sense!!!
@@ -356,25 +349,26 @@
                     Ok(MapStrategy::Map)
                 }
             }
             // no aggregations, just return column
             // or an aggregation that has been flattened
             // we have to check which one
             //`col("foo").over("groups")`
-            (false, false, AggState::NotAggregated(_)) => {
+            (WindowMapping::GroupsToRows, AggState::NotAggregated(_)) => {
                 // col()
                 // or col().alias()
                 if self.is_simple_column_expr() {
                     Ok(MapStrategy::Nothing)
                 } else {
                     Ok(MapStrategy::Map)
                 }
             }
+            (WindowMapping::Join, AggState::NotAggregated(_)) => Ok(MapStrategy::Join),
             // literals, do nothing and let broadcast
-            (false, false, AggState::Literal(_)) => Ok(MapStrategy::Nothing),
+            (_, AggState::Literal(_)) => Ok(MapStrategy::Nothing),
         }
     }
 }
 
 impl PhysicalExpr for WindowExpr {
     // Note: this was first implemented with expression evaluation but this performed really bad.
     // Therefore we choose the groupby -> apply -> self join approach
@@ -424,26 +418,16 @@
             matches!(
                 s.is_sorted_flag(),
                 IsSorted::Ascending | IsSorted::Descending
             )
         });
         let explicit_list_agg = self.is_explicit_list_agg();
 
-        // A `sort()` in a window function is one level flatter
-        // Assume we have column a : i32
-        // than a sort in a groupby. A groupby sorts the groups and returns array: list[i32]
-        // whereas a window function returns array: i32
-        // So a `sort().list()` in a groupby returns: list[list[i32]]
-        // whereas in a window function would return: list[i32]
-        if explicit_list_agg {
-            state.set_finalize_window_as_list();
-        }
-
         // if we flatten this column we need to make sure the groups are sorted.
-        let mut sort_groups = self.options.explode ||
+        let mut sort_groups = matches!(self.options.mapping, WindowMapping::Explode) ||
             // if not
             //      `col().over()`
             // and not
             //      `col().list().over`
             // and not
             //      `col().sum()`
             // and keys are sorted
@@ -459,32 +443,29 @@
         let create_groups = || {
             let gb = df.groupby_with_series(groupby_columns.clone(), true, sort_groups)?;
             let out: PolarsResult<GroupsProxy> = Ok(gb.take_groups());
             out
         };
 
         // Try to get cached grouptuples
-        let (groups, _, cache_key) = if state.cache_window() {
+        let (mut groups, _, cache_key) = if state.cache_window() {
             let mut cache_key = String::with_capacity(32 * groupby_columns.len());
             write!(&mut cache_key, "{}", state.branch_idx).unwrap();
             for s in &groupby_columns {
                 cache_key.push_str(s.name());
             }
 
             let mut gt_map = state.group_tuples.lock().unwrap();
             // we run sequential and partitioned
             // and every partition run the cache should be empty so we expect a max of 1.
             debug_assert!(gt_map.len() <= 1);
             if let Some(gt) = gt_map.get_mut(&cache_key) {
                 if df.height() > 0 {
                     assert!(!gt.is_empty());
                 };
-                if sort_groups {
-                    gt.sort()
-                }
 
                 // We take now, but it is important that we set this before we return!
                 // a next windows function may get this cached key and get an empty if this
                 // does not happen
                 (std::mem::take(gt), true, cache_key)
             } else {
                 (create_groups()?, false, cache_key)
@@ -495,20 +476,28 @@
 
         // 2. create GroupBy object and apply aggregation
         let apply_columns = self
             .apply_columns
             .iter()
             .map(|s| s.as_ref().to_string())
             .collect();
+
+        // some window expressions need sorted groups
+        // to make sure that the caches align we sort
+        // the groups, so that the cached groups and join keys
+        // are consistent among all windows
+        if sort_groups || state.cache_window() {
+            groups.sort()
+        }
         let gb = GroupBy::new(df, groupby_columns.clone(), groups, Some(apply_columns));
 
         let mut ac = self.run_aggregation(df, state, &gb)?;
 
         use MapStrategy::*;
-        match self.determine_map_strategy(ac.agg_state(), sorted_keys, explicit_list_agg, &gb)? {
+        match self.determine_map_strategy(ac.agg_state(), sorted_keys, &gb)? {
             Nothing => {
                 let mut out = ac.flat_naive().into_owned();
                 cache_gb(gb, state, &cache_key);
                 if let Some(name) = &self.out_name {
                     out.rename(name.as_ref());
                 }
                 Ok(out)
@@ -517,22 +506,14 @@
                 let mut out = ac.aggregated().explode()?;
                 cache_gb(gb, state, &cache_key);
                 if let Some(name) = &self.out_name {
                     out.rename(name.as_ref());
                 }
                 Ok(out)
             }
-            ExplodeLater => {
-                let mut out = ac.aggregated();
-                cache_gb(gb, state, &cache_key);
-                if let Some(name) = &self.out_name {
-                    out.rename(name.as_ref());
-                }
-                Ok(out)
-            }
             Map => {
                 // TODO!
                 // investigate if sorted arrays can be return directly
                 let out_column = ac.aggregated();
                 let flattened = out_column.explode()?;
                 // we extend the lifetime as we must convince the compiler that ac lives
                 // long enough. We drop `GrouBy` when we are done with `ac`.
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/file_cache.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/file_cache.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/node_timer.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/node_timer.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/planner/expr.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/planner/lp.rs`

 * *Files 24% similar despite different names*

```diff
@@ -1,615 +1,574 @@
-use polars_core::frame::groupby::GroupByMethod;
 use polars_core::prelude::*;
-use polars_core::series::IsSorted;
-use polars_core::utils::parallel_op_series;
+use polars_core::POOL;
 
-use super::super::expressions as phys_expr;
-use crate::prelude::*;
-
-pub(crate) fn create_physical_expressions(
-    exprs: &[Node],
-    context: Context,
+use super::super::executors::{self, Executor};
+use super::*;
+use crate::utils::*;
+
+fn partitionable_gb(
+    keys: &[Node],
+    aggs: &[Node],
+    _input_schema: &Schema,
     expr_arena: &Arena<AExpr>,
-    schema: Option<&SchemaRef>,
-) -> PolarsResult<Vec<Arc<dyn PhysicalExpr>>> {
-    exprs
-        .iter()
-        .map(|e| create_physical_expr(*e, context, expr_arena, schema))
-        .collect()
-}
-
-pub(crate) fn create_physical_expr(
-    expression: Node,
-    ctxt: Context,
-    expr_arena: &Arena<AExpr>,
-    schema: Option<&SchemaRef>,
-) -> PolarsResult<Arc<dyn PhysicalExpr>> {
-    use AExpr::*;
-
-    match expr_arena.get(expression).clone() {
-        Count => Ok(Arc::new(phys_expr::CountExpr::new())),
-        Window {
-            mut function,
-            partition_by,
-            order_by: _,
-            options,
-        } => {
-            // TODO! Order by
-            let group_by =
-                create_physical_expressions(&partition_by, Context::Default, expr_arena, schema)?;
-            let phys_function =
-                create_physical_expr(function, Context::Aggregation, expr_arena, schema)?;
-            let mut out_name = None;
-            let mut apply_columns = aexpr_to_leaf_names(function, expr_arena);
-            // sort and then dedup removes consecutive duplicates == all duplicates
-            apply_columns.sort();
-            apply_columns.dedup();
-
-            if apply_columns.is_empty() {
-                if has_aexpr(function, expr_arena, |e| matches!(e, AExpr::Literal(_))) {
-                    apply_columns.push(Arc::from("literal"))
-                } else if has_aexpr(function, expr_arena, |e| matches!(e, AExpr::Count)) {
-                    apply_columns.push(Arc::from("count"))
-                } else {
-                    let e = node_to_expr(function, expr_arena);
-                    polars_bail!(
-                        ComputeError:
-                        "cannot apply a window function, did not find a root column; \
-                        this is likely due to a syntax error in this expression: {:?}", e
-                    );
-                }
+    apply: &Option<Arc<dyn DataFrameUdf>>,
+) -> bool {
+    // We first check if we can partition the groupby on the latest moment.
+    let mut partitionable = true;
+
+    // checks:
+    //      1. complex expressions in the groupby itself are also not partitionable
+    //          in this case anything more than col("foo")
+    //      2. a custom function cannot be partitioned
+    //      3. we don't bother with more than 2 keys, as the cardinality likely explodes
+    //         by the combinations
+    if !keys.is_empty() && keys.len() < 3 && apply.is_none() {
+        // complex expressions in the groupby itself are also not partitionable
+        // in this case anything more than col("foo")
+        for key in keys {
+            if (expr_arena).iter(*key).count() > 1 {
+                partitionable = false;
+                break;
             }
-
-            if let Alias(expr, name) = expr_arena.get(function) {
-                function = *expr;
-                out_name = Some(name.clone());
-            };
-            let function = node_to_expr(function, expr_arena);
-
-            Ok(Arc::new(WindowExpr {
-                group_by,
-                apply_columns,
-                out_name,
-                function,
-                phys_function,
-                options,
-                expr: node_to_expr(expression, expr_arena),
-            }))
-        }
-        Literal(value) => Ok(Arc::new(LiteralExpr::new(
-            value,
-            node_to_expr(expression, expr_arena),
-        ))),
-        BinaryExpr { left, op, right } => {
-            let lhs = create_physical_expr(left, ctxt, expr_arena, schema)?;
-            let rhs = create_physical_expr(right, ctxt, expr_arena, schema)?;
-            Ok(Arc::new(phys_expr::BinaryExpr::new(
-                lhs,
-                op,
-                rhs,
-                node_to_expr(expression, expr_arena),
-            )))
-        }
-        Column(column) => Ok(Arc::new(ColumnExpr::new(
-            column,
-            node_to_expr(expression, expr_arena),
-            schema.cloned(),
-        ))),
-        Sort { expr, options } => {
-            let phys_expr = create_physical_expr(expr, ctxt, expr_arena, schema)?;
-            Ok(Arc::new(SortExpr::new(
-                phys_expr,
-                options,
-                node_to_expr(expression, expr_arena),
-            )))
         }
-        Take { expr, idx } => {
-            let phys_expr = create_physical_expr(expr, ctxt, expr_arena, schema)?;
-            let phys_idx = create_physical_expr(idx, ctxt, expr_arena, schema)?;
-            Ok(Arc::new(TakeExpr {
-                phys_expr,
-                idx: phys_idx,
-                expr: node_to_expr(expression, expr_arena),
-            }))
-        }
-        SortBy {
-            expr,
-            by,
-            descending,
-        } => {
-            let phys_expr = create_physical_expr(expr, ctxt, expr_arena, schema)?;
-            let phys_by = create_physical_expressions(&by, ctxt, expr_arena, schema)?;
-            Ok(Arc::new(SortByExpr::new(
-                phys_expr,
-                phys_by,
-                descending,
-                node_to_expr(expression, expr_arena),
-            )))
-        }
-        Filter { input, by } => {
-            let phys_input = create_physical_expr(input, ctxt, expr_arena, schema)?;
-            let phys_by = create_physical_expr(by, ctxt, expr_arena, schema)?;
-            Ok(Arc::new(FilterExpr::new(
-                phys_input,
-                phys_by,
-                node_to_expr(expression, expr_arena),
-            )))
-        }
-        Alias(expr, name) => {
-            let phys_expr = create_physical_expr(expr, ctxt, expr_arena, schema)?;
-            Ok(Arc::new(AliasExpr::new(
-                phys_expr,
-                name,
-                node_to_expr(expression, expr_arena),
-            )))
-        }
-        Agg(agg) => {
-            match agg {
-                AAggExpr::Min {
-                    input: expr,
-                    propagate_nans,
-                } => {
-                    let input = create_physical_expr(expr, ctxt, expr_arena, schema)?;
-                    match ctxt {
-                        Context::Aggregation => {
-                            if propagate_nans {
-                                Ok(Arc::new(AggregationExpr::new(input, GroupByMethod::NanMin)))
-                            } else {
-                                Ok(Arc::new(AggregationExpr::new(input, GroupByMethod::Min)))
-                            }
-                        }
-                        Context::Default => {
-                            let function = SpecialEq::new(Arc::new(move |s: &mut [Series]| {
-                                let s = std::mem::take(&mut s[0]);
-
-                                if propagate_nans && s.dtype().is_float() {
-                                    #[cfg(feature = "propagate_nans")]
-                                    {
-                                        return parallel_op_series(
-                                            |s| {
-                                                Ok(polars_ops::prelude::nan_propagating_aggregate::nan_min_s(&s, s.name()))
-                                            },
-                                            s,
-                                            None,
-                                        );
-                                    }
-                                    #[cfg(not(feature = "propagate_nans"))]
-                                    {
-                                        panic!("activate 'propagate_nans' feature")
-                                    }
-                                }
-
-                                match s.is_sorted_flag() {
-                                    IsSorted::Ascending | IsSorted::Descending => {
-                                        Ok(Some(s.min_as_series()))
-                                    }
-                                    IsSorted::Not => {
-                                        parallel_op_series(|s| Ok(s.min_as_series()), s, None)
-                                    }
-                                }
-                            })
-                                as Arc<dyn SeriesUdf>);
-                            Ok(Arc::new(ApplyExpr::new_minimal(
-                                vec![input],
-                                function,
-                                node_to_expr(expression, expr_arena),
-                                ApplyOptions::ApplyFlat,
-                            )))
-                        }
-                    }
-                }
-                AAggExpr::Max {
-                    input: expr,
-                    propagate_nans,
-                } => {
-                    let input = create_physical_expr(expr, ctxt, expr_arena, schema)?;
-                    match ctxt {
-                        Context::Aggregation => {
-                            if propagate_nans {
-                                Ok(Arc::new(AggregationExpr::new(input, GroupByMethod::NanMax)))
-                            } else {
-                                Ok(Arc::new(AggregationExpr::new(input, GroupByMethod::Max)))
+
+        if partitionable {
+            for agg in aggs {
+                let aexpr = expr_arena.get(*agg);
+                let depth = (expr_arena).iter(*agg).count();
+
+                // These single expressions are partitionable
+                if matches!(aexpr, AExpr::Count) {
+                    continue;
+                }
+                // col()
+                // lit() etc.
+                if depth == 1 {
+                    partitionable = false;
+                    break;
+                }
+
+                // it should end with an aggregation
+                if let AExpr::Alias(input, _) = aexpr {
+                    // col().agg().alias() is allowed: count of 3
+                    // col().alias() is not allowed: count of 2
+                    // count().alias() is allowed: count of 2
+                    if depth <= 2 {
+                        match expr_arena.get(*input) {
+                            AExpr::Count => {}
+                            _ => {
+                                partitionable = false;
+                                break;
                             }
                         }
-                        Context::Default => {
-                            let function = SpecialEq::new(Arc::new(move |s: &mut [Series]| {
-                                let s = std::mem::take(&mut s[0]);
-
-                                if propagate_nans && s.dtype().is_float() {
-                                    #[cfg(feature = "propagate_nans")]
-                                    {
-                                        return parallel_op_series(
-                                            |s| {
-                                                Ok(polars_ops::prelude::nan_propagating_aggregate::nan_max_s(&s, s.name()))
-                                            },
-                                            s,
-                                            None,
-                                        );
-                                    }
-                                    #[cfg(not(feature = "propagate_nans"))]
-                                    {
-                                        panic!("activate 'propagate_nans' feature")
-                                    }
-                                }
-
-                                match s.is_sorted_flag() {
-                                    IsSorted::Ascending | IsSorted::Descending => {
-                                        Ok(Some(s.max_as_series()))
-                                    }
-                                    IsSorted::Not => {
-                                        parallel_op_series(|s| Ok(s.max_as_series()), s, None)
-                                    }
-                                }
-                            })
-                                as Arc<dyn SeriesUdf>);
-                            Ok(Arc::new(ApplyExpr::new_minimal(
-                                vec![input],
-                                function,
-                                node_to_expr(expression, expr_arena),
-                                ApplyOptions::ApplyFlat,
-                            )))
-                        }
                     }
                 }
-                AAggExpr::Sum(expr) => {
-                    let input = create_physical_expr(expr, ctxt, expr_arena, schema)?;
-                    match ctxt {
-                        Context::Aggregation => {
-                            Ok(Arc::new(AggregationExpr::new(input, GroupByMethod::Sum)))
-                        }
-                        Context::Default => {
-                            let function = SpecialEq::new(Arc::new(move |s: &mut [Series]| {
-                                let s = std::mem::take(&mut s[0]);
-                                parallel_op_series(|s| Ok(s.sum_as_series()), s, None)
-                            })
-                                as Arc<dyn SeriesUdf>);
-                            Ok(Arc::new(ApplyExpr::new_minimal(
-                                vec![input],
-                                function,
-                                node_to_expr(expression, expr_arena),
-                                ApplyOptions::ApplyFlat,
-                            )))
-                        }
-                    }
-                }
-                AAggExpr::Std(expr, ddof) => {
-                    let input = create_physical_expr(expr, ctxt, expr_arena, schema)?;
-                    match ctxt {
-                        Context::Aggregation => Ok(Arc::new(AggregationExpr::new(
-                            input,
-                            GroupByMethod::Std(ddof),
-                        ))),
-                        Context::Default => {
-                            let function = SpecialEq::new(Arc::new(move |s: &mut [Series]| {
-                                let s = std::mem::take(&mut s[0]);
-                                Ok(Some(s.std_as_series(ddof)))
-                            })
-                                as Arc<dyn SeriesUdf>);
-                            Ok(Arc::new(ApplyExpr::new_minimal(
-                                vec![input],
-                                function,
-                                node_to_expr(expression, expr_arena),
-                                ApplyOptions::ApplyFlat,
-                            )))
-                        }
-                    }
-                }
-                AAggExpr::Var(expr, ddof) => {
-                    let input = create_physical_expr(expr, ctxt, expr_arena, schema)?;
-                    match ctxt {
-                        Context::Aggregation => Ok(Arc::new(AggregationExpr::new(
-                            input,
-                            GroupByMethod::Var(ddof),
-                        ))),
-                        Context::Default => {
-                            let function = SpecialEq::new(Arc::new(move |s: &mut [Series]| {
-                                let s = std::mem::take(&mut s[0]);
-                                Ok(Some(s.var_as_series(ddof)))
-                            })
-                                as Arc<dyn SeriesUdf>);
-                            Ok(Arc::new(ApplyExpr::new_minimal(
-                                vec![input],
-                                function,
-                                node_to_expr(expression, expr_arena),
-                                ApplyOptions::ApplyFlat,
-                            )))
-                        }
-                    }
-                }
-                AAggExpr::Mean(expr) => {
-                    let input = create_physical_expr(expr, ctxt, expr_arena, schema)?;
-                    match ctxt {
-                        Context::Aggregation => {
-                            Ok(Arc::new(AggregationExpr::new(input, GroupByMethod::Mean)))
-                        }
-                        Context::Default => {
-                            let function = SpecialEq::new(Arc::new(move |s: &mut [Series]| {
-                                let s = std::mem::take(&mut s[0]);
-                                Ok(Some(s.mean_as_series()))
-                            })
-                                as Arc<dyn SeriesUdf>);
-                            Ok(Arc::new(ApplyExpr::new_minimal(
-                                vec![input],
-                                function,
-                                node_to_expr(expression, expr_arena),
-                                ApplyOptions::ApplyFlat,
-                            )))
-                        }
-                    }
-                }
-                AAggExpr::Median(expr) => {
-                    let input = create_physical_expr(expr, ctxt, expr_arena, schema)?;
-                    match ctxt {
-                        Context::Aggregation => {
-                            Ok(Arc::new(AggregationExpr::new(input, GroupByMethod::Median)))
-                        }
-                        Context::Default => {
-                            let function = SpecialEq::new(Arc::new(move |s: &mut [Series]| {
-                                let s = std::mem::take(&mut s[0]);
-                                Ok(Some(s.median_as_series()))
-                            })
-                                as Arc<dyn SeriesUdf>);
-                            Ok(Arc::new(ApplyExpr::new_minimal(
-                                vec![input],
-                                function,
-                                node_to_expr(expression, expr_arena),
-                                ApplyOptions::ApplyFlat,
-                            )))
-                        }
-                    }
-                }
-                AAggExpr::First(expr) => {
-                    let input = create_physical_expr(expr, ctxt, expr_arena, schema)?;
-                    match ctxt {
-                        Context::Aggregation => {
-                            Ok(Arc::new(AggregationExpr::new(input, GroupByMethod::First)))
-                        }
-                        Context::Default => {
-                            let function = SpecialEq::new(Arc::new(move |s: &mut [Series]| {
-                                let s = std::mem::take(&mut s[0]);
-                                Ok(Some(s.head(Some(1))))
-                            })
-                                as Arc<dyn SeriesUdf>);
-                            Ok(Arc::new(ApplyExpr::new_minimal(
-                                vec![input],
-                                function,
-                                node_to_expr(expression, expr_arena),
-                                ApplyOptions::ApplyFlat,
-                            )))
-                        }
-                    }
-                }
-                AAggExpr::Last(expr) => {
-                    let input = create_physical_expr(expr, ctxt, expr_arena, schema)?;
-                    match ctxt {
-                        Context::Aggregation => {
-                            Ok(Arc::new(AggregationExpr::new(input, GroupByMethod::Last)))
-                        }
-                        Context::Default => {
-                            let function = SpecialEq::new(Arc::new(move |s: &mut [Series]| {
-                                let s = std::mem::take(&mut s[0]);
-                                Ok(Some(s.tail(Some(1))))
-                            })
-                                as Arc<dyn SeriesUdf>);
-                            Ok(Arc::new(ApplyExpr::new_minimal(
-                                vec![input],
-                                function,
-                                node_to_expr(expression, expr_arena),
-                                ApplyOptions::ApplyFlat,
-                            )))
-                        }
-                    }
-                }
-                AAggExpr::Implode(expr) => {
-                    let input = create_physical_expr(expr, ctxt, expr_arena, schema)?;
-                    match ctxt {
-                        Context::Aggregation => Ok(Arc::new(AggregationExpr::new(
-                            input,
-                            GroupByMethod::Implode,
-                        ))),
-                        Context::Default => {
-                            let function = SpecialEq::new(Arc::new(move |s: &mut [Series]| {
-                                let s = &s[0];
-                                s.implode().map(|ca| Some(ca.into_series()))
-                            })
-                                as Arc<dyn SeriesUdf>);
-                            Ok(Arc::new(ApplyExpr::new_minimal(
-                                vec![input],
-                                function,
-                                node_to_expr(expression, expr_arena),
-                                ApplyOptions::ApplyFlat,
-                            )))
-                        }
-                    }
-                }
-                AAggExpr::NUnique(expr) => {
-                    let input = create_physical_expr(expr, ctxt, expr_arena, schema)?;
-                    match ctxt {
-                        Context::Aggregation => Ok(Arc::new(AggregationExpr::new(
-                            input,
-                            GroupByMethod::NUnique,
-                        ))),
-                        Context::Default => {
-                            let function = SpecialEq::new(Arc::new(move |s: &mut [Series]| {
-                                let s = std::mem::take(&mut s[0]);
-                                s.n_unique().map(|count| {
-                                    Some(
-                                        UInt32Chunked::from_slice(s.name(), &[count as u32])
-                                            .into_series(),
-                                    )
-                                })
-                            })
-                                as Arc<dyn SeriesUdf>);
-                            Ok(Arc::new(ApplyExpr::new_minimal(
-                                vec![input],
-                                function,
-                                node_to_expr(expression, expr_arena),
-                                ApplyOptions::ApplyFlat,
-                            )))
+
+                let has_aggregation =
+                    |node: Node| has_aexpr(node, expr_arena, |ae| matches!(ae, AExpr::Agg(_)));
+
+                // check if the aggregation type is partitionable
+                // only simple aggregation like col().sum
+                // that can be divided in to the aggregation of their partitions are allowed
+                if !((expr_arena).iter(*agg).all(|(_, ae)| {
+                    use AExpr::*;
+                    match ae {
+                        // struct is needed to keep both states
+                        #[cfg(feature = "dtype-struct")]
+                        Agg(AAggExpr::Mean(_)) => {
+                            // only numeric means for now.
+                            // logical types seem to break because of casts to float.
+                            matches!(expr_arena.get(*agg).get_type(_input_schema, Context::Default, expr_arena).map(|dt| {
+                                        dt.is_numeric()}), Ok(true))
+                        },
+                        // only allowed expressions
+                        Agg(agg_e) => {
+                            matches!(
+                                            agg_e,
+                                            AAggExpr::Min{..}
+                                                | AAggExpr::Max{..}
+                                                | AAggExpr::Sum(_)
+                                                | AAggExpr::Last(_)
+                                                | AAggExpr::First(_)
+                                                | AAggExpr::Count(_)
+                                        )
+                        },
+                        Function {input, options, ..} => {
+                            matches!(options.collect_groups, ApplyOptions::ApplyFlat) && input.len() == 1 &&
+                                !has_aggregation(input[0])
+                        }
+                        BinaryExpr {left, right, ..} => {
+                            !has_aggregation(*left) && !has_aggregation(*right)
+                        }
+                        Ternary {truthy, falsy, predicate,..} => {
+                            !has_aggregation(*truthy) && !has_aggregation(*falsy) && !has_aggregation(*predicate)
+                        }
+                        Column(_) | Alias(_, _) | Count | Literal(_) | Cast {..} => {
+                            true
+                        }
+                        _ => {
+                            false
+                        },
+                    }
+                }) &&
+                    // we only allow expressions that end with an aggregation
+                    matches!(aexpr, AExpr::Alias(_, _) | AExpr::Agg(_)))
+                {
+                    partitionable = false;
+                    break;
+                }
+
+                #[cfg(feature = "object")]
+                {
+                    for name in aexpr_to_leaf_names(*agg, expr_arena) {
+                        let dtype = _input_schema.get(&name).unwrap();
+
+                        if let DataType::Object(_) = dtype {
+                            partitionable = false;
+                            break;
                         }
                     }
-                }
-                AAggExpr::Quantile {
-                    expr,
-                    quantile,
-                    interpol,
-                } => {
-                    // todo! add schema to get correct output type
-                    let input = create_physical_expr(expr, ctxt, expr_arena, schema)?;
-                    let quantile = create_physical_expr(quantile, ctxt, expr_arena, schema)?;
-                    Ok(Arc::new(AggQuantileExpr::new(input, quantile, interpol)))
-                    //
-                    // match ctxt {
-                    //     Context::Aggregation => {
-                    //
-                    //         Ok(Arc::new(AggQuantileExpr::new(input, quantile, interpol)))
-                    //     }
-                    //     Context::Default => {
-                    //         let function = SpecialEq::new(Arc::new(move |s: &mut [Series]| {
-                    //             let s = std::mem::take(&mut s[0]);
-                    //             s.quantile_as_series(quantile, interpol)
-                    //         })
-                    //             as Arc<dyn SeriesUdf>);
-                    //         Ok(Arc::new(ApplyExpr::new_minimal(
-                    //             vec![input],
-                    //             function,
-                    //             node_to_expr(expression, expr_arena),
-                    //             ApplyOptions::ApplyFlat,
-                    //         )))
-                    //     }
-                    // }
-                }
-                AAggExpr::AggGroups(expr) => {
-                    if let Context::Default = ctxt {
-                        panic!("agg groups expression only supported in aggregation context")
-                    }
-                    let phys_expr = create_physical_expr(expr, ctxt, expr_arena, schema)?;
-                    Ok(Arc::new(AggregationExpr::new(
-                        phys_expr,
-                        GroupByMethod::Groups,
-                    )))
-                }
-                AAggExpr::Count(expr) => {
-                    let input = create_physical_expr(expr, ctxt, expr_arena, schema)?;
-                    match ctxt {
-                        Context::Aggregation => {
-                            Ok(Arc::new(AggregationExpr::new(input, GroupByMethod::Count)))
-                        }
-                        Context::Default => {
-                            let function = SpecialEq::new(Arc::new(move |s: &mut [Series]| {
-                                let s = std::mem::take(&mut s[0]);
-                                let count = s.len();
-                                Ok(Some(
-                                    UInt32Chunked::from_slice(s.name(), &[count as u32])
-                                        .into_series(),
-                                ))
-                            })
-                                as Arc<dyn SeriesUdf>);
-                            Ok(Arc::new(ApplyExpr::new_minimal(
-                                vec![input],
-                                function,
-                                node_to_expr(expression, expr_arena),
-                                ApplyOptions::ApplyFlat,
-                            )))
-                        }
+                    if !partitionable {
+                        break;
                     }
                 }
             }
         }
-        Cast {
-            expr,
-            data_type,
-            strict,
+    } else {
+        partitionable = false;
+    }
+    partitionable
+}
+
+pub fn create_physical_plan(
+    root: Node,
+    lp_arena: &mut Arena<ALogicalPlan>,
+    expr_arena: &mut Arena<AExpr>,
+) -> PolarsResult<Box<dyn Executor>> {
+    use ALogicalPlan::*;
+
+    let logical_plan = lp_arena.take(root);
+    match logical_plan {
+        #[cfg(feature = "python")]
+        PythonScan { options, .. } => Ok(Box::new(executors::PythonScanExec { options })),
+        FileSink { .. } => panic!(
+            "sink_parquet not yet supported in standard engine. Use 'collect().write_parquet()'"
+        ),
+        Union { inputs, options } => {
+            let inputs = inputs
+                .into_iter()
+                .map(|node| create_physical_plan(node, lp_arena, expr_arena))
+                .collect::<PolarsResult<Vec<_>>>()?;
+            Ok(Box::new(executors::UnionExec { inputs, options }))
+        }
+        Slice { input, offset, len } => {
+            let input = create_physical_plan(input, lp_arena, expr_arena)?;
+            Ok(Box::new(executors::SliceExec { input, offset, len }))
+        }
+        Selection { input, predicate } => {
+            let input = create_physical_plan(input, lp_arena, expr_arena)?;
+            let mut state = ExpressionConversionState::default();
+            let predicate =
+                create_physical_expr(predicate, Context::Default, expr_arena, None, &mut state)?;
+            Ok(Box::new(executors::FilterExec::new(
+                predicate,
+                input,
+                state.has_windows,
+            )))
+        }
+        #[cfg(feature = "csv")]
+        CsvScan {
+            path,
+            file_info,
+            output_schema,
+            options,
+            predicate,
         } => {
-            let phys_expr = create_physical_expr(expr, ctxt, expr_arena, schema)?;
-            Ok(Arc::new(CastExpr {
-                input: phys_expr,
-                data_type,
-                expr: node_to_expr(expression, expr_arena),
-                strict,
+            let predicate = predicate
+                .map(|pred| {
+                    create_physical_expr(
+                        pred,
+                        Context::Default,
+                        expr_arena,
+                        output_schema.as_ref(),
+                        &mut Default::default(),
+                    )
+                })
+                .map_or(Ok(None), |v| v.map(Some))?;
+            Ok(Box::new(executors::CsvExec {
+                path,
+                schema: file_info.schema,
+                options,
+                predicate,
             }))
         }
-        Ternary {
+        #[cfg(feature = "ipc")]
+        IpcScan {
+            path,
+            file_info,
+            output_schema,
             predicate,
-            truthy,
-            falsy,
+            options,
         } => {
-            let predicate = create_physical_expr(predicate, ctxt, expr_arena, schema)?;
-            let truthy = create_physical_expr(truthy, ctxt, expr_arena, schema)?;
-            let falsy = create_physical_expr(falsy, ctxt, expr_arena, schema)?;
-            Ok(Arc::new(TernaryExpr::new(
+            let predicate = predicate
+                .map(|pred| {
+                    create_physical_expr(
+                        pred,
+                        Context::Default,
+                        expr_arena,
+                        output_schema.as_ref(),
+                        &mut Default::default(),
+                    )
+                })
+                .map_or(Ok(None), |v| v.map(Some))?;
+
+            Ok(Box::new(executors::IpcExec {
+                path,
+                schema: file_info.schema,
                 predicate,
-                truthy,
-                falsy,
-                node_to_expr(expression, expr_arena),
+                options,
+            }))
+        }
+        #[cfg(feature = "parquet")]
+        ParquetScan {
+            path,
+            file_info,
+            output_schema,
+            predicate,
+            options,
+            cloud_options,
+        } => {
+            let predicate = predicate
+                .map(|pred| {
+                    create_physical_expr(
+                        pred,
+                        Context::Default,
+                        expr_arena,
+                        output_schema.as_ref(),
+                        &mut Default::default(),
+                    )
+                })
+                .map_or(Ok(None), |v| v.map(Some))?;
+
+            Ok(Box::new(executors::ParquetExec::new(
+                path,
+                file_info.schema,
+                predicate,
+                options,
+                cloud_options,
             )))
         }
-        AnonymousFunction {
+        Projection {
+            expr,
             input,
-            function,
-            output_type: _,
-            options,
+            schema: _schema,
+            ..
         } => {
-            let input = create_physical_expressions(&input, ctxt, expr_arena, schema)?;
-
-            Ok(Arc::new(ApplyExpr {
-                inputs: input,
-                function,
-                expr: node_to_expr(expression, expr_arena),
-                collect_groups: options.collect_groups,
-                auto_explode: options.auto_explode,
-                allow_rename: options.allow_rename,
-                pass_name_to_apply: options.pass_name_to_apply,
-                input_schema: schema.cloned(),
+            let input_schema = lp_arena.get(input).schema(lp_arena).into_owned();
+            let input = create_physical_plan(input, lp_arena, expr_arena)?;
+            let mut state = ExpressionConversionState::new(POOL.current_num_threads() > expr.len());
+            let phys_expr = create_physical_expressions(
+                &expr,
+                Context::Default,
+                expr_arena,
+                Some(&input_schema),
+                &mut state,
+            )?;
+            Ok(Box::new(executors::ProjectionExec {
+                input,
+                expr: phys_expr,
+                has_windows: state.has_windows,
+                input_schema,
+                #[cfg(test)]
+                schema: _schema,
             }))
         }
-        Function {
+        LocalProjection {
+            expr,
             input,
+            schema: _schema,
+            ..
+        } => {
+            let input_schema = lp_arena.get(input).schema(lp_arena).into_owned();
+
+            let input = create_physical_plan(input, lp_arena, expr_arena)?;
+            let mut state = ExpressionConversionState::new(POOL.current_num_threads() > expr.len());
+            let phys_expr = create_physical_expressions(
+                &expr,
+                Context::Default,
+                expr_arena,
+                Some(&input_schema),
+                &mut state,
+            )?;
+            Ok(Box::new(executors::ProjectionExec {
+                input,
+                expr: phys_expr,
+                has_windows: state.has_windows,
+                input_schema,
+                #[cfg(test)]
+                schema: _schema,
+            }))
+        }
+        DataFrameScan {
+            df,
+            projection,
+            selection: predicate,
+            schema,
+            ..
+        } => {
+            let mut state = ExpressionConversionState::default();
+            let selection = predicate
+                .map(|pred| {
+                    create_physical_expr(
+                        pred,
+                        Context::Default,
+                        expr_arena,
+                        Some(&schema),
+                        &mut state,
+                    )
+                })
+                .transpose()?;
+            Ok(Box::new(executors::DataFrameExec {
+                df,
+                projection,
+                selection,
+                predicate_has_windows: state.has_windows,
+            }))
+        }
+        AnonymousScan {
             function,
+            predicate,
             options,
+            output_schema,
             ..
         } => {
-            let input = create_physical_expressions(&input, ctxt, expr_arena, schema)?;
-
-            Ok(Arc::new(ApplyExpr {
-                inputs: input,
-                function: function.into(),
-                expr: node_to_expr(expression, expr_arena),
-                collect_groups: options.collect_groups,
-                auto_explode: options.auto_explode,
-                allow_rename: options.allow_rename,
-                pass_name_to_apply: options.pass_name_to_apply,
-                input_schema: schema.cloned(),
+            let predicate = predicate
+                .map(|pred| {
+                    create_physical_expr(
+                        pred,
+                        Context::Default,
+                        expr_arena,
+                        output_schema.as_ref(),
+                        &mut Default::default(),
+                    )
+                })
+                .map_or(Ok(None), |v| v.map(Some))?;
+            Ok(Box::new(executors::AnonymousScanExec {
+                function,
+                predicate,
+                options,
             }))
         }
-        Slice {
+        Sort {
             input,
-            offset,
-            length,
+            by_column,
+            args,
         } => {
-            let input = create_physical_expr(input, ctxt, expr_arena, schema)?;
-            let offset = create_physical_expr(offset, ctxt, expr_arena, schema)?;
-            let length = create_physical_expr(length, ctxt, expr_arena, schema)?;
-            Ok(Arc::new(SliceExpr {
+            let input_schema = lp_arena.get(input).schema(lp_arena);
+            let by_column = create_physical_expressions(
+                &by_column,
+                Context::Default,
+                expr_arena,
+                Some(input_schema.as_ref()),
+                &mut Default::default(),
+            )?;
+            let input = create_physical_plan(input, lp_arena, expr_arena)?;
+            Ok(Box::new(executors::SortExec {
                 input,
-                offset,
-                length,
-                expr: node_to_expr(expression, expr_arena),
+                by_column,
+                args,
             }))
         }
-        Explode(expr) => {
-            let input = create_physical_expr(expr, ctxt, expr_arena, schema)?;
-            let function =
-                SpecialEq::new(Arc::new(move |s: &mut [Series]| s[0].explode().map(Some))
-                    as Arc<dyn SeriesUdf>);
-            Ok(Arc::new(ApplyExpr::new_minimal(
-                vec![input],
-                function,
-                node_to_expr(expression, expr_arena),
-                ApplyOptions::ApplyGroups,
+        Cache { input, id, count } => {
+            let input = create_physical_plan(input, lp_arena, expr_arena)?;
+            Ok(Box::new(executors::CacheExec { id, input, count }))
+        }
+        Distinct { input, options } => {
+            let input = create_physical_plan(input, lp_arena, expr_arena)?;
+            Ok(Box::new(executors::UniqueExec { input, options }))
+        }
+        Aggregate {
+            input,
+            keys,
+            aggs,
+            apply,
+            schema,
+            maintain_order,
+            options,
+        } => {
+            let input_schema = lp_arena.get(input).schema(lp_arena).into_owned();
+            let phys_keys = create_physical_expressions(
+                &keys,
+                Context::Default,
+                expr_arena,
+                Some(&input_schema),
+                &mut Default::default(),
+            )?;
+            let phys_aggs = create_physical_expressions(
+                &aggs,
+                Context::Aggregation,
+                expr_arena,
+                Some(&input_schema),
+                &mut Default::default(),
+            )?;
+
+            let _slice = options.slice;
+            #[cfg(feature = "dynamic_groupby")]
+            if let Some(options) = options.dynamic {
+                let input = create_physical_plan(input, lp_arena, expr_arena)?;
+                return Ok(Box::new(executors::GroupByDynamicExec {
+                    input,
+                    keys: phys_keys,
+                    aggs: phys_aggs,
+                    options,
+                    input_schema,
+                    slice: _slice,
+                    apply,
+                }));
+            }
+
+            #[cfg(feature = "dynamic_groupby")]
+            if let Some(options) = options.rolling {
+                let input = create_physical_plan(input, lp_arena, expr_arena)?;
+                return Ok(Box::new(executors::GroupByRollingExec {
+                    input,
+                    keys: phys_keys,
+                    aggs: phys_aggs,
+                    options,
+                    input_schema,
+                    slice: _slice,
+                    apply,
+                }));
+            }
+
+            // We first check if we can partition the groupby on the latest moment.
+            let partitionable = partitionable_gb(&keys, &aggs, &input_schema, expr_arena, &apply);
+            if partitionable {
+                let from_partitioned_ds = (&*lp_arena).iter(input).any(|(_, lp)| {
+                    if let Union { options, .. } = lp {
+                        options.from_partitioned_ds
+                    } else {
+                        false
+                    }
+                });
+                let input = create_physical_plan(input, lp_arena, expr_arena)?;
+                let keys = keys
+                    .iter()
+                    .map(|node| node_to_expr(*node, expr_arena))
+                    .collect::<Vec<_>>();
+                let aggs = aggs
+                    .iter()
+                    .map(|node| node_to_expr(*node, expr_arena))
+                    .collect::<Vec<_>>();
+                Ok(Box::new(executors::PartitionGroupByExec::new(
+                    input,
+                    phys_keys,
+                    phys_aggs,
+                    maintain_order,
+                    options.slice,
+                    input_schema,
+                    schema,
+                    from_partitioned_ds,
+                    keys,
+                    aggs,
+                )))
+            } else {
+                let input = create_physical_plan(input, lp_arena, expr_arena)?;
+                Ok(Box::new(executors::GroupByExec::new(
+                    input,
+                    phys_keys,
+                    phys_aggs,
+                    apply,
+                    maintain_order,
+                    input_schema,
+                    options.slice,
+                )))
+            }
+        }
+        Join {
+            input_left,
+            input_right,
+            left_on,
+            right_on,
+            options,
+            ..
+        } => {
+            let parallel = if options.force_parallel {
+                true
+            } else if options.allow_parallel {
+                // check if two DataFrames come from a separate source.
+                // If they don't we can parallelize,
+                // we may deadlock if we don't check this
+                let mut sources_left = PlHashSet::new();
+                agg_source_paths(input_left, &mut sources_left, lp_arena);
+                let mut sources_right = PlHashSet::new();
+                agg_source_paths(input_right, &mut sources_right, lp_arena);
+                sources_left.intersection(&sources_right).next().is_none()
+            } else {
+                false
+            };
+
+            let input_left = create_physical_plan(input_left, lp_arena, expr_arena)?;
+            let input_right = create_physical_plan(input_right, lp_arena, expr_arena)?;
+            let left_on = create_physical_expressions(
+                &left_on,
+                Context::Default,
+                expr_arena,
+                None,
+                &mut Default::default(),
+            )?;
+            let right_on = create_physical_expressions(
+                &right_on,
+                Context::Default,
+                expr_arena,
+                None,
+                &mut Default::default(),
+            )?;
+            Ok(Box::new(executors::JoinExec::new(
+                input_left,
+                input_right,
+                options.how,
+                left_on,
+                right_on,
+                parallel,
+                options.suffix,
+                options.slice,
             )))
         }
-        Wildcard => panic!("should be no wildcard at this point"),
-        Nth(_) => panic!("should be no nth at this point"),
+        HStack { input, exprs, .. } => {
+            let input_schema = lp_arena.get(input).schema(lp_arena).into_owned();
+            let input = create_physical_plan(input, lp_arena, expr_arena)?;
+
+            let mut state =
+                ExpressionConversionState::new(POOL.current_num_threads() > exprs.len());
+            let phys_expr = create_physical_expressions(
+                &exprs,
+                Context::Default,
+                expr_arena,
+                Some(&input_schema),
+                &mut state,
+            )?;
+            Ok(Box::new(executors::StackExec {
+                input,
+                has_windows: state.has_windows,
+                expr: phys_expr,
+                input_schema,
+            }))
+        }
+        MapFunction {
+            input, function, ..
+        } => {
+            let input = create_physical_plan(input, lp_arena, expr_arena)?;
+            Ok(Box::new(executors::UdfExec { input, function }))
+        }
+        ExtContext {
+            input, contexts, ..
+        } => {
+            let input = create_physical_plan(input, lp_arena, expr_arena)?;
+            let contexts = contexts
+                .into_iter()
+                .map(|node| create_physical_plan(node, lp_arena, expr_arena))
+                .collect::<PolarsResult<_>>()?;
+            Ok(Box::new(executors::ExternalContext { input, contexts }))
+        }
     }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/physical_plan/state.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/physical_plan/state.rs`

 * *Files 7% similar despite different names*

```diff
@@ -21,21 +21,16 @@
 bitflags! {
     #[repr(transparent)]
     pub(super) struct StateFlags: u8 {
         /// More verbose logging
         const VERBOSE = 0x01;
         /// Indicates that window expression's [`GroupTuples`] may be cached.
         const CACHE_WINDOW_EXPR = 0x02;
-        /// A `sort()` in a window function is one level flatter
-        /// Assume we have column a : i32
-        /// than a sort in a groupby. A groupby sorts the groups and returns array: list[i32]
-        /// whereas a window function returns array: i32
-        /// So a `sort().list()` in a groupby returns: list[list[i32]]
-        /// whereas in a window function would return: list[i32]
-        const FINALIZE_WINDOW_AS_LIST = 0x04;
+        /// Indicates the expression has a window function
+        const HAS_WINDOW = 0x04;
     }
 }
 
 impl Default for StateFlags {
     fn default() -> Self {
         StateFlags::CACHE_WINDOW_EXPR
     }
@@ -61,14 +56,16 @@
     }
 }
 
 /// State/ cache that is maintained during the Execution of the physical plan.
 pub struct ExecutionState {
     // cached by a `.cache` call and kept in memory for the duration of the plan.
     df_cache: Arc<Mutex<PlHashMap<usize, Arc<OnceCell<DataFrame>>>>>,
+    #[allow(clippy::type_complexity)]
+    pub(crate) expr_cache: Option<Arc<Mutex<PlHashMap<usize, Arc<OnceCell<Series>>>>>>,
     // cache file reads until all branches got there file, then we delete it
     #[cfg(any(feature = "ipc", feature = "parquet", feature = "csv"))]
     pub(crate) file_cache: FileCache,
     pub(super) schema_cache: RwLock<Option<SchemaRef>>,
     /// Used by Window Expression to prevent redundant grouping
     pub(super) group_tuples: GroupsProxyCache,
     /// Used by Window Expression to prevent redundant joins
@@ -107,14 +104,15 @@
         }
     }
 
     /// Partially clones and partially clears state
     pub(super) fn split(&self) -> Self {
         Self {
             df_cache: self.df_cache.clone(),
+            expr_cache: self.expr_cache.clone(),
             #[cfg(any(feature = "ipc", feature = "parquet", feature = "csv"))]
             file_cache: self.file_cache.clone(),
             schema_cache: Default::default(),
             group_tuples: Default::default(),
             join_tuples: Default::default(),
             branch_idx: self.branch_idx,
             flags: AtomicU8::new(self.flags.load(Ordering::Relaxed)),
@@ -123,14 +121,15 @@
         }
     }
 
     /// clones and partially clears state
     pub(super) fn clone(&self) -> Self {
         Self {
             df_cache: self.df_cache.clone(),
+            expr_cache: self.expr_cache.clone(),
             #[cfg(any(feature = "ipc", feature = "parquet", feature = "csv"))]
             file_cache: self.file_cache.clone(),
             schema_cache: self.schema_cache.read().unwrap().clone().into(),
             group_tuples: self.group_tuples.clone(),
             join_tuples: self.join_tuples.clone(),
             branch_idx: self.branch_idx,
             flags: AtomicU8::new(self.flags.load(Ordering::Relaxed)),
@@ -143,14 +142,15 @@
     pub(crate) fn with_finger_prints(_finger_prints: Option<usize>) -> Self {
         Self::new()
     }
     #[cfg(any(feature = "parquet", feature = "csv", feature = "ipc"))]
     pub(crate) fn with_finger_prints(finger_prints: Option<Vec<FileFingerPrint>>) -> Self {
         Self {
             df_cache: Arc::new(Mutex::new(PlHashMap::default())),
+            expr_cache: None,
             schema_cache: Default::default(),
             #[cfg(any(feature = "ipc", feature = "parquet", feature = "csv"))]
             file_cache: FileCache::new(finger_prints),
             group_tuples: Arc::new(Mutex::new(PlHashMap::default())),
             join_tuples: Arc::new(Mutex::new(PlHashMap::default())),
             branch_idx: 0,
             flags: AtomicU8::new(StateFlags::init().as_u8()),
@@ -162,14 +162,15 @@
     pub fn new() -> Self {
         let mut flags: StateFlags = Default::default();
         if verbose() {
             flags |= StateFlags::VERBOSE;
         }
         Self {
             df_cache: Default::default(),
+            expr_cache: None,
             schema_cache: Default::default(),
             #[cfg(any(feature = "ipc", feature = "parquet", feature = "csv"))]
             file_cache: FileCache::new(None),
             group_tuples: Default::default(),
             join_tuples: Default::default(),
             branch_idx: 0,
             flags: AtomicU8::new(StateFlags::init().as_u8()),
@@ -198,16 +199,26 @@
         let mut guard = self.df_cache.lock().unwrap();
         guard
             .entry(key)
             .or_insert_with(|| Arc::new(OnceCell::new()))
             .clone()
     }
 
+    pub(crate) fn get_expr_cache(&self, key: usize) -> Option<Arc<OnceCell<Series>>> {
+        self.expr_cache.as_ref().map(|cache| {
+            let mut guard = cache.lock().unwrap();
+            guard
+                .entry(key)
+                .or_insert_with(|| Arc::new(OnceCell::new()))
+                .clone()
+        })
+    }
+
     /// Clear the cache used by the Window expressions
-    pub(crate) fn clear_expr_cache(&self) {
+    pub(crate) fn clear_window_expr_cache(&self) {
         {
             let mut lock = self.group_tuples.lock().unwrap();
             lock.clear();
         }
         let mut lock = self.join_tuples.lock().unwrap();
         lock.clear();
     }
@@ -220,48 +231,46 @@
 
     /// Indicates that window expression's [`GroupTuples`] may be cached.
     pub(super) fn cache_window(&self) -> bool {
         let flags: StateFlags = self.flags.load(Ordering::Relaxed).into();
         flags.contains(StateFlags::CACHE_WINDOW_EXPR)
     }
 
+    /// Indicates that window expression's [`GroupTuples`] may be cached.
+    pub(super) fn has_window(&self) -> bool {
+        let flags: StateFlags = self.flags.load(Ordering::Relaxed).into();
+        flags.contains(StateFlags::HAS_WINDOW)
+    }
+
     /// More verbose logging
     pub(super) fn verbose(&self) -> bool {
         let flags: StateFlags = self.flags.load(Ordering::Relaxed).into();
         flags.contains(StateFlags::VERBOSE)
     }
 
-    pub(super) fn set_finalize_window_as_list(&self) {
-        self.set_flags(&|mut flags| {
-            flags |= StateFlags::FINALIZE_WINDOW_AS_LIST;
-            flags
-        })
-    }
-
-    pub(super) fn unset_finalize_window_as_list(&self) -> bool {
-        let mut flags: StateFlags = self.flags.load(Ordering::Relaxed).into();
-        let is_set = flags.contains(StateFlags::FINALIZE_WINDOW_AS_LIST);
-        flags.remove(StateFlags::FINALIZE_WINDOW_AS_LIST);
-        self.flags.store(flags.as_u8(), Ordering::Relaxed);
-        is_set
-    }
-
     pub(super) fn remove_cache_window_flag(&mut self) {
         self.set_flags(&|mut flags| {
             flags.remove(StateFlags::CACHE_WINDOW_EXPR);
             flags
         });
     }
 
     pub(super) fn insert_cache_window_flag(&mut self) {
         self.set_flags(&|mut flags| {
             flags.insert(StateFlags::CACHE_WINDOW_EXPR);
             flags
         });
     }
+    // this will trigger some conservative
+    pub(super) fn insert_has_window_function_flag(&mut self) {
+        self.set_flags(&|mut flags| {
+            flags.insert(StateFlags::HAS_WINDOW);
+            flags
+        });
+    }
 }
 
 impl Default for ExecutionState {
     fn default() -> Self {
         ExecutionState::new()
     }
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/prelude.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/prelude.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/tests/aggregations.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/tests/aggregations.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/tests/arity.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/tests/arity.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/tests/cse.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/tests/cse.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/tests/io.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/tests/io.rs`

 * *Files 1% similar despite different names*

```diff
@@ -191,15 +191,15 @@
 
     Ok(())
 }
 
 fn slice_at_union(lp_arena: &Arena<ALogicalPlan>, lp: Node) -> bool {
     (&lp_arena).iter(lp).all(|(_, lp)| {
         if let ALogicalPlan::Union { options, .. } = lp {
-            options.slice
+            options.slice.is_some()
         } else {
             true
         }
     })
 }
 
 #[test]
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/tests/logical.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/tests/logical.rs`

 * *Files 5% similar despite different names*

```diff
@@ -8,16 +8,15 @@
     let df = df![
         "date" => ["2021-01-01", "2021-01-02", "2021-01-03"],
         "groups" => [1, 1, 1]
     ]?;
 
     let out = df
         .lazy()
-        .with_columns(&[col("date").str().strptime(StrpTimeOptions {
-            date_dtype: DataType::Date,
+        .with_columns(&[col("date").str().to_date(StrptimeOptions {
             ..Default::default()
         })])
         .with_column(
             col("date")
                 .cast(DataType::Datetime(TimeUnit::Milliseconds, None))
                 .alias("datetime"),
         )
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/tests/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/tests/mod.rs`

 * *Files 1% similar despite different names*

```diff
@@ -39,15 +39,15 @@
 use polars_core::prelude::*;
 pub(crate) use polars_core::SINGLE_LOCK;
 use polars_io::prelude::*;
 use polars_plan::logical_plan::{
     ArenaLpIter, OptimizationRule, SimplifyExprRule, StackOptimizer, TypeCoercionRule,
 };
 
-use crate::dsl::{arg_sort_by, pearson_corr};
+use crate::dsl::pearson_corr;
 use crate::prelude::*;
 
 static GLOB_PARQUET: &str = "../../examples/datasets/*.parquet";
 static GLOB_CSV: &str = "../../examples/datasets/*.csv";
 static GLOB_IPC: &str = "../../examples/datasets/*.ipc";
 static FOODS_CSV: &str = "../../examples/datasets/foods1.csv";
 static FOODS_IPC: &str = "../../examples/datasets/foods1.ipc";
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/tests/optimization_checks.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/tests/optimization_checks.rs`

 * *Files 2% similar despite different names*

```diff
@@ -53,14 +53,40 @@
                 predicate: Some(_), ..
             } => true,
             _ => false,
         }
     })
 }
 
+pub(crate) fn is_pipeline(q: LazyFrame) -> bool {
+    let (mut expr_arena, mut lp_arena) = get_arenas();
+    let lp = q.optimize(&mut lp_arena, &mut expr_arena).unwrap();
+    matches!(
+        lp_arena.get(lp),
+        ALogicalPlan::MapFunction {
+            function: FunctionNode::Pipeline { .. },
+            ..
+        }
+    )
+}
+
+pub(crate) fn has_pipeline(q: LazyFrame) -> bool {
+    let (mut expr_arena, mut lp_arena) = get_arenas();
+    let lp = q.optimize(&mut lp_arena, &mut expr_arena).unwrap();
+    (&lp_arena).iter(lp).any(|(_, lp)| {
+        matches!(
+            lp,
+            ALogicalPlan::MapFunction {
+                function: FunctionNode::Pipeline { .. },
+                ..
+            }
+        )
+    })
+}
+
 fn slice_at_scan(q: LazyFrame) -> bool {
     let (mut expr_arena, mut lp_arena) = get_arenas();
     let lp = q.optimize(&mut lp_arena, &mut expr_arena).unwrap();
     (&lp_arena).iter(lp).any(|(_, lp)| {
         use ALogicalPlan::*;
         match lp {
             CsvScan { options, .. } => options.n_rows.is_some(),
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/tests/predicate_queries.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/tests/predicate_queries.rs`

 * *Files 2% similar despite different names*

```diff
@@ -122,16 +122,15 @@
 fn test_strptime_block_predicate() -> PolarsResult<()> {
     let df = df![
         "date" => ["2021-01-01", "2021-01-02"]
     ]?;
 
     let q = df
         .lazy()
-        .with_column(col("date").str().strptime(StrpTimeOptions {
-            date_dtype: DataType::Date,
+        .with_column(col("date").str().to_date(StrptimeOptions {
             ..Default::default()
         }))
         .filter(
             col("date").gt(NaiveDate::from_ymd_opt(2021, 1, 1)
                 .unwrap()
                 .and_hms_opt(0, 0, 0)
                 .unwrap()
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/tests/projection_queries.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/tests/projection_queries.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/tests/queries.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/tests/queries.rs`

 * *Files 7% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 use polars_core::frame::explode::MeltArgs;
 #[cfg(feature = "diff")]
 use polars_core::series::ops::NullBehavior;
 
 use super::*;
+#[cfg(feature = "arange")]
+use crate::dsl::arg_sort_by;
 
 #[test]
 fn test_lazy_with_column() {
     let df = get_df()
         .lazy()
         .with_column(lit(10).alias("foo"))
         .collect()
@@ -1011,14 +1013,15 @@
             .collect::<Vec<_>>()
     );
 
     Ok(())
 }
 
 #[test]
+#[cfg(feature = "arange")]
 fn test_arg_sort_multiple() -> PolarsResult<()> {
     let df = df![
         "int" => [1, 2, 3, 1, 2],
         "flt" => [3.0, 2.0, 1.0, 2.0, 1.0],
         "str" => ["a", "a", "a", "b", "b"]
     ]?;
 
@@ -1124,15 +1127,20 @@
     let df = df![
         "a" => ["a", "b", "a"],
         "b" => [Some(1), None, None]
     ]?;
 
     let out = df
         .lazy()
-        .select([col("b").forward_fill(None).implode().over([col("a")])])
+        .select([col("b").forward_fill(None).over_with_options(
+            [col("a")],
+            WindowOptions {
+                mapping: WindowMapping::Join,
+            },
+        )])
         .collect()?;
     let agg = out.column("b")?.list()?;
 
     let a: Series = agg.get(0).unwrap();
     assert!(a.series_equal(&Series::new("b", &[1, 1])));
     let a: Series = agg.get(2).unwrap();
     assert!(a.series_equal(&Series::new("b", &[1, 1])));
@@ -1284,16 +1292,20 @@
     let out = df
         .lazy()
         .select([
             col("fruits"),
             col("B")
                 .shift(1)
                 .filter(col("B").shift(1).gt(lit(4)))
-                .implode()
-                .over([col("fruits")])
+                .over_with_options(
+                    [col("fruits")],
+                    WindowOptions {
+                        mapping: WindowMapping::Join,
+                    },
+                )
                 .alias("filtered"),
         ])
         .collect()?;
 
     assert_eq!(
         out.column("filtered")?
             .list()?
@@ -1371,15 +1383,15 @@
     let df = fruits_cars();
     let out = df
         .lazy()
         .select([col("fruits")
             .filter(col("fruits").eq(lit("banana")))
             .count()])
         .collect()?;
-    assert_eq!(out.column("fruits")?.u32()?.get(0), Some(3));
+    assert_eq!(out.column("fruits")?.idx()?.get(0), Some(3));
     Ok(())
 }
 
 #[test]
 #[cfg(feature = "dtype-i16")]
 fn test_groupby_small_ints() -> PolarsResult<()> {
     let df = df![
@@ -1429,38 +1441,14 @@
         .collect()?;
 
     assert!(out.column("foo")?.len() > 1);
     Ok(())
 }
 
 #[test]
-fn test_sort_by_suffix() -> PolarsResult<()> {
-    let df = fruits_cars();
-    let out = df
-        .lazy()
-        .select([col("*")
-            .sort_by([col("A")], [false])
-            .implode()
-            .over([col("fruits")])
-            .flatten()
-            .suffix("_sorted")])
-        .collect()?;
-
-    let expected = df!(
-            "A_sorted"=> [1, 2, 5, 3, 4],
-            "fruits_sorted"=> ["banana", "banana", "banana", "apple", "apple"],
-            "B_sorted"=> [5, 4, 1, 3, 2],
-            "cars_sorted"=> ["beetle", "audi", "beetle", "beetle", "beetle"]
-    )?;
-
-    assert!(expected.frame_equal(&out));
-    Ok(())
-}
-
-#[test]
 fn test_list_in_select_context() -> PolarsResult<()> {
     let s = Series::new("a", &[1, 2, 3]);
     let mut builder = get_list_builder(s.dtype(), s.len(), 1, s.name()).unwrap();
     builder.append_series(&s);
     let expected = builder.finish().into_series();
 
     let df = DataFrame::new(vec![s])?;
@@ -1636,17 +1624,15 @@
         .lazy()
         .select([col("a")
             .arg_sort(SortOptions {
                 descending: false,
                 nulls_last: false,
                 multithreaded: true,
             })
-            .implode()
-            .over([col("a")])
-            .flatten()])
+            .over([col("a")])])
         .collect()?;
 
     let a = out.column("a")?.idx()?;
     assert_eq!(Vec::from(a), &[Some(0), Some(0)]);
 
     Ok(())
 }
@@ -1665,16 +1651,20 @@
             .rank(
                 RankOptions {
                     method: RankMethod::Average,
                     ..Default::default()
                 },
                 None,
             )
-            .implode()
-            .over([col("group")])])
+            .over_with_options(
+                [col("group")],
+                WindowOptions {
+                    mapping: WindowMapping::Join,
+                },
+            )])
         .collect()?;
 
     let out = out.column("value")?.explode()?;
     let out = out.f32()?;
     assert_eq!(
         Vec::from(out),
         &[Some(1.0), Some(2.0), Some(1.0), Some(2.0), Some(1.0)]
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/tests/streaming.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/tests/streaming.rs`

 * *Files 11% similar despite different names*

```diff
@@ -11,62 +11,85 @@
 }
 
 fn get_csv_glob() -> LazyFrame {
     let file = "../../examples/datasets/foods*.csv";
     LazyCsvReader::new(file).finish().unwrap()
 }
 
-fn assert_streaming_with_default(q: LazyFrame) {
+fn assert_streaming_with_default(q: LazyFrame, total: bool, check_shape_only: bool) {
     let q_streaming = q.clone().with_streaming(true);
+    if total {
+        assert!(optimization_checks::is_pipeline(q_streaming.clone()));
+    } else {
+        assert!(optimization_checks::has_pipeline(q_streaming.clone()));
+    }
     let q_expected = q.with_streaming(false);
-
     let out = q_streaming.collect().unwrap();
     let expected = q_expected.collect().unwrap();
-    assert_eq!(out, expected);
+    if check_shape_only {
+        assert_eq!(out.shape(), expected.shape())
+    } else {
+        assert_eq!(out, expected);
+    }
 }
 
 #[test]
 fn test_streaming_parquet() -> PolarsResult<()> {
     let q = get_parquet_file();
 
     let q = q
         .groupby([col("sugars_g")])
         .agg([((lit(1) - col("fats_g")) + col("calories")).sum()])
         .sort("sugars_g", Default::default());
 
-    assert_streaming_with_default(q);
+    assert_streaming_with_default(q, true, false);
     Ok(())
 }
 
 #[test]
 fn test_streaming_csv() -> PolarsResult<()> {
     let q = get_csv_file();
 
     let q = q
         .select([col("sugars_g"), col("calories")])
         .groupby([col("sugars_g")])
         .agg([col("calories").sum()])
         .sort("sugars_g", Default::default());
 
-    assert_streaming_with_default(q);
+    assert_streaming_with_default(q, true, false);
     Ok(())
 }
 
 #[test]
 fn test_streaming_glob() -> PolarsResult<()> {
     let q = get_csv_glob();
+    let q = q.sort("sugars_g", Default::default());
 
-    let q = q
-        .select([col("sugars_g"), col("calories")])
-        .filter(col("sugars_g").gt(lit(10)))
-        .groupby([col("sugars_g")])
-        .agg([col("calories").sum() * lit(10)])
-        .sort("sugars_g", Default::default());
+    assert_streaming_with_default(q, true, false);
+    Ok(())
+}
 
-    assert_streaming_with_default(q);
+#[test]
+fn test_streaming_union_order() -> PolarsResult<()> {
+    let q = get_csv_glob();
+    let q = concat([q.clone(), q], false, false)?;
+    let q = q.select([col("sugars_g"), col("calories")]);
+
+    assert_streaming_with_default(q, true, false);
+    Ok(())
+}
+
+#[test]
+#[cfg(feature = "cross_join")]
+fn test_streaming_union_join() -> PolarsResult<()> {
+    let q = get_csv_glob();
+    let q = q.select([col("sugars_g"), col("calories")]);
+    let q = q.clone().cross_join(q);
+
+    assert_streaming_with_default(q, true, true);
     Ok(())
 }
 
 #[test]
 fn test_streaming_multiple_keys_aggregate() -> PolarsResult<()> {
     let q = get_csv_glob();
 
@@ -75,15 +98,15 @@
         .groupby([col("sugars_g"), col("calories")])
         .agg([
             (col("fats_g") * lit(10)).sum(),
             col("calories").mean().alias("cal_mean"),
         ])
         .sort_by_exprs([col("sugars_g"), col("calories")], [false, false], false);
 
-    assert_streaming_with_default(q);
+    assert_streaming_with_default(q, true, false);
     Ok(())
 }
 
 #[test]
 fn test_streaming_first_sum() -> PolarsResult<()> {
     let q = get_csv_file();
 
@@ -92,15 +115,28 @@
         .groupby([col("sugars_g")])
         .agg([
             col("calories").sum(),
             col("calories").first().alias("calories_first"),
         ])
         .sort("sugars_g", Default::default());
 
-    assert_streaming_with_default(q);
+    assert_streaming_with_default(q, true, false);
+    Ok(())
+}
+
+#[test]
+fn test_streaming_unique() -> PolarsResult<()> {
+    let q = get_csv_file();
+
+    let q = q
+        .select([col("sugars_g"), col("calories")])
+        .unique(None, Default::default())
+        .sort_by_exprs([cols(["sugars_g", "calories"])], [false], false);
+
+    assert_streaming_with_default(q, true, false);
     Ok(())
 }
 
 #[test]
 fn test_streaming_aggregate_slice() -> PolarsResult<()> {
     let q = get_parquet_file();
 
@@ -166,15 +202,15 @@
         "col2" => ["a", "a", "a", "a", "a", "c"],
         "floats" => [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]
     ]?
     .lazy();
 
     let q = lf_left.inner_join(lf_right, col("col1"), col("col1"));
 
-    assert_streaming_with_default(q);
+    assert_streaming_with_default(q, true, false);
     Ok(())
 }
 #[test]
 fn test_streaming_inner_join2() -> PolarsResult<()> {
     let lf_left = df![
            "a"=> [0, 0, 0, 3, 0, 1, 3, 3, 3, 1, 4, 4, 2, 1, 1, 3, 1, 4, 2, 2],
     "b"=> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
@@ -185,15 +221,15 @@
            "a"=> [10, 18, 13, 9, 1, 13, 14, 12, 15, 11],
     "b"=> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
        ]?
     .lazy();
 
     let q = lf_left.inner_join(lf_right, col("a"), col("a"));
 
-    assert_streaming_with_default(q);
+    assert_streaming_with_default(q, true, false);
     Ok(())
 }
 #[test]
 fn test_streaming_left_join() -> PolarsResult<()> {
     let lf_left = df![
            "a"=> [0, 0, 0, 3, 0, 1, 3, 3, 3, 1, 4, 4, 2, 1, 1, 3, 1, 4, 2, 2],
     "b"=> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
@@ -204,15 +240,15 @@
            "a"=> [10, 18, 13, 9, 1, 13, 14, 12, 15, 11],
     "b"=> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
        ]?
     .lazy();
 
     let q = lf_left.left_join(lf_right, col("a"), col("a"));
 
-    assert_streaming_with_default(q);
+    assert_streaming_with_default(q, true, false);
     Ok(())
 }
 
 #[test]
 #[cfg(feature = "cross_join")]
 fn test_streaming_slice() -> PolarsResult<()> {
     let vals = (0..100).collect::<Vec<_>>();
@@ -242,30 +278,30 @@
          "b"=> [0],
     ]?
     .lazy();
 
     let q = lf_left.clone().left_join(lf_right, col("a"), col("a"));
 
     // we add a join that is not supported streaming (for now)
-    // so we can test if the partial query is executed with out panics
+    // so we can test if the partial query is executed without panics
     let q = q
         .join_builder()
         .with(lf_left.clone())
         .left_on([col("a")])
         .right_on([col("a")])
         .suffix("_foo")
         .how(JoinType::Outer)
         .finish();
 
     let q = q.left_join(
         lf_left.clone().select([all().suffix("_foo")]),
         col("a"),
         col("a_foo"),
     );
-    assert_streaming_with_default(q);
+    assert_streaming_with_default(q, false, true);
 
     Ok(())
 }
 
 #[test]
 fn test_streaming_aggregate_join() -> PolarsResult<()> {
     let q = get_parquet_file();
@@ -305,28 +341,26 @@
     .lazy();
 
     let q = q1
         .clone()
         .left_join(q2.clone(), col("p_id"), col("p_id2"))
         .left_join(q3.clone(), col("m_id"), col("m_id3"));
 
-    assert_streaming_with_default(q.clone());
+    assert_streaming_with_default(q.clone(), true, false);
 
     // more joins
     let q = q.left_join(q1.clone(), col("p_id"), col("p_id")).left_join(
         q3.clone(),
         col("m_id"),
         col("m_id3"),
     );
 
-    assert_streaming_with_default(q);
-
+    assert_streaming_with_default(q, true, false);
     // empty tables
     let q = q1
         .slice(0, 0)
         .left_join(q2.slice(0, 0), col("p_id"), col("p_id2"))
         .left_join(q3.slice(0, 0), col("m_id"), col("m_id3"));
 
-    assert_streaming_with_default(q);
-
+    assert_streaming_with_default(q, true, false);
     Ok(())
 }
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/tests/tpch.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/tests/tpch.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-lazy/src/utils.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-lazy/src/utils.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-error/Cargo.toml` & `polars_lts_cpu-0.18.0/local_dependencies/polars-error/Cargo.toml`

 * *Files 12% similar despite different names*

```diff
@@ -1,28 +1,28 @@
 [package]
 name = "polars-error"
-version = "0.28.0"
+version = "0.30.0"
 edition = "2021"
 license = "MIT"
 repository = "https://github.com/pola-rs/polars"
 description = "Error definitions for the Polars DataFrame library"
 
 # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html
 
 [dependencies]
 regex = { version = "1.6", optional = true }
 thiserror= "^1"
 
 [dependencies.arrow]
 package = "arrow2"
 # git = "https://github.com/jorgecarleitao/arrow2"
-git = "https://github.com/ritchie46/arrow2"
+# git = "https://github.com/ritchie46/arrow2"
 # rev = "1491c6e8f4fd100f53c358e4f3ef1536d9e75090"
 # path = "../arrow2"
-branch = "polars_2023-04-20"
+# branch = "polars_2023-05-25"
 version = "0.17"
 default-features = false
 features = [
   "compute_aggregate",
   "compute_arithmetics",
   "compute_boolean",
   "compute_boolean_kleene",
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-error/LICENSE` & `polars_lts_cpu-0.18.0/local_dependencies/polars-time/LICENSE`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-error/src/lib.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-error/src/lib.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/Cargo.toml` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/Cargo.toml`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 [package]
 name = "polars-ops"
-version= "0.28.0"
+version= "0.30.0"
 authors = ["ritchie46 <ritchie46@gmail.com>"]
 edition = "2021"
 license = "MIT"
 repository = "https://github.com/pola-rs/polars"
 description = "More operations on polars data structures"
 
 # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html
@@ -12,17 +12,18 @@
 [dependencies]
 argminmax = { version = "0.6.1", default-features = false, features = ["float"] }
 base64 = { version = "0.21", optional = true }
 either= "1.8"
 hex = { version = "0.4", optional = true }
 jsonpath_lib = { version = "0.3.0", optional = true, git = "https://github.com/ritchie46/jsonpath", branch = "improve_compiled" }
 memchr= "2"
-polars-arrow = { version = "0.28.0", path = "../polars-arrow", default-features = false }
-polars-core = { version = "0.28.0", path = "../polars-core", features = ["private"], default-features = false }
-polars-utils = { version = "0.28.0", path = "../polars-utils", default-features = false }
+polars-arrow = { version = "0.30.0", path = "../polars-arrow", default-features = false }
+polars-core = { version = "0.30.0", path = "../polars-core", features = ["private"], default-features = false }
+polars-json = { version = "0.30.0", optional = true, path = "../polars-json", default-features = false }
+polars-utils = { version = "0.30.0", path = "../polars-utils", default-features = false }
 serde = { version = "1", features = ["derive"], optional = true }
 serde_json = { version = "1", optional = true }
 smartstring= { version = "1" }
 
 [features]
 simd = ["argminmax/nightly_simd"]
 nightly = ["polars-utils/nightly"]
@@ -32,38 +33,40 @@
 dtype-time = ["polars-core/dtype-time", "polars-core/temporal"]
 dtype-duration = ["polars-core/dtype-duration", "polars-core/temporal"]
 dtype-struct = ["polars-core/dtype-struct", "polars-core/temporal"]
 dtype-u8 = ["polars-core/dtype-u8"]
 dtype-u16 = ["polars-core/dtype-u16"]
 dtype-i8 = ["polars-core/dtype-i8"]
 dtype-i16 = ["polars-core/dtype-i16"]
+dtype-array = ["polars-core/dtype-array"]
 dtype-decimal = ["polars-core/dtype-decimal"]
 object = ["polars-core/object"]
 propagate_nans = []
-performant = ["polars-core/performant"]
+performant = ["polars-core/performant", "fused"]
 big_idx = ["polars-core/bigidx"]
 round_series = []
 is_first = []
 is_unique = []
 approx_unique = []
+fused = []
 
 # extra utilities for BinaryChunked
 binary_encoding = ["base64", "hex"]
 string_encoding = ["base64", "hex"]
 
 # ops
 to_dummies = []
 interpolate = []
 list_to_struct = ["polars-core/dtype-struct"]
 list_count = []
 diff = ["polars-core/diff"]
 strings = ["polars-core/strings"]
 string_justify = ["polars-core/strings"]
 string_from_radix = ["polars-core/strings"]
-extract_jsonpath = ["arrow/io_json", "serde_json", "jsonpath_lib"]
+extract_jsonpath = ["serde_json", "jsonpath_lib", "polars-json"]
 log = []
 hash = []
 rolling_window = ["polars-core/rolling_window"]
 moment = ["polars-core/moment"]
 search_sorted = []
 merge_sorted = []
 top_k = []
@@ -73,18 +76,18 @@
 asof_join = ["polars-core/asof_join"]
 semi_anti_join = ["polars-core/semi_anti_join"]
 list_take = []
 
 [dependencies.arrow]
 package = "arrow2"
 # git = "https://github.com/jorgecarleitao/arrow2"
-git = "https://github.com/ritchie46/arrow2"
+# git = "https://github.com/ritchie46/arrow2"
 # rev = "1491c6e8f4fd100f53c358e4f3ef1536d9e75090"
 # path = "../arrow2"
-branch = "polars_2023-04-20"
+# branch = "polars_2023-05-25"
 version = "0.17"
 default-features = false
 features = [
   "compute_aggregate",
   "compute_arithmetics",
   "compute_boolean",
   "compute_boolean_kleene",
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/LICENSE` & `polars_lts_cpu-0.18.0/local_dependencies/polars-error/LICENSE`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/binary/namespace.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/binary/namespace.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/interpolate.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/interpolate.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/list/count.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/list/count.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/list/hash.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/list/hash.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/list/min_max.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/list/min_max.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/list/namespace.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/list/namespace.rs`

 * *Files 14% similar despite different names*

```diff
@@ -10,14 +10,15 @@
 use polars_core::export::num::{NumCast, Signed, Zero};
 #[cfg(feature = "diff")]
 use polars_core::series::ops::NullBehavior;
 use polars_core::utils::{try_get_supertype, CustomIterTools};
 
 use super::*;
 use crate::chunked_array::list::min_max::{list_max_function, list_min_function};
+use crate::chunked_array::list::sum_mean::sum_with_nulls;
 use crate::prelude::list::sum_mean::{mean_list_numerical, sum_list_numerical};
 use crate::series::ArgAgg;
 
 pub(super) fn has_inner_nulls(ca: &ListChunked) -> bool {
     for arr in ca.downcast_iter() {
         if arr.values().null_count() > 0 {
             return true;
@@ -114,123 +115,37 @@
     }
 
     fn lst_min(&self) -> Series {
         list_min_function(self.as_list())
     }
 
     fn lst_sum(&self) -> Series {
-        fn inner(ca: &ListChunked, inner_dtype: &DataType) -> Series {
-            use DataType::*;
-            // TODO: add fast path for smaller ints?
-            let mut out = match inner_dtype {
-                Boolean => {
-                    let out: IdxCa = ca
-                        .amortized_iter()
-                        .map(|s| s.and_then(|s| s.as_ref().sum()))
-                        .collect();
-                    out.into_series()
-                }
-                UInt32 => {
-                    let out: UInt32Chunked = ca
-                        .amortized_iter()
-                        .map(|s| s.and_then(|s| s.as_ref().sum()))
-                        .collect();
-                    out.into_series()
-                }
-                UInt64 => {
-                    let out: UInt64Chunked = ca
-                        .amortized_iter()
-                        .map(|s| s.and_then(|s| s.as_ref().sum()))
-                        .collect();
-                    out.into_series()
-                }
-                Int32 => {
-                    let out: Int32Chunked = ca
-                        .amortized_iter()
-                        .map(|s| s.and_then(|s| s.as_ref().sum()))
-                        .collect();
-                    out.into_series()
-                }
-                Int64 => {
-                    let out: Int64Chunked = ca
-                        .amortized_iter()
-                        .map(|s| s.and_then(|s| s.as_ref().sum()))
-                        .collect();
-                    out.into_series()
-                }
-                Float32 => {
-                    let out: Float32Chunked = ca
-                        .amortized_iter()
-                        .map(|s| s.and_then(|s| s.as_ref().sum()))
-                        .collect();
-                    out.into_series()
-                }
-                Float64 => {
-                    let out: Float64Chunked = ca
-                        .amortized_iter()
-                        .map(|s| s.and_then(|s| s.as_ref().sum()))
-                        .collect();
-                    out.into_series()
-                }
-                // slowest sum_as_series path
-                _ => ca
-                    .apply_amortized(|s| s.as_ref().sum_as_series())
-                    .explode()
-                    .unwrap()
-                    .into_series(),
-            };
-            out.rename(ca.name());
-            out
-        }
-
         let ca = self.as_list();
 
         if has_inner_nulls(ca) {
-            return inner(ca, &ca.inner_dtype());
+            return sum_with_nulls(ca, &ca.inner_dtype());
         };
 
         match ca.inner_dtype() {
             DataType::Boolean => count_boolean_bits(ca).into_series(),
             dt if dt.is_numeric() => sum_list_numerical(ca, &dt),
-            dt => inner(ca, &dt),
+            dt => sum_with_nulls(ca, &dt),
         }
     }
 
     fn lst_mean(&self) -> Series {
-        fn inner(ca: &ListChunked) -> Series {
-            let mut out: Float64Chunked = ca
-                .amortized_iter()
-                .map(|s| s.and_then(|s| s.as_ref().mean()))
-                .collect();
-
-            out.rename(ca.name());
-            out.into_series()
-        }
-        use DataType::*;
-
         let ca = self.as_list();
 
         if has_inner_nulls(ca) {
-            return match ca.inner_dtype() {
-                Float32 => {
-                    let mut out: Float32Chunked = ca
-                        .amortized_iter()
-                        .map(|s| s.and_then(|s| s.as_ref().mean().map(|v| v as f32)))
-                        .collect();
-
-                    out.rename(ca.name());
-                    out.into_series()
-                }
-                _ => inner(ca),
-            };
+            return sum_mean::mean_with_nulls(ca);
         };
 
         match ca.inner_dtype() {
             dt if dt.is_numeric() => mean_list_numerical(ca, &dt),
-            _ => inner(ca),
+            _ => sum_mean::mean_with_nulls(ca),
         }
     }
 
     #[must_use]
     fn lst_sort(&self, options: SortOptions) -> ListChunked {
         let ca = self.as_list();
         ca.apply_amortized(|s| s.as_ref().sort_with(options))
@@ -638,7 +553,9 @@
     };
     polars_ensure!(
         out.null_count() == idx_null_count || null_on_oob,
         ComputeError: "take indices are out of bounds"
     );
     Ok(out)
 }
+
+// TODO: implement the above for ArrayChunked as well?
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/list/to_struct.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/list/to_struct.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/nan_propagating_aggregate.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/nan_propagating_aggregate.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/set.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/set.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/strings/case.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/strings/case.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/strings/json_path.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/strings/json_path.rs`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,9 @@
 use std::borrow::Cow;
 
-use arrow::io::ndjson;
 use jsonpath_lib::PathCompiled;
 use serde_json::Value;
 
 use super::*;
 
 pub fn extract_json<'a>(expr: &PathCompiled, json_str: &'a str) -> Option<Cow<'a, str>> {
     serde_json::from_str(json_str).ok().and_then(|value| {
@@ -55,30 +54,37 @@
     fn json_infer(&self, number_of_rows: Option<usize>) -> PolarsResult<DataType> {
         let ca = self.as_utf8();
         let values_iter = ca
             .into_iter()
             .map(|x| x.unwrap_or("null"))
             .take(number_of_rows.unwrap_or(ca.len()));
 
-        ndjson::read::infer_iter(values_iter)
+        polars_json::ndjson::infer_iter(values_iter)
             .map(|d| DataType::from(&d))
             .map_err(|e| polars_err!(ComputeError: "error inferring JSON: {}", e))
     }
 
     /// Extracts a typed-JSON value for each row in the Utf8Chunked
     fn json_extract(&self, dtype: Option<DataType>) -> PolarsResult<Series> {
         let ca = self.as_utf8();
         let dtype = match dtype {
             Some(dt) => dt,
             None => ca.json_infer(None)?,
         };
 
+        let buf_size = ca.get_values_size() + ca.null_count() * "null".len();
         let iter = ca.into_iter().map(|x| x.unwrap_or("null"));
-        let array = ndjson::read::deserialize_iter(iter, dtype.to_arrow())
-            .map_err(|e| polars_err!(ComputeError: "error deserializing JSON: {}", e))?;
+
+        let array = polars_json::ndjson::deserialize::deserialize_iter(
+            iter,
+            dtype.to_arrow(),
+            buf_size,
+            ca.len(),
+        )
+        .map_err(|e| polars_err!(ComputeError: "error deserializing JSON: {}", e))?;
         Series::try_from(("", array))
     }
 
     fn json_path_select(&self, json_path: &str) -> PolarsResult<Utf8Chunked> {
         let pat = PathCompiled::compile(json_path)
             .map_err(|e| polars_err!(ComputeError: "error compiling JSONpath expression: {}", e))?;
         Ok(self
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/strings/justify.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/strings/justify.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/strings/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/strings/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/strings/namespace.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/strings/namespace.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/strings/replace.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/strings/replace.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/chunked_array/top_k.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/chunked_array/top_k.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/frame/join/merge_sorted.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/frame/join/merge_sorted.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/frame/join/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/frame/join/mod.rs`

 * *Files 2% similar despite different names*

```diff
@@ -154,23 +154,35 @@
                     _verbose,
                 );
             }
         }
 
         polars_ensure!(
             selected_left.len() == selected_right.len(),
-            ComputeError: "the number of columns given as join key should be equal"
+            ComputeError:
+                format!(
+                    "the number of columns given as join key (left: {}, right:{}) should be equal",
+                    selected_left.len(),
+                    selected_right.len()
+                )
         );
-        polars_ensure!(
-            selected_left
+
+        if let Some((l, r)) = selected_left
             .iter()
             .zip(&selected_right)
-            .all(|(l, r)| l.dtype() == r.dtype()),
-            ComputeError: "datatypes of join keys don't match"
-        );
+            .find(|(l, r)| l.dtype() != r.dtype())
+        {
+            polars_bail!(
+                ComputeError:
+                    format!(
+                        "datatypes of join keys don't match - `{}`: {} on left does not match `{}`: {} on right",
+                        l.name(), l.dtype(), r.name(), r.dtype()
+                    )
+            );
+        };
 
         #[cfg(feature = "dtype-categorical")]
         for (l, r) in selected_left.iter().zip(&selected_right) {
             _check_categorical_src(l.dtype(), r.dtype())?
         }
 
         // Single keys
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/frame/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/frame/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/frame/pivot/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/frame/pivot/mod.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/frame/pivot/positioning.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/frame/pivot/positioning.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/ops/approx_algo/hyperloglogplus.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/approx_algo/hyperloglogplus.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/ops/approx_unique.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/approx_unique.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/ops/arg_min_max.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/arg_min_max.rs`

 * *Files 1% similar despite different names*

```diff
@@ -30,15 +30,15 @@
             }
             dt if dt.is_numeric() => {
                 with_match_physical_numeric_polars_type!(s.dtype(), |$T| {
                     let ca: &ChunkedArray<$T> = s.as_ref().as_ref().as_ref();
                     if ca.is_empty() { // because argminmax assumes not empty
                         None
                     } else if let Ok(vals) = ca.cont_slice() {
-                        arg_min_numeric_slice(vals, ca.is_sorted_flag2())
+                        arg_min_numeric_slice(vals, ca.is_sorted_flag())
                     } else {
                         arg_min_numeric(ca)
                     }
                 })
             }
             _ => None,
         }
@@ -58,15 +58,15 @@
             }
             dt if dt.is_numeric() => {
                 with_match_physical_numeric_polars_type!(s.dtype(), |$T| {
                     let ca: &ChunkedArray<$T> = s.as_ref().as_ref().as_ref();
                     if ca.is_empty() { // because argminmax assumes not empty
                         None
                     } else if let Ok(vals) = ca.cont_slice() {
-                        arg_max_numeric_slice(vals, ca.is_sorted_flag2())
+                        arg_max_numeric_slice(vals, ca.is_sorted_flag())
                     } else {
                         arg_max_numeric(ca)
                     }
                 })
             }
             _ => None,
         }
@@ -191,27 +191,27 @@
     } else {
         let mask_chunks = mask.chunks::<u64>();
         first_unset_bit_impl(mask_chunks)
     }
 }
 
 fn arg_min_str(ca: &Utf8Chunked) -> Option<usize> {
-    match ca.is_sorted_flag2() {
+    match ca.is_sorted_flag() {
         IsSorted::Ascending => Some(0),
         IsSorted::Descending => Some(ca.len() - 1),
         IsSorted::Not => ca
             .into_iter()
             .enumerate()
             .reduce(|acc, (idx, val)| if acc.1 > val { (idx, val) } else { acc })
             .map(|tpl| tpl.0),
     }
 }
 
 fn arg_max_str(ca: &Utf8Chunked) -> Option<usize> {
-    match ca.is_sorted_flag2() {
+    match ca.is_sorted_flag() {
         IsSorted::Ascending => Some(ca.len() - 1),
         IsSorted::Descending => Some(0),
         IsSorted::Not => ca
             .into_iter()
             .enumerate()
             .reduce(|acc, (idx, val)| if acc.1 < val { (idx, val) } else { acc })
             .map(|tpl| tpl.0),
@@ -219,15 +219,15 @@
 }
 
 fn arg_min_numeric<'a, T>(ca: &'a ChunkedArray<T>) -> Option<usize>
 where
     T: PolarsNumericType,
     for<'b> &'b [T::Native]: ArgMinMax,
 {
-    match ca.is_sorted_flag2() {
+    match ca.is_sorted_flag() {
         IsSorted::Ascending => Some(0),
         IsSorted::Descending => Some(ca.len() - 1),
         IsSorted::Not => {
             ca.downcast_iter()
                 .fold((None, None, 0), |acc, arr| {
                     if arr.len() == 0 {
                         return acc;
@@ -274,15 +274,15 @@
 }
 
 fn arg_max_numeric<'a, T>(ca: &'a ChunkedArray<T>) -> Option<usize>
 where
     T: PolarsNumericType,
     for<'b> &'b [T::Native]: ArgMinMax,
 {
-    match ca.is_sorted_flag2() {
+    match ca.is_sorted_flag() {
         IsSorted::Ascending => Some(ca.len() - 1),
         IsSorted::Descending => Some(0),
         IsSorted::Not => {
             ca.downcast_iter()
                 .fold((None, None, 0), |acc, arr| {
                     if arr.len() == 0 {
                         return acc;
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/ops/floor_divide.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/floor_divide.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/ops/is_first.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/is_first.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/ops/is_unique.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/is_unique.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/ops/log.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/log.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/ops/mod.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/mod.rs`

 * *Files 13% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 mod approx_algo;
 #[cfg(feature = "approx_unique")]
 mod approx_unique;
 mod arg_min_max;
 #[cfg(feature = "round_series")]
 mod floor_divide;
+#[cfg(feature = "fused")]
+mod fused;
 #[cfg(feature = "is_first")]
 mod is_first;
 #[cfg(feature = "is_unique")]
 mod is_unique;
 #[cfg(feature = "log")]
 mod log;
 #[cfg(feature = "rolling_window")]
@@ -20,14 +22,16 @@
 
 pub use approx_algo::*;
 #[cfg(feature = "approx_unique")]
 pub use approx_unique::*;
 pub use arg_min_max::ArgAgg;
 #[cfg(feature = "round_series")]
 pub use floor_divide::*;
+#[cfg(feature = "fused")]
+pub use fused::*;
 #[cfg(feature = "is_first")]
 pub use is_first::*;
 #[cfg(feature = "is_unique")]
 pub use is_unique::*;
 #[cfg(feature = "log")]
 pub use log::*;
 use polars_core::prelude::*;
```

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/ops/rolling.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/rolling.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/ops/search_sorted.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/search_sorted.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/local_dependencies/polars-ops/src/series/ops/to_dummies.rs` & `polars_lts_cpu-0.18.0/local_dependencies/polars-ops/src/series/ops/to_dummies.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/Cargo.toml` & `polars_lts_cpu-0.18.0/Cargo.toml`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 [package]
 name = "py-polars"
-version = "0.17.9"
+version = "0.18.0"
 edition = "2021"
 
 # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html
 
 [workspace]
 # prevents package from thinking it's in the workspace
 [target.'cfg(any(not(target_os = "linux"), use_mimalloc))'.dependencies]
@@ -90,14 +90,15 @@
   "object",
   "pivot",
   "top_k",
   "build_info",
   "cse",
   "propagate_nans",
   "polars/groupby_list",
+  "polars/fused",
   "sql",
   "binary_encoding",
   "streaming",
   "performant",
   "list_take",
   "list_count",
 ]
@@ -179,8 +180,8 @@
 # Should not be used when packaging
 # target-cpu = "native"
 
 [build-dependencies]
 built = { version = "0.6", features = ["chrono", "git2"], optional = true }
 
 [patch.crates-io]
-simd-json = { git = "https://github.com/ritchie46/simd-json", branch = "alignment" }
+# simd-json = { git = "https://github.com/ritchie46/simd-json", branch = "alignment" }
```

### Comparing `polars_lts_cpu-0.17.9/LICENSE` & `polars_lts_cpu-0.18.0/LICENSE`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/Makefile` & `polars_lts_cpu-0.18.0/Makefile`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/README.md` & `polars_lts_cpu-0.18.0/README.md`

 * *Files 2% similar despite different names*

```diff
@@ -200,25 +200,25 @@
 ```sh
 pip install 'polars[all]'
 pip install 'polars[numpy,pandas,pyarrow]'  # install a subset of all optional dependencies
 ```
 
 You can also install the dependencies directly.
 
-| Tag        | Description                                                                                                                           |
-| ---------- | ------------------------------------------------------------------------------------------------------------------------------------- |
-| all        | Install all optional dependencies (all of the following)                                                                              |
-| pandas     | Install with Pandas for converting data to and from Pandas Dataframes/Series                                                          |
-| numpy      | Install with numpy for converting data to and from numpy arrays                                                                       |
-| pyarrow    | Reading data formats using PyArrow                                                                                                    |
-| fsspec     | Support for reading from remote file systems                                                                                          |
-| connectorx | Support for reading from SQL databases                                                                                                |
-| xlsx2csv   | Support for reading from Excel files                                                                                                  |
-| deltalake  | Support for reading from Delta Lake Tables                                                                                            |
-| timezone   | Timezone support, only needed if 1. you are on Python < 3.9 and/or 2. you are on Windows, otherwise no dependencies will be installed |
+| Tag        | Description                                                                  |
+| ---------- | ---------------------------------------------------------------------------- |
+| **all**    | Install all optional dependencies (all of the following)                     |
+| pandas     | Install with Pandas for converting data to and from Pandas Dataframes/Series |
+| numpy      | Install with numpy for converting data to and from numpy arrays              |
+| pyarrow    | Reading data formats using PyArrow                                           |
+| fsspec     | Support for reading from remote file systems                                 |
+| connectorx | Support for reading from SQL databases                                       |
+| xlsx2csv   | Support for reading from Excel files                                         |
+| deltalake  | Support for reading from Delta Lake Tables                                   |
+| timezone   | Timezone support, only needed if are on Python<3.9 or you are on Windows     |
 
 Releases happen quite often (weekly / every few days) at the moment, so updating polars regularly to get the latest bugfixes / features might not be a bad idea.
 
 ### Rust
 
 You can take latest release from `crates.io`, or if you want to use the latest features / performance improvements
 point to the `main` branch of this repo.
```

#### html2text {}

```diff
@@ -72,28 +72,26 @@
 Install the latest polars version with: ```sh pip install polars ``` We also
 have a conda package (`conda install -c conda-forge polars`), however pip is
 the preferred way to install Polars. Install Polars with all optional
 dependencies. ```sh pip install 'polars[all]' pip install 'polars
 [numpy,pandas,pyarrow]' # install a subset of all optional dependencies ``` You
 can also install the dependencies directly. | Tag | Description | | ---------
 - | ---------------------------------------------------------------------------
----------------------------------------------------------- | | all | Install
-all optional dependencies (all of the following) | | pandas | Install with
-Pandas for converting data to and from Pandas Dataframes/Series | | numpy |
-Install with numpy for converting data to and from numpy arrays | | pyarrow |
-Reading data formats using PyArrow | | fsspec | Support for reading from remote
-file systems | | connectorx | Support for reading from SQL databases | |
-xlsx2csv | Support for reading from Excel files | | deltalake | Support for
-reading from Delta Lake Tables | | timezone | Timezone support, only needed if
-1. you are on Python < 3.9 and/or 2. you are on Windows, otherwise no
-dependencies will be installed | Releases happen quite often (weekly / every
-few days) at the moment, so updating polars regularly to get the latest
-bugfixes / features might not be a bad idea. ### Rust You can take latest
-release from `crates.io`, or if you want to use the latest features /
-performance improvements point to the `main` branch of this repo. ```toml
+- | | **all** | Install all optional dependencies (all of the following) | |
+pandas | Install with Pandas for converting data to and from Pandas Dataframes/
+Series | | numpy | Install with numpy for converting data to and from numpy
+arrays | | pyarrow | Reading data formats using PyArrow | | fsspec | Support
+for reading from remote file systems | | connectorx | Support for reading from
+SQL databases | | xlsx2csv | Support for reading from Excel files | | deltalake
+| Support for reading from Delta Lake Tables | | timezone | Timezone support,
+only needed if are on Python<3.9 or you are on Windows | Releases happen quite
+often (weekly / every few days) at the moment, so updating polars regularly to
+get the latest bugfixes / features might not be a bad idea. ### Rust You can
+take latest release from `crates.io`, or if you want to use the latest features
+/ performance improvements point to the `main` branch of this repo. ```toml
 polars = { git = "https://github.com/pola-rs/polars", rev = "" } ``` Required
 Rust version `>=1.62` ## Contributing Want to contribute? Read our
 [contribution guideline](./CONTRIBUTING.md). ## Python: compile polars from
 source If you want a bleeding edge release or maximal performance you should
 compile **polars** from source. This can be done by going through the following
 steps in sequence: 1. Install the latest [Rust compiler](https://www.rust-
 lang.org/tools/install) 2. Install [maturin](https://maturin.rs/): `pip install
```

### Comparing `polars_lts_cpu-0.17.9/build.rs` & `polars_lts_cpu-0.18.0/build.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/docs/Makefile` & `polars_lts_cpu-0.18.0/docs/Makefile`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/docs/_templates/autosummary/class.rst` & `polars_lts_cpu-0.18.0/docs/_templates/autosummary/class.rst`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/docs/run_live_docs_server.py` & `polars_lts_cpu-0.18.0/docs/run_live_docs_server.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/docs/source/_static/css/custom.css` & `polars_lts_cpu-0.18.0/docs/source/_static/css/custom.css`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/docs/source/conf.py` & `polars_lts_cpu-0.18.0/docs/source/conf.py`

 * *Files 3% similar despite different names*

```diff
@@ -52,14 +52,15 @@
     # third-party extensions
     # ----------------------
     "autodocsumm",
     "numpydoc",
     "sphinx_autosummary_accessors",
     "sphinx_copybutton",
     "sphinx_design",
+    "sphinx_favicon",
 ]
 
 # Add any paths that contain templates here, relative to this directory.
 templates_path = ["_templates", sphinx_autosummary_accessors.templates_path]
 
 # List of patterns, relative to source directory, that match files and
 # directories to ignore when looking for source files.
@@ -110,32 +111,33 @@
         },
         {
             "name": "Twitter",
             "url": "https://twitter.com/DataPolars",
             "icon": "fa-brands fa-twitter",
         },
     ],
-    "favicons": [
-        {
-            "rel": "icon",
-            "sizes": "32x32",
-            "href": "https://raw.githubusercontent.com/pola-rs/polars-static/master/icons/favicon-32x32.png",
-        },
-        {
-            "rel": "apple-touch-icon",
-            "sizes": "180x180",
-            "href": "https://raw.githubusercontent.com/pola-rs/polars-static/master/icons/touchicon-180x180.png",
-        },
-    ],
     "logo": {
         "image_light": "https://raw.githubusercontent.com/pola-rs/polars-static/master/logos/polars-logo-dark-medium.png",
         "image_dark": "https://raw.githubusercontent.com/pola-rs/polars-static/master/logos/polars-logo-dimmed-medium.png",
     },
 }
 
+favicons = [
+    {
+        "rel": "icon",
+        "sizes": "32x32",
+        "href": "https://raw.githubusercontent.com/pola-rs/polars-static/master/icons/favicon-32x32.png",
+    },
+    {
+        "rel": "apple-touch-icon",
+        "sizes": "180x180",
+        "href": "https://raw.githubusercontent.com/pola-rs/polars-static/master/icons/touchicon-180x180.png",
+    },
+]
+
 intersphinx_mapping = {
     "numpy": ("https://numpy.org/doc/stable/", None),
     "pandas": ("https://pandas.pydata.org/docs/", None),
     "pyarrow": ("https://arrow.apache.org/docs/", None),
     "python": ("https://docs.python.org/3", None),
 }
 
@@ -197,27 +199,27 @@
     return (
         f"https://github.com/pola-rs/polars/blob/main/py-polars/polars/{fn}{linespec}"
     )
 
 
 def _minify_classpaths(s: str) -> str:
     # strip private polars classpaths, leaving the classname:
-    # * "pli.Expr" -> "Expr"
+    # * "pl.Expr" -> "Expr"
     # * "polars.expr.expr.Expr" -> "Expr"
     # * "polars.lazyframe.frame.LazyFrame" -> "LazyFrame"
     # also:
     # * "datetime.date" => "date"
     s = s.replace("datetime.", "")
     return re.sub(
         pattern=r"""
         ~?
         (
-          (?:pli|
+          (?:pl|
             (?:polars\.
-              (?:internals|datatypes)
+              (?:_reexport|datatypes)
             )
           )
           (?:\.[a-z.]+)?\.
           ([A-Z][\w.]+)
         )
         """,
         repl=r"\2",
```

### Comparing `polars_lts_cpu-0.17.9/docs/source/reference/api.rst` & `polars_lts_cpu-0.18.0/docs/source/reference/api.rst`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/docs/source/reference/config.rst` & `polars_lts_cpu-0.18.0/docs/source/reference/config.rst`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/docs/source/reference/dataframe/modify_select.rst` & `polars_lts_cpu-0.18.0/docs/source/reference/dataframe/modify_select.rst`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/docs/source/reference/datatypes.rst` & `polars_lts_cpu-0.18.0/docs/source/reference/datatypes.rst`

 * *Files 18% similar despite different names*

```diff
@@ -14,22 +14,22 @@
 .. autosummary::
     :toctree: api/
     :nosignatures:
 
     Decimal
     Float32
     Float64
+    Int8
     Int16
     Int32
     Int64
-    Int8
+    UInt8
     UInt16
     UInt32
     UInt64
-    UInt8
 
 Temporal
 ~~~~~~~~~~~
 .. autosummary::
     :toctree: api/
     :nosignatures:
 
@@ -39,14 +39,15 @@
     Time
 
 Nested
 ~~~~~~
 .. autosummary::
     :toctree: api/
 
+    Array
     List
     Struct
 
 Other
 ~~~~~
 .. autosummary::
     :toctree: api/
```

### Comparing `polars_lts_cpu-0.17.9/docs/source/reference/expressions/computation.rst` & `polars_lts_cpu-0.18.0/docs/source/reference/expressions/computation.rst`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/docs/source/reference/expressions/functions.rst` & `polars_lts_cpu-0.18.0/docs/source/reference/expressions/functions.rst`

 * *Files 12% similar despite different names*

```diff
@@ -35,32 +35,33 @@
    exclude
    first
    fold
    format
    from_epoch
    groups
    head
-   list
+   implode
    lit
    map
    max
    mean
    median
    min
    n_unique
-   pearson_corr
    quantile
    reduce
    repeat
+   rolling_corr
+   rolling_cov
    select
-   spearman_rank_corr
    std
    struct
    sum
    tail
+   time
    var
    when
 
 
 **Available in expression namespace:**
 
 .. autosummary::
```

### Comparing `polars_lts_cpu-0.17.9/docs/source/reference/expressions/modify_select.rst` & `polars_lts_cpu-0.18.0/docs/source/reference/expressions/modify_select.rst`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/docs/source/reference/expressions/operators.rst` & `polars_lts_cpu-0.18.0/docs/source/reference/expressions/operators.rst`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/docs/source/reference/expressions/string.rst` & `polars_lts_cpu-0.18.0/docs/source/reference/expressions/string.rst`

 * *Files 16% similar despite different names*

```diff
@@ -31,11 +31,15 @@
     Expr.str.slice
     Expr.str.split
     Expr.str.split_exact
     Expr.str.splitn
     Expr.str.starts_with
     Expr.str.strip
     Expr.str.strptime
+    Expr.str.to_date
+    Expr.str.to_datetime
+    Expr.str.to_decimal
     Expr.str.to_lowercase
+    Expr.str.to_time
     Expr.str.to_uppercase
     Expr.str.zfill
     Expr.str.parse_int
```

### Comparing `polars_lts_cpu-0.17.9/docs/source/reference/expressions/temporal.rst` & `polars_lts_cpu-0.18.0/docs/source/reference/expressions/temporal.rst`

 * *Files 1% similar despite different names*

```diff
@@ -37,13 +37,14 @@
     Expr.dt.quarter
     Expr.dt.round
     Expr.dt.second
     Expr.dt.seconds
     Expr.dt.strftime
     Expr.dt.time
     Expr.dt.timestamp
+    Expr.dt.to_string
     Expr.dt.truncate
     Expr.dt.week
     Expr.dt.weekday
     Expr.dt.with_time_unit
     Expr.dt.convert_time_zone
     Expr.dt.year
```

### Comparing `polars_lts_cpu-0.17.9/docs/source/reference/functions.rst` & `polars_lts_cpu-0.18.0/docs/source/reference/functions.rst`

 * *Files 23% similar despite different names*

```diff
@@ -20,18 +20,17 @@
 Eager/Lazy functions
 ~~~~~~~~~~~~~~~~~~~~
 .. autosummary::
    :toctree: api/
 
     arg_where
     concat
-    cut
     date_range
-    get_dummies
     ones
+    time_range
     zeros
 
 Miscellaneous
 ~~~~~~~~~~~~~~~~~~~~
 .. autosummary::
    :toctree: api/
```

### Comparing `polars_lts_cpu-0.17.9/docs/source/reference/io.rst` & `polars_lts_cpu-0.18.0/docs/source/reference/io.rst`

 * *Files 3% similar despite different names*

```diff
@@ -73,14 +73,15 @@
 Delta Lake
 ~~~~~~~~~~
 .. autosummary::
    :toctree: api/
 
    scan_delta
    read_delta
+   DataFrame.write_delta
 
 Datasets
 ~~~~~~~~
 Connect to pyarrow datasets.
 
 .. autosummary::
    :toctree: api/
```

### Comparing `polars_lts_cpu-0.17.9/docs/source/reference/lazyframe/modify_select.rst` & `polars_lts_cpu-0.18.0/docs/source/reference/lazyframe/modify_select.rst`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/docs/source/reference/series/computation.rst` & `polars_lts_cpu-0.18.0/docs/source/reference/series/computation.rst`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/docs/source/reference/series/descriptive.rst` & `polars_lts_cpu-0.18.0/docs/source/reference/series/descriptive.rst`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/docs/source/reference/series/list.rst` & `polars_lts_cpu-0.18.0/docs/source/reference/series/string.rst`

 * *Files 25% similar despite different names*

```diff
@@ -1,37 +1,45 @@
-====
-List
-====
+======
+String
+======
 
-The following methods are available under the `Series.arr` attribute.
+The following methods are available under the `Series.str` attribute.
 
 .. currentmodule:: polars
 .. autosummary::
    :toctree: api/
    :template: autosummary/accessor_method.rst
 
-    Series.arr.arg_max
-    Series.arr.arg_min
-    Series.arr.concat
-    Series.arr.contains
-    Series.arr.count_match
-    Series.arr.diff
-    Series.arr.eval
-    Series.arr.explode
-    Series.arr.first
-    Series.arr.get
-    Series.arr.head
-    Series.arr.join
-    Series.arr.last
-    Series.arr.lengths
-    Series.arr.max
-    Series.arr.mean
-    Series.arr.min
-    Series.arr.reverse
-    Series.arr.shift
-    Series.arr.slice
-    Series.arr.sort
-    Series.arr.sum
-    Series.arr.tail
-    Series.arr.take
-    Series.arr.to_struct
-    Series.arr.unique
+    Series.str.concat
+    Series.str.contains
+    Series.str.count_match
+    Series.str.decode
+    Series.str.encode
+    Series.str.ends_with
+    Series.str.explode
+    Series.str.extract
+    Series.str.extract_all
+    Series.str.json_extract
+    Series.str.json_path_match
+    Series.str.lengths
+    Series.str.ljust
+    Series.str.lstrip
+    Series.str.n_chars
+    Series.str.replace
+    Series.str.replace_all
+    Series.str.rjust
+    Series.str.rstrip
+    Series.str.slice
+    Series.str.split
+    Series.str.split_exact
+    Series.str.splitn
+    Series.str.starts_with
+    Series.str.strip
+    Series.str.strptime
+    Series.str.to_date
+    Series.str.to_datetime
+    Series.str.to_decimal
+    Series.str.to_lowercase
+    Series.str.to_time
+    Series.str.to_uppercase
+    Series.str.zfill
+    Series.str.parse_int
```

### Comparing `polars_lts_cpu-0.17.9/docs/source/reference/series/modify_select.rst` & `polars_lts_cpu-0.18.0/docs/source/reference/series/modify_select.rst`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/docs/source/reference/series/temporal.rst` & `polars_lts_cpu-0.18.0/docs/source/reference/series/temporal.rst`

 * *Files 1% similar despite different names*

```diff
@@ -41,13 +41,14 @@
     Series.dt.quarter
     Series.dt.round
     Series.dt.second
     Series.dt.seconds
     Series.dt.strftime
     Series.dt.time
     Series.dt.timestamp
+    Series.dt.to_string
     Series.dt.truncate
     Series.dt.week
     Series.dt.weekday
     Series.dt.with_time_unit
     Series.dt.convert_time_zone
     Series.dt.year
```

### Comparing `polars_lts_cpu-0.17.9/docs/source/reference/testing.rst` & `polars_lts_cpu-0.18.0/docs/source/reference/testing.rst`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/polars/__init__.py` & `polars_lts_cpu-0.18.0/polars/__init__.py`

 * *Files 5% similar despite different names*

```diff
@@ -24,14 +24,15 @@
 from polars.datatypes import (
     DATETIME_DTYPES,
     DURATION_DTYPES,
     FLOAT_DTYPES,
     INTEGER_DTYPES,
     NUMERIC_DTYPES,
     TEMPORAL_DTYPES,
+    Array,
     Binary,
     Boolean,
     Categorical,
     DataType,
     Date,
     Datetime,
     Decimal,
@@ -66,43 +67,39 @@
     PolarsPanicError,
     SchemaError,
     SchemaFieldNotFoundError,
     ShapeError,
     StructFieldNotFoundError,
 )
 from polars.expr import Expr
-from polars.functions.eager import (
+from polars.functions import (
     align_frames,
-    concat,
-    cut,
-    date_range,
-    get_dummies,
-    ones,
-    zeros,
-)
-from polars.functions.lazy import (
     all,
     any,
     apply,
     approx_unique,
     arange,
     arg_sort_by,
     arg_where,
     avg,
     coalesce,
     col,
     collect_all,
+    concat,
     concat_list,
     concat_str,
     corr,
     count,
     cov,
     cumfold,
     cumreduce,
     cumsum,
+    date,
+    date_range,
+    datetime,
     duration,
     element,
     exclude,
     first,
     fold,
     format,
     from_epoch,
@@ -113,44 +110,44 @@
     lit,
     map,
     max,
     mean,
     median,
     min,
     n_unique,
-    pearson_corr,
+    ones,
     quantile,
     reduce,
     repeat,
+    rolling_corr,
+    rolling_cov,
     select,
-    spearman_rank_corr,
     std,
     struct,
     sum,
     tail,
+    time,
+    time_range,
     var,
+    when,
+    zeros,
 )
-from polars.functions.lazy import date_ as date
-from polars.functions.lazy import datetime_ as datetime
-from polars.functions.lazy import list_ as list
-from polars.functions.whenthen import when
 from polars.io import (
     read_avro,
     read_csv,
     read_csv_batched,
     read_database,
     read_delta,
     read_excel,
     read_ipc,
     read_ipc_schema,
     read_json,
     read_ndjson,
     read_parquet,
     read_parquet_schema,
-    read_sql,
     scan_csv,
     scan_delta,
     scan_ds,
     scan_ipc,
     scan_ndjson,
     scan_parquet,
     scan_pyarrow_dataset,
@@ -198,14 +195,15 @@
     "StructFieldNotFoundError",
     # core classes
     "DataFrame",
     "Expr",
     "LazyFrame",
     "Series",
     # polars.datatypes
+    "Array",
     "Binary",
     "Boolean",
     "Categorical",
     "DataType",
     "Date",
     "Datetime",
     "Decimal",
@@ -246,15 +244,14 @@
     "read_excel",
     "read_ipc",
     "read_ipc_schema",
     "read_json",
     "read_ndjson",
     "read_parquet",
     "read_parquet_schema",
-    "read_sql",
     "scan_csv",
     "scan_delta",
     "scan_ds",
     "scan_ipc",
     "scan_ndjson",
     "scan_parquet",
     "scan_pyarrow_dataset",
@@ -267,20 +264,19 @@
     "Config",
     # polars.functions.whenthen
     "when",
     # polars.functions
     "align_frames",
     "arg_where",
     "concat",
-    "cut",
     "date_range",
     "element",
-    "get_dummies",
     "ones",
     "repeat",
+    "time_range",
     "zeros",
     # polars.functions.lazy
     "all",
     "any",
     "apply",
     "arange",
     "arg_sort_by",
@@ -304,32 +300,32 @@
     "fold",
     "format",
     "from_epoch",
     "groups",
     "head",
     "implode",
     "last",
-    "list",  # named list_, see import above
     "lit",
     "map",
     "max",
     "mean",
     "median",
     "min",
     "n_unique",
     "approx_unique",
-    "pearson_corr",
     "quantile",
     "reduce",
+    "rolling_corr",
+    "rolling_cov",
     "select",
-    "spearman_rank_corr",
     "std",
     "struct",
     "sum",
     "tail",
+    "time",  # named time_, see import above
     "var",
     # polars.convert
     "from_arrow",
     "from_dataframe",
     "from_dict",
     "from_dicts",
     "from_numpy",
```

### Comparing `polars_lts_cpu-0.17.9/polars/api.py` & `polars_lts_cpu-0.18.0/polars/api.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,36 +1,33 @@
 from __future__ import annotations
 
 from functools import reduce
 from operator import or_
 from typing import TYPE_CHECKING, Callable, TypeVar
 from warnings import warn
 
-from polars import internals as pli
+import polars._reexport as pl
 from polars.utils.various import find_stacklevel
 
 if TYPE_CHECKING:
-    from polars.dataframe import DataFrame
-    from polars.expr import Expr
-    from polars.lazyframe import LazyFrame
-    from polars.series import Series
+    from polars import DataFrame, Expr, LazyFrame, Series
 
 __all__ = [
     "register_expr_namespace",
     "register_dataframe_namespace",
     "register_lazyframe_namespace",
     "register_series_namespace",
 ]
 
 # do not allow override of polars' own namespaces (as registered by '_accessors')
 _reserved_namespaces: set[str] = reduce(
     or_,
     (
         cls._accessors  # type: ignore[attr-defined]
-        for cls in (pli.DataFrame, pli.Expr, pli.LazyFrame, pli.Series)
+        for cls in (pl.DataFrame, pl.Expr, pl.LazyFrame, pl.Series)
     ),
 )
 
 
 NS = TypeVar("NS")
 
 
@@ -96,20 +93,18 @@
     ...
     ...     def nearest(self, p: int) -> pl.Expr:
     ...         return (p ** (self._expr.log(p)).round(0).cast(pl.Int64)).cast(pl.Int64)
     ...
     >>>
     >>> df = pl.DataFrame([1.4, 24.3, 55.0, 64.001], schema=["n"])
     >>> df.select(
-    ...     [
-    ...         pl.col("n"),
-    ...         pl.col("n").pow_n.next(p=2).alias("next_pow2"),
-    ...         pl.col("n").pow_n.previous(p=2).alias("prev_pow2"),
-    ...         pl.col("n").pow_n.nearest(p=2).alias("nearest_pow2"),
-    ...     ]
+    ...     pl.col("n"),
+    ...     pl.col("n").pow_n.next(p=2).alias("next_pow2"),
+    ...     pl.col("n").pow_n.previous(p=2).alias("prev_pow2"),
+    ...     pl.col("n").pow_n.nearest(p=2).alias("nearest_pow2"),
     ... )
     shape: (4, 4)
     
      n       next_pow2  prev_pow2  nearest_pow2 
      ---     ---        ---        ---          
      f64     i64        i64        i64          
     
@@ -122,15 +117,15 @@
     See Also
     --------
     register_dataframe_namespace: Register functionality on a DataFrame.
     register_lazyframe_namespace: Register functionality on a LazyFrame.
     register_series_namespace: Register functionality on a Series.
 
     """
-    return _create_namespace(name, pli.Expr)
+    return _create_namespace(name, pl.Expr)
 
 
 def register_dataframe_namespace(name: str) -> Callable[[type[NS]], type[NS]]:
     """
     Decorator for registering custom functionality with a polars DataFrame.
 
     Parameters
@@ -222,15 +217,15 @@
     See Also
     --------
     register_expr_namespace: Register functionality on an Expr.
     register_lazyframe_namespace: Register functionality on a LazyFrame.
     register_series_namespace: Register functionality on a Series.
 
     """
-    return _create_namespace(name, pli.DataFrame)
+    return _create_namespace(name, pl.DataFrame)
 
 
 def register_lazyframe_namespace(name: str) -> Callable[[type[NS]], type[NS]]:
     """
     Decorator for registering custom functionality with a polars LazyFrame.
 
     Parameters
@@ -297,15 +292,15 @@
      str  i64  i64  i64 
     
      xx   2    3    4   
      xy   4    5    6   
      yy   5    6    7   
      yz   6    7    8   
     
-    >>> [ldf.collect() for ldf in ldf.types.split_by_column_dtypes()]
+    >>> pl.collect_all(ldf.types.split_by_column_dtypes())
     [shape: (4, 1)
     
      a1  
      --- 
      str 
     
      xx  
@@ -327,15 +322,15 @@
     See Also
     --------
     register_expr_namespace: Register functionality on an Expr.
     register_dataframe_namespace: Register functionality on a DataFrame.
     register_series_namespace: Register functionality on a Series.
 
     """
-    return _create_namespace(name, pli.LazyFrame)
+    return _create_namespace(name, pl.LazyFrame)
 
 
 def register_series_namespace(name: str) -> Callable[[type[NS]], type[NS]]:
     """
     Decorator for registering custom functionality with a polars Series.
 
     Parameters
@@ -382,8 +377,8 @@
     See Also
     --------
     register_expr_namespace: Register functionality on an Expr.
     register_dataframe_namespace: Register functionality on a DataFrame.
     register_lazyframe_namespace: Register functionality on a LazyFrame.
 
     """
-    return _create_namespace(name, pli.Series)
+    return _create_namespace(name, pl.Series)
```

### Comparing `polars_lts_cpu-0.17.9/polars/config.py` & `polars_lts_cpu-0.18.0/polars/config.py`

 * *Files 4% similar despite different names*

```diff
@@ -80,23 +80,26 @@
 
     (The compact format is available for all `Config` methods that take a single value).
 
     """
 
     _original_state: str = ""
 
-    def __init__(self, **options: Any) -> None:
+    def __init__(self, *, restore_defaults: bool = False, **options: Any) -> None:
         """
         Initialise a Config object instance for context manager usage.
 
         Any `options` kwargs should correspond to the available named "set_"
         methods, but can optionally to omit the "set_" prefix for brevity.
 
         Parameters
         ----------
+        restore_defaults
+            set all options to their default values (this is applied before
+            setting any other options).
         options
             keyword args that will set the option; equivalent to calling the
             named "set_<option>" method with the given value.
 
         Examples
         --------
         >>> with pl.Config(
@@ -106,14 +109,17 @@
         ... ):
         ...     pass
 
         """
         # save original state _before_ any changes are made
         self._original_state = self.save()
 
+        if restore_defaults:
+            self.restore_defaults()
+
         for opt, value in options.items():
             if not hasattr(self, opt) and not opt.startswith("set_"):
                 opt = f"set_{opt}"
             if not hasattr(self, opt):
                 raise AttributeError(f"Config has no {opt!r} option")
             getattr(self, opt)(value)
 
@@ -302,14 +308,47 @@
         Set the number of characters used to display string values.
 
         Parameters
         ----------
         n : int
             number of characters to display
 
+        Examples
+        --------
+        >>> df = pl.DataFrame(
+        ...     {
+        ...         "txt": [
+        ...             "Play it, Sam. Play 'As Time Goes By'.",
+        ...             "This is the beginning of a beautiful friendship.",
+        ...         ]
+        ...     }
+        ... )
+        >>> df.with_columns(pl.col("txt").str.lengths().alias("len"))
+        shape: (2, 2)
+        
+         txt                                len 
+         ---                                --- 
+         str                                u32 
+        
+         Play it, Sam. Play 'As Time Goes  37  
+         This is the beginning of a beaut  48  
+        
+        >>> with pl.Config(fmt_str_lengths=50):
+        ...     print(df)
+        ...
+        shape: (2, 1)
+        
+         txt                                              
+         ---                                              
+         str                                              
+        
+         Play it, Sam. Play 'As Time Goes By'.            
+         This is the beginning of a beautiful friendship. 
+        
+
         """
         if n <= 0:
             raise ValueError("number of characters must be > 0")
 
         os.environ["POLARS_FMT_STR_LEN"] = str(n)
         return cls
 
@@ -519,14 +558,30 @@
 
         Notes
         -----
         The UTF8 styles all use one or more of the semigraphic box-drawing characters
         found in the Unicode Box Drawing block, which are not ASCII compatible:
         https://en.wikipedia.org/wiki/Box-drawing_character#Box_Drawing
 
+        Examples
+        --------
+        >>> df = pl.DataFrame(
+        ...     {"abc": [-2.5, 5.0], "mno": ["hello", "world"], "xyz": [True, False]}
+        ... )
+        >>> with pl.Config(
+        ...     tbl_formatting="ASCII_MARKDOWN",
+        ...     tbl_hide_column_data_types=True,
+        ...     tbl_hide_dataframe_shape=True,
+        ... ):
+        ...     print(df)
+        | abc  | mno   | xyz   |
+        |------|-------|-------|
+        | -2.5 | hello | true  |
+        | 5.0  | world | false |
+
         Raises
         ------
         KeyError: if format string not recognised.
 
         """
         # can see what the different styles look like in the comfy-table tests:
         # https://github.com/Nukesor/comfy-table/blob/main/tests/all/presets_test.rs
```

### Comparing `polars_lts_cpu-0.17.9/polars/convert.py` & `polars_lts_cpu-0.18.0/polars/convert.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,32 +1,34 @@
 from __future__ import annotations
 
+import io
 import re
 from itertools import zip_longest
 from typing import TYPE_CHECKING, Any, Mapping, Sequence, overload
 
-from polars import internals as pli
+import polars._reexport as pl
+from polars import functions as F
 from polars.datatypes import (
     N_INFER_DEFAULT,
     Categorical,
     List,
     Object,
     Struct,
     Utf8,
 )
 from polars.dependencies import _PYARROW_AVAILABLE
 from polars.dependencies import pandas as pd
 from polars.dependencies import pyarrow as pa
 from polars.exceptions import NoDataError
+from polars.io import read_csv
 from polars.utils.various import _cast_repr_strings_with_schema, parse_version
 
 if TYPE_CHECKING:
-    from polars.dataframe import DataFrame
+    from polars import DataFrame, Series
     from polars.dependencies import numpy as np
-    from polars.series import Series
     from polars.type_aliases import Orientation, SchemaDefinition, SchemaDict
 
 
 def from_dict(
     data: Mapping[str, Sequence[object] | Mapping[str, Sequence[object]] | Series],
     schema: SchemaDefinition | None = None,
     *,
@@ -71,15 +73,15 @@
      i64  i64 
     
      1    3   
      2    4   
     
 
     """
-    return pli.DataFrame._from_dict(
+    return pl.DataFrame._from_dict(
         data, schema=schema, schema_overrides=schema_overrides
     )
 
 
 def from_dicts(
     data: Sequence[dict[str, Any]],
     schema: SchemaDefinition | None = None,
@@ -170,15 +172,15 @@
      3    6    null  null 
     
 
     """
     if not data and not (schema or schema_overrides):
         raise NoDataError("No rows. Cannot infer schema.")
 
-    return pli.DataFrame(
+    return pl.DataFrame(
         data,
         schema=schema,
         schema_overrides=schema_overrides,
         infer_schema_length=infer_schema_length,
     )
 
 
@@ -237,15 +239,15 @@
     
      1    4   
      2    5   
      3    6   
     
 
     """
-    return pli.DataFrame._from_records(
+    return pl.DataFrame._from_records(
         data,
         schema=schema,
         schema_overrides=schema_overrides,
         orient=orient,
         infer_schema_length=infer_schema_length,
     )
 
@@ -267,39 +269,58 @@
     for idx, (elem, *_) in enumerate(rows):
         if re.match(r"^\W*", elem):
             table_body_start = idx
             break
 
     # handle headers with wrapped column names and determine headers/dtypes
     header_block = ["".join(h).split("---") for h in zip(*rows[:table_body_start])]
-    headers, dtypes = (list(h) for h in zip_longest(*header_block))
+    dtypes: list[str | None]
+    if all(len(h) == 1 for h in header_block):
+        headers = [h[0] for h in header_block]
+        dtypes = [None] * len(headers)
+    else:
+        headers, dtypes = (list(h) for h in zip_longest(*header_block))
+
     body = rows[table_body_start + 1 :]
+    no_dtypes = all(d is None for d in dtypes)
 
     # transpose rows into columns, detect/omit truncated columns
     coldata = list(zip(*(row for row in body if not all((e == "") for e in row))))
     for el in ("", "..."):
         if el in headers:
             idx = headers.index(el)
             for table_elem in (headers, dtypes):
-                table_elem.pop(idx)
+                table_elem.pop(idx)  # type: ignore[attr-defined]
             if coldata:
                 coldata.pop(idx)
 
     # init cols as utf8 Series, handle "null" -> None, create schema from repr dtype
-    data = [pli.Series([(None if v == "null" else v) for v in cd]) for cd in coldata]
+    data = [pl.Series([(None if v == "null" else v) for v in cd]) for cd in coldata]
     schema = dict(zip(headers, (dtype_short_repr_to_dtype(d) for d in dtypes)))
     for dtype in set(schema.values()):
         if dtype in (List, Struct, Object):
             raise NotImplementedError(
                 f"'from_repr' does not support {dtype.base_type()} dtype"
             )
 
     # construct DataFrame from string series and cast from repr to native dtype
-    df = pli.DataFrame(data=data, orient="col", schema=list(schema))
-    return _cast_repr_strings_with_schema(df, schema)
+    df = pl.DataFrame(data=data, orient="col", schema=list(schema))
+    if no_dtypes:
+        if df.is_empty():
+            # if no dtypes *and* empty, default to string
+            return df.with_columns(F.all().cast(Utf8))
+        else:
+            # otherwise, take a trip through our CSV inference logic
+            if all(tp == Utf8 for tp in df.schema.values()):
+                buf = io.BytesIO()
+                df.write_csv(file=buf)
+                df = read_csv(buf, new_columns=df.columns, try_parse_dates=True)
+            return df
+    else:
+        return _cast_repr_strings_with_schema(df, schema)
 
 
 def _from_series_repr(m: re.Match[str]) -> Series:
     """Reconstruct a Series from a regex-matched series repr."""
     from polars.datatypes.convert import dtype_short_repr_to_dtype
 
     shape = m.groups()[0]
@@ -319,17 +340,17 @@
         elif string_values and string_values[0].lstrip("#> ") == "[":
             string_values = string_values[1:]
 
     values = string_values[:length] if length > 0 else string_values
     values = [(None if v == "null" else v) for v in values if v not in ("", "...")]
 
     if not values:
-        return pli.Series(name=name, values=values, dtype=dtype)
+        return pl.Series(name=name, values=values, dtype=dtype)
     else:
-        srs = pli.Series(name=name, values=values, dtype=Utf8)
+        srs = pl.Series(name=name, values=values, dtype=Utf8)
         if dtype is None:
             return srs
         elif dtype in (Categorical, Utf8):
             return srs.str.replace('^"(.*)"$', r"$1").cast(dtype)
 
         return _cast_repr_strings_with_schema(
             srs.to_frame(), schema={srs.name: dtype}
@@ -347,25 +368,28 @@
         to be trimmed of whitespace (or leading prompts) as the repr will be
         found/extracted automatically.
 
     Notes
     -----
     This function handles the default UTF8_FULL and UTF8_FULL_CONDENSED DataFrame
     tables (with or without rounded corners). Truncated columns/rows are omitted,
-    wrapped headers are accounted for, and dtypes identified.
+    wrapped headers are accounted for, and dtypes automatically identified.
 
-    Currently compound/nested types such as List and Struct are not supported.
+    Currently compound/nested dtypes such as List and Struct are not supported;
+    neither are Object dtypes.
 
     See Also
     --------
     polars.DataFrame.to_init_repr
     polars.Series.to_init_repr
 
     Examples
     --------
+    From DataFrame table repr:
+
     >>> df = pl.from_repr(
     ...     '''
     ...     Out[3]:
     ...     shape: (1, 5)
     ...     
     ...      source_ac  source_cha    ident  timestamp                      
     ...      tor_id     nnel_id        ---    ---                            
@@ -390,22 +414,29 @@
     
     >>> df.schema
     {'source_actor_id': Int32,
      'source_channel_id': Int64,
      'ident': Utf8,
      'timestamp': Datetime(time_unit='us', time_zone='Asia/Tokyo')}
 
-    srs = pl.from_repr'''
-    ... shape: (3,)
-    ... Series: 'a' [bool]
-    ... [
-    ...     true
-    ...     false
-    ...     true
-    ... ]
+    From Series repr:
+
+    >>> srs = pl.from_repr(
+    ...     '''
+    ...     shape: (3,)
+    ...     Series: 's' [bool]
+    ...     [
+    ...        true
+    ...        false
+    ...        true
+    ...     ]
+    ...     '''
+    ... )
+    >>> srs.to_list()
+    [True, False, True]
 
     """
     # find DataFrame table...
     m = re.search(r"([].*?[])", tbl, re.DOTALL)
     if m is not None:
         return _from_dataframe_repr(m)
 
@@ -473,15 +504,15 @@
     
      1    4   
      2    5   
      3    6   
     
 
     """
-    return pli.DataFrame._from_numpy(
+    return pl.DataFrame._from_numpy(
         data, schema=schema, orient=orient, schema_overrides=schema_overrides
     )
 
 
 # Note: we cannot @overload the typing (Series vs DataFrame) here, as pyarrow
 # does not implement any support for type hints; attempts to hint here will
 # simply result in mypy inferring "Any", which isn't useful...
@@ -561,39 +592,37 @@
         1
         2
         3
     ]
 
     """  # noqa: W505
     if isinstance(data, pa.Table):
-        return pli.DataFrame._from_arrow(
+        return pl.DataFrame._from_arrow(
             data=data, rechunk=rechunk, schema=schema, schema_overrides=schema_overrides
         )
     elif isinstance(data, (pa.Array, pa.ChunkedArray)):
         name = getattr(data, "_name", "") or ""
-        s = pli.DataFrame(
-            data=pli.Series._from_arrow(name, data, rechunk=rechunk),
+        s = pl.DataFrame(
+            data=pl.Series._from_arrow(name, data, rechunk=rechunk),
             schema=schema,
             schema_overrides=schema_overrides,
         ).to_series()
-        return (
-            s if (name or schema or schema_overrides) else s.rename("", in_place=True)
-        )
+        return s if (name or schema or schema_overrides) else s.alias("")
 
     if isinstance(data, pa.RecordBatch):
         data = [data]
     if isinstance(data, Sequence) and data and isinstance(data[0], pa.RecordBatch):
-        return pli.DataFrame._from_arrow(
+        return pl.DataFrame._from_arrow(
             data=pa.Table.from_batches(data),
             rechunk=rechunk,
             schema=schema,
             schema_overrides=schema_overrides,
         )
     elif isinstance(data, Sequence) and (schema or schema_overrides) and not data:
-        return pli.DataFrame(data=[], schema=schema, schema_overrides=schema_overrides)
+        return pl.DataFrame(data=[], schema=schema, schema_overrides=schema_overrides)
     else:
         raise ValueError(
             f"expected pyarrow Table, Array, or sequence of RecordBatches; got {type(data)}."
         )
 
 
 @overload
@@ -682,17 +711,17 @@
         1
         2
         3
     ]
 
     """  # noqa: W505
     if isinstance(data, (pd.Series, pd.DatetimeIndex)):
-        return pli.Series._from_pandas("", data, nan_to_null=nan_to_null)
+        return pl.Series._from_pandas("", data, nan_to_null=nan_to_null)
     elif isinstance(data, pd.DataFrame):
-        return pli.DataFrame._from_pandas(
+        return pl.DataFrame._from_pandas(
             data,
             rechunk=rechunk,
             nan_to_null=nan_to_null,
             schema_overrides=schema_overrides,
             include_index=include_index,
         )
     else:
@@ -720,15 +749,15 @@
     Zero-copy conversions currently cannot be guaranteed and will throw a
     ``RuntimeError``.
 
     Using a dedicated function like :func:`from_pandas` or :func:`from_arrow` is a more
     efficient method of conversion.
 
     """
-    if isinstance(df, pli.DataFrame):
+    if isinstance(df, pl.DataFrame):
         return df
     if not hasattr(df, "__dataframe__"):
         raise TypeError(
             f"`df` of type {type(df)} does not support the dataframe interchange"
             " protocol."
         )
     if not _PYARROW_AVAILABLE or parse_version(pa.__version__) < parse_version("11"):
```

### Comparing `polars_lts_cpu-0.17.9/polars/dataframe/_html.py` & `polars_lts_cpu-0.18.0/polars/dataframe/_html.py`

 * *Files 0% similar despite different names*

```diff
@@ -6,15 +6,15 @@
 from typing import TYPE_CHECKING, Iterable
 
 from polars.dependencies import html
 
 if TYPE_CHECKING:
     from types import TracebackType
 
-    from polars.dataframe import DataFrame
+    from polars import DataFrame
 
 
 class Tag:
     """Class for representing an HTML tag."""
 
     def __init__(
         self,
```

### Comparing `polars_lts_cpu-0.17.9/polars/dataframe/frame.py` & `polars_lts_cpu-0.18.0/polars/dataframe/frame.py`

 * *Files 0% similar despite different names*

```diff
@@ -10,26 +10,27 @@
 from io import BytesIO, StringIO
 from pathlib import Path
 from typing import (
     TYPE_CHECKING,
     Any,
     BinaryIO,
     Callable,
+    Collection,
     Generator,
     Iterable,
     Iterator,
     Mapping,
     NoReturn,
     Sequence,
     TypeVar,
     overload,
 )
 
+import polars._reexport as pl
 from polars import functions as F
-from polars import internals as pli
 from polars.dataframe._html import NotebookFormatter
 from polars.dataframe.groupby import DynamicGroupBy, GroupBy, RollingGroupBy
 from polars.datatypes import (
     FLOAT_DTYPES,
     INTEGER_DTYPES,
     N_INFER_DEFAULT,
     NUMERIC_DTYPES,
@@ -38,15 +39,19 @@
     Categorical,
     DataTypeClass,
     Float64,
     Int8,
     Int16,
     Int32,
     Int64,
+    List,
+    Null,
     Object,
+    Struct,
+    Time,
     UInt8,
     UInt16,
     UInt32,
     UInt64,
     Utf8,
     py_type_to_dtype,
     unpack_dtypes,
@@ -81,15 +86,15 @@
     dict_to_pydf,
     iterable_to_pydf,
     numpy_to_pydf,
     pandas_to_pydf,
     sequence_to_pydf,
     series_to_pydf,
 )
-from polars.utils._parse_expr_input import expr_to_lit_or_expr
+from polars.utils._parse_expr_input import parse_as_expression
 from polars.utils._wrap import wrap_ldf, wrap_s
 from polars.utils.convert import _timedelta_to_pl_duration
 from polars.utils.decorators import deprecated_alias
 from polars.utils.meta import get_index_type
 from polars.utils.various import (
     _prepare_row_count_args,
     _process_null_values,
@@ -109,20 +114,19 @@
 
 
 if TYPE_CHECKING:
     import sys
     from datetime import timedelta
     from io import IOBase
 
+    import deltalake
     from pyarrow.interchange.dataframe import _PyArrowDataFrame
     from xlsxwriter import Workbook
 
-    from polars.expr import Expr
-    from polars.lazyframe import LazyFrame
-    from polars.series import Series
+    from polars import Expr, LazyFrame, Series
     from polars.type_aliases import (
         AsofJoinStrategy,
         AvroCompression,
         ClosedInterval,
         ColumnTotalsDefinition,
         ComparisonOperator,
         ConditionalFormatDict,
@@ -364,15 +368,15 @@
             self._df = sequence_to_pydf(
                 data,
                 schema=schema,
                 schema_overrides=schema_overrides,
                 orient=orient,
                 infer_schema_length=infer_schema_length,
             )
-        elif isinstance(data, pli.Series):
+        elif isinstance(data, pl.Series):
             self._df = series_to_pydf(
                 data, schema=schema, schema_overrides=schema_overrides
             )
 
         elif _check_for_numpy(data) and isinstance(data, np.ndarray):
             self._df = numpy_to_pydf(
                 data,
@@ -691,15 +695,15 @@
         low_memory: bool = False,
         rechunk: bool = True,
         skip_rows_after_header: int = 0,
         row_count_name: str | None = None,
         row_count_offset: int = 0,
         sample_size: int = 1024,
         eol_char: str = "\n",
-    ) -> Self:
+    ) -> DataFrame:
         """
         Read a CSV file into a DataFrame.
 
         Use ``pl.read_csv`` to dispatch to this method.
 
         See Also
         --------
@@ -762,17 +766,17 @@
                 rechunk=rechunk,
                 skip_rows_after_header=skip_rows_after_header,
                 row_count_name=row_count_name,
                 row_count_offset=row_count_offset,
                 eol_char=eol_char,
             )
             if columns is None:
-                return self._from_pydf(scan.collect()._df)
+                return scan.collect()
             elif is_str_sequence(columns, allow_str=False):
-                return self._from_pydf(scan.select(columns).collect()._df)
+                return scan.select(columns).collect()
             else:
                 raise ValueError(
                     "cannot use glob patterns and integer based projection as `columns`"
                     " argument; Use columns: List[str]"
                 )
 
         projection, columns = handle_projection_columns(columns)
@@ -816,15 +820,15 @@
         n_rows: int | None = None,
         parallel: ParallelStrategy = "auto",
         row_count_name: str | None = None,
         row_count_offset: int = 0,
         low_memory: bool = False,
         use_statistics: bool = True,
         rechunk: bool = True,
-    ) -> Self:
+    ) -> DataFrame:
         """
         Read into a DataFrame from a parquet file.
 
         Use ``pl.read_parquet`` to dispatch to this method.
 
         See Also
         --------
@@ -846,17 +850,17 @@
                 parallel=parallel,
                 row_count_name=row_count_name,
                 row_count_offset=row_count_offset,
                 low_memory=low_memory,
             )
 
             if columns is None:
-                return cls._from_pydf(scan.collect()._df)
+                return scan.collect()
             elif is_str_sequence(columns, allow_str=False):
-                return cls._from_pydf(scan.select(columns).collect()._df)
+                return scan.select(columns).collect()
             else:
                 raise ValueError(
                     "cannot use glob patterns and integer based projection as `columns`"
                     " argument; Use columns: List[str]"
                 )
 
         projection, columns = handle_projection_columns(columns)
@@ -1258,15 +1262,15 @@
 
         return combined.select(expr)
 
     def _compare_to_non_df(
         self,
         other: Any,
         op: ComparisonOperator,
-    ) -> Self:
+    ) -> DataFrame:
         """Compare a DataFrame with a non-DataFrame object."""
         if op == "eq":
             return self.select(F.all() == other)
         elif op == "neq":
             return self.select(F.all() != other)
         elif op == "gt":
             return self.select(F.all() > other)
@@ -1275,23 +1279,23 @@
         elif op == "gt_eq":
             return self.select(F.all() >= other)
         elif op == "lt_eq":
             return self.select(F.all() <= other)
         else:
             raise ValueError(f"got unexpected comparison operator: {op}")
 
-    def _div(self, other: Any, floordiv: bool) -> Self:
-        if isinstance(other, pli.Series):
+    def _div(self, other: Any, floordiv: bool) -> DataFrame:
+        if isinstance(other, pl.Series):
             if floordiv:
                 return self.select(F.all() // lit(other))
             return self.select(F.all() / lit(other))
 
         elif not isinstance(other, DataFrame):
             s = _prepare_other_arg(other, length=len(self))
-            other = DataFrame([s.rename(f"n{i}") for i in range(len(self.columns))])
+            other = DataFrame([s.alias(f"n{i}") for i in range(len(self.columns))])
 
         orig_dtypes = other.dtypes
         other = self._cast_all_from_to(other, INTEGER_DTYPES, Float64)
         df = self._from_pydf(self._df.div_df(other._df))
 
         df = (
             df
@@ -1310,18 +1314,18 @@
 
     def _cast_all_from_to(
         self, df: DataFrame, from_: frozenset[PolarsDataType], to: PolarsDataType
     ) -> DataFrame:
         casts = [s.cast(to).alias(s.name) for s in df if s.dtype() in from_]
         return df.with_columns(casts) if casts else df
 
-    def __floordiv__(self, other: DataFrame | Series | int | float) -> Self:
+    def __floordiv__(self, other: DataFrame | Series | int | float) -> DataFrame:
         return self._div(other, floordiv=True)
 
-    def __truediv__(self, other: DataFrame | Series | int | float) -> Self:
+    def __truediv__(self, other: DataFrame | Series | int | float) -> DataFrame:
         return self._div(other, floordiv=False)
 
     def __bool__(self) -> NoReturn:
         raise ValueError(
             "The truth value of a DataFrame is ambiguous. "
             "Hint: to check if a DataFrame contains any values, use 'is_empty()'"
         )
@@ -1356,21 +1360,25 @@
 
         other = _prepare_other_arg(other)
         return self._from_pydf(self._df.mul(other._s))
 
     def __rmul__(self, other: DataFrame | Series | int | float) -> Self:
         return self * other
 
-    def __add__(self, other: DataFrame | Series | int | float | bool | str) -> Self:
+    def __add__(
+        self, other: DataFrame | Series | int | float | bool | str
+    ) -> DataFrame:
         if isinstance(other, DataFrame):
             return self._from_pydf(self._df.add_df(other._df))
         other = _prepare_other_arg(other)
         return self._from_pydf(self._df.add(other._s))
 
-    def __radd__(self, other: DataFrame | Series | int | float | bool | str) -> Self:
+    def __radd__(  # type: ignore[misc]
+        self, other: DataFrame | Series | int | float | bool | str
+    ) -> DataFrame:
         if isinstance(other, str):
             return self.select((lit(other) + F.col("*")).keep_name())
         return self + other
 
     def __sub__(self, other: DataFrame | Series | int | float) -> Self:
         if isinstance(other, DataFrame):
             return self._from_pydf(self._df.sub_df(other._df))
@@ -1401,15 +1409,15 @@
         else:
             return self.shape[dim] + idx
 
     def _pos_idxs(self, idxs: np.ndarray[Any, Any] | Series, dim: int) -> Series:
         # pl.UInt32 (polars) or pl.UInt64 (polars_u64_idx).
         idx_type = get_index_type()
 
-        if isinstance(idxs, pli.Series):
+        if isinstance(idxs, pl.Series):
             if idxs.dtype == idx_type:
                 return idxs
             if idxs.dtype in {
                 UInt8,
                 UInt16,
                 UInt64 if idx_type == UInt32 else UInt32,
                 Int8,
@@ -1465,15 +1473,15 @@
                     else:
                         if idxs.dtype in (np.int8, np.int16, np.int32):
                             idxs = idxs.astype(np.int64)
 
                     # Update negative indexes to absolute indexes.
                     idxs = np.where(idxs < 0, self.shape[dim] + idxs, idxs)
 
-                return pli.Series("", idxs, dtype=idx_type)
+                return pl.Series("", idxs, dtype=idx_type)
 
         raise NotImplementedError("Unsupported idxs datatype.")
 
     @overload
     def __getitem__(self, item: str) -> Series:
         ...
 
@@ -1546,15 +1554,15 @@
                     col_selection = slice(start, stop, col_selection.step)
 
                     df = self.__getitem__(self.columns[col_selection])
                     return df[row_selection]
 
                 # df[:, [True, False]]
                 if is_bool_sequence(col_selection) or (
-                    isinstance(col_selection, pli.Series)
+                    isinstance(col_selection, pl.Series)
                     and col_selection.dtype == Boolean
                 ):
                     if len(col_selection) != self.width:
                         raise ValueError(
                             f"Expected {self.width} values when selecting columns by"
                             f" boolean mask. Got {len(col_selection)}."
                         )
@@ -1640,17 +1648,17 @@
                 return self._from_pydf(self._df.select(item))
 
         if is_str_sequence(item, allow_str=False):
             # select multiple columns
             # df[["foo", "bar"]]
             return self._from_pydf(self._df.select(item))
         elif is_int_sequence(item):
-            item = pli.Series("", item)  # fall through to next if isinstance
+            item = pl.Series("", item)  # fall through to next if isinstance
 
-        if isinstance(item, pli.Series):
+        if isinstance(item, pl.Series):
             dtype = item.dtype
             if dtype == Utf8:
                 return self._from_pydf(self._df.select(item))
             elif dtype == UInt32:
                 return self._from_pydf(self._df.take_with_series(item._s))
             elif dtype in INTEGER_DTYPES:
                 return self._from_pydf(
@@ -1686,23 +1694,23 @@
                     "matrix columns should be equal to list use to determine column"
                     " names"
                 )
 
             # todo! we can parallelize this by calling from_numpy
             columns = []
             for i, name in enumerate(key):
-                columns.append(pli.Series(name, value[:, i]))
+                columns.append(pl.Series(name, value[:, i]))
             self._df = self.with_columns(columns)._df
 
         # df[a, b]
         elif isinstance(key, tuple):
             row_selection, col_selection = key
 
             if (
-                isinstance(row_selection, pli.Series) and row_selection.dtype == Boolean
+                isinstance(row_selection, pl.Series) and row_selection.dtype == Boolean
             ) or is_bool_sequence(row_selection):
                 raise ValueError(
                     "Not allowed to set 'DataFrame' by boolean mask in the "
                     "row position. Consider using 'DataFrame.with_columns'"
                 )
 
             # get series column selection
@@ -1960,42 +1968,89 @@
         >>> df = pl.DataFrame({"foo": [1, 2, 3], "bar": [4, 5, 6]})
         >>> df.to_dicts()
         [{'foo': 1, 'bar': 4}, {'foo': 2, 'bar': 5}, {'foo': 3, 'bar': 6}]
 
         """
         return list(self.iter_rows(named=True))
 
-    def to_numpy(self) -> np.ndarray[Any, Any]:
+    def to_numpy(self, structured: bool = False) -> np.ndarray[Any, Any]:
         """
         Convert DataFrame to a 2D NumPy array.
 
         This operation clones data.
 
+        Parameters
+        ----------
+        structured
+            Optionally return a structured array, with field names and
+            dtypes that correspond to the DataFrame schema.
+
         Notes
         -----
         If you're attempting to convert Utf8 to an array you'll need to install
         ``pyarrow``.
 
         Examples
         --------
         >>> df = pl.DataFrame(
-        ...     {"foo": [1, 2, 3], "bar": [6, 7, 8], "ham": ["a", "b", "c"]}
+        ...     {
+        ...         "foo": [1, 2, 3],
+        ...         "bar": [6.5, 7.0, 8.5],
+        ...         "ham": ["a", "b", "c"],
+        ...     },
+        ...     schema_overrides={"foo": pl.UInt8, "bar": pl.Float32},
         ... )
-        >>> numpy_array = df.to_numpy()
-        >>> type(numpy_array)
-        <class 'numpy.ndarray'>
+
+        Export to a standard 2D numpy array.
+
+        >>> df.to_numpy()
+        array([[1, 6.5, 'a'],
+               [2, 7.0, 'b'],
+               [3, 8.5, 'c']], dtype=object)
+
+        Export to a structured array, which can better-preserve individual
+        column data, such as name and dtype...
+
+        >>> df.to_numpy(structured=True)
+        array([(1, 6.5, 'a'), (2, 7. , 'b'), (3, 8.5, 'c')],
+              dtype=[('foo', 'u1'), ('bar', '<f4'), ('ham', '<U1')])
+
+        ...optionally zero-copying as a record array view:
+
+        >>> import numpy as np
+        >>> df.to_numpy(True).view(np.recarray)
+        rec.array([(1, 6.5, 'a'), (2, 7. , 'b'), (3, 8.5, 'c')],
+                  dtype=[('foo', 'u1'), ('bar', '<f4'), ('ham', '<U1')])
 
         """
-        out = self._df.to_numpy()
-        if out is None:
-            return np.vstack(
-                [self.to_series(i).to_numpy() for i in range(self.width)]
-            ).T
+        if structured:
+            # see: https://numpy.org/doc/stable/user/basics.rec.html
+            arrays = []
+            for c, tp in self.schema.items():
+                s = self[c]
+                a = s.to_numpy()
+                arrays.append(
+                    a.astype(str, copy=False)
+                    if tp == Utf8 and not s.has_validity()
+                    else a
+                )
+
+            out = np.empty(
+                len(self), dtype=list(zip(self.columns, (a.dtype for a in arrays)))
+            )
+            for idx, c in enumerate(self.columns):
+                out[c] = arrays[idx]
         else:
-            return out
+            out = self._df.to_numpy()
+            if out is None:
+                return np.vstack(
+                    [self.to_series(i).to_numpy() for i in range(self.width)]
+                ).T
+
+        return out
 
     def to_pandas(  # noqa: D417
         self,
         *args: Any,
         use_pyarrow_extension_array: bool = False,
         **kwargs: Any,
     ) -> pd.DataFrame:
@@ -3044,19 +3099,15 @@
             - "gzip" : min-level: 0, max-level: 10.
             - "brotli" : min-level: 0, max-level: 11.
             - "zstd" : min-level: 1, max-level: 22.
 
         statistics
             Write statistics to the parquet headers. This requires extra compute.
         row_group_size
-            Size of the row groups in number of rows.
-            If None (default), the chunks of the `DataFrame` are
-            used. Writing in smaller chunks may reduce memory pressure and improve
-            writing speeds. If None and ``use_pyarrow=True``, the row group size
-            will be the minimum of the DataFrame size and 64 * 1024 * 1024.
+            Size of the row groups in number of rows. Defaults to 512^2 rows.
         use_pyarrow
             Use C++ parquet implementation vs Rust parquet implementation.
             At the moment C++ supports more features.
         pyarrow_options
             Arguments passed to ``pyarrow.parquet.write_table``.
 
         Examples
@@ -3174,14 +3225,155 @@
             self.to_pandas(use_pyarrow_extension_array=True).to_sql(
                 name=table_name, con=engine, if_exists=if_exists, index=False
             )
 
         else:
             raise ValueError(f"'engine' {engine} is not supported.")
 
+    def write_delta(
+        self,
+        target: str | Path | deltalake.DeltaTable,
+        *,
+        mode: Literal["error", "append", "overwrite", "ignore"] = "error",
+        overwrite_schema: bool = False,
+        storage_options: dict[str, str] | None = None,
+        delta_write_options: dict[str, Any] | None = None,
+    ) -> None:
+        """
+        Write DataFrame as delta table.
+
+        Note: Some polars data types like `Null`, `Categorical` and `Time` are
+        not supported by the delta protocol specification.
+
+        Parameters
+        ----------
+        target
+            URI of a table or a DeltaTable object.
+        mode : {'error', 'append', 'overwrite', 'ignore'}
+            How to handle existing data.
+
+            * If 'error', throw an error if the table already exists (default).
+            * If 'append', will add new data.
+            * If 'overwrite', will replace table with new data.
+            * If 'ignore', will not write anything if table already exists.
+        overwrite_schema
+            If True, allows updating the schema of the table.
+        storage_options
+            Extra options for the storage backends supported by `deltalake`.
+            For cloud storages, this may include configurations for authentication etc.
+
+            * See a list of supported storage options for S3 `here <https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html#variants>`__.
+            * See a list of supported storage options for GCS `here <https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html#variants>`__.
+            * See a list of supported storage options for Azure `here <https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html#variants>`__.
+        delta_write_options
+            Additional keyword arguments while writing a Delta lake Table.
+            See a list of supported write options `here <https://github.com/delta-io/delta-rs/blob/395d48b47ea638b70415899dc035cc895b220e55/python/deltalake/writer.py#L65>`__.
+
+        Examples
+        --------
+        Instantiate a basic dataframe:
+
+        >>> df = pl.DataFrame(
+        ...     {
+        ...         "foo": [1, 2, 3, 4, 5],
+        ...         "bar": [6, 7, 8, 9, 10],
+        ...         "ham": ["a", "b", "c", "d", "e"],
+        ...     }
+        ... )
+
+        Write DataFrame as a Delta Lake table on local filesystem.
+
+        >>> table_path = "/path/to/delta-table/"
+        >>> df.write_delta(table_path)  # doctest: +SKIP
+
+        Append data to an existing Delta Lake table on local filesystem.
+        Note: This will fail if schema of the new data does not match the
+        schema of existing table.
+
+        >>> df.write_delta(table_path, mode="append")  # doctest: +SKIP
+
+        Overwrite a Delta Lake table as a new version.
+        Note: If the schema of the new and old data is same,
+        then setting `overwrite_schema` is not required.
+
+        >>> existing_table_path = "/path/to/delta-table/"
+        >>> df.write_delta(
+        ...     existing_table_path, mode="overwrite", overwrite_schema=True
+        ... )  # doctest: +SKIP
+
+        Write DataFrame as a Delta Lake table on cloud object store like S3.
+
+        >>> table_path = "s3://bucket/prefix/to/delta-table/"
+        >>> df.write_delta(
+        ...     table_path,
+        ...     storage_options={
+        ...         "AWS_REGION": "THE_AWS_REGION",
+        ...         "AWS_ACCESS_KEY_ID": "THE_AWS_ACCESS_KEY_ID",
+        ...         "AWS_SECRET_ACCESS_KEY": "THE_AWS_SECRET_ACCESS_KEY",
+        ...     },
+        ... )  # doctest: +SKIP
+
+        """
+        from polars.io.delta import check_if_delta_available, resolve_delta_lake_uri
+
+        check_if_delta_available()
+
+        from deltalake.writer import (  # type: ignore[import]
+            try_get_deltatable,
+            write_deltalake,
+        )
+
+        if delta_write_options is None:
+            delta_write_options = {}
+
+        if isinstance(target, (str, Path)):
+            target = resolve_delta_lake_uri(str(target), strict=False)
+
+        unsupported_cols = {}
+        unsupported_types = [Time, Categorical, Null]
+
+        def check_unsupported_types(n: str, t: PolarsDataType | None) -> None:
+            if t is None or t in unsupported_types:
+                unsupported_cols[n] = t
+            elif isinstance(t, Struct):
+                for i in t.fields:
+                    check_unsupported_types(f"{n}.{i.name}", i.dtype)
+            elif isinstance(t, List):
+                check_unsupported_types(n, t.inner)
+
+        for name, data_type in self.schema.items():
+            check_unsupported_types(name, data_type)
+
+        if len(unsupported_cols) != 0:
+            raise TypeError(
+                f"Column(s) in {unsupported_cols} have unsupported data types."
+            )
+
+        data = self.to_arrow()
+        data_schema = data.schema
+
+        # Workaround to prevent manual casting of large types
+        table = try_get_deltatable(target, storage_options)
+
+        if table is not None:
+            table_schema = table.schema()
+
+            if data_schema == table_schema.to_pyarrow(as_large_types=True):
+                data_schema = table_schema.to_pyarrow()
+
+        write_deltalake(
+            table_or_uri=target,
+            data=data,
+            mode=mode,
+            schema=data_schema,
+            overwrite_schema=overwrite_schema,
+            storage_options=storage_options,
+            **delta_write_options,
+        )
+
     def estimated_size(self, unit: SizeUnit = "b") -> int | float:
         """
         Return an estimation of the total (heap) allocated size of the `DataFrame`.
 
         Estimated size is given in the specified unit (bytes by default).
 
         This estimation is the sum of the size of its buffers, validity, including
@@ -3220,28 +3412,28 @@
         return scale_bytes(sz, unit)
 
     def transpose(
         self,
         *,
         include_header: bool = False,
         header_name: str = "column",
-        column_names: Iterator[str] | Sequence[str] | None = None,
+        column_names: Iterable[str] | None = None,
     ) -> Self:
         """
         Transpose a DataFrame over the diagonal.
 
         Parameters
         ----------
         include_header
             If set, the column names will be added as first column.
         header_name
             If `include_header` is set, this determines the name of the column that will
             be inserted.
         column_names
-            Optional generator/iterator that yields column names. Will be used to
+            Optional iterable that yields column names. Will be used to
             replace the columns in the DataFrame.
 
         Notes
         -----
         This is a very expensive operation. Perhaps you can do it differently.
 
         Returns
@@ -3321,15 +3513,15 @@
 
             column_names = iter(column_names)
             for _ in range(n):
                 names.append(next(column_names))
             df.columns = names
         return df
 
-    def reverse(self) -> Self:
+    def reverse(self) -> DataFrame:
         """
         Reverse the DataFrame.
 
         Examples
         --------
         >>> df = pl.DataFrame(
         ...     {
@@ -3348,15 +3540,15 @@
          b    2   
          a    1   
         
 
         """
         return self.select(F.col("*").reverse())
 
-    def rename(self, mapping: dict[str, str]) -> Self:
+    def rename(self, mapping: dict[str, str]) -> DataFrame:
         """
         Rename column names.
 
         Parameters
         ----------
         mapping
             Key value pairs that map from old name to new name.
@@ -3375,17 +3567,15 @@
         
          1      6    a   
          2      7    b   
          3      8    c   
         
 
         """
-        return self._from_pydf(
-            self.lazy().rename(mapping).collect(no_optimization=True)._df
-        )
+        return self.lazy().rename(mapping).collect(no_optimization=True)
 
     def insert_at_idx(self, index: int, series: Series) -> Self:
         """
         Insert a Series at a certain column index. This operation is in place.
 
         Parameters
         ----------
@@ -3436,15 +3626,15 @@
             index = len(self.columns) + index
         self._df.insert_at_idx(index, series._s)
         return self
 
     def filter(
         self,
         predicate: (Expr | str | Series | list[bool] | np.ndarray[Any, Any] | bool),
-    ) -> Self:
+    ) -> DataFrame:
         """
         Filter the rows in the DataFrame based on a predicate expression.
 
         Parameters
         ----------
         predicate
             Expression that evaluates to a boolean Series.
@@ -3495,21 +3685,20 @@
         
          1    6    a   
          3    8    c   
         
 
         """
         if _check_for_numpy(predicate) and isinstance(predicate, np.ndarray):
-            predicate = pli.Series(predicate)
+            predicate = pl.Series(predicate)
 
-        return self._from_pydf(
+        return (
             self.lazy()
             .filter(predicate)  # type: ignore[arg-type]
             .collect(no_optimization=True)
-            ._df
         )
 
     @overload
     def glimpse(self, *, return_as_string: Literal[False]) -> None:
         ...
 
     @overload
@@ -3673,32 +3862,34 @@
             F.all().std().prefix("std:"),
             F.all().min().prefix("min:"),
             F.all().max().prefix("max:"),
             F.all().median().prefix("median:"),
             *percentile_exprs,
         ).row(0)
 
-        # reshape/cast wide result
+        # reshape wide result
         n_cols = len(self.columns)
         described = [
             df_metrics[(n * n_cols) : (n + 1) * n_cols] for n in range(0, len(metrics))
         ]
+
+        # cast by column type (numeric/bool -> float), (other -> string)
         summary = dict(zip(self.columns, list(zip(*described))))
         num_or_bool = NUMERIC_DTYPES | {Boolean}
         for c, tp in self.schema.items():
             summary[c] = [
                 None
                 if (v is None or isinstance(v, dict))
                 else (float(v) if tp in num_or_bool else str(v))
                 for v in summary[c]
             ]
 
-        # return as frame
+        # return results as a frame
         df_summary = self.__class__(summary)
-        df_summary.insert_at_idx(0, pli.Series("describe", metrics))
+        df_summary.insert_at_idx(0, pl.Series("describe", metrics))
         return df_summary
 
     def find_idx_by_name(self, name: str) -> int:
         """
         Find the index of a column by name.
 
         Parameters
@@ -3758,15 +3949,15 @@
 
     def sort(
         self,
         by: IntoExpr | Iterable[IntoExpr],
         *more_by: IntoExpr,
         descending: bool | Sequence[bool] = False,
         nulls_last: bool = False,
-    ) -> Self:
+    ) -> DataFrame:
         """
         Sort the dataframe by the given columns.
 
         Parameters
         ----------
         by
             Column(s) to sort by. Accepts expression input. Strings are parsed as column
@@ -3841,29 +4032,28 @@
         
          1     6.0  a   
          null  4.0  b   
          2     5.0  c   
         
 
         """
-        return self._from_pydf(
+        return (
             self.lazy()
             .sort(by, *more_by, descending=descending, nulls_last=nulls_last)
             .collect(no_optimization=True)
-            ._df
         )
 
     def top_k(
         self,
         k: int,
         *,
         by: IntoExpr | Iterable[IntoExpr],
         descending: bool | Sequence[bool] = False,
         nulls_last: bool = False,
-    ) -> Self:
+    ) -> DataFrame:
         """
         Return the `k` largest elements.
 
         If 'descending=True` the smallest elements will be given.
 
         Parameters
         ----------
@@ -3918,34 +4108,33 @@
          b    3   
          b    2   
          a    2   
          c    1   
         
 
         """
-        return self._from_pydf(
+        return (
             self.lazy()
             .top_k(k, by=by, descending=descending, nulls_last=nulls_last)
             .collect(
                 projection_pushdown=False,
                 predicate_pushdown=False,
                 common_subplan_elimination=False,
                 slice_pushdown=True,
             )
-            ._df
         )
 
     def bottom_k(
         self,
         k: int,
         *,
         by: IntoExpr | Iterable[IntoExpr],
         descending: bool | Sequence[bool] = False,
         nulls_last: bool = False,
-    ) -> Self:
+    ) -> DataFrame:
         """
         Return the `k` smallest elements.
 
         If 'descending=True` the largest elements will be given.
 
         Parameters
         ----------
@@ -4000,24 +4189,23 @@
          a    1   
          a    2   
          b    1   
          b    2   
         
 
         """
-        return self._from_pydf(
+        return (
             self.lazy()
             .bottom_k(k, by=by, descending=descending, nulls_last=nulls_last)
             .collect(
                 projection_pushdown=False,
                 predicate_pushdown=False,
                 common_subplan_elimination=False,
                 slice_pushdown=True,
             )
-            ._df
         )
 
     def frame_equal(self, other: DataFrame, *, null_equal: bool = True) -> bool:
         """
         Check if DataFrame is equal to other.
 
         Parameters
@@ -4240,15 +4428,15 @@
         See Also
         --------
         head
 
         """
         return self.head(n)
 
-    def drop_nulls(self, subset: str | Sequence[str] | None = None) -> Self:
+    def drop_nulls(self, subset: str | Collection[str] | None = None) -> DataFrame:
         """
         Drop all rows that contain null values.
 
         Returns a new DataFrame.
 
         Parameters
         ----------
@@ -4327,17 +4515,15 @@
          1     1    
          2     null 
          null  null 
          1     1    
         
 
         """
-        return self._from_pydf(
-            self.lazy().drop_nulls(subset).collect(no_optimization=True)._df
-        )
+        return self.lazy().drop_nulls(subset).collect(no_optimization=True)
 
     def pipe(
         self,
         function: Callable[Concatenate[DataFrame, P], T],
         *args: P.args,
         **kwargs: P.kwargs,
     ) -> T:
@@ -4439,15 +4625,15 @@
         return self._from_pydf(self._df.with_row_count(name, offset))
 
     def groupby(
         self,
         by: IntoExpr | Iterable[IntoExpr],
         *more_by: IntoExpr,
         maintain_order: bool = False,
-    ) -> GroupBy[Self]:
+    ) -> GroupBy:
         """
         Start a groupby operation.
 
         Parameters
         ----------
         by
             Column(s) to group by. Accepts expression input. Strings are parsed as
@@ -4567,21 +4753,22 @@
         
 
         """
         return GroupBy(self, by, *more_by, maintain_order=maintain_order)
 
     def groupby_rolling(
         self,
-        index_column: str,
+        index_column: IntoExpr,
         *,
         period: str | timedelta,
         offset: str | timedelta | None = None,
         closed: ClosedInterval = "right",
         by: IntoExpr | Iterable[IntoExpr] | None = None,
-    ) -> RollingGroupBy[Self]:
+        check_sorted: bool = True,
+    ) -> RollingGroupBy:
         """
         Create rolling groups based on a time column.
 
         Also works for index values of type Int32 or Int64.
 
         Different from a ``dynamic_groupby`` the windows are now determined by the
         individual values and are not of constant intervals. For constant intervals use
@@ -4629,14 +4816,27 @@
             length of the window
         offset
             offset of the window. Default is -period
         closed : {'right', 'left', 'both', 'none'}
             Define which sides of the temporal interval are closed (inclusive).
         by
             Also group by this column/these columns
+        check_sorted
+            When the ``by`` argument is given, polars can not check sortedness
+            by the metadata and has to do a full scan on the index column to
+            verify data is sorted. This is expensive. If you are sure the
+            data within the by groups is sorted, you can set this to ``False``.
+            Doing so incorrectly will lead to incorrect output
+
+        Returns
+        -------
+        RollingGroupBy
+            Object you can call ``.agg`` on to aggregate by groups, the result
+            of which will be sorted by `index_column` (but note that if `by` columns are
+            passed, it will only be sorted within each `by` group).
 
         See Also
         --------
         groupby_dynamic
 
         Examples
         --------
@@ -4673,29 +4873,32 @@
          2020-01-01 16:45:09  15     3      7     
          2020-01-02 18:12:48  24     3      9     
          2020-01-03 19:45:32  11     2      9     
          2020-01-08 23:16:43  1      1      1     
         
 
         """
-        return RollingGroupBy(self, index_column, period, offset, closed, by)
+        return RollingGroupBy(
+            self, index_column, period, offset, closed, by, check_sorted
+        )
 
     def groupby_dynamic(
         self,
-        index_column: str,
+        index_column: IntoExpr,
         *,
         every: str | timedelta,
         period: str | timedelta | None = None,
         offset: str | timedelta | None = None,
         truncate: bool = True,
         include_boundaries: bool = False,
         closed: ClosedInterval = "left",
         by: IntoExpr | Iterable[IntoExpr] | None = None,
         start_by: StartBy = "window",
-    ) -> DynamicGroupBy[Self]:
+        check_sorted: bool = True,
+    ) -> DynamicGroupBy:
         """
         Group based on a time value (or index value of type Int32, Int64).
 
         Time windows are calculated and rows are assigned to windows. Different from a
         normal groupby is that a row can be member of multiple groups. The time/index
         window could be seen as a rolling window, with a window size determined by
         dates/times/values instead of slots in the DataFrame.
@@ -4760,31 +4963,48 @@
             Add the lower and upper bound of the window to the "_lower_bound" and
             "_upper_bound" columns. This will impact performance because it's harder to
             parallelize
         closed : {'left', 'right', 'both', 'none'}
             Define which sides of the temporal interval are closed (inclusive).
         by
             Also group by this column/these columns
-        start_by : {'window', 'datapoint', 'monday'}
+        start_by : {'window', 'datapoint', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday'}
             The strategy to determine the start of the first window by.
 
             - 'window': Truncate the start of the window with the 'every' argument.
             - 'datapoint': Start from the first encountered data point.
             - 'monday': Start the window on the monday before the first data point.
+            - 'tuesday': Start the window on the tuesday before the first data point.
+            - ...
+            - 'sunday': Start the window on the sunday before the first data point.
+        check_sorted
+            When the ``by`` argument is given, polars can not check sortedness
+            by the metadata and has to do a full scan on the index column to
+            verify data is sorted. This is expensive. If you are sure the
+            data within the by groups is sorted, you can set this to ``False``.
+            Doing so incorrectly will lead to incorrect output
+
+        Returns
+        -------
+        DynamicGroupBy
+            Object you can call ``.agg`` on to aggregate by groups, the result
+            of which will be sorted by `index_column` (but note that if `by` columns are
+            passed, it will only be sorted within each `by` group).
 
         Examples
         --------
         >>> from datetime import datetime
         >>> # create an example dataframe
         >>> df = pl.DataFrame(
         ...     {
         ...         "time": pl.date_range(
         ...             start=datetime(2021, 12, 16),
         ...             end=datetime(2021, 12, 16, 3),
         ...             interval="30m",
+        ...             eager=True,
         ...         ),
         ...         "n": range(7),
         ...     }
         ... )
         >>> df
         shape: (7, 2)
         
@@ -4881,14 +5101,15 @@
 
         >>> df = pl.DataFrame(
         ...     {
         ...         "time": pl.date_range(
         ...             start=datetime(2021, 12, 16),
         ...             end=datetime(2021, 12, 16, 3),
         ...             interval="30m",
+        ...             eager=True,
         ...         ),
         ...         "groups": ["a", "a", "a", "b", "b", "a", "a"],
         ...     }
         ... )
         >>> df
         shape: (7, 2)
         
@@ -4962,14 +5183,15 @@
             period,
             offset,
             truncate,
             include_boundaries,
             closed,
             by,
             start_by,
+            check_sorted,
         )
 
     def upsample(
         self,
         time_column: str,
         *,
         every: str | timedelta,
@@ -5012,14 +5234,20 @@
         Or combine them:
         "3d12h4m25s" # 3 days, 12 hours, 4 minutes, and 25 seconds
 
         Suffix with `"_saturating"` to indicate that dates too large for
         their month should saturate at the largest date (e.g. 2022-02-29 -> 2022-02-28)
         instead of erroring.
 
+        Returns
+        -------
+        DataFrame
+            Result will be sorted by `time_column` (but note that if `by` columns are
+            passed, it will only be sorted within each `by` group).
+
         Examples
         --------
         Upsample a DataFrame by a certain interval.
 
         >>> from datetime import datetime
         >>> df = pl.DataFrame(
         ...     {
@@ -5077,15 +5305,15 @@
         by_right: str | Sequence[str] | None = None,
         by: str | Sequence[str] | None = None,
         strategy: AsofJoinStrategy = "backward",
         suffix: str = "_right",
         tolerance: str | int | float | None = None,
         allow_parallel: bool = True,
         force_parallel: bool = False,
-    ) -> Self:
+    ) -> DataFrame:
         """
         Perform an asof join.
 
         This is similar to a left-join except that we match on nearest key rather than
         equal keys.
 
         Both DataFrames must be sorted by the asof_join key.
@@ -5094,14 +5322,17 @@
 
           - A "backward" search selects the last row in the right DataFrame whose
             'on' key is less than or equal to the left's key.
 
           - A "forward" search selects the first row in the right DataFrame whose
             'on' key is greater than or equal to the left's key.
 
+          - A "nearest" search selects the last row in the right DataFrame whose value
+            is nearest to the left's key.
+
         The default is "backward".
 
         Parameters
         ----------
         other
             Lazy DataFrame to join with.
         left_on
@@ -5113,22 +5344,22 @@
             None.
         by
             join on these columns before doing asof join
         by_left
             join on these columns before doing asof join
         by_right
             join on these columns before doing asof join
-        strategy : {'backward', 'forward'}
+        strategy : {'backward', 'forward', 'nearest'}
             Join strategy.
         suffix
             Suffix to append to columns with a duplicate name.
         tolerance
             Numeric tolerance. By setting this the join will only be done if the near
             keys are within this distance. If an asof join is done on columns of dtype
-            "Date", "Datetime", "Duration" or "Time" you use the following string
+            "Date", "Datetime", "Duration" or "Time", use the following string
             language:
 
                 - 1ns   (1 nanosecond)
                 - 1us   (1 microsecond)
                 - 1ms   (1 millisecond)
                 - 1s    (1 second)
                 - 1m    (1 minute)
@@ -5193,15 +5424,15 @@
 
         """
         if not isinstance(other, DataFrame):
             raise TypeError(
                 f"Expected 'other' join table to be a DataFrame, not a {type(other).__name__}"
             )
 
-        return self._from_pydf(
+        return (
             self.lazy()
             .join_asof(
                 other.lazy(),
                 left_on=left_on,
                 right_on=right_on,
                 on=on,
                 by_left=by_left,
@@ -5210,27 +5441,26 @@
                 strategy=strategy,
                 suffix=suffix,
                 tolerance=tolerance,
                 allow_parallel=allow_parallel,
                 force_parallel=force_parallel,
             )
             .collect(no_optimization=True)
-            ._df
         )
 
     def join(
         self,
         other: DataFrame,
         on: str | Expr | Sequence[str | Expr] | None = None,
         how: JoinStrategy = "inner",
         *,
         left_on: str | Expr | Sequence[str | Expr] | None = None,
         right_on: str | Expr | Sequence[str | Expr] | None = None,
         suffix: str = "_right",
-    ) -> Self:
+    ) -> DataFrame:
         """
         Join in SQL-like fashion.
 
         Parameters
         ----------
         other
             DataFrame to join with.
@@ -5331,35 +5561,34 @@
 
         """
         if not isinstance(other, DataFrame):
             raise TypeError(
                 f"Expected 'other' join table to be a DataFrame, not a {type(other).__name__}"
             )
 
-        return self._from_pydf(
+        return (
             self.lazy()
             .join(
                 other=other.lazy(),
                 left_on=left_on,
                 right_on=right_on,
                 on=on,
                 how=how,
                 suffix=suffix,
             )
             .collect(no_optimization=True)
-            ._df
         )
 
     def apply(
         self,
         function: Callable[[tuple[Any, ...]], Any],
         return_dtype: PolarsDataType | None = None,
         *,
         inference_size: int = 256,
-    ) -> Self:
+    ) -> DataFrame:
         """
         Apply a custom/user-defined function (UDF) over the rows of the DataFrame.
 
         The UDF will receive each row as a tuple of values: ``udf(row)``.
 
         Implementing logic using a Python function is almost always _significantly_
         slower and more memory intensive than implementing the same logic using
@@ -5410,15 +5639,18 @@
          2         -3       
          4         15       
          6         24       
         
 
         It is better to implement this with an expression:
 
-        >>> df.select([pl.col("foo") * 2, pl.col("bar") * 3])  # doctest: +IGNORE_RESULT
+        >>> df.select(
+        ...     pl.col("foo") * 2,
+        ...     pl.col("bar") * 3,
+        ... )  # doctest: +IGNORE_RESULT
 
         Return a Series by mapping each row to a scalar:
 
         >>> df.apply(lambda t: (t[0] * 2 + t[1]))
         shape: (3, 1)
         
          apply 
@@ -5435,15 +5667,15 @@
         >>> df.select(pl.col("foo") * 2 + pl.col("bar"))  # doctest: +IGNORE_RESULT
 
         """
         out, is_df = self._df.apply(function, return_dtype, inference_size)
         if is_df:
             return self._from_pydf(out)
         else:
-            return self._from_pydf(wrap_s(out).to_frame()._df)
+            return wrap_s(out).to_frame()
 
     def hstack(
         self, columns: list[Series] | DataFrame, *, in_place: bool = False
     ) -> Self:
         """
         Return a new DataFrame grown horizontally by stacking multiple Series to it.
 
@@ -5575,15 +5807,15 @@
          30   60  
         
 
         """
         self._df.extend(other._df)
         return self
 
-    def drop(self, columns: str | Sequence[str], *more_columns: str) -> Self:
+    def drop(self, columns: str | Collection[str], *more_columns: str) -> DataFrame:
         """
         Remove columns from the dataframe.
 
         Parameters
         ----------
         columns
             Name of the column(s) that should be removed from the dataframe.
@@ -5638,17 +5870,15 @@
         
          a   
          b   
          c   
         
 
         """
-        return self._from_pydf(
-            self.lazy().drop(columns, *more_columns).collect(no_optimization=True)._df
-        )
+        return self.lazy().drop(columns, *more_columns).collect(no_optimization=True)
 
     def drop_in_place(self, name: str) -> Series:
         """
         Drop a single column in-place and return the dropped column.
 
         Parameters
         ----------
@@ -5727,15 +5957,15 @@
         """
         # faster path
         if n == 0:
             return self._from_pydf(self._df.clear())
         if n > 0 or len(self) > 0:
             return self.__class__(
                 {
-                    nm: pli.Series(name=nm, dtype=tp).extend_constant(None, n)
+                    nm: pl.Series(name=nm, dtype=tp).extend_constant(None, n)
                     for nm, tp in self.schema.items()
                 }
             )
         return self.clone()
 
     def clone(self) -> Self:
         """
@@ -5862,15 +6092,15 @@
     def fill_null(
         self,
         value: Any | None = None,
         strategy: FillNullStrategy | None = None,
         limit: int | None = None,
         *,
         matches_supertype: bool = True,
-    ) -> Self:
+    ) -> DataFrame:
         """
         Fill null values using the specified value or strategy.
 
         Parameters
         ----------
         value
             Value used to fill null values.
@@ -5946,22 +6176,21 @@
          1    0.5  
          2    4.0  
          0    0.0  
          4    13.0 
         
 
         """
-        return self._from_pydf(
+        return (
             self.lazy()
             .fill_null(value, strategy, limit, matches_supertype=matches_supertype)
             .collect(no_optimization=True)
-            ._df
         )
 
-    def fill_nan(self, value: Expr | int | float | None) -> Self:
+    def fill_nan(self, value: Expr | int | float | None) -> DataFrame:
         """
         Fill floating point NaN values by an Expression evaluation.
 
         Parameters
         ----------
         value
             Value to fill NaN with.
@@ -5997,23 +6226,21 @@
          1.5   0.5  
          2.0   4.0  
          99.0  99.0 
          4.0   13.0 
         
 
         """
-        return self._from_pydf(
-            self.lazy().fill_nan(value).collect(no_optimization=True)._df
-        )
+        return self.lazy().fill_nan(value).collect(no_optimization=True)
 
     def explode(
         self,
         columns: str | Sequence[str] | Expr | Sequence[Expr],
         *more_columns: str | Expr,
-    ) -> Self:
+    ) -> DataFrame:
         """
         Explode the dataframe to long format by exploding the given columns.
 
         Parameters
         ----------
         columns
             Name of the column(s) to explode. Columns must be of datatype List or Utf8.
@@ -6059,20 +6286,15 @@
          b        5       
          c        6       
          c        7       
          c        8       
         
 
         """
-        return self._from_pydf(
-            self.lazy()
-            .explode(columns, *more_columns)
-            .collect(no_optimization=True)
-            ._df
-        )
+        return self.lazy().explode(columns, *more_columns).collect(no_optimization=True)
 
     def pivot(
         self,
         values: Sequence[str] | str,
         index: Sequence[str] | str,
         columns: Sequence[str] | str,
         aggregate_function: PivotAgg | Expr | None | NoDefault = no_default,
@@ -6278,15 +6500,15 @@
 
     def unstack(
         self,
         step: int,
         how: UnstackDirection = "vertical",
         columns: str | Sequence[str] | None = None,
         fill_values: list[Any] | None = None,
-    ) -> Self:
+    ) -> DataFrame:
         """
         Unstack a long table to a wide form without doing an aggregation.
 
         This can be much faster than a pivot, because it can skip the grouping phase.
 
         Warnings
         --------
@@ -6396,15 +6618,15 @@
             s.slice(slice_nbr * n_rows, n_rows).alias(
                 s.name + "_" + str(slice_nbr).zfill(zfill_val)
             )
             for s in df
             for slice_nbr in range(0, n_cols)
         ]
 
-        return self._from_pydf(DataFrame(slices)._df)
+        return DataFrame(slices)
 
     @overload
     def partition_by(
         self,
         by: str | Iterable[str],
         *more_by: str,
         maintain_order: bool = ...,
@@ -6614,15 +6836,15 @@
         return self._from_pydf(self._df.shift(periods))
 
     def shift_and_fill(
         self,
         fill_value: int | str | float,
         *,
         periods: int = 1,
-    ) -> Self:
+    ) -> DataFrame:
         """
         Shift the values by a given period and fill the resulting null values.
 
         Parameters
         ----------
         fill_value
             fill None values with this value.
@@ -6647,19 +6869,18 @@
         
          0    0    0   
          1    6    a   
          2    7    b   
         
 
         """
-        return self._from_pydf(
+        return (
             self.lazy()
             .shift_and_fill(fill_value=fill_value, periods=periods)
             .collect(no_optimization=True)
-            ._df
         )
 
     def is_duplicated(self) -> Series:
         """
         Get a mask of all duplicated rows in this DataFrame.
 
         Examples
@@ -6773,15 +6994,15 @@
         return wrap_ldf(self._df.lazy())
 
     def select(
         self,
         exprs: IntoExpr | Iterable[IntoExpr] | None = None,
         *more_exprs: IntoExpr,
         **named_exprs: IntoExpr,
-    ) -> Self:
+    ) -> DataFrame:
         """
         Select columns from this DataFrame.
 
         Parameters
         ----------
         exprs
             Column(s) to select. Accepts expression input. Strings are parsed as column
@@ -6874,27 +7095,26 @@
         
          {1,0}     
          {0,1}     
          {1,0}     
         
 
         """
-        return self._from_pydf(
+        return (
             self.lazy()
             .select(exprs, *more_exprs, **named_exprs)
             .collect(no_optimization=True)
-            ._df
         )
 
     def with_columns(
         self,
-        exprs: IntoExpr | Iterable[IntoExpr] = None,
+        exprs: IntoExpr | Iterable[IntoExpr] | None = None,
         *more_exprs: IntoExpr,
         **named_exprs: IntoExpr,
-    ) -> Self:
+    ) -> DataFrame:
         """
         Add columns to this DataFrame.
 
         Added columns will replace existing columns with the same name.
 
         Parameters
         ----------
@@ -7030,19 +7250,18 @@
          1    0.5   {null,null} 
          2    4.0   {1,3.5}     
          3    10.0  {1,6.0}     
          4    13.0  {1,3.0}     
         
 
         """
-        return self._from_pydf(
+        return (
             self.lazy()
             .with_columns(exprs, *more_exprs, **named_exprs)
             .collect(no_optimization=True)
-            ._df
         )
 
     @overload
     def n_chunks(self, strategy: Literal["first"] = ...) -> int:
         ...
 
     @overload
@@ -7430,15 +7649,15 @@
         
          2.0  7.0  null 
         
 
         """
         return self._from_pydf(self._df.median())
 
-    def product(self) -> Self:
+    def product(self) -> DataFrame:
         """
         Aggregate the columns of this DataFrame to their product values.
 
         Examples
         --------
         >>> df = pl.DataFrame(
         ...     {
@@ -7537,15 +7756,15 @@
 
     def unique(
         self,
         subset: str | Sequence[str] | None = None,
         *,
         keep: UniqueKeepStrategy = "any",
         maintain_order: bool = False,
-    ) -> Self:
+    ) -> DataFrame:
         """
         Drop duplicate rows from this dataframe.
 
         Parameters
         ----------
         subset
             Column name(s) to consider when identifying duplicates.
@@ -7611,19 +7830,18 @@
         
          2    a    b   
          3    a    b   
          1    a    b   
         
 
         """
-        return self._from_pydf(
+        return (
             self.lazy()
             .unique(subset=subset, keep=keep, maintain_order=maintain_order)
             .collect(no_optimization=True)
-            ._df
         )
 
     def n_unique(self, subset: str | Expr | Sequence[str | Expr] | None = None) -> int:
         """
         Return the number of unique rows, or the number of unique row-subsets.
 
         Parameters
@@ -7676,19 +7894,19 @@
         ...     ],
         ... )
         3
 
         """
         if isinstance(subset, str):
             subset = [F.col(subset)]
-        elif isinstance(subset, pli.Expr):
+        elif isinstance(subset, pl.Expr):
             subset = [subset]
 
         if isinstance(subset, Sequence) and len(subset) == 1:
-            expr = expr_to_lit_or_expr(subset[0], str_to_lit=False)
+            expr = parse_as_expression(subset[0])
         else:
             struct_fields = F.all() if (subset is None) else subset
             expr = F.struct(struct_fields)  # type: ignore[call-overload]
 
         df = self.lazy().select(expr.n_unique()).collect()
         return 0 if df.is_empty() else df.row(0)[0]
 
@@ -7978,26 +8196,26 @@
         item: Return dataframe element as a scalar.
 
         """
         if index is not None and by_predicate is not None:
             raise ValueError(
                 "Cannot set both 'index' and 'by_predicate'; mutually exclusive"
             )
-        elif isinstance(index, pli.Expr):
+        elif isinstance(index, pl.Expr):
             raise TypeError("Expressions should be passed to the 'by_predicate' param")
 
         if index is not None:
             row = self._df.row_tuple(index)
             if named:
                 return dict(zip(self.columns, row))
             else:
                 return row
 
         elif by_predicate is not None:
-            if not isinstance(by_predicate, pli.Expr):
+            if not isinstance(by_predicate, pl.Expr):
                 raise TypeError(
                     f"Expected 'by_predicate to be an expression; "
                     f"found {type(by_predicate)}"
                 )
             rows = self.filter(by_predicate).rows()
             n_rows = len(rows)
             if n_rows > 1:
@@ -8152,15 +8370,15 @@
         if buffer_size and not has_object:
             load_pyarrow_dicts = (
                 named
                 and _PYARROW_AVAILABLE
                 # note: 'ns' precision instantiates values as pandas types - avoid
                 and not any(
                     (getattr(tp, "time_unit", None) == "ns")
-                    for tp in unpack_dtypes(self.dtypes)
+                    for tp in unpack_dtypes(*self.dtypes)
                 )
             )
             for offset in range(0, self.height, buffer_size):
                 zerocopy_slice = self.slice(offset, buffer_size)
                 if load_pyarrow_dicts:
                     yield from zerocopy_slice.to_arrow().to_pylist()
                 else:
@@ -8241,15 +8459,15 @@
             self._df.shrink_to_fit()
             return self
         else:
             df = self.clone()
             df._df.shrink_to_fit()
             return df
 
-    def take_every(self, n: int) -> Self:
+    def take_every(self, n: int) -> DataFrame:
         """
         Take every nth row in the DataFrame and return as a new DataFrame.
 
         Examples
         --------
         >>> s = pl.DataFrame({"a": [1, 2, 3, 4], "b": [5, 6, 7, 8]})
         >>> s.take_every(2)
@@ -8310,15 +8528,15 @@
         """
         k0 = seed
         k1 = seed_1 if seed_1 is not None else seed
         k2 = seed_2 if seed_2 is not None else seed
         k3 = seed_3 if seed_3 is not None else seed
         return wrap_s(self._df.hash_rows(k0, k1, k2, k3))
 
-    def interpolate(self) -> Self:
+    def interpolate(self) -> DataFrame:
         """
         Interpolate intermediate values. The interpolation method is linear.
 
         Examples
         --------
         >>> df = pl.DataFrame(
         ...     {
@@ -8441,15 +8659,15 @@
             columns = [columns]
         if more_columns:
             columns = list(columns)
             columns.extend(more_columns)
         return self._from_pydf(self._df.unnest(columns))
 
     @typing.no_type_check
-    def corr(self, **kwargs: Any) -> Self:
+    def corr(self, **kwargs: Any) -> DataFrame:
         """
         Return Pearson product-moment correlation coefficients.
 
         See numpy ``corrcoef`` for more information:
         https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html
 
         Notes
@@ -8473,19 +8691,17 @@
         
          1.0   -1.0  1.0  
          -1.0  1.0   -1.0 
          1.0   -1.0  1.0  
         
 
         """
-        return self._from_pydf(
-            DataFrame(np.corrcoef(self.to_numpy().T, **kwargs), schema=self.columns)._df
-        )
+        return DataFrame(np.corrcoef(self.to_numpy().T, **kwargs), schema=self.columns)
 
-    def merge_sorted(self, other: DataFrame, key: str) -> Self:
+    def merge_sorted(self, other: DataFrame, key: str) -> DataFrame:
         """
         Take two sorted DataFrames and merge them by the sorted key.
 
         The output of this operation will also be sorted.
         It is the callers responsibility that the frames are sorted
         by that key otherwise the output will not make sense.
 
@@ -8495,52 +8711,46 @@
         ----------
         other
             Other DataFrame that must be merged
         key
             Key that is sorted.
 
         """
-        return self._from_pydf(
-            self.lazy()
-            .merge_sorted(other.lazy(), key)
-            .collect(no_optimization=True)
-            ._df
-        )
+        return self.lazy().merge_sorted(other.lazy(), key).collect(no_optimization=True)
 
     def set_sorted(
         self,
-        column: IntoExpr | Iterable[IntoExpr],
-        *more_columns: IntoExpr,
+        column: str | Iterable[str],
+        *more_columns: str,
         descending: bool = False,
-    ) -> Self:
+    ) -> DataFrame:
         """
         Indicate that one or multiple columns are sorted.
 
         Parameters
         ----------
         column
             Columns that are sorted
         more_columns
             Additional columns that are sorted, specified as positional arguments.
         descending
             Whether the columns are sorted in descending order.
         """
-        return self._from_pydf(
+        return (
             self.lazy()
             .set_sorted(column, *more_columns, descending=descending)
             .collect(no_optimization=True)
-            ._df
         )
 
     def update(
         self,
         other: DataFrame,
         on: str | Sequence[str] | None = None,
         how: Literal["left", "inner"] = "left",
-    ) -> Self:
+    ) -> DataFrame:
         """
         Update the values in this `DataFrame` with the non-null values in `other`.
 
         Notes
         -----
         This is syntactic sugar for a left/inner join + coalesce
 
@@ -8607,26 +8817,24 @@
          1    4   
          2    500 
          3    6   
          4    700 
         
 
         """
-        return self._from_pydf(
-            self.lazy().update(other.lazy(), on, how).collect(no_optimization=True)._df
-        )
+        return self.lazy().update(other.lazy(), on, how).collect(no_optimization=True)
 
 
 def _prepare_other_arg(other: Any, length: int | None = None) -> Series:
     # if not a series create singleton series such that it will broadcast
     value = other
-    if not isinstance(other, pli.Series):
+    if not isinstance(other, pl.Series):
         if isinstance(other, str):
             pass
         elif isinstance(other, Sequence):
             raise ValueError("Operation not supported.")
-        other = pli.Series("", [other])
+        other = pl.Series("", [other])
 
     if length and length > 1:
         other = other.extend_constant(value=value, n=length - 1)
 
     return other
```

### Comparing `polars_lts_cpu-0.17.9/polars/dataframe/groupby.py` & `polars_lts_cpu-0.18.0/polars/dataframe/groupby.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,37 +1,41 @@
 from __future__ import annotations
 
-from typing import TYPE_CHECKING, Callable, Generic, Iterable, Iterator, TypeVar
+from typing import TYPE_CHECKING, Callable, Iterable, Iterator
 
+import polars._reexport as pl
 from polars import functions as F
-from polars import internals as pli
 from polars.functions.whenthen import WhenThen, WhenThenThen
 from polars.utils.convert import _timedelta_to_pl_duration
 
 if TYPE_CHECKING:
+    import sys
     from datetime import timedelta
 
-    from polars.dataframe import DataFrame
+    from polars import DataFrame
     from polars.type_aliases import (
         ClosedInterval,
         IntoExpr,
         RollingInterpolationMethod,
+        SchemaDict,
         StartBy,
     )
 
-# A type variable used to refer to a polars.DataFrame or any subclass of it
-DF = TypeVar("DF", bound="DataFrame")
+    if sys.version_info >= (3, 11):
+        from typing import Self
+    else:
+        from typing_extensions import Self
 
 
-class GroupBy(Generic[DF]):
+class GroupBy:
     """Starts a new GroupBy operation."""
 
     def __init__(
         self,
-        df: DF,
+        df: DataFrame,
         by: IntoExpr | Iterable[IntoExpr],
         *more_by: IntoExpr,
         maintain_order: bool,
     ):
         """
         Utility class for performing a groupby operation over the given dataframe.
 
@@ -52,15 +56,15 @@
 
         """
         self.df = df
         self.by = by
         self.more_by = more_by
         self.maintain_order = maintain_order
 
-    def __iter__(self) -> GroupBy[DF]:
+    def __iter__(self) -> Self:
         """
         Allows iteration over the groups of the groupby operation.
 
         Returns
         -------
         Iterator returning tuples of (name, data) for each group.
 
@@ -103,42 +107,44 @@
 
         group_names = groups_df.select(F.all().exclude(temp_col))
 
         # When grouping by a single column, group name is a single value
         # When grouping by multiple columns, group name is a tuple of values
         self._group_names: Iterator[object] | Iterator[tuple[object, ...]]
         if (
-            isinstance(self.by, (str, pli.Expr, WhenThen, WhenThenThen))
+            isinstance(self.by, (str, pl.Expr, WhenThen, WhenThenThen))
             and not self.more_by
         ):
             self._group_names = iter(group_names.to_series())
         else:
             self._group_names = group_names.iter_rows()
 
         self._group_indices = groups_df.select(temp_col).to_series()
         self._current_index = 0
 
         return self
 
-    def __next__(self) -> tuple[object, DF] | tuple[tuple[object, ...], DF]:
+    def __next__(
+        self,
+    ) -> tuple[object, DataFrame] | tuple[tuple[object, ...], DataFrame]:
         if self._current_index >= len(self._group_indices):
             raise StopIteration
 
         group_name = next(self._group_names)
         group_data = self.df[self._group_indices[self._current_index]]
         self._current_index += 1
 
         return group_name, group_data
 
     def agg(
         self,
         aggs: IntoExpr | Iterable[IntoExpr] | None = None,
         *more_aggs: IntoExpr,
         **named_aggs: IntoExpr,
-    ) -> DF:
+    ) -> DataFrame:
         """
         Compute aggregations for each group of a groupby operation.
 
         Parameters
         ----------
         aggs
             Aggregations to compute for each group of the groupby operation.
@@ -217,23 +223,22 @@
         
          a    2      17.0           
          c    3      1.0            
          b    5      10.0           
         
 
         """
-        df = (
+        return (
             self.df.lazy()
             .groupby(self.by, *self.more_by, maintain_order=self.maintain_order)
             .agg(aggs, *more_aggs, **named_aggs)
             .collect(no_optimization=True)
         )
-        return self.df.__class__._from_pydf(df._df)
 
-    def apply(self, function: Callable[[DataFrame], DataFrame]) -> DF:
+    def apply(self, function: Callable[[DataFrame], DataFrame]) -> DataFrame:
         """
         Apply a custom/user-defined function (UDF) over the groups as a sub-DataFrame.
 
         Implementing logic using a Python function is almost always _significantly_
         slower and more memory intensive than implementing the same logic using
         the native expression API because:
 
@@ -315,15 +320,15 @@
         else:
             raise TypeError("Cannot call `apply` when grouping by an expression.")
 
         return self.df.__class__._from_pydf(
             self.df._df.groupby_apply(by, function, self.maintain_order)
         )
 
-    def head(self, n: int = 5) -> DF:
+    def head(self, n: int = 5) -> DataFrame:
         """
         Get the first `n` rows of each group.
 
         Parameters
         ----------
         n
             Number of rows to return.
@@ -361,23 +366,22 @@
          a        5   
          b        6   
          c        1   
          c        2   
         
 
         """
-        df = (
+        return (
             self.df.lazy()
             .groupby(self.by, *self.more_by, maintain_order=self.maintain_order)
             .head(n)
             .collect(no_optimization=True)
         )
-        return self.df.__class__._from_pydf(df._df)
 
-    def tail(self, n: int = 5) -> DF:
+    def tail(self, n: int = 5) -> DataFrame:
         """
         Get the last `n` rows of each group.
 
         Parameters
         ----------
         n
             Number of rows to return.
@@ -415,23 +419,22 @@
          a        5   
          b        6   
          c        2   
          c        4   
         
 
         """
-        df = (
+        return (
             self.df.lazy()
             .groupby(self.by, *self.more_by, maintain_order=self.maintain_order)
             .tail(n)
             .collect(no_optimization=True)
         )
-        return self.df.__class__._from_pydf(df._df)
 
-    def all(self) -> DF:
+    def all(self) -> DataFrame:
         """
         Aggregate the groups into Series.
 
         Examples
         --------
         >>> df = pl.DataFrame({"a": ["one", "two", "one", "two"], "b": [1, 2, 3, 4]})
         >>> df.groupby("a", maintain_order=True).all()
@@ -444,18 +447,21 @@
          one  [1, 3]    
          two  [2, 4]    
         
 
         """
         return self.agg(F.all())
 
-    def count(self) -> DF:
+    def count(self) -> DataFrame:
         """
         Count the number of values in each group.
 
+        .. warning::
+            `null` is deemed a value in this context.
+
         Examples
         --------
         >>> df = pl.DataFrame(
         ...     {
         ...         "a": [1, 2, 2, 3, 4, 5],
         ...         "b": [0.5, 0.5, 4, 10, 13, 14],
         ...         "c": [True, True, True, False, False, True],
@@ -473,15 +479,15 @@
          Orange  1     
          Banana  2     
         
 
         """
         return self.agg(F.count())
 
-    def first(self) -> DF:
+    def first(self) -> DataFrame:
         """
         Aggregate the first values in the group.
 
         Examples
         --------
         >>> df = pl.DataFrame(
         ...     {
@@ -502,15 +508,15 @@
          Orange  2    0.5   true  
          Banana  4    13.0  false 
         
 
         """
         return self.agg(F.all().first())
 
-    def last(self) -> DF:
+    def last(self) -> DataFrame:
         """
         Aggregate the last values in the group.
 
         Examples
         --------
         >>> df = pl.DataFrame(
         ...     {
@@ -531,15 +537,15 @@
          Orange  2    0.5   true  
          Banana  5    14.0  true  
         
 
         """
         return self.agg(F.all().last())
 
-    def max(self) -> DF:
+    def max(self) -> DataFrame:
         """
         Reduce the groups to the maximal value.
 
         Examples
         --------
         >>> df = pl.DataFrame(
         ...     {
@@ -560,15 +566,15 @@
          Orange  2    0.5   true 
          Banana  5    14.0  true 
         
 
         """
         return self.agg(F.all().max())
 
-    def mean(self) -> DF:
+    def mean(self) -> DataFrame:
         """
         Reduce the groups to the mean values.
 
         Examples
         --------
         >>> df = pl.DataFrame(
         ...     {
@@ -589,15 +595,15 @@
          Orange  2.0  0.5       1.0      
          Banana  4.5  13.5      0.5      
         
 
         """
         return self.agg(F.all().mean())
 
-    def median(self) -> DF:
+    def median(self) -> DataFrame:
         """
         Return the median per group.
 
         Examples
         --------
         >>> df = pl.DataFrame(
         ...     {
@@ -616,15 +622,15 @@
          Apple   2.0  4.0  
          Banana  4.0  13.0 
         
 
         """
         return self.agg(F.all().median())
 
-    def min(self) -> DF:
+    def min(self) -> DataFrame:
         """
         Reduce the groups to the minimal value.
 
         Examples
         --------
         >>> df = pl.DataFrame(
         ...     {
@@ -645,15 +651,15 @@
          Orange  2    0.5   true  
          Banana  4    13.0  false 
         
 
         """
         return self.agg(F.all().min())
 
-    def n_unique(self) -> DF:
+    def n_unique(self) -> DataFrame:
         """
         Count the unique values per group.
 
         Examples
         --------
         >>> df = pl.DataFrame(
         ...     {
@@ -674,15 +680,15 @@
         
 
         """
         return self.agg(F.all().n_unique())
 
     def quantile(
         self, quantile: float, interpolation: RollingInterpolationMethod = "nearest"
-    ) -> DF:
+    ) -> DataFrame:
         """
         Compute the quantile per group.
 
         Parameters
         ----------
         quantile
             Quantile between 0.0 and 1.0.
@@ -709,15 +715,15 @@
          Orange  2.0  0.5  
          Banana  5.0  14.0 
         
 
         """
         return self.agg(F.all().quantile(quantile, interpolation=interpolation))
 
-    def sum(self) -> DF:
+    def sum(self) -> DataFrame:
         """
         Reduce the groups to the sum.
 
         Examples
         --------
         >>> df = pl.DataFrame(
         ...     {
@@ -739,52 +745,55 @@
          Banana  9    27.0  1   
         
 
         """
         return self.agg(F.all().sum())
 
 
-class RollingGroupBy(Generic[DF]):
+class RollingGroupBy:
     """
     A rolling grouper.
 
     This has an `.agg` method which will allow you to run all polars expressions in a
     groupby context.
     """
 
     def __init__(
         self,
-        df: DF,
-        index_column: str,
+        df: DataFrame,
+        index_column: IntoExpr,
         period: str | timedelta,
         offset: str | timedelta | None,
         closed: ClosedInterval,
         by: IntoExpr | Iterable[IntoExpr] | None,
+        check_sorted: bool,
     ):
         period = _timedelta_to_pl_duration(period)
         offset = _timedelta_to_pl_duration(offset)
 
         self.df = df
         self.time_column = index_column
         self.period = period
         self.offset = offset
         self.closed = closed
         self.by = by
+        self.check_sorted = check_sorted
 
-    def __iter__(self) -> RollingGroupBy[DF]:
+    def __iter__(self) -> Self:
         temp_col = "__POLARS_GB_GROUP_INDICES"
         groups_df = (
             self.df.lazy()
             .with_row_count(name=temp_col)
             .groupby_rolling(
                 index_column=self.time_column,
                 period=self.period,
                 offset=self.offset,
                 closed=self.closed,
                 by=self.by,
+                check_sorted=self.check_sorted,
             )
             .agg(F.col(temp_col))
             .collect(no_optimization=True)
         )
 
         group_names = groups_df.select(F.all().exclude(temp_col))
 
@@ -797,65 +806,163 @@
             self._group_names = group_names.iter_rows()
 
         self._group_indices = groups_df.select(temp_col).to_series()
         self._current_index = 0
 
         return self
 
-    def __next__(self) -> tuple[object, DF] | tuple[tuple[object, ...], DF]:
+    def __next__(
+        self,
+    ) -> tuple[object, DataFrame] | tuple[tuple[object, ...], DataFrame]:
         if self._current_index >= len(self._group_indices):
             raise StopIteration
 
         group_name = next(self._group_names)
         group_data = self.df[self._group_indices[self._current_index]]
         self._current_index += 1
 
         return group_name, group_data
 
     def agg(
         self,
         aggs: IntoExpr | Iterable[IntoExpr] | None = None,
         *more_aggs: IntoExpr,
         **named_aggs: IntoExpr,
-    ) -> DF:
-        df = (
+    ) -> DataFrame:
+        return (
             self.df.lazy()
             .groupby_rolling(
                 index_column=self.time_column,
                 period=self.period,
                 offset=self.offset,
                 closed=self.closed,
                 by=self.by,
+                check_sorted=self.check_sorted,
             )
             .agg(aggs, *more_aggs, **named_aggs)
             .collect(no_optimization=True)
         )
-        return self.df.__class__._from_pydf(df._df)
+
+    def apply(
+        self,
+        function: Callable[[DataFrame], DataFrame],
+        schema: SchemaDict | None,
+    ) -> DataFrame:
+        """
+        Apply a custom/user-defined function (UDF) over the groups as a new DataFrame.
+
+        Using this is considered an anti-pattern. This will be very slow because:
+
+        - it forces the engine to materialize the whole `DataFrames` for the groups.
+        - it is not parallelized
+        - it blocks optimizations as the passed python function is opaque to the
+          optimizer
+
+        The idiomatic way to apply custom functions over multiple columns is using:
+
+        `pl.struct([my_columns]).apply(lambda struct_series: ..)`
+
+        Parameters
+        ----------
+        function
+            Function to apply over each group of the `LazyFrame`.
+        schema
+            Schema of the output function. This has to be known statically. If the
+            given schema is incorrect, this is a bug in the caller's query and may
+            lead to errors. If set to None, polars assumes the schema is unchanged.
+
+
+        Examples
+        --------
+        >>> df = pl.DataFrame(
+        ...     {
+        ...         "id": [0, 1, 2, 3, 4],
+        ...         "color": ["red", "green", "green", "red", "red"],
+        ...         "shape": ["square", "triangle", "square", "triangle", "square"],
+        ...     }
+        ... )
+        >>> df
+        shape: (5, 3)
+        
+         id   color  shape    
+         ---  ---    ---      
+         i64  str    str      
+        
+         0    red    square   
+         1    green  triangle 
+         2    green  square   
+         3    red    triangle 
+         4    red    square   
+        
+
+        For each color group sample two rows:
+
+        >>> (
+        ...     df.lazy()
+        ...     .groupby("color")
+        ...     .apply(lambda group_df: group_df.sample(2), schema=None)
+        ...     .collect()
+        ... )  # doctest: +IGNORE_RESULT
+        shape: (4, 3)
+        
+         id   color  shape    
+         ---  ---    ---      
+         i64  str    str      
+        
+         1    green  triangle 
+         2    green  square   
+         4    red    square   
+         3    red    triangle 
+        
+
+        It is better to implement this with an expression:
+
+        >>> (
+        ...     df.lazy()
+        ...     .filter(pl.arange(0, pl.count()).shuffle().over("color") < 2)
+        ...     .collect()
+        ... )  # doctest: +IGNORE_RESULT
+
+        """
+        return (
+            self.df.lazy()
+            .groupby_rolling(
+                index_column=self.time_column,
+                period=self.period,
+                offset=self.offset,
+                closed=self.closed,
+                by=self.by,
+                check_sorted=self.check_sorted,
+            )
+            .apply(function, schema)
+            .collect(no_optimization=True)
+        )
 
 
-class DynamicGroupBy(Generic[DF]):
+class DynamicGroupBy:
     """
     A dynamic grouper.
 
     This has an `.agg` method which allows you to run all polars expressions in a
     groupby context.
     """
 
     def __init__(
         self,
-        df: DF,
-        index_column: str,
+        df: DataFrame,
+        index_column: IntoExpr,
         every: str | timedelta,
         period: str | timedelta | None,
         offset: str | timedelta | None,
         truncate: bool,
         include_boundaries: bool,
         closed: ClosedInterval,
         by: IntoExpr | Iterable[IntoExpr] | None,
         start_by: StartBy,
+        check_sorted: bool,
     ):
         period = _timedelta_to_pl_duration(period)
         offset = _timedelta_to_pl_duration(offset)
         every = _timedelta_to_pl_duration(every)
 
         self.df = df
         self.time_column = index_column
@@ -863,30 +970,32 @@
         self.period = period
         self.offset = offset
         self.truncate = truncate
         self.include_boundaries = include_boundaries
         self.closed = closed
         self.by = by
         self.start_by = start_by
+        self.check_sorted = check_sorted
 
-    def __iter__(self) -> DynamicGroupBy[DF]:
+    def __iter__(self) -> Self:
         temp_col = "__POLARS_GB_GROUP_INDICES"
         groups_df = (
             self.df.lazy()
             .with_row_count(name=temp_col)
             .groupby_dynamic(
                 index_column=self.time_column,
                 every=self.every,
                 period=self.period,
                 offset=self.offset,
                 truncate=self.truncate,
                 include_boundaries=self.include_boundaries,
                 closed=self.closed,
                 by=self.by,
                 start_by=self.start_by,
+                check_sorted=self.check_sorted,
             )
             .agg(F.col(temp_col))
             .collect(no_optimization=True)
         )
 
         group_names = groups_df.select(F.all().exclude(temp_col))
 
@@ -899,40 +1008,141 @@
             self._group_names = group_names.iter_rows()
 
         self._group_indices = groups_df.select(temp_col).to_series()
         self._current_index = 0
 
         return self
 
-    def __next__(self) -> tuple[object, DF] | tuple[tuple[object, ...], DF]:
+    def __next__(
+        self,
+    ) -> tuple[object, DataFrame] | tuple[tuple[object, ...], DataFrame]:
         if self._current_index >= len(self._group_indices):
             raise StopIteration
 
         group_name = next(self._group_names)
         group_data = self.df[self._group_indices[self._current_index]]
         self._current_index += 1
 
         return group_name, group_data
 
     def agg(
         self,
         aggs: IntoExpr | Iterable[IntoExpr] | None = None,
         *more_aggs: IntoExpr,
         **named_aggs: IntoExpr,
-    ) -> DF:
-        df = (
+    ) -> DataFrame:
+        return (
             self.df.lazy()
             .groupby_dynamic(
                 index_column=self.time_column,
                 every=self.every,
                 period=self.period,
                 offset=self.offset,
                 truncate=self.truncate,
                 include_boundaries=self.include_boundaries,
                 closed=self.closed,
                 by=self.by,
                 start_by=self.start_by,
+                check_sorted=self.check_sorted,
             )
             .agg(aggs, *more_aggs, **named_aggs)
             .collect(no_optimization=True)
         )
-        return self.df.__class__._from_pydf(df._df)
+
+    def apply(
+        self,
+        function: Callable[[DataFrame], DataFrame],
+        schema: SchemaDict | None,
+    ) -> DataFrame:
+        """
+        Apply a custom/user-defined function (UDF) over the groups as a new DataFrame.
+
+        Using this is considered an anti-pattern. This will be very slow because:
+
+        - it forces the engine to materialize the whole `DataFrames` for the groups.
+        - it is not parallelized
+        - it blocks optimizations as the passed python function is opaque to the
+          optimizer
+
+        The idiomatic way to apply custom functions over multiple columns is using:
+
+        `pl.struct([my_columns]).apply(lambda struct_series: ..)`
+
+        Parameters
+        ----------
+        function
+            Function to apply over each group of the `LazyFrame`.
+        schema
+            Schema of the output function. This has to be known statically. If the
+            given schema is incorrect, this is a bug in the caller's query and may
+            lead to errors. If set to None, polars assumes the schema is unchanged.
+
+
+        Examples
+        --------
+        >>> df = pl.DataFrame(
+        ...     {
+        ...         "id": [0, 1, 2, 3, 4],
+        ...         "color": ["red", "green", "green", "red", "red"],
+        ...         "shape": ["square", "triangle", "square", "triangle", "square"],
+        ...     }
+        ... )
+        >>> df
+        shape: (5, 3)
+        
+         id   color  shape    
+         ---  ---    ---      
+         i64  str    str      
+        
+         0    red    square   
+         1    green  triangle 
+         2    green  square   
+         3    red    triangle 
+         4    red    square   
+        
+
+        For each color group sample two rows:
+
+        >>> (
+        ...     df.lazy()
+        ...     .groupby("color")
+        ...     .apply(lambda group_df: group_df.sample(2), schema=None)
+        ...     .collect()
+        ... )  # doctest: +IGNORE_RESULT
+        shape: (4, 3)
+        
+         id   color  shape    
+         ---  ---    ---      
+         i64  str    str      
+        
+         1    green  triangle 
+         2    green  square   
+         4    red    square   
+         3    red    triangle 
+        
+
+        It is better to implement this with an expression:
+
+        >>> (
+        ...     df.lazy()
+        ...     .filter(pl.arange(0, pl.count()).shuffle().over("color") < 2)
+        ...     .collect()
+        ... )  # doctest: +IGNORE_RESULT
+
+        """
+        return (
+            self.df.lazy()
+            .groupby_dynamic(
+                index_column=self.time_column,
+                every=self.every,
+                period=self.period,
+                offset=self.offset,
+                truncate=self.truncate,
+                include_boundaries=self.include_boundaries,
+                closed=self.closed,
+                by=self.by,
+                start_by=self.start_by,
+                check_sorted=self.check_sorted,
+            )
+            .apply(function, schema)
+            .collect(no_optimization=True)
+        )
```

### Comparing `polars_lts_cpu-0.17.9/polars/datatypes/__init__.py` & `polars_lts_cpu-0.18.0/polars/datatypes/__init__.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 from polars.datatypes.classes import (
+    Array,
     Binary,
     Boolean,
     Categorical,
     DataType,
     DataTypeClass,
     DataTypeGroup,
     Date,
@@ -69,14 +70,15 @@
     PythonDataType,
     SchemaDefinition,
     SchemaDict,
 )
 
 __all__ = [
     # classes
+    "Array",
     "Binary",
     "Boolean",
     "Categorical",
     "DataType",
     "DataTypeClass",
     "DataTypeGroup",
     "Date",
```

### Comparing `polars_lts_cpu-0.17.9/polars/datatypes/classes.py` & `polars_lts_cpu-0.18.0/polars/datatypes/classes.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,35 +1,69 @@
 from __future__ import annotations
 
 import contextlib
 from datetime import timezone
 from inspect import isclass
-from typing import TYPE_CHECKING, Any, Iterator, Mapping, Sequence
+from typing import TYPE_CHECKING, Any, Callable, Iterator, Mapping, Sequence
 
 import polars.datatypes
 
 with contextlib.suppress(ImportError):  # Module not available when building docs
     from polars.polars import dtype_str_repr as _dtype_str_repr
 
 if TYPE_CHECKING:
     from polars.type_aliases import PolarsDataType, PythonDataType, SchemaDict, TimeUnit
 
 
+class classinstmethod(classmethod):  # type: ignore[type-arg]
+    """Decorator that allows a method to be called from the class OR instance."""
+
+    def __get__(self, instance: Any, type_: type) -> Any:  # type: ignore[override]
+        get = super().__get__ if instance is None else self.__func__.__get__
+        return get(instance, type_)
+
+
+class classproperty:
+    """Equivalent to @property, but works on a class (doesn't require an instance)."""
+
+    def __init__(self, method: Callable[..., Any] | None = None) -> None:
+        self.fget = method
+
+    def __get__(self, instance: Any, cls: type | None = None) -> Any:
+        return self.fget(cls)  # type: ignore[misc]
+
+    def getter(self, method: Callable[..., Any]) -> Any:
+        self.fget = method
+        return self
+
+
 class DataTypeClass(type):
     """Metaclass for nicely printing DataType classes."""
 
     def __repr__(cls) -> str:
         return cls.__name__
 
     def _string_repr(cls) -> str:
         return _dtype_str_repr(cls)
 
     def base_type(cls) -> PolarsDataType:
         return cls
 
+    @classproperty
+    def is_nested(self) -> bool:
+        return False
+
+    @classmethod
+    def is_(cls, other: PolarsDataType) -> bool:
+        return cls == other and hash(cls) == hash(other)
+
+    @classmethod
+    def is_not(cls, other: PolarsDataType) -> bool:
+        return not cls.is_(other)
+
 
 class DataType(metaclass=DataTypeClass):
     """Base class for all Polars data types."""
 
     def __new__(cls, *args: Any, **kwargs: Any) -> PolarsDataType:  # type: ignore[misc]
         # this formulation allows for equivalent use of "pl.Type" and "pl.Type()", while
         # still respecting types that take initialisation params (eg: Duration/Datetime)
@@ -55,14 +89,64 @@
         >>> pl.List(pl.Int32).base_type()
         List
         >>> pl.Struct([pl.Field("a", pl.Int64), pl.Field("b", pl.Boolean)]).base_type()
         Struct
         """
         return cls
 
+    @classproperty
+    def is_nested(self) -> bool:
+        return False
+
+    @classinstmethod  # type: ignore[arg-type]
+    def is_(self, other: PolarsDataType) -> bool:
+        """
+        Check if this DataType is the same as another DataType.
+
+        This is a stricter check than ``self == other``, as it enforces an exact
+        match of all dtype attributes for nested and/or uninitialised dtypes.
+
+        Parameters
+        ----------
+        other
+            the other polars dtype to compare with.
+
+        Examples
+        --------
+        >>> pl.List == pl.List(pl.Int32)
+        True
+        >>> pl.List.is_(pl.List(pl.Int32))
+        False
+
+        """
+        return self == other and hash(self) == hash(other)
+
+    @classinstmethod  # type: ignore[arg-type]
+    def is_not(self, other: PolarsDataType) -> bool:
+        """
+        Check if this DataType is NOT the same as another DataType.
+
+        This is a stricter check than ``self != other``, as it enforces an exact
+        match of all dtype attributes for nested and/or uninitialised dtypes.
+
+        Parameters
+        ----------
+        other
+            the other polars dtype to compare with.
+
+        Examples
+        --------
+        >>> pl.List != pl.List(pl.Int32)
+        False
+        >>> pl.List.is_not(pl.List(pl.Int32))
+        True
+
+        """
+        return not self.is_(other)
+
 
 def _custom_reconstruct(
     cls: type[Any], base: type[Any], state: Any
 ) -> PolarsDataType | type:
     """Helper function for unpickling DataType objects."""
     if state:
         obj = base.__new__(cls, state)
@@ -111,14 +195,18 @@
 class TemporalType(DataType):
     """Base class for temporal data types."""
 
 
 class NestedType(DataType):
     """Base class for nested data types."""
 
+    @classproperty
+    def is_nested(self) -> bool:
+        return True
+
 
 class Int8(IntegralType):
     """8-bit signed integer type."""
 
 
 class Int16(IntegralType):
     """16-bit signed integer type."""
@@ -344,14 +432,59 @@
             if self.inner is None or other.inner is None:
                 return True
             else:
                 return self.inner == other.inner
         else:
             return False
 
+    def __hash__(self) -> int:
+        return hash((self.__class__, self.inner))
+
+    def __repr__(self) -> str:
+        class_name = self.__class__.__name__
+        return f"{class_name}({self.inner!r})"
+
+
+class Array(NestedType):
+    inner: PolarsDataType | None = None
+    width: int
+
+    def __init__(self, width: int, inner: PolarsDataType | PythonDataType = Null):
+        """
+        Nested list/array type.
+
+        Parameters
+        ----------
+        width
+            The fixed size length of the inner arrays.
+        inner
+            The `DataType` of values within the list
+
+        """
+        self.width = width
+        self.inner = polars.datatypes.py_type_to_dtype(inner)
+
+    def __eq__(self, other: PolarsDataType) -> bool:  # type: ignore[override]
+        # This equality check allows comparison of type classes and type instances.
+        # If a parent type is not specific about its inner type, we infer it as equal:
+        # > fixed-size-list[i64] == fixed-size-list[i64] -> True
+        # > fixed-size-list[i64] == fixed-size-list[f32] -> False
+        # > fixed-size-list[i64] == fixed-size-list      -> True
+
+        # allow comparing object instances to class
+        if type(other) is DataTypeClass and issubclass(other, Array):
+            return True
+        if isinstance(other, Array):
+            if self.inner is None or other.inner is None:
+                return True
+            else:
+                return self.inner == other.inner
+        else:
+            return False
+
     def __hash__(self) -> int:
         return hash((self.__class__, self.inner))
 
     def __repr__(self) -> str:
         class_name = self.__class__.__name__
         return f"{class_name}({self.inner!r})"
```

### Comparing `polars_lts_cpu-0.17.9/polars/datatypes/constants.py` & `polars_lts_cpu-0.18.0/polars/datatypes/constants.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/polars/datatypes/constructor.py` & `polars_lts_cpu-0.18.0/polars/datatypes/constructor.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 from __future__ import annotations
 
+import functools
 from decimal import Decimal as PyDecimal
 from typing import TYPE_CHECKING, Any, Callable, Sequence
 
 from polars import datatypes as dt
 from polars.dependencies import numpy as np
 
 # Module not available when building docs
@@ -46,16 +47,21 @@
 
 
 def polars_type_to_constructor(
     dtype: PolarsDataType,
 ) -> Callable[[str, Sequence[Any], bool], PySeries]:
     """Get the right PySeries constructor for the given Polars dtype."""
     try:
-        dtype = dtype.base_type()
-        return _POLARS_TYPE_TO_CONSTRUCTOR[dtype]
+        base_type = dtype.base_type()
+        # special case for Array as it needs to pass the width argument
+        # upon construction
+        if base_type == dt.Array:
+            return functools.partial(PySeries.new_array, dtype.width, dtype.inner)  # type: ignore[union-attr]
+
+        return _POLARS_TYPE_TO_CONSTRUCTOR[base_type]
     except KeyError:  # pragma: no cover
         raise ValueError(f"Cannot construct PySeries for type {dtype}.") from None
 
 
 _NUMPY_TYPE_TO_CONSTRUCTOR = None
```

### Comparing `polars_lts_cpu-0.17.9/polars/datatypes/convert.py` & `polars_lts_cpu-0.18.0/polars/datatypes/convert.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,20 +12,16 @@
     Any,
     Callable,
     Collection,
     ForwardRef,
     Optional,
     TypeVar,
     Union,
-    cast,
     overload,
 )
-from typing import (
-    List as TypingList,
-)
 
 from polars.datatypes import (
     Binary,
     Boolean,
     Categorical,
     DataType,
     DataTypeClass,
@@ -50,15 +46,14 @@
     UInt32,
     UInt64,
     Unknown,
     Utf8,
 )
 from polars.dependencies import numpy as np
 from polars.dependencies import pyarrow as pa
-from polars.type_aliases import PolarsDataType
 
 with contextlib.suppress(ImportError):  # Module not available when building docs
     from polars.polars import dtype_str_repr as _dtype_str_repr
 
 if sys.version_info >= (3, 8):
     from typing import get_args
 else:
@@ -73,15 +68,15 @@
     from types import NoneType, UnionType
 else:
     # infer equivalent class
     NoneType = type(None)
     UnionType = type(Union[int, float])
 
 if TYPE_CHECKING:
-    from polars.type_aliases import PythonDataType, SchemaDict, TimeUnit
+    from polars.type_aliases import PolarsDataType, PythonDataType, SchemaDict, TimeUnit
 
     if sys.version_info >= (3, 8):
         from typing import Literal
     else:
         from typing_extensions import Literal
 
 
@@ -140,14 +135,29 @@
         return Decimal
     if python_dtype is bytes:
         return Binary
     if python_dtype is object:
         return Object
     if python_dtype is None.__class__:
         return Null
+
+    # cover generic typing aliases, such as 'list[str]'
+    if hasattr(python_dtype, "__origin__") and hasattr(python_dtype, "__args__"):
+        base_type = python_dtype.__origin__
+        if base_type is not None:
+            dtype = map_py_type_to_dtype(base_type)
+            nested = python_dtype.__args__
+            if len(nested) == 1:
+                nested = nested[0]
+            return (
+                dtype
+                if nested is None
+                else dtype(map_py_type_to_dtype(nested))  # type: ignore[operator]
+            )
+
     raise TypeError("Invalid type")
 
 
 def is_polars_dtype(dtype: Any, include_unknown: bool = False) -> bool:
     """Indicate whether the given input is a Polars dtype, or dtype specialisation."""
     try:
         if dtype == Unknown:
@@ -156,24 +166,24 @@
         else:
             return isinstance(dtype, (DataType, DataTypeClass))
     except ValueError:
         return False
 
 
 def unpack_dtypes(
-    dtypes: PolarsDataType | Collection[PolarsDataType] | None,
+    *dtypes: PolarsDataType | None,
     include_compound: bool = False,
 ) -> set[PolarsDataType]:
     """
     Return a set of unique dtypes found in one or more (potentially compound) dtypes.
 
     Parameters
     ----------
-    dtypes : PolarsDataType | Collection[PolarsDataType] | None
-        polars dtype, collection of dtypes, or None.
+    *dtypes : PolarsDataType | Collection[PolarsDataType] | None
+        one or more polars dtypes.
 
     include_compound : bool, default True
         * if True, any parent/compound dtypes (List, Struct) are included in the result.
         * if False, only the child/scalar dtypes are returned from these types.
 
     Examples
     --------
@@ -192,30 +202,30 @@
     ...     [struct_dtype, list_dtype], include_compound=True
     ... )  # doctest: +IGNORE_RESULT
     {Float64, Int64, Utf8, List(Float64), Struct([Field('a', Int64), Field('b', Utf8), Field('c', List(Float64))])}
 
     """  # noqa: W505
     if not dtypes:
         return set()
-    elif is_polars_dtype(dtypes):
-        dtypes = [dtypes]  # type: ignore[list-item]
+    elif len(dtypes) == 1 and isinstance(dtypes[0], Collection):
+        dtypes = dtypes[0]
 
     unpacked: set[PolarsDataType] = set()
-    for tp in cast(TypingList[PolarsDataType], dtypes):
+    for tp in dtypes:
         if isinstance(tp, List):
             if include_compound:
                 unpacked.add(tp)
             unpacked.update(unpack_dtypes(tp.inner, include_compound=include_compound))
         elif isinstance(tp, Struct):
             if include_compound:
                 unpacked.add(tp)
             unpacked.update(unpack_dtypes(tp.fields, include_compound=include_compound))  # type: ignore[arg-type]
         elif isinstance(tp, Field):
             unpacked.update(unpack_dtypes(tp.dtype, include_compound=include_compound))
-        elif is_polars_dtype(tp):
+        elif tp is not None and is_polars_dtype(tp):
             unpacked.add(tp)
     return unpacked
 
 
 class _DataTypeMappings:
     @property
     @cache
@@ -399,27 +409,32 @@
         data_type = (
             PY_STR_TO_DTYPE.get(
                 re.sub(r"(^None \|)|(\| None$)", "", annotation).strip(), data_type
             )
             if isinstance(annotation, str)  # type: ignore[redundant-expr]
             else annotation
         )
+    elif type(data_type).__name__ == "InitVar":
+        data_type = data_type.type
 
     if is_polars_dtype(data_type):
         return data_type
 
     elif isinstance(data_type, (OptionType, UnionType)):
         # not exhaustive; handles the common "type | None" case, but
         # should probably pick appropriate supertype when n_types > 1?
         possible_types = [tp for tp in get_args(data_type) if tp is not NoneType]
         if len(possible_types) == 1:
             data_type = possible_types[0]
 
     elif isinstance(data_type, str):
-        data_type = DataTypeMappings.REPR_TO_DTYPE.get(data_type, data_type)
+        data_type = DataTypeMappings.REPR_TO_DTYPE.get(
+            re.sub(r"^(?:dataclasses\.)?InitVar\[(.+)\]$", r"\1", data_type),
+            data_type,
+        )
         if is_polars_dtype(data_type):
             return data_type
     try:
         return map_py_type_to_dtype(data_type)
     except (KeyError, TypeError):  # pragma: no cover
         if not raise_unmatched:
             return None
@@ -471,15 +486,16 @@
         dtype.itemsize,
     ) in DataTypeMappings.NUMPY_KIND_AND_ITEMSIZE_TO_DTYPE
 
 
 def numpy_char_code_to_dtype(dtype_char: str) -> PolarsDataType:
     """Convert a numpy character dtype to a Polars dtype."""
     dtype = np.dtype(dtype_char)
-
+    if dtype.kind == "U":
+        return Utf8
     try:
         return DataTypeMappings.NUMPY_KIND_AND_ITEMSIZE_TO_DTYPE[
             (dtype.kind, dtype.itemsize)
         ]
     except KeyError:  # pragma: no cover
         raise ValueError(
             f"Cannot parse numpy data type {dtype} into Polars data type."
```

### Comparing `polars_lts_cpu-0.17.9/polars/dependencies.py` & `polars_lts_cpu-0.18.0/polars/dependencies.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/polars/exceptions.py` & `polars_lts_cpu-0.18.0/polars/exceptions.py`

 * *Files 2% similar despite different names*

```diff
@@ -62,14 +62,18 @@
     """Exception raised when no rows are returned, but at least one row is expected."""
 
 
 class TooManyRowsReturnedError(RowsError):
     """Exception raised when more rows than expected are returned."""
 
 
+class TimeZoneAwareConstructorWarning(Warning):
+    """Warning raised when constructing Series from non-UTC time-zone-aware inputs."""
+
+
 class ChronoFormatWarning(Warning):
     """
     Warning raised when a chrono format string contains dubious patterns.
 
     Polars uses Rust's chrono crate to convert between string data and temporal data.
     The patterns used by chrono differ slightly from Python's built-in datetime module.
     Refer to the `chrono strftime documentation
```

### Comparing `polars_lts_cpu-0.17.9/polars/expr/binary.py` & `polars_lts_cpu-0.18.0/polars/expr/binary.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING
 
 from polars.utils._wrap import wrap_expr
 
 if TYPE_CHECKING:
-    from polars.expr import Expr
+    from polars import Expr
     from polars.type_aliases import TransferEncoding
 
 
 class ExprBinaryNameSpace:
     """Namespace for bin related expressions."""
 
     _accessor = "bin"
@@ -27,39 +27,39 @@
             The binary substring to look for
 
         Returns
         -------
         Boolean mask
 
         """
-        return wrap_expr(self._pyexpr.binary_contains(literal))
+        return wrap_expr(self._pyexpr.bin_contains(literal))
 
     def ends_with(self, suffix: bytes) -> Expr:
         """
         Check if string values end with a binary substring.
 
         Parameters
         ----------
         suffix
             Suffix substring.
 
         """
-        return wrap_expr(self._pyexpr.binary_ends_with(suffix))
+        return wrap_expr(self._pyexpr.bin_ends_with(suffix))
 
     def starts_with(self, prefix: bytes) -> Expr:
         """
         Check if values start with a binary substring.
 
         Parameters
         ----------
         prefix
             Prefix substring.
 
         """
-        return wrap_expr(self._pyexpr.binary_starts_with(prefix))
+        return wrap_expr(self._pyexpr.bin_starts_with(prefix))
 
     def decode(self, encoding: TransferEncoding, *, strict: bool = True) -> Expr:
         """
         Decode a value using the provided encoding.
 
         Parameters
         ----------
@@ -67,17 +67,17 @@
             The encoding to use.
         strict
             Raise an error if the underlying value cannot be decoded,
             otherwise mask out with a null value.
 
         """
         if encoding == "hex":
-            return wrap_expr(self._pyexpr.binary_hex_decode(strict))
+            return wrap_expr(self._pyexpr.bin_hex_decode(strict))
         elif encoding == "base64":
-            return wrap_expr(self._pyexpr.binary_base64_decode(strict))
+            return wrap_expr(self._pyexpr.bin_base64_decode(strict))
         else:
             raise ValueError(
                 f"encoding must be one of {{'hex', 'base64'}}, got {encoding}"
             )
 
     def encode(self, encoding: TransferEncoding) -> Expr:
         """
@@ -90,14 +90,14 @@
 
         Returns
         -------
         Binary array with values encoded using provided encoding
 
         """
         if encoding == "hex":
-            return wrap_expr(self._pyexpr.binary_hex_encode())
+            return wrap_expr(self._pyexpr.bin_hex_encode())
         elif encoding == "base64":
-            return wrap_expr(self._pyexpr.binary_base64_encode())
+            return wrap_expr(self._pyexpr.bin_base64_encode())
         else:
             raise ValueError(
                 f"encoding must be one of {{'hex', 'base64'}}, got {encoding}"
             )
```

### Comparing `polars_lts_cpu-0.17.9/polars/expr/categorical.py` & `polars_lts_cpu-0.18.0/polars/expr/categorical.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING
 
 from polars.utils._wrap import wrap_expr
 
 if TYPE_CHECKING:
-    from polars.expr.expr import Expr
+    from polars import Expr
     from polars.type_aliases import CategoricalOrdering
 
 
 class ExprCatNameSpace:
     """Namespace for categorical related expressions."""
 
     _accessor = "cat"
```

### Comparing `polars_lts_cpu-0.17.9/polars/expr/datetime.py` & `polars_lts_cpu-0.18.0/polars/expr/datetime.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,26 +1,33 @@
 from __future__ import annotations
 
 import datetime as dt
+import warnings
 from typing import TYPE_CHECKING
 
+import polars._reexport as pl
 from polars import functions as F
-from polars import internals as pli
 from polars.datatypes import DTYPE_TEMPORAL_UNITS, Date, Int32
-from polars.utils._parse_expr_input import expr_to_lit_or_expr
+from polars.utils._parse_expr_input import parse_as_expression
 from polars.utils._wrap import wrap_expr
 from polars.utils.convert import _timedelta_to_pl_duration
 from polars.utils.decorators import deprecated_alias
+from polars.utils.various import find_stacklevel
 
 if TYPE_CHECKING:
     from datetime import timedelta
 
-    from polars.expr import Expr
+    from polars import Expr
     from polars.type_aliases import EpochTimeUnit, TimeUnit
 
+TIME_ZONE_DEPRECATION_MESSAGE = (
+    "In a future version of polars, time zones other than those in `zoneinfo.available_timezones()` "
+    "will no longer be supported. Please use one of them instead."
+)
+
 
 class ExprDateTimeNameSpace:
     """Namespace for datetime related expressions."""
 
     _accessor = "dt"
 
     def __init__(self, expr: Expr):
@@ -75,58 +82,58 @@
 
         Examples
         --------
         >>> from datetime import timedelta, datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 2)
         >>> df = pl.date_range(
-        ...     start, stop, timedelta(minutes=225), name="dates"
+        ...     start, stop, timedelta(minutes=225), eager=True
         ... ).to_frame()
         >>> df
         shape: (7, 1)
         
-         dates               
+         date                
          ---                 
          datetime[s]        
         
          2001-01-01 00:00:00 
          2001-01-01 03:45:00 
          2001-01-01 07:30:00 
          2001-01-01 11:15:00 
          2001-01-01 15:00:00 
          2001-01-01 18:45:00 
          2001-01-01 22:30:00 
         
-        >>> df.select(pl.col("dates").dt.truncate("1h"))
+        >>> df.select(pl.col("date").dt.truncate("1h"))
         shape: (7, 1)
         
-         dates               
+         date                
          ---                 
          datetime[s]        
         
          2001-01-01 00:00:00 
          2001-01-01 03:00:00 
          2001-01-01 07:00:00 
          2001-01-01 11:00:00 
          2001-01-01 15:00:00 
          2001-01-01 18:00:00 
          2001-01-01 22:00:00 
         
-        >>> df.select(pl.col("dates").dt.truncate("1h")).frame_equal(
-        ...     df.select(pl.col("dates").dt.truncate(timedelta(hours=1)))
+        >>> df.select(pl.col("date").dt.truncate("1h")).frame_equal(
+        ...     df.select(pl.col("date").dt.truncate(timedelta(hours=1)))
         ... )
         True
 
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 1, 1)
-        >>> df = pl.date_range(start, stop, "10m", name="dates").to_frame()
-        >>> df.select(["dates", pl.col("dates").dt.truncate("30m").alias("truncate")])
+        >>> df = pl.date_range(start, stop, "10m", eager=True).to_frame()
+        >>> df.select(["date", pl.col("date").dt.truncate("30m").alias("truncate")])
         shape: (7, 2)
         
-         dates                truncate            
+         date                 truncate            
          ---                  ---                 
          datetime[s]         datetime[s]        
         
          2001-01-01 00:00:00  2001-01-01 00:00:00 
          2001-01-01 00:10:00  2001-01-01 00:00:00 
          2001-01-01 00:20:00  2001-01-01 00:00:00 
          2001-01-01 00:30:00  2001-01-01 00:30:00 
@@ -199,58 +206,58 @@
 
         Examples
         --------
         >>> from datetime import timedelta, datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 2)
         >>> df = pl.date_range(
-        ...     start, stop, timedelta(minutes=225), name="dates"
+        ...     start, stop, timedelta(minutes=225), eager=True
         ... ).to_frame()
         >>> df
         shape: (7, 1)
         
-         dates               
+         date                
          ---                 
          datetime[s]        
         
          2001-01-01 00:00:00 
          2001-01-01 03:45:00 
          2001-01-01 07:30:00 
          2001-01-01 11:15:00 
          2001-01-01 15:00:00 
          2001-01-01 18:45:00 
          2001-01-01 22:30:00 
         
-        >>> df.select(pl.col("dates").dt.round("1h"))
+        >>> df.select(pl.col("date").dt.round("1h"))
         shape: (7, 1)
         
-         dates               
+         date                
          ---                 
          datetime[s]        
         
          2001-01-01 00:00:00 
          2001-01-01 04:00:00 
          2001-01-01 08:00:00 
          2001-01-01 11:00:00 
          2001-01-01 15:00:00 
          2001-01-01 19:00:00 
          2001-01-01 23:00:00 
         
-        >>> df.select(pl.col("dates").dt.round("1h")).frame_equal(
-        ...     df.select(pl.col("dates").dt.round(timedelta(hours=1)))
+        >>> df.select(pl.col("date").dt.round("1h")).frame_equal(
+        ...     df.select(pl.col("date").dt.round(timedelta(hours=1)))
         ... )
         True
 
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 1, 1)
-        >>> df = pl.date_range(start, stop, "10m", name="dates").to_frame()
-        >>> df.select(["dates", pl.col("dates").dt.round("30m").alias("round")])
+        >>> df = pl.date_range(start, stop, "10m", eager=True).to_frame()
+        >>> df.select(["date", pl.col("date").dt.round("30m").alias("round")])
         shape: (7, 2)
         
-         dates                round               
+         date                 round               
          ---                  ---                 
          datetime[s]         datetime[s]        
         
          2001-01-01 00:00:00  2001-01-01 00:00:00 
          2001-01-01 00:10:00  2001-01-01 00:00:00 
          2001-01-01 00:20:00  2001-01-01 00:30:00 
          2001-01-01 00:30:00  2001-01-01 00:30:00 
@@ -320,64 +327,117 @@
          ---                      ---                      ---                 
          datetime[s]             datetime[s]             datetime[s]        
         
          2022-12-31 01:02:03.456  2022-10-10 01:02:03.456  2022-10-10 04:05:06 
          2023-07-05 07:08:09.101  2022-07-05 07:08:09.101  2022-07-05 04:05:06 
         
         """
-        if not isinstance(time, (dt.time, pli.Expr)):
+        if not isinstance(time, (dt.time, pl.Expr)):
             raise TypeError(
                 f"expected 'time' to be a python time or polars expression, found {time!r}"
             )
-        time = expr_to_lit_or_expr(time)
-        return wrap_expr(self._pyexpr.dt_combine(time._pyexpr, time_unit))
+        time = parse_as_expression(time)._pyexpr
+        return wrap_expr(self._pyexpr.dt_combine(time, time_unit))
+
+    def to_string(self, format: str) -> Expr:
+        """
+        Convert a Date/Time/Datetime column into a Utf8 column with the given format.
+
+        Similar to ``cast(pl.Utf8)``, but this method allows you to customize the
+        formatting of the resulting string.
+
+        Parameters
+        ----------
+        format
+            Format to use, refer to the `chrono strftime documentation
+            <https://docs.rs/chrono/latest/chrono/format/strftime/index.html>`_
+            for specification. Example: ``"%y-%m-%d"``.
+
+        Examples
+        --------
+        >>> from datetime import datetime
+        >>> df = pl.DataFrame(
+        ...     {
+        ...         "datetime": [
+        ...             datetime(2020, 3, 1),
+        ...             datetime(2020, 4, 1),
+        ...             datetime(2020, 5, 1),
+        ...         ]
+        ...     }
+        ... )
+        >>> df.with_columns(
+        ...     pl.col("datetime")
+        ...     .dt.to_string("%Y/%m/%d %H:%M:%S")
+        ...     .alias("datetime_string")
+        ... )
+        shape: (3, 2)
+        
+         datetime             datetime_string     
+         ---                  ---                 
+         datetime[s]         str                 
+        
+         2020-03-01 00:00:00  2020/03/01 00:00:00 
+         2020-04-01 00:00:00  2020/04/01 00:00:00 
+         2020-05-01 00:00:00  2020/05/01 00:00:00 
+        
+
+        """
+        return wrap_expr(self._pyexpr.dt_to_string(format))
 
     @deprecated_alias(fmt="format")
     def strftime(self, format: str) -> Expr:
         """
-        Format Date/Datetime with a formatting rule.
+        Convert a Date/Time/Datetime column into a Utf8 column with the given format.
+
+        Similar to ``cast(pl.Utf8)``, but this method allows you to customize the
+        formatting of the resulting string.
+
+        Alias for :func:`to_string`.
 
         Parameters
         ----------
         format
             Format to use, refer to the `chrono strftime documentation
             <https://docs.rs/chrono/latest/chrono/format/strftime/index.html>`_
             for specification. Example: ``"%y-%m-%d"``.
 
         Examples
         --------
-        >>> from datetime import timedelta, datetime
+        >>> from datetime import datetime
         >>> df = pl.DataFrame(
         ...     {
-        ...         "date": pl.date_range(
-        ...             datetime(2020, 3, 1), datetime(2020, 5, 1), "1mo"
-        ...         ),
+        ...         "datetime": [
+        ...             datetime(2020, 3, 1),
+        ...             datetime(2020, 4, 1),
+        ...             datetime(2020, 5, 1),
+        ...         ]
         ...     }
         ... )
-        >>> df.select(
-        ...     [
-        ...         pl.col("date"),
-        ...         pl.col("date")
-        ...         .dt.strftime("%Y/%m/%d %H:%M:%S")
-        ...         .alias("date_formatted"),
-        ...     ]
+        >>> df.with_columns(
+        ...     pl.col("datetime")
+        ...     .dt.strftime("%Y/%m/%d %H:%M:%S")
+        ...     .alias("datetime_string")
         ... )
         shape: (3, 2)
         
-         date                 date_formatted      
+         datetime             datetime_string     
          ---                  ---                 
          datetime[s]         str                 
         
          2020-03-01 00:00:00  2020/03/01 00:00:00 
          2020-04-01 00:00:00  2020/04/01 00:00:00 
          2020-05-01 00:00:00  2020/05/01 00:00:00 
         
 
+        See Also
+        --------
+        to_string : The identical expression for which ``strftime`` is an alias.
+
         """
-        return wrap_expr(self._pyexpr.strftime(format))
+        return self.to_string(format)
 
     def year(self) -> Expr:
         """
         Extract year from underlying Date representation.
 
         Applies to Date and Datetime columns.
 
@@ -388,15 +448,17 @@
         Year as Int32
 
         Examples
         --------
         >>> from datetime import timedelta, datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2002, 7, 1)
-        >>> df = pl.DataFrame({"date": pl.date_range(start, stop, timedelta(days=180))})
+        >>> df = pl.DataFrame(
+        ...     {"date": pl.date_range(start, stop, timedelta(days=180), eager=True)}
+        ... )
         >>> df
         shape: (4, 1)
         
          date                
          ---                 
          datetime[s]        
         
@@ -415,15 +477,15 @@
          2001 
          2001 
          2001 
          2002 
         
 
         """
-        return wrap_expr(self._pyexpr.year())
+        return wrap_expr(self._pyexpr.dt_year())
 
     def is_leap_year(self) -> Expr:
         """
         Determine whether the year of the underlying date is a leap year.
 
         Applies to Date and Datetime columns.
 
@@ -432,15 +494,17 @@
         Leap year info as Boolean
 
         Examples
         --------
         >>> from datetime import datetime
         >>> start = datetime(2000, 1, 1)
         >>> stop = datetime(2002, 1, 1)
-        >>> df = pl.DataFrame({"date": pl.date_range(start, stop, interval="1y")})
+        >>> df = pl.DataFrame(
+        ...     {"date": pl.date_range(start, stop, interval="1y", eager=True)}
+        ... )
         >>> df
         shape: (3, 1)
         
          date                
          ---                 
          datetime[s]        
         
@@ -457,15 +521,15 @@
         
          true  
          false 
          false 
         
 
         """
-        return wrap_expr(self._pyexpr.is_leap_year())
+        return wrap_expr(self._pyexpr.dt_is_leap_year())
 
     def iso_year(self) -> Expr:
         """
         Extract ISO year from underlying Date representation.
 
         Applies to Date and Datetime columns.
 
@@ -477,15 +541,17 @@
         ISO Year as Int32
 
         Examples
         --------
         >>> from datetime import timedelta, datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2006, 1, 1)
-        >>> df = pl.DataFrame({"date": pl.date_range(start, stop, timedelta(days=180))})
+        >>> df = pl.DataFrame(
+        ...     {"date": pl.date_range(start, stop, timedelta(days=180), eager=True)}
+        ... )
         >>> df.select(
         ...     [
         ...         pl.col("date"),
         ...         pl.col("date").dt.iso_year().alias("iso_year"),
         ...     ]
         ... )
         shape: (11, 2)
@@ -502,15 +568,15 @@
          2004-06-14 00:00:00  2004     
          2004-12-11 00:00:00  2004     
          2005-06-09 00:00:00  2005     
          2005-12-06 00:00:00  2005     
         
 
         """
-        return wrap_expr(self._pyexpr.iso_year())
+        return wrap_expr(self._pyexpr.dt_iso_year())
 
     def quarter(self) -> Expr:
         """
         Extract quarter from underlying Date representation.
 
         Applies to Date and Datetime columns.
 
@@ -521,15 +587,17 @@
         Quarter as UInt32
 
         Examples
         --------
         >>> from datetime import timedelta, datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2002, 6, 1)
-        >>> df = pl.DataFrame({"date": pl.date_range(start, stop, timedelta(days=180))})
+        >>> df = pl.DataFrame(
+        ...     {"date": pl.date_range(start, stop, timedelta(days=180), eager=True)}
+        ... )
         >>> df
         shape: (3, 1)
         
          date                
          ---                 
          datetime[s]        
         
@@ -546,15 +614,15 @@
         
          1    
          2    
          4    
         
 
         """
-        return wrap_expr(self._pyexpr.quarter())
+        return wrap_expr(self._pyexpr.dt_quarter())
 
     def month(self) -> Expr:
         """
         Extract month from underlying Date representation.
 
         Applies to Date and Datetime columns.
 
@@ -566,15 +634,17 @@
         Month as UInt32
 
         Examples
         --------
         >>> from datetime import timedelta, datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 4, 1)
-        >>> df = pl.DataFrame({"date": pl.date_range(start, stop, timedelta(days=31))})
+        >>> df = pl.DataFrame(
+        ...     {"date": pl.date_range(start, stop, timedelta(days=31), eager=True)}
+        ... )
         >>> df
         shape: (3, 1)
         
          date                
          ---                 
          datetime[s]        
         
@@ -591,15 +661,15 @@
         
          1    
          2    
          3    
         
 
         """
-        return wrap_expr(self._pyexpr.month())
+        return wrap_expr(self._pyexpr.dt_month())
 
     def week(self) -> Expr:
         """
         Extract the week from the underlying Date representation.
 
         Applies to Date and Datetime columns.
 
@@ -611,15 +681,17 @@
         Week number as UInt32
 
         Examples
         --------
         >>> from datetime import timedelta, datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 4, 1)
-        >>> df = pl.DataFrame({"date": pl.date_range(start, stop, timedelta(days=31))})
+        >>> df = pl.DataFrame(
+        ...     {"date": pl.date_range(start, stop, timedelta(days=31), eager=True)}
+        ... )
         >>> df
         shape: (3, 1)
         
          date                
          ---                 
          datetime[s]        
         
@@ -636,15 +708,15 @@
         
          1    
          5    
          9    
         
 
         """
-        return wrap_expr(self._pyexpr.week())
+        return wrap_expr(self._pyexpr.dt_week())
 
     def weekday(self) -> Expr:
         """
         Extract the week day from the underlying Date representation.
 
         Applies to Date and Datetime columns.
 
@@ -655,15 +727,17 @@
         Week day as UInt32
 
         Examples
         --------
         >>> from datetime import timedelta, datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 9)
-        >>> df = pl.DataFrame({"date": pl.date_range(start, stop, timedelta(days=3))})
+        >>> df = pl.DataFrame(
+        ...     {"date": pl.date_range(start, stop, timedelta(days=3), eager=True)}
+        ... )
         >>> df
         shape: (3, 1)
         
          date                
          ---                 
          datetime[s]        
         
@@ -686,15 +760,15 @@
         
          1        1             1           
          4        4             4           
          7        7             7           
         
 
         """
-        return wrap_expr(self._pyexpr.weekday())
+        return wrap_expr(self._pyexpr.dt_weekday())
 
     def day(self) -> Expr:
         """
         Extract day from underlying Date representation.
 
         Applies to Date and Datetime columns.
 
@@ -706,15 +780,17 @@
         Day as UInt32
 
         Examples
         --------
         >>> from datetime import timedelta, datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 9)
-        >>> df = pl.DataFrame({"date": pl.date_range(start, stop, timedelta(days=3))})
+        >>> df = pl.DataFrame(
+        ...     {"date": pl.date_range(start, stop, timedelta(days=3), eager=True)}
+        ... )
         >>> df
         shape: (3, 1)
         
          date                
          ---                 
          datetime[s]        
         
@@ -737,15 +813,15 @@
         
          1        1             1           
          4        4             4           
          7        7             7           
         
 
         """
-        return wrap_expr(self._pyexpr.day())
+        return wrap_expr(self._pyexpr.dt_day())
 
     def ordinal_day(self) -> Expr:
         """
         Extract ordinal day from underlying Date representation.
 
         Applies to Date and Datetime columns.
 
@@ -757,15 +833,17 @@
         Day as UInt32
 
         Examples
         --------
         >>> from datetime import timedelta, datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 9)
-        >>> df = pl.DataFrame({"date": pl.date_range(start, stop, timedelta(days=3))})
+        >>> df = pl.DataFrame(
+        ...     {"date": pl.date_range(start, stop, timedelta(days=3), eager=True)}
+        ... )
         >>> df
         shape: (3, 1)
         
          date                
          ---                 
          datetime[s]        
         
@@ -788,24 +866,24 @@
         
          1        1             1           
          4        4             4           
          7        7             7           
         
 
         """
-        return wrap_expr(self._pyexpr.ordinal_day())
+        return wrap_expr(self._pyexpr.dt_ordinal_day())
 
     def time(self) -> Expr:
-        return wrap_expr(self._pyexpr.time())
+        return wrap_expr(self._pyexpr.dt_time())
 
     def date(self) -> Expr:
-        return wrap_expr(self._pyexpr.date())
+        return wrap_expr(self._pyexpr.dt_date())
 
     def datetime(self) -> Expr:
-        return wrap_expr(self._pyexpr.datetime())
+        return wrap_expr(self._pyexpr.dt_datetime())
 
     def hour(self) -> Expr:
         """
         Extract hour from underlying DateTime representation.
 
         Applies to Datetime columns.
 
@@ -816,15 +894,17 @@
         Hour as UInt32
 
         Examples
         --------
         >>> from datetime import timedelta, datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 2)
-        >>> df = pl.DataFrame({"date": pl.date_range(start, stop, timedelta(hours=12))})
+        >>> df = pl.DataFrame(
+        ...     {"date": pl.date_range(start, stop, timedelta(hours=12), eager=True)}
+        ... )
         >>> df
         shape: (3, 1)
         
          date                
          ---                 
          datetime[s]        
         
@@ -841,15 +921,15 @@
         
          0    
          12   
          0    
         
 
         """
-        return wrap_expr(self._pyexpr.hour())
+        return wrap_expr(self._pyexpr.dt_hour())
 
     def minute(self) -> Expr:
         """
         Extract minutes from underlying DateTime representation.
 
         Applies to Datetime columns.
 
@@ -861,15 +941,15 @@
 
         Examples
         --------
         >>> from datetime import timedelta, datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 1, 0, 4, 0)
         >>> df = pl.DataFrame(
-        ...     {"date": pl.date_range(start, stop, timedelta(minutes=2))}
+        ...     {"date": pl.date_range(start, stop, timedelta(minutes=2), eager=True)}
         ... )
         >>> df
         shape: (3, 1)
         
          date                
          ---                 
          datetime[s]        
@@ -887,15 +967,15 @@
         
          0    
          2    
          4    
         
 
         """
-        return wrap_expr(self._pyexpr.minute())
+        return wrap_expr(self._pyexpr.dt_minute())
 
     def second(self, *, fractional: bool = False) -> Expr:
         """
         Extract seconds from underlying DateTime representation.
 
         Applies to Datetime columns.
 
@@ -917,14 +997,15 @@
         >>> from datetime import timedelta, datetime
         >>> df = pl.DataFrame(
         ...     data={
         ...         "date": pl.date_range(
         ...             start=datetime(2001, 1, 1, 0, 0, 0, 456789),
         ...             end=datetime(2001, 1, 1, 0, 0, 6),
         ...             interval=timedelta(seconds=2, microseconds=654321),
+        ...             eager=True,
         ...         )
         ...     }
         ... )
         >>> df
         shape: (3, 1)
         
          date                       
@@ -959,15 +1040,15 @@
          5.765431 
         
 
         >>> from datetime import timedelta, datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 1, 0, 0, 4)
         >>> df = pl.DataFrame(
-        ...     {"date": pl.date_range(start, stop, timedelta(seconds=2))}
+        ...     {"date": pl.date_range(start, stop, timedelta(seconds=2), eager=True)}
         ... )
         >>> df
         shape: (3, 1)
         
          date                
          ---                 
          datetime[s]        
@@ -985,17 +1066,17 @@
         
          0    
          2    
          4    
         
 
         """
-        sec = wrap_expr(self._pyexpr.second())
+        sec = wrap_expr(self._pyexpr.dt_second())
         return (
-            sec + (wrap_expr(self._pyexpr.nanosecond()) / F.lit(1_000_000_000.0))
+            sec + (wrap_expr(self._pyexpr.dt_nanosecond()) / F.lit(1_000_000_000.0))
             if fractional
             else sec
         )
 
     def millisecond(self) -> Expr:
         """
         Extract milliseconds from underlying DateTime representation.
@@ -1003,15 +1084,15 @@
         Applies to Datetime columns.
 
         Returns
         -------
         Milliseconds as UInt32
 
         """
-        return wrap_expr(self._pyexpr.millisecond())
+        return wrap_expr(self._pyexpr.dt_millisecond())
 
     def microsecond(self) -> Expr:
         """
         Extract microseconds from underlying DateTime representation.
 
         Applies to Datetime columns.
 
@@ -1021,15 +1102,18 @@
 
         Examples
         --------
         >>> from datetime import datetime
         >>> df = pl.DataFrame(
         ...     {
         ...         "date": pl.date_range(
-        ...             datetime(2020, 1, 1), datetime(2020, 1, 1, 0, 0, 1, 0), "1ms"
+        ...             datetime(2020, 1, 1),
+        ...             datetime(2020, 1, 1, 0, 0, 1, 0),
+        ...             "1ms",
+        ...             eager=True,
         ...         ),
         ...     }
         ... )
         >>> df.select(
         ...     [
         ...         pl.col("date"),
         ...         pl.col("date").dt.microsecond().alias("microseconds"),
@@ -1049,28 +1133,28 @@
          2020-01-01 00:00:00.997  997000       
          2020-01-01 00:00:00.998  998000       
          2020-01-01 00:00:00.999  999000       
          2020-01-01 00:00:01      0            
         
 
         """
-        return wrap_expr(self._pyexpr.microsecond())
+        return wrap_expr(self._pyexpr.dt_microsecond())
 
     def nanosecond(self) -> Expr:
         """
         Extract nanoseconds from underlying DateTime representation.
 
         Applies to Datetime columns.
 
         Returns
         -------
         Nanoseconds as UInt32
 
         """
-        return wrap_expr(self._pyexpr.nanosecond())
+        return wrap_expr(self._pyexpr.dt_nanosecond())
 
     def epoch(self, time_unit: EpochTimeUnit = "us") -> Expr:
         """
         Get the time passed since the Unix EPOCH in the give time unit.
 
         Parameters
         ----------
@@ -1078,15 +1162,17 @@
             Time unit.
 
         Examples
         --------
         >>> from datetime import timedelta, datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 3)
-        >>> df = pl.DataFrame({"date": pl.date_range(start, stop, timedelta(days=1))})
+        >>> df = pl.DataFrame(
+        ...     {"date": pl.date_range(start, stop, timedelta(days=1), eager=True)}
+        ... )
         >>> df.select(
         ...     [
         ...         pl.col("date"),
         ...         pl.col("date").dt.epoch().alias("epoch_ns"),
         ...         pl.col("date").dt.epoch(time_unit="s").alias("epoch_s"),
         ...     ]
         ... )
@@ -1123,15 +1209,17 @@
             Time unit.
 
         Examples
         --------
         >>> from datetime import timedelta, datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 3)
-        >>> df = pl.DataFrame({"date": pl.date_range(start, stop, timedelta(days=1))})
+        >>> df = pl.DataFrame(
+        ...     {"date": pl.date_range(start, stop, timedelta(days=1), eager=True)}
+        ... )
         >>> df.select(
         ...     [
         ...         pl.col("date"),
         ...         pl.col("date").dt.timestamp().alias("timestamp_ns"),
         ...         pl.col("date").dt.timestamp("ms").alias("timestamp_ms"),
         ...     ]
         ... )
@@ -1143,15 +1231,15 @@
         
          2001-01-01 00:00:00  978307200000000  978307200000 
          2001-01-02 00:00:00  978393600000000  978393600000 
          2001-01-03 00:00:00  978480000000000  978480000000 
         
 
         """
-        return wrap_expr(self._pyexpr.timestamp(time_unit))
+        return wrap_expr(self._pyexpr.dt_timestamp(time_unit))
 
     def with_time_unit(self, time_unit: TimeUnit) -> Expr:
         """
         Set time unit of a Series of dtype Datetime or Duration.
 
         This does not modify underlying data, and should be used to fix an incorrect
         time unit.
@@ -1163,15 +1251,19 @@
 
         Examples
         --------
         >>> from datetime import datetime
         >>> df = pl.DataFrame(
         ...     {
         ...         "date": pl.date_range(
-        ...             datetime(2001, 1, 1), datetime(2001, 1, 3), "1d", time_unit="ns"
+        ...             datetime(2001, 1, 1),
+        ...             datetime(2001, 1, 3),
+        ...             "1d",
+        ...             time_unit="ns",
+        ...             eager=True,
         ...         )
         ...     }
         ... )
         >>> df.select(
         ...     [
         ...         pl.col("date"),
         ...         pl.col("date").dt.with_time_unit("us").alias("time_unit_us"),
@@ -1202,15 +1294,15 @@
 
         Examples
         --------
         >>> from datetime import datetime
         >>> df = pl.DataFrame(
         ...     {
         ...         "date": pl.date_range(
-        ...             datetime(2001, 1, 1), datetime(2001, 1, 3), "1d"
+        ...             datetime(2001, 1, 1), datetime(2001, 1, 3), "1d", eager=True
         ...         )
         ...     }
         ... )
         >>> df.select(
         ...     [
         ...         pl.col("date"),
         ...         pl.col("date").dt.cast_time_unit("ms").alias("time_unit_ms"),
@@ -1246,14 +1338,15 @@
         >>> df = pl.DataFrame(
         ...     {
         ...         "date": pl.date_range(
         ...             datetime(2020, 3, 1),
         ...             datetime(2020, 5, 1),
         ...             "1mo",
         ...             time_zone="UTC",
+        ...             eager=True,
         ...         ),
         ...     }
         ... )
         >>> df.select(
         ...     [
         ...         pl.col("date"),
         ...         pl.col("date")
@@ -1268,14 +1361,22 @@
          datetime[s, UTC]        datetime[s, Europe/London] 
         
          2020-03-01 00:00:00 UTC  2020-03-01 00:00:00 GMT     
          2020-04-01 00:00:00 UTC  2020-04-01 01:00:00 BST     
          2020-05-01 00:00:00 UTC  2020-05-01 01:00:00 BST     
         
         """
+        from polars.dependencies import zoneinfo
+
+        if time_zone not in zoneinfo.available_timezones():
+            warnings.warn(
+                TIME_ZONE_DEPRECATION_MESSAGE,
+                DeprecationWarning,
+                stacklevel=find_stacklevel(),
+            )
         return wrap_expr(self._pyexpr.dt_convert_time_zone(time_zone))
 
     def replace_time_zone(
         self, time_zone: str | None, *, use_earliest: bool | None = None
     ) -> Expr:
         """
         Replace time zone for a Series of type Datetime.
@@ -1298,14 +1399,15 @@
         >>> df = pl.DataFrame(
         ...     {
         ...         "london_timezone": pl.date_range(
         ...             datetime(2020, 3, 1),
         ...             datetime(2020, 7, 1),
         ...             "1mo",
         ...             time_zone="UTC",
+        ...             eager=True,
         ...         ).dt.convert_time_zone(time_zone="Europe/London"),
         ...     }
         ... )
         >>> df.select(
         ...     [
         ...         pl.col("london_timezone"),
         ...         pl.col("london_timezone")
@@ -1364,14 +1466,22 @@
          2018-10-28 02:00:00  true   2018-10-28 02:00:00 CEST      
          2018-10-28 02:30:00  true   2018-10-28 02:30:00 CEST      
          2018-10-28 02:00:00  false  2018-10-28 02:00:00 CET       
          2018-10-28 02:30:00  false  2018-10-28 02:30:00 CET       
         
 
         """
+        from polars.dependencies import zoneinfo
+
+        if time_zone is not None and time_zone not in zoneinfo.available_timezones():
+            warnings.warn(
+                TIME_ZONE_DEPRECATION_MESSAGE,
+                DeprecationWarning,
+                stacklevel=find_stacklevel(),
+            )
         return wrap_expr(self._pyexpr.dt_replace_time_zone(time_zone, use_earliest))
 
     def days(self) -> Expr:
         """
         Extract the days from a Duration type.
 
         Returns
@@ -1380,15 +1490,15 @@
 
         Examples
         --------
         >>> from datetime import datetime
         >>> df = pl.DataFrame(
         ...     {
         ...         "date": pl.date_range(
-        ...             datetime(2020, 3, 1), datetime(2020, 5, 1), "1mo"
+        ...             datetime(2020, 3, 1), datetime(2020, 5, 1), "1mo", eager=True
         ...         ),
         ...     }
         ... )
         >>> df.select(
         ...     [
         ...         pl.col("date"),
         ...         pl.col("date").diff().dt.days().alias("days_diff"),
@@ -1418,15 +1528,15 @@
 
         Examples
         --------
         >>> from datetime import datetime
         >>> df = pl.DataFrame(
         ...     {
         ...         "date": pl.date_range(
-        ...             datetime(2020, 1, 1), datetime(2020, 1, 4), "1d"
+        ...             datetime(2020, 1, 1), datetime(2020, 1, 4), "1d", eager=True
         ...         ),
         ...     }
         ... )
         >>> df.select(
         ...     [
         ...         pl.col("date"),
         ...         pl.col("date").diff().dt.hours().alias("hours_diff"),
@@ -1457,15 +1567,15 @@
 
         Examples
         --------
         >>> from datetime import datetime
         >>> df = pl.DataFrame(
         ...     {
         ...         "date": pl.date_range(
-        ...             datetime(2020, 1, 1), datetime(2020, 1, 4), "1d"
+        ...             datetime(2020, 1, 1), datetime(2020, 1, 4), "1d", eager=True
         ...         ),
         ...     }
         ... )
         >>> df.select(
         ...     [
         ...         pl.col("date"),
         ...         pl.col("date").diff().dt.minutes().alias("minutes_diff"),
@@ -1496,15 +1606,18 @@
 
         Examples
         --------
         >>> from datetime import datetime
         >>> df = pl.DataFrame(
         ...     {
         ...         "date": pl.date_range(
-        ...             datetime(2020, 1, 1), datetime(2020, 1, 1, 0, 4, 0), "1m"
+        ...             datetime(2020, 1, 1),
+        ...             datetime(2020, 1, 1, 0, 4, 0),
+        ...             "1m",
+        ...             eager=True,
         ...         ),
         ...     }
         ... )
         >>> df.select(
         ...     [
         ...         pl.col("date"),
         ...         pl.col("date").diff().dt.seconds().alias("seconds_diff"),
@@ -1536,15 +1649,18 @@
 
         Examples
         --------
         >>> from datetime import datetime
         >>> df = pl.DataFrame(
         ...     {
         ...         "date": pl.date_range(
-        ...             datetime(2020, 1, 1), datetime(2020, 1, 1, 0, 0, 1, 0), "1ms"
+        ...             datetime(2020, 1, 1),
+        ...             datetime(2020, 1, 1, 0, 0, 1, 0),
+        ...             "1ms",
+        ...             eager=True,
         ...         ),
         ...     }
         ... )
         >>> df.select(
         ...     [
         ...         pl.col("date"),
         ...         pl.col("date").diff().dt.milliseconds().alias("milliseconds_diff"),
@@ -1580,15 +1696,18 @@
 
         Examples
         --------
         >>> from datetime import datetime
         >>> df = pl.DataFrame(
         ...     {
         ...         "date": pl.date_range(
-        ...             datetime(2020, 1, 1), datetime(2020, 1, 1, 0, 0, 1, 0), "1ms"
+        ...             datetime(2020, 1, 1),
+        ...             datetime(2020, 1, 1, 0, 0, 1, 0),
+        ...             "1ms",
+        ...             eager=True,
         ...         ),
         ...     }
         ... )
         >>> df.select(
         ...     [
         ...         pl.col("date"),
         ...         pl.col("date").diff().dt.microseconds().alias("microseconds_diff"),
@@ -1624,15 +1743,18 @@
 
         Examples
         --------
         >>> from datetime import datetime
         >>> df = pl.DataFrame(
         ...     {
         ...         "date": pl.date_range(
-        ...             datetime(2020, 1, 1), datetime(2020, 1, 1, 0, 0, 1, 0), "1ms"
+        ...             datetime(2020, 1, 1),
+        ...             datetime(2020, 1, 1, 0, 0, 1, 0),
+        ...             "1ms",
+        ...             eager=True,
         ...         ),
         ...     }
         ... )
         >>> df.select(
         ...     [
         ...         pl.col("date"),
         ...         pl.col("date").diff().dt.nanoseconds().alias("nanoseconds_diff"),
@@ -1693,15 +1815,15 @@
 
         Examples
         --------
         >>> from datetime import datetime
         >>> df = pl.DataFrame(
         ...     {
         ...         "dates": pl.date_range(
-        ...             datetime(2000, 1, 1), datetime(2005, 1, 1), "1y"
+        ...             datetime(2000, 1, 1), datetime(2005, 1, 1), "1y", eager=True
         ...         )
         ...     }
         ... )
         >>> df.select(
         ...     [
         ...         pl.col("dates").dt.offset_by("1y").alias("date_plus_1y"),
         ...         pl.col("dates").dt.offset_by("-1y2mo").alias("date_min"),
@@ -1760,15 +1882,18 @@
 
         Examples
         --------
         >>> from datetime import datetime
         >>> df = pl.DataFrame(
         ...     {
         ...         "dates": pl.date_range(
-        ...             datetime(2000, 1, 15, 2), datetime(2000, 12, 15, 2), "1mo"
+        ...             datetime(2000, 1, 15, 2),
+        ...             datetime(2000, 12, 15, 2),
+        ...             "1mo",
+        ...             eager=True,
         ...         )
         ...     }
         ... )
         >>> df.select(pl.col("dates").dt.month_start())
         shape: (12, 1)
         
          dates               
@@ -1803,15 +1928,18 @@
 
         Examples
         --------
         >>> from datetime import datetime
         >>> df = pl.DataFrame(
         ...     {
         ...         "dates": pl.date_range(
-        ...             datetime(2000, 1, 1, 2), datetime(2000, 12, 1, 2), "1mo"
+        ...             datetime(2000, 1, 1, 2),
+        ...             datetime(2000, 12, 1, 2),
+        ...             "1mo",
+        ...             eager=True,
         ...         )
         ...     }
         ... )
         >>> df.select(pl.col("dates").dt.month_end())
         shape: (12, 1)
         
          dates               
```

### Comparing `polars_lts_cpu-0.17.9/polars/expr/expr.py` & `polars_lts_cpu-0.18.0/polars/expr/expr.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,86 +14,88 @@
     Collection,
     FrozenSet,
     Iterable,
     NoReturn,
     Sequence,
     Set,
     TypeVar,
-    cast,
 )
 
+import polars._reexport as pl
 from polars import functions as F
-from polars import internals as pli
 from polars.datatypes import (
     FLOAT_DTYPES,
     INTEGER_DTYPES,
     Categorical,
     Struct,
     UInt32,
     Utf8,
     is_polars_dtype,
     py_type_to_dtype,
 )
 from polars.dependencies import _check_for_numpy
 from polars.dependencies import numpy as np
+from polars.expr.array import ExprArrayNameSpace
 from polars.expr.binary import ExprBinaryNameSpace
 from polars.expr.categorical import ExprCatNameSpace
 from polars.expr.datetime import ExprDateTimeNameSpace
 from polars.expr.list import ExprListNameSpace
 from polars.expr.meta import ExprMetaNameSpace
 from polars.expr.string import ExprStringNameSpace
 from polars.expr.struct import ExprStructNameSpace
-from polars.utils._parse_expr_input import expr_to_lit_or_expr, selection_to_pyexpr_list
+from polars.utils._parse_expr_input import (
+    parse_as_expression,
+    parse_as_list_of_expressions,
+)
 from polars.utils.convert import _timedelta_to_pl_duration
-from polars.utils.decorators import deprecated_alias, redirect
+from polars.utils.decorators import deprecated_alias
 from polars.utils.meta import threadpool_size
 from polars.utils.various import sphinx_accessor
 
 with contextlib.suppress(ImportError):  # Module not available when building docs
     from polars.polars import arg_where as py_arg_where
     from polars.polars import reduce as pyreduce
+
 if TYPE_CHECKING:
     import sys
 
-    from polars.dataframe import DataFrame
-    from polars.lazyframe import LazyFrame
+    from polars import DataFrame, LazyFrame, Series
     from polars.polars import PyExpr
-    from polars.series import Series
     from polars.type_aliases import (
         ApplyStrategy,
         ClosedInterval,
         FillNullStrategy,
         InterpolationMethod,
         IntoExpr,
         NullBehavior,
         PolarsDataType,
         PythonLiteral,
         RankMethod,
         RollingInterpolationMethod,
         SearchSortedSide,
+        WindowMappingStrategy,
     )
 
     if sys.version_info >= (3, 11):
         from typing import Concatenate, ParamSpec, Self
     else:
         from typing_extensions import Concatenate, ParamSpec, Self
 
     T = TypeVar("T")
     P = ParamSpec("P")
 
 elif os.getenv("BUILDING_SPHINX_DOCS"):
     property = sphinx_accessor
 
 
-@redirect({"list": "implode"})
 class Expr:
     """Expressions that can be used in various contexts."""
 
     _pyexpr: PyExpr = None
-    _accessors: set[str] = {"arr", "cat", "dt", "meta", "str", "bin", "struct"}
+    _accessors: set[str] = {"arr", "cat", "dt", "list", "meta", "str", "bin", "struct"}
 
     @classmethod
     def _from_pyexpr(cls, pyexpr: PyExpr) -> Self:
         expr = cls.__new__(cls)
         expr._pyexpr = pyexpr
         return expr
 
@@ -124,15 +126,15 @@
     # operators
     def __add__(self, other: Any) -> Self:
         return self._from_pyexpr(self._pyexpr + self._to_pyexpr(other))
 
     def __radd__(self, other: Any) -> Self:
         return self._from_pyexpr(self._to_pyexpr(other) + self._pyexpr)
 
-    def __and__(self, other: Expr) -> Self:
+    def __and__(self, other: Expr | int) -> Self:
         return self._from_pyexpr(self._pyexpr._and(self._to_pyexpr(other)))
 
     def __rand__(self, other: Any) -> Self:
         return self._from_pyexpr(self._pyexpr._and(self._to_pyexpr(other)))
 
     def __eq__(self, other: Any) -> Self:  # type: ignore[override]
         return self._from_pyexpr(self._pyexpr.eq(self._to_expr(other)._pyexpr))
@@ -172,28 +174,28 @@
 
     def __ne__(self, other: Any) -> Self:  # type: ignore[override]
         return self._from_pyexpr(self._pyexpr.neq(self._to_expr(other)._pyexpr))
 
     def __neg__(self) -> Expr:
         return F.lit(0) - self
 
-    def __or__(self, other: Expr) -> Self:
+    def __or__(self, other: Expr | int) -> Self:
         return self._from_pyexpr(self._pyexpr._or(self._to_pyexpr(other)))
 
     def __ror__(self, other: Any) -> Self:
         return self._from_pyexpr(self._to_pyexpr(other)._or(self._pyexpr))
 
     def __pos__(self) -> Expr:
         return F.lit(0) + self
 
     def __pow__(self, power: int | float | Series | Expr) -> Self:
         return self.pow(power)
 
     def __rpow__(self, base: int | float | Expr) -> Expr:
-        return expr_to_lit_or_expr(base) ** self
+        return parse_as_expression(base) ** self
 
     def __sub__(self, other: Any) -> Self:
         return self._from_pyexpr(self._pyexpr - self._to_pyexpr(other))
 
     def __rsub__(self, other: Any) -> Self:
         return self._from_pyexpr(self._to_pyexpr(other) - self._pyexpr)
 
@@ -225,15 +227,15 @@
         num_expr = sum(isinstance(inp, Expr) for inp in inputs)
         if num_expr > 1:
             if num_expr < len(inputs):
                 raise ValueError(
                     "Numpy ufunc with more than one expression can only be used if all non-expression inputs are provided as keyword arguments only"
                 )
 
-            exprs = selection_to_pyexpr_list(inputs)
+            exprs = parse_as_list_of_expressions(inputs)
             return self._from_pyexpr(pyreduce(partial(ufunc, **kwargs), exprs))
 
         def function(s: Series) -> Series:  # pragma: no cover
             args = [inp if not isinstance(inp, Expr) else s for inp in inputs]
             return ufunc(*args, **kwargs)
 
         return self.map(function)
@@ -243,14 +245,15 @@
         Cast to physical representation of the logical dtype.
 
         - :func:`polars.datatypes.Date` -> :func:`polars.datatypes.Int32`
         - :func:`polars.datatypes.Datetime` -> :func:`polars.datatypes.Int64`
         - :func:`polars.datatypes.Time` -> :func:`polars.datatypes.Int64`
         - :func:`polars.datatypes.Duration` -> :func:`polars.datatypes.Int64`
         - :func:`polars.datatypes.Categorical` -> :func:`polars.datatypes.UInt32`
+        - ``List(inner)`` -> ``List(physical of inner)``
 
         Other data types will be left unchanged.
 
         Examples
         --------
         Replicating the pandas
         `pd.factorize
@@ -436,14 +439,20 @@
         Rename the output of an expression.
 
         Parameters
         ----------
         name
             New name.
 
+        See Also
+        --------
+        map_alias
+        prefix
+        suffix
+
         Examples
         --------
         >>> df = pl.DataFrame(
         ...     {
         ...         "a": [1, 2, 3],
         ...         "b": ["a", "b", None],
         ...     }
@@ -456,18 +465,16 @@
          i64  str  
         
          1    a    
          2    b    
          3    null 
         
         >>> df.select(
-        ...     [
-        ...         pl.col("a").alias("bar"),
-        ...         pl.col("b").alias("foo"),
-        ...     ]
+        ...     pl.col("a").alias("bar"),
+        ...     pl.col("b").alias("foo"),
         ... )
         shape: (3, 2)
         
          bar  foo  
          ---  ---  
          i64  str  
         
@@ -626,15 +633,15 @@
          18   4   
         
 
         Prevent
         "DuplicateError: Column with name: 'literal' has more than one occurrences"
         errors.
 
-        >>> df.select([(pl.lit(10) / pl.all()).keep_name()])
+        >>> df.select((pl.lit(10) / pl.all()).keep_name())
         shape: (2, 2)
         
          a     b        
          ---   ---      
          f64   f64      
         
          10.0  3.333333 
@@ -646,52 +653,74 @@
 
     def pipe(
         self,
         function: Callable[Concatenate[Expr, P], T],
         *args: P.args,
         **kwargs: P.kwargs,
     ) -> T:
-        r"""
+        r'''
         Offers a structured way to apply a sequence of user-defined functions (UDFs).
 
         Parameters
         ----------
         function
             Callable; will receive the expression as the first parameter,
             followed by any given args/kwargs.
         *args
             Arguments to pass to the UDF.
         **kwargs
             Keyword arguments to pass to the UDF.
 
         Examples
         --------
-        >>> def extract_number(expr):
+        >>> def extract_number(expr: pl.Expr) -> pl.Expr:
+        ...     """Extract the digits from a string."""
         ...     return expr.str.extract(r"\d+", 0).cast(pl.Int64)
-        ...
-        >>> df = pl.DataFrame({"a": ["a: 1", "b: 2", "c: 3"]})
-        >>> df.with_columns(pl.col("a").pipe(extract_number))
-        shape: (3, 1)
-        
-         a   
-         --- 
-         i64 
-        
-         1   
-         2   
-         3   
-        
+        >>>
+        >>> def scale_negative_even(expr: pl.Expr, *, n: int = 1) -> pl.Expr:
+        ...     """Set even numbers negative, and scale by a user-supplied value."""
+        ...     expr = pl.when(expr % 2 == 0).then(-expr).otherwise(expr)
+        ...     return expr * n
+        >>>
+        >>> df = pl.DataFrame({"val": ["a: 1", "b: 2", "c: 3", "d: 4"]})
+        >>> df.with_columns(
+        ...     udfs=(
+        ...         pl.col("val").pipe(extract_number).pipe(scale_negative_even, n=5)
+        ...     ),
+        ... )
+        shape: (4, 2)
+        
+         val   udfs 
+         ---   ---  
+         str   i64  
+        
+         a: 1  5    
+         b: 2  -10  
+         c: 3  15   
+         d: 4  -20  
+        
 
-        """
+        '''
         return function(self, *args, **kwargs)
 
     def prefix(self, prefix: str) -> Self:
         """
         Add a prefix to the root column name of the expression.
 
+        Parameters
+        ----------
+        prefix
+            Prefix to add to root column name.
+
+        See Also
+        --------
+        alias
+        map_alias
+        suffix
+
         Examples
         --------
         >>> df = pl.DataFrame(
         ...     {
         ...         "A": [1, 2, 3, 4, 5],
         ...         "fruits": ["banana", "banana", "apple", "apple", "banana"],
         ...         "B": [5, 4, 3, 2, 1],
@@ -708,18 +737,16 @@
          1    banana  5    beetle 
          2    banana  4    audi   
          3    apple   3    beetle 
          4    apple   2    beetle 
          5    banana  1    beetle 
         
         >>> df.select(
-        ...     [
-        ...         pl.all(),
-        ...         pl.all().reverse().prefix("reverse_"),
-        ...     ]
+        ...     pl.all(),
+        ...     pl.all().reverse().prefix("reverse_"),
         ... )
         shape: (5, 8)
         
          A    fruits  B    cars    reverse_A  reverse_fruits  reverse_B  reverse_cars 
          ---  ---     ---  ---     ---        ---             ---        ---          
          i64  str     i64  str     i64        str             i64        str          
         
@@ -733,14 +760,25 @@
         """  # noqa: W505
         return self._from_pyexpr(self._pyexpr.prefix(prefix))
 
     def suffix(self, suffix: str) -> Self:
         """
         Add a suffix to the root column name of the expression.
 
+        Parameters
+        ----------
+        suffix
+            Suffix to add to root column name.
+
+        See Also
+        --------
+        alias
+        map_alias
+        prefix
+
         Examples
         --------
         >>> df = pl.DataFrame(
         ...     {
         ...         "A": [1, 2, 3, 4, 5],
         ...         "fruits": ["banana", "banana", "apple", "apple", "banana"],
         ...         "B": [5, 4, 3, 2, 1],
@@ -757,18 +795,16 @@
          1    banana  5    beetle 
          2    banana  4    audi   
          3    apple   3    beetle 
          4    apple   2    beetle 
          5    banana  1    beetle 
         
         >>> df.select(
-        ...     [
-        ...         pl.all(),
-        ...         pl.all().reverse().suffix("_reverse"),
-        ...     ]
+        ...     pl.all(),
+        ...     pl.all().reverse().suffix("_reverse"),
         ... )
         shape: (5, 8)
         
          A    fruits  B    cars    A_reverse  fruits_reverse  B_reverse  cars_reverse 
          ---  ---     ---  ---     ---        ---             ---        ---          
          i64  str     i64  str     i64        str             i64        str          
         
@@ -787,34 +823,45 @@
         Rename the output of an expression by mapping a function over the root name.
 
         Parameters
         ----------
         function
             Function that maps root name to new name.
 
+        See Also
+        --------
+        alias
+        prefix
+        suffix
+
         Examples
         --------
         >>> df = pl.DataFrame(
         ...     {
         ...         "A": [1, 2],
         ...         "B": [3, 4],
         ...     }
         ... )
-        >>> df.select(
-        ...     pl.all().reverse().map_alias(lambda colName: colName + "_reverse")
+
+        >>> df.select(pl.all().reverse().suffix("_reverse")).with_columns(
+        ...     pl.all().map_alias(
+        ...         # Remove "_reverse" suffix and convert to lower case.
+        ...         lambda col_name: col_name.rsplit("_reverse", 1)[0].lower()
+        ...     )
         ... )
-        shape: (2, 2)
-        
-         A_reverse  B_reverse 
-         ---        ---       
-         i64        i64       
-        
-         2          4         
-         1          3         
-        
+        shape: (2, 4)
+        
+         A_reverse  B_reverse  a    b   
+         ---        ---        ---  --- 
+         i64        i64        i64  i64 
+        
+         2          4          2    4   
+         1          3          1    3   
+        
+
 
         """
         return self._from_pyexpr(self._pyexpr.map_alias(function))
 
     def is_not(self) -> Self:
         """
         Negate a boolean expression.
@@ -1076,14 +1123,17 @@
         """
         return self._from_pyexpr(self._pyexpr.agg_groups())
 
     def count(self) -> Self:
         """
         Count the number of values in this expression.
 
+        .. warning::
+            `null` is deemed a value in this context.
+
         Examples
         --------
         >>> df = pl.DataFrame({"a": [8, 9, 10], "b": [None, 4, 4]})
         >>> df.select(pl.all().count())  # counts nulls
         shape: (1, 2)
         
          a    b   
@@ -1190,41 +1240,41 @@
          i64  i64  
         
          8    null 
          10   4    
         
 
         """
-        other = expr_to_lit_or_expr(other)
-        return self._from_pyexpr(self._pyexpr.append(other._pyexpr, upcast))
+        other = parse_as_expression(other)._pyexpr
+        return self._from_pyexpr(self._pyexpr.append(other, upcast))
 
     def rechunk(self) -> Self:
         """
         Create a single chunk of memory for this Series.
 
         Examples
         --------
         >>> df = pl.DataFrame({"a": [1, 1, 2]})
 
         Create a Series with 3 nulls, append column a then rechunk
 
         >>> df.select(pl.repeat(None, 3).append(pl.col("a")).rechunk())
         shape: (6, 1)
-        
-         literal 
-         ---     
-         i64     
-        
-         null    
-         null    
-         null    
-         1       
-         1       
-         2       
-        
+        
+         repeat 
+         ---    
+         i64    
+        
+         null   
+         null   
+         null   
+         1      
+         1      
+         2      
+        
 
         """
         return self._from_pyexpr(self._pyexpr.rechunk())
 
     def drop_nulls(self) -> Self:
         """
         Drop all null values.
@@ -1568,15 +1618,15 @@
          1.0 
          2.0 
         
 
         """
         return self._from_pyexpr(self._pyexpr.ceil())
 
-    def round(self, decimals: int) -> Self:
+    def round(self, decimals: int = 0) -> Self:
         """
         Round underlying floating point data by `decimals` digits.
 
         Parameters
         ----------
         decimals
             Number of decimals to round by.
@@ -1624,16 +1674,16 @@
          --- 
          i64 
         
          44  
         
 
         """
-        other = expr_to_lit_or_expr(other, str_to_lit=False)
-        return self._from_pyexpr(self._pyexpr.dot(other._pyexpr))
+        other = parse_as_expression(other)._pyexpr
+        return self._from_pyexpr(self._pyexpr.dot(other))
 
     def mode(self) -> Self:
         """
         Compute the most occurring value(s).
 
         Can return multiple Values.
 
@@ -1995,16 +2045,16 @@
          ---   ---    --- 
          u32   u32    u32 
         
          0     2      4   
         
 
         """
-        element = expr_to_lit_or_expr(element, str_to_lit=False)
-        return self._from_pyexpr(self._pyexpr.search_sorted(element._pyexpr, side))
+        element = parse_as_expression(element)._pyexpr
+        return self._from_pyexpr(self._pyexpr.search_sorted(element, side))
 
     def sort_by(
         self,
         by: IntoExpr | Iterable[IntoExpr],
         *more_by: IntoExpr,
         descending: bool | Sequence[bool] = False,
     ) -> Self:
@@ -2122,17 +2172,15 @@
          str    i64     i64    |
         
          a      3       7      |
          b      2       5      |
         
 
         """
-        by = selection_to_pyexpr_list(by)
-        if more_by:
-            by.extend(selection_to_pyexpr_list(more_by))
+        by = parse_as_list_of_expressions(by, *more_by)
         if isinstance(descending, bool):
             descending = [descending]
         elif len(by) != len(descending):
             raise ValueError(
                 f"the length of `descending` ({len(descending)}) does not match the length of `by` ({len(by)})"
             )
         return self._from_pyexpr(self._pyexpr.sort_by(by, descending))
@@ -2178,18 +2226,17 @@
          two    99    
         
 
         """
         if isinstance(indices, list) or (
             _check_for_numpy(indices) and isinstance(indices, np.ndarray)
         ):
-            indices = cast("np.ndarray[Any, Any]", indices)
-            indices_lit = F.lit(pli.Series("", indices, dtype=UInt32))
+            indices_lit = F.lit(pl.Series("", indices, dtype=UInt32))
         else:
-            indices_lit = expr_to_lit_or_expr(indices, str_to_lit=False)
+            indices_lit = parse_as_expression(indices)  # type: ignore[arg-type]
         return self._from_pyexpr(self._pyexpr.take(indices_lit._pyexpr))
 
     def shift(self, periods: int = 1) -> Self:
         """
         Shift the values by a given period.
 
         Parameters
@@ -2214,15 +2261,15 @@
         
 
         """
         return self._from_pyexpr(self._pyexpr.shift(periods))
 
     def shift_and_fill(
         self,
-        fill_value: int | float | bool | str | Expr | list[Any],
+        fill_value: IntoExpr,
         *,
         periods: int = 1,
     ) -> Self:
         """
         Shift the values by a given period and fill the resulting null values.
 
         Parameters
@@ -2245,18 +2292,16 @@
          a   
          1   
          2   
          3   
         
 
         """
-        fill_value = expr_to_lit_or_expr(fill_value, str_to_lit=True)
-        return self._from_pyexpr(
-            self._pyexpr.shift_and_fill(periods, fill_value._pyexpr)
-        )
+        fill_value = parse_as_expression(fill_value, str_as_lit=True)._pyexpr
+        return self._from_pyexpr(self._pyexpr.shift_and_fill(periods, fill_value))
 
     def fill_null(
         self,
         value: Any | None = None,
         strategy: FillNullStrategy | None = None,
         limit: int | None = None,
     ) -> Self:
@@ -2325,16 +2370,16 @@
         elif strategy not in ("forward", "backward") and limit is not None:
             raise ValueError(
                 "can only specify 'limit' when strategy is set to"
                 " 'backward' or 'forward'"
             )
 
         if value is not None:
-            value = expr_to_lit_or_expr(value, str_to_lit=True)
-            return self._from_pyexpr(self._pyexpr.fill_null(value._pyexpr))
+            value = parse_as_expression(value, str_as_lit=True)._pyexpr
+            return self._from_pyexpr(self._pyexpr.fill_null(value))
         else:
             return self._from_pyexpr(
                 self._pyexpr.fill_null_with_strategy(strategy, limit)
             )
 
     def fill_nan(self, value: int | float | Expr | None) -> Self:
         """
@@ -2357,16 +2402,16 @@
         
          1.0   4.0  
          null  zero 
          zero  6.0  
         
 
         """
-        fill_value = expr_to_lit_or_expr(value, str_to_lit=True)
-        return self._from_pyexpr(self._pyexpr.fill_nan(fill_value._pyexpr))
+        fill_value = parse_as_expression(value, str_as_lit=True)._pyexpr
+        return self._from_pyexpr(self._pyexpr.fill_nan(fill_value))
 
     def forward_fill(self, limit: int | None = None) -> Self:
         """
         Fill missing values with the latest seen values.
 
         Parameters
         ----------
@@ -2867,15 +2912,20 @@
         
          2   
         
 
         """
         return self._from_pyexpr(self._pyexpr.last())
 
-    def over(self, expr: IntoExpr | Iterable[IntoExpr], *more_exprs: IntoExpr) -> Self:
+    def over(
+        self,
+        expr: IntoExpr | Iterable[IntoExpr],
+        *more_exprs: IntoExpr,
+        mapping_strategy: WindowMappingStrategy = "group_to_rows",
+    ) -> Self:
         """
         Compute expressions over the given groups.
 
         This expression is similar to performing a groupby aggregation and joining the
         result back into the original dataframe.
 
         The outcome is similar to how `window functions
@@ -2885,14 +2935,25 @@
         Parameters
         ----------
         expr
             Column(s) to group by. Accepts expression input. Strings are parsed as
             column names.
         *more_exprs
             Additional columns to group by, specified as positional arguments.
+        mapping_strategy: {'group_to_rows', 'join', 'explode'}
+            - group_to_rows
+                If the aggregation results in multiple values, assign them back to there
+                position in the DataFrame. This can only be done if the group yields
+                the same elements before aggregation as after
+            - join
+                Join the groups as 'List<group_dtype>' to the row positions.
+                warning: this can be memory intensive
+            - explode
+                Don't do any mapping, but simply flatten the group.
+                This only makes sense if the input data is sorted.
 
         Examples
         --------
         Pass the name of a column to compute the expression over that column.
 
         >>> df = pl.DataFrame(
         ...     {
@@ -2960,18 +3021,16 @@
          a    2    4    4     
          b    3    3    1     
          b    5    2    1     
          b    3    1    1     
         
 
         """
-        exprs = selection_to_pyexpr_list(expr)
-        if more_exprs:
-            exprs.extend(selection_to_pyexpr_list(more_exprs))
-        return self._from_pyexpr(self._pyexpr.over(exprs))
+        exprs = parse_as_list_of_expressions(expr, *more_exprs)
+        return self._from_pyexpr(self._pyexpr.over(exprs, mapping_strategy))
 
     def is_unique(self) -> Self:
         """
         Get mask of unique values.
 
         Examples
         --------
@@ -3106,16 +3165,16 @@
          --- 
          f64 
         
          1.5 
         
 
         """
-        quantile = expr_to_lit_or_expr(quantile, str_to_lit=False)
-        return self._from_pyexpr(self._pyexpr.quantile(quantile._pyexpr, interpolation))
+        quantile = parse_as_expression(quantile)._pyexpr
+        return self._from_pyexpr(self._pyexpr.quantile(quantile, interpolation))
 
     def filter(self, predicate: Expr) -> Self:
         """
         Filter a single column.
 
         Mostly useful in an aggregation context. If you want to filter on a DataFrame
         level, use `LazyFrame.filter`.
@@ -3202,25 +3261,29 @@
 
         The output of this custom function must be a Series.
         If you want to apply a custom function elementwise over single values, see
         :func:`apply`. A use case for ``map`` is when you want to transform an
         expression with a third-party library.
 
         Read more in `the book
-        <https://pola-rs.github.io/polars-book/user-guide/dsl/custom_functions.html>`_.
+        <https://pola-rs.github.io/polars-book/user-guide/expressions/user-defined-functions>`_.
 
         Parameters
         ----------
         function
             Lambda/ function to apply.
         return_dtype
             Dtype of the output Series.
         agg_list
             Aggregate list
 
+        See Also
+        --------
+        map_dict
+
         Examples
         --------
         >>> df = pl.DataFrame(
         ...     {
         ...         "sine": [0.0, 1.0, 0.0, -1.0],
         ...         "cosine": [1.0, 0.0, -1.0, 0.0],
         ...     }
@@ -3352,16 +3415,15 @@
 
         """
         # input x: Series of type list containing the group values
         if pass_name:
 
             def wrap_f(x: Series) -> Series:  # pragma: no cover
                 def inner(s: Series) -> Series:  # pragma: no cover
-                    s.rename(x.name, in_place=True)
-                    return function(s)
+                    return function(s.alias(x.name))
 
                 return x.apply(inner, return_dtype=return_dtype, skip_nulls=skip_nulls)
 
         else:
 
             def wrap_f(x: Series) -> Series:  # pragma: no cover
                 return x.apply(
@@ -3434,15 +3496,15 @@
         
 
         """
         return self._from_pyexpr(self._pyexpr.explode())
 
     def explode(self) -> Self:
         """
-        Explode a list or utf8 Series.
+        Explode a list Series.
 
         This means that every item is expanded to a new row.
 
         Returns
         -------
         Exploded Series of same dtype
 
@@ -3549,15 +3611,15 @@
         
          5   
          6   
          7   
         
 
         """
-        offset = -expr_to_lit_or_expr(n, str_to_lit=False)
+        offset = -parse_as_expression(n)
         return self.slice(offset, n)
 
     def limit(self, n: int | Expr = 10) -> Self:
         """
         Get the first `n` rows (alias for :func:`Expr.head`).
 
         Parameters
@@ -3701,14 +3763,28 @@
          NaN  NaN  false  
          4.0  4.0  true   
         
 
         """
         return self.__eq__(other)
 
+    def eq_missing(self, other: Any) -> Self:
+        """
+        Method equivalent of equality operator ``expr == other`` where `None` == None`.
+
+        This differs from default ``eq`` where null values are propagated.
+
+        Parameters
+        ----------
+        other
+            A literal or expression value to compare with.
+
+        """
+        return self._from_pyexpr(self._pyexpr.eq_missing(self._to_expr(other)._pyexpr))
+
     def ge(self, other: Any) -> Self:
         """
         Method equivalent of "greater than or equal" operator ``expr >= other``.
 
         Parameters
         ----------
         other
@@ -3876,14 +3952,28 @@
          NaN  NaN  true   
          4.0  4.0  false  
         
 
         """
         return self.__ne__(other)
 
+    def ne_missing(self, other: Any) -> Self:
+        """
+        Method equivalent of equality operator ``expr != other`` where `None` == None`.
+
+        This differs from default ``ne`` where null values are propagated.
+
+        Parameters
+        ----------
+        other
+            A literal or expression value to compare with.
+
+        """
+        return self._from_pyexpr(self._pyexpr.neq_missing(self._to_expr(other)._pyexpr))
+
     def add(self, other: Any) -> Self:
         """
         Method equivalent of addition operator ``expr + other``.
 
         Parameters
         ----------
         other
@@ -4126,16 +4216,16 @@
          1    1.0    1.0        
          2    8.0    2.0        
          4    64.0   16.0       
          8    512.0  512.0      
         
 
         """
-        exponent = expr_to_lit_or_expr(exponent)
-        return self._from_pyexpr(self._pyexpr.pow(exponent._pyexpr))
+        exponent = parse_as_expression(exponent)._pyexpr
+        return self._from_pyexpr(self._pyexpr.pow(exponent))
 
     def xor(self, other: Any) -> Self:
         """
         Method equivalent of logical exclusive-or operator ``expr ^ other``.
 
         Parameters
         ----------
@@ -4218,20 +4308,20 @@
          false    
         
 
         """
         if isinstance(other, Collection) and not isinstance(other, str):
             if isinstance(other, (Set, FrozenSet)):
                 other = sorted(other)
-            other = F.lit(None) if len(other) == 0 else F.lit(pli.Series(other))
+            other = F.lit(None) if len(other) == 0 else F.lit(pl.Series(other))
         else:
-            other = expr_to_lit_or_expr(other, str_to_lit=False)
+            other = parse_as_expression(other)
         return self._from_pyexpr(self._pyexpr.is_in(other._pyexpr))
 
-    def repeat_by(self, by: Expr | str) -> Self:
+    def repeat_by(self, by: pl.Series | Expr | str | int) -> Self:
         """
         Repeat the elements in this Series as specified in the given expression.
 
         The repeated elements are expanded into a `List`.
 
         Parameters
         ----------
@@ -4261,16 +4351,16 @@
         
          ["x"]           
          ["y", "y"]      
          ["z", "z", "z"] 
         
 
         """
-        by = expr_to_lit_or_expr(by, False)
-        return self._from_pyexpr(self._pyexpr.repeat_by(by._pyexpr))
+        by = parse_as_expression(by)._pyexpr
+        return self._from_pyexpr(self._pyexpr.repeat_by(by))
 
     def is_between(
         self,
         lower_bound: IntoExpr,
         upper_bound: IntoExpr,
         closed: ClosedInterval = "both",
     ) -> Self:
@@ -4347,16 +4437,16 @@
          b    true       
          c    true       
          d    false      
          e    false      
         
 
         """
-        lower_bound = expr_to_lit_or_expr(lower_bound, str_to_lit=False)
-        upper_bound = expr_to_lit_or_expr(upper_bound, str_to_lit=False)
+        lower_bound = parse_as_expression(lower_bound)
+        upper_bound = parse_as_expression(upper_bound)
 
         if closed == "none":
             return (self > lower_bound) & (self < upper_bound)
         elif closed == "both":
             return (self >= lower_bound) & (self <= upper_bound)
         elif closed == "right":
             return (self > lower_bound) & (self <= upper_bound)
@@ -4595,15 +4685,15 @@
             elementwise with the values in the window.
         min_periods
             The number of values in the window that should be non-null before computing
             a result. If None, it will be set equal to window size.
         center
             Set the labels at the center of the window
         by
-            If the `window_size` is temporal for instance `"5h"` or `"3s`, you must
+            If the `window_size` is temporal for instance `"5h"` or `"3s"`, you must
             set the column that will be used to determine the windows. This column must
             be of dtype `{Date, Datetime}`
         closed : {'left', 'right', 'both', 'none'}
             Define which sides of the temporal interval are closed (inclusive).
 
         Warnings
         --------
@@ -4694,15 +4784,15 @@
             elementwise with the values in the window.
         min_periods
             The number of values in the window that should be non-null before computing
             a result. If None, it will be set equal to window size.
         center
             Set the labels at the center of the window
         by
-            If the `window_size` is temporal, for instance `"5h"` or `"3s`, you must
+            If the `window_size` is temporal, for instance `"5h"` or `"3s"`, you must
             set the column that will be used to determine the windows. This column must
             be of dtype `{Date, Datetime}`
         closed : {'left', 'right', 'both', 'none'}
             Define which sides of the temporal interval are closed (inclusive).
 
         Warnings
         --------
@@ -4793,15 +4883,15 @@
             elementwise with the values in the window.
         min_periods
             The number of values in the window that should be non-null before computing
             a result. If None, it will be set equal to window size.
         center
             Set the labels at the center of the window
         by
-            If the `window_size` is temporal for instance `"5h"` or `"3s`, you must
+            If the `window_size` is temporal for instance `"5h"` or `"3s"`, you must
             set the column that will be used to determine the windows. This column must
             be of dtype `{Date, Datetime}`
         closed : {'left', 'right', 'both', 'none'}
             Define which sides of the temporal interval are closed (inclusive).
 
         Warnings
         --------
@@ -4892,15 +4982,15 @@
             elementwise with the values in the window.
         min_periods
             The number of values in the window that should be non-null before computing
             a result. If None, it will be set equal to window size.
         center
             Set the labels at the center of the window
         by
-            If the `window_size` is temporal for instance `"5h"` or `"3s`, you must
+            If the `window_size` is temporal for instance `"5h"` or `"3s"`, you must
             set the column that will be used to determine the windows. This column must
             of dtype `{Date, Datetime}`
         closed : {'left', 'right', 'both', 'none'}
             Define which sides of the temporal interval are closed (inclusive).
 
         Warnings
         --------
@@ -4991,15 +5081,15 @@
             elementwise with the values in the window.
         min_periods
             The number of values in the window that should be non-null before computing
             a result. If None, it will be set equal to window size.
         center
             Set the labels at the center of the window
         by
-            If the `window_size` is temporal for instance `"5h"` or `"3s`, you must
+            If the `window_size` is temporal for instance `"5h"` or `"3s"`, you must
             set the column that will be used to determine the windows. This column must
             be of dtype `{Date, Datetime}`
         closed : {'left', 'right', 'both', 'none'}
             Define which sides of the temporal interval are closed (inclusive).
 
         Warnings
         --------
@@ -5090,15 +5180,15 @@
             elementwise with the values in the window.
         min_periods
             The number of values in the window that should be non-null before computing
             a result. If None, it will be set equal to window size.
         center
             Set the labels at the center of the window
         by
-            If the `window_size` is temporal for instance `"5h"` or `"3s`, you must
+            If the `window_size` is temporal for instance `"5h"` or `"3s"`, you must
             set the column that will be used to determine the windows. This column must
             be of dtype `{Date, Datetime}`
         closed : {'left', 'right', 'both', 'none'}
             Define which sides of the temporal interval are closed (inclusive).
 
         Warnings
         --------
@@ -5185,15 +5275,15 @@
             elementwise with the values in the window.
         min_periods
             The number of values in the window that should be non-null before computing
             a result. If None, it will be set equal to window size.
         center
             Set the labels at the center of the window
         by
-            If the `window_size` is temporal for instance `"5h"` or `"3s`, you must
+            If the `window_size` is temporal for instance `"5h"` or `"3s"`, you must
             set the column that will be used to determine the windows. This column must
             be of dtype `{Date, Datetime}`
         closed : {'left', 'right', 'both', 'none'}
             Define which sides of the temporal interval are closed (inclusive).
 
         Warnings
         --------
@@ -5286,15 +5376,15 @@
             elementwise with the values in the window.
         min_periods
             The number of values in the window that should be non-null before computing
             a result. If None, it will be set equal to window size.
         center
             Set the labels at the center of the window
         by
-            If the `window_size` is temporal for instance `"5h"` or `"3s`, you must
+            If the `window_size` is temporal for instance `"5h"` or `"3s"`, you must
             set the column that will be used to determine the windows. This column must
             be of dtype `{Date, Datetime}`
         closed : {'left', 'right', 'both', 'none'}
             Define which sides of the temporal interval are closed (inclusive).
 
         Warnings
         --------
@@ -5526,14 +5616,31 @@
          3   
          4   
          1   
          2   
          5   
         
 
+        Use 'rank' with 'over' to rank within groups:
+
+        >>> df = pl.DataFrame({"a": [1, 1, 2, 2, 2], "b": [6, 7, 5, 14, 11]})
+        >>> df.with_columns(pl.col("b").rank().over("a").alias("rank"))
+        shape: (5, 3)
+        
+         a    b    rank 
+         ---  ---  ---  
+         i64  i64  f32  
+        
+         1    6    1.0  
+         1    7    2.0  
+         2    5    1.0  
+         2    14   3.0  
+         2    11   2.0  
+        
+
         """
         return self._from_pyexpr(self._pyexpr.rank(method, descending, seed))
 
     def diff(self, n: int = 1, null_behavior: NullBehavior = "ignore") -> Self:
         """
         Calculate the n-th discrete difference.
 
@@ -6903,14 +7010,24 @@
          2    2           2           2     2     b    1.32  null  
          3    8589934592  1073741824  112   129   c    0.12  false 
         
 
         """
         return self._from_pyexpr(self._pyexpr.shrink_dtype())
 
+    def cache(self) -> Self:
+        """
+        Cache this expression so that it only is executed once per context.
+
+        This can actually hurt performance and can have a lot of contention.
+        It is advised not to use it until actually benchmarked on your problem.
+
+        """
+        return self._from_pyexpr(self._pyexpr.cache())
+
     def map_dict(
         self,
         remapping: dict[Any, Any],
         *,
         default: Any = None,
         return_dtype: PolarsDataType | None = None,
     ) -> Self:
@@ -6926,14 +7043,18 @@
             Dictionary containing the before/after values to map.
         default
             Value to use when the remapping dict does not contain the lookup value.
             Use ``pl.first()``, to keep the original value.
         return_dtype
             Set return dtype to override automatic return dtype determination.
 
+        See Also
+        --------
+        map
+
         Examples
         --------
         >>> country_code_dict = {
         ...     "CA": "Canada",
         ...     "DE": "Germany",
         ...     "FR": "France",
         ...     None: "Not specified",
@@ -7106,15 +7227,15 @@
 
             """
             try:
                 if dtype is None:
                     # If no dtype was set, which should only happen when:
                     #     values = remapping.values()
                     # create a Series from those values and infer the dtype.
-                    s = pli.Series(
+                    s = pl.Series(
                         name,
                         values,
                         dtype=None,
                         dtype_if_empty=dtype_if_empty,
                         strict=True,
                     )
 
@@ -7127,15 +7248,15 @@
                             or (s.dtype in FLOAT_DTYPES and dtype_keys in FLOAT_DTYPES)
                             or (s.dtype == Utf8 and dtype_keys == Categorical)
                         ):
                             # Values Series and keys Series are of similar dtypes,
                             # that we can assume that the user wants the values Series
                             # of the same dtype as the key Series.
                             dtype = dtype_keys
-                            s = pli.Series(
+                            s = pl.Series(
                                 name,
                                 values,
                                 dtype=dtype_keys,
                                 dtype_if_empty=dtype_if_empty,
                                 strict=True,
                             )
                             if dtype != s.dtype:
@@ -7143,15 +7264,15 @@
                                     f"Remapping values for map_dict could not be converted to {dtype}: found {s.dtype}"
                                 )
                 else:
                     # dtype was set, which should always be the case when:
                     #     values = remapping.keys()
                     # and in cases where the user set the output dtype when:
                     #     values = remapping.values()
-                    s = pli.Series(
+                    s = pl.Series(
                         name,
                         values,
                         dtype=dtype,
                         dtype_if_empty=dtype_if_empty,
                         strict=True,
                     )
                     if dtype != s.dtype:
@@ -7224,15 +7345,15 @@
                 dtype_if_empty=input_dtype,
                 dtype_keys=input_dtype,
                 is_keys=True,
             )
 
             if return_dtype_:
                 # Create remap value Series with specified output dtype.
-                remap_value_s = pli.Series(
+                remap_value_s = pl.Series(
                     remap_value_column,
                     remapping.values(),
                     dtype=return_dtype_,
                     dtype_if_empty=input_dtype,
                 )
             else:
                 # Create remap value Series with same output dtype as remap key Series,
@@ -7247,15 +7368,15 @@
                     is_keys=False,
                 )
 
             return (
                 (
                     df.lazy()
                     .join(
-                        pli.DataFrame(
+                        pl.DataFrame(
                             [
                                 remap_key_s,
                                 remap_value_s,
                             ]
                         )
                         .lazy()
                         .with_columns(F.lit(True).alias(is_remapped_column)),
@@ -7288,15 +7409,15 @@
                 dtype_if_empty=input_dtype,
                 dtype_keys=input_dtype,
                 is_keys=True,
             )
 
             if return_dtype:
                 # Create remap value Series with specified output dtype.
-                remap_value_s = pli.Series(
+                remap_value_s = pl.Series(
                     remap_value_column,
                     remapping.values(),
                     dtype=return_dtype,
                     dtype_if_empty=input_dtype,
                 )
             else:
                 # Create remap value Series with same output dtype as remap key Series,
@@ -7312,15 +7433,15 @@
                 )
 
             return (
                 (
                     s.to_frame()
                     .lazy()
                     .join(
-                        pli.DataFrame(
+                        pl.DataFrame(
                             [
                                 remap_key_s,
                                 remap_value_s,
                             ]
                         )
                         .lazy()
                         .with_columns(F.lit(True).alias(is_remapped_column)),
@@ -7333,24 +7454,14 @@
                 .to_series(1)
             )
 
         func = inner_with_default if default is not None else inner
         return self.map(func)
 
     @property
-    def arr(self) -> ExprListNameSpace:
-        """
-        Create an object namespace of all list related methods.
-
-        See the individual method pages for full details
-
-        """
-        return ExprListNameSpace(self)
-
-    @property
     def bin(self) -> ExprBinaryNameSpace:
         """
         Create an object namespace of all binary related methods.
 
         See the individual method pages for full details
         """
         return ExprBinaryNameSpace(self)
@@ -7382,27 +7493,47 @@
         return ExprCatNameSpace(self)
 
     @property
     def dt(self) -> ExprDateTimeNameSpace:
         """Create an object namespace of all datetime related methods."""
         return ExprDateTimeNameSpace(self)
 
+    # Keep the `list` and `str` properties below at the end of the definition of Expr,
+    # as to not confuse mypy with the type annotation `str` and `list`
+
+    @property
+    def list(self) -> ExprListNameSpace:
+        """
+        Create an object namespace of all list related methods.
+
+        See the individual method pages for full details
+
+        """
+        return ExprListNameSpace(self)
+
+    @property
+    def arr(self) -> ExprArrayNameSpace:
+        """
+        Create an object namespace of all array related methods.
+
+        See the individual method pages for full details
+
+        """
+        return ExprArrayNameSpace(self)
+
     @property
     def meta(self) -> ExprMetaNameSpace:
         """
         Create an object namespace of all meta related expression methods.
 
         This can be used to modify and traverse existing expressions
 
         """
         return ExprMetaNameSpace(self)
 
-    # Keep the `str` property below at the end of the definition of Expr,
-    # as to not confuse mypy with the type annotation `str`
-
     @property
     def str(self) -> ExprStringNameSpace:
         """
         Create an object namespace of all string related methods.
 
         See the individual method pages for full details
```

### Comparing `polars_lts_cpu-0.17.9/polars/expr/list.py` & `polars_lts_cpu-0.18.0/polars/expr/list.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,137 +1,140 @@
 from __future__ import annotations
 
 import copy
-from typing import TYPE_CHECKING, Any, Callable
+from typing import TYPE_CHECKING, Any, Callable, Sequence
 
+import polars._reexport as pl
 from polars import functions as F
-from polars import internals as pli
-from polars.utils._parse_expr_input import expr_to_lit_or_expr
+from polars.utils._parse_expr_input import parse_as_expression
 from polars.utils._wrap import wrap_expr
+from polars.utils.decorators import deprecated_alias
 
 if TYPE_CHECKING:
     from datetime import date, datetime, time
 
-    from polars.expr import Expr
-    from polars.series import Series
-    from polars.type_aliases import NullBehavior, ToStructStrategy
+    from polars import Expr, Series
+    from polars.type_aliases import IntoExpr, NullBehavior, ToStructStrategy
 
 
 class ExprListNameSpace:
     """Namespace for list related expressions."""
 
-    _accessor = "arr"
+    _accessor = "list"
 
     def __init__(self, expr: Expr):
         self._pyexpr = expr._pyexpr
 
+    def __getitem__(self, item: int) -> Expr:
+        return self.get(item)
+
     def lengths(self) -> Expr:
         """
         Get the length of the arrays as UInt32.
 
         Examples
         --------
         >>> df = pl.DataFrame({"foo": [1, 2], "bar": [["a", "b"], ["c"]]})
-        >>> df.select(pl.col("bar").arr.lengths())
+        >>> df.select(pl.col("bar").list.lengths())
         shape: (2, 1)
         
          bar 
          --- 
          u32 
         
          2   
          1   
         
 
         """
-        return wrap_expr(self._pyexpr.arr_lengths())
+        return wrap_expr(self._pyexpr.list_lengths())
 
     def sum(self) -> Expr:
         """
         Sum all the lists in the array.
 
         Examples
         --------
         >>> df = pl.DataFrame({"values": [[1], [2, 3]]})
-        >>> df.select(pl.col("values").arr.sum())
+        >>> df.select(pl.col("values").list.sum())
         shape: (2, 1)
         
          values 
          ---    
          i64    
         
          1      
          5      
         
 
         """
-        return wrap_expr(self._pyexpr.lst_sum())
+        return wrap_expr(self._pyexpr.list_sum())
 
     def max(self) -> Expr:
         """
         Compute the max value of the lists in the array.
 
         Examples
         --------
         >>> df = pl.DataFrame({"values": [[1], [2, 3]]})
-        >>> df.select(pl.col("values").arr.max())
+        >>> df.select(pl.col("values").list.max())
         shape: (2, 1)
         
          values 
          ---    
          i64    
         
          1      
          3      
         
 
         """
-        return wrap_expr(self._pyexpr.lst_max())
+        return wrap_expr(self._pyexpr.list_max())
 
     def min(self) -> Expr:
         """
         Compute the min value of the lists in the array.
 
         Examples
         --------
         >>> df = pl.DataFrame({"values": [[1], [2, 3]]})
-        >>> df.select(pl.col("values").arr.min())
+        >>> df.select(pl.col("values").list.min())
         shape: (2, 1)
         
          values 
          ---    
          i64    
         
          1      
          2      
         
 
         """
-        return wrap_expr(self._pyexpr.lst_min())
+        return wrap_expr(self._pyexpr.list_min())
 
     def mean(self) -> Expr:
         """
         Compute the mean value of the lists in the array.
 
         Examples
         --------
         >>> df = pl.DataFrame({"values": [[1], [2, 3]]})
-        >>> df.select(pl.col("values").arr.mean())
+        >>> df.select(pl.col("values").list.mean())
         shape: (2, 1)
         
          values 
          ---    
          f64    
         
          1.0    
          2.5    
         
 
         """
-        return wrap_expr(self._pyexpr.lst_mean())
+        return wrap_expr(self._pyexpr.list_mean())
 
     def sort(self, *, descending: bool = False) -> Expr:
         """
         Sort the arrays in this column.
 
         Parameters
         ----------
@@ -141,62 +144,62 @@
         Examples
         --------
         >>> df = pl.DataFrame(
         ...     {
         ...         "a": [[3, 2, 1], [9, 1, 2]],
         ...     }
         ... )
-        >>> df.select(pl.col("a").arr.sort())
+        >>> df.select(pl.col("a").list.sort())
         shape: (2, 1)
         
          a         
          ---       
          list[i64] 
         
          [1, 2, 3] 
          [1, 2, 9] 
         
-        >>> df.select(pl.col("a").arr.sort(descending=True))
+        >>> df.select(pl.col("a").list.sort(descending=True))
         shape: (2, 1)
         
          a         
          ---       
          list[i64] 
         
          [3, 2, 1] 
          [9, 2, 1] 
         
 
         """
-        return wrap_expr(self._pyexpr.lst_sort(descending))
+        return wrap_expr(self._pyexpr.list_sort(descending))
 
     def reverse(self) -> Expr:
         """
         Reverse the arrays in the list.
 
         Examples
         --------
         >>> df = pl.DataFrame(
         ...     {
         ...         "a": [[3, 2, 1], [9, 1, 2]],
         ...     }
         ... )
-        >>> df.select(pl.col("a").arr.reverse())
+        >>> df.select(pl.col("a").list.reverse())
         shape: (2, 1)
         
          a         
          ---       
          list[i64] 
         
          [1, 2, 3] 
          [2, 1, 9] 
         
 
         """
-        return wrap_expr(self._pyexpr.lst_reverse())
+        return wrap_expr(self._pyexpr.list_reverse())
 
     def unique(self, *, maintain_order: bool = False) -> Expr:
         """
         Get the unique/distinct values in the list.
 
         Parameters
         ----------
@@ -206,26 +209,26 @@
         Examples
         --------
         >>> df = pl.DataFrame(
         ...     {
         ...         "a": [[1, 1, 2]],
         ...     }
         ... )
-        >>> df.select(pl.col("a").arr.unique())
+        >>> df.select(pl.col("a").list.unique())
         shape: (1, 1)
         
          a         
          ---       
          list[i64] 
         
          [1, 2]    
         
 
         """
-        return wrap_expr(self._pyexpr.lst_unique(maintain_order))
+        return wrap_expr(self._pyexpr.list_unique(maintain_order))
 
     def concat(self, other: list[Expr | str] | Expr | str | Series | list[Any]) -> Expr:
         """
         Concat the arrays in a Series dtype List in linear time.
 
         Parameters
         ----------
@@ -236,30 +239,30 @@
         --------
         >>> df = pl.DataFrame(
         ...     {
         ...         "a": [["a"], ["x"]],
         ...         "b": [["b", "c"], ["y", "z"]],
         ...     }
         ... )
-        >>> df.select(pl.col("a").arr.concat("b"))
+        >>> df.select(pl.col("a").list.concat("b"))
         shape: (2, 1)
         
          a               
          ---             
          list[str]       
         
          ["a", "b", "c"] 
          ["x", "y", "z"] 
         
 
         """
         if isinstance(other, list) and (
-            not isinstance(other[0], (pli.Expr, str, pli.Series))
+            not isinstance(other[0], (pl.Expr, str, pl.Series))
         ):
-            return self.concat(pli.Series([other]))
+            return self.concat(pl.Series([other]))
 
         other_list: list[Expr | str | Series]
         other_list = [other] if not isinstance(other, list) else copy.copy(other)  # type: ignore[arg-type]
 
         other_list.insert(0, wrap_expr(self._pyexpr))
         return F.concat_list(other_list)
 
@@ -275,29 +278,29 @@
         ----------
         index
             Index to return per sublist
 
         Examples
         --------
         >>> df = pl.DataFrame({"foo": [[3, 2, 1], [], [1, 2]]})
-        >>> df.select(pl.col("foo").arr.get(0))
+        >>> df.select(pl.col("foo").list.get(0))
         shape: (3, 1)
         
          foo  
          ---  
          i64  
         
          3    
          null 
          1    
         
 
         """
-        index = expr_to_lit_or_expr(index, str_to_lit=False)._pyexpr
-        return wrap_expr(self._pyexpr.lst_get(index))
+        index = parse_as_expression(index)._pyexpr
+        return wrap_expr(self._pyexpr.list_get(index))
 
     def take(
         self,
         index: Expr | Series | list[int] | list[list[int]],
         *,
         null_on_oob: bool = False,
     ) -> Expr:
@@ -315,29 +318,26 @@
             Behavior if an index is out of bounds:
             True -> set as null
             False -> raise an error
             Note that defaulting to raising an error is much cheaper
 
         """
         if isinstance(index, list):
-            index = pli.Series(index)
-        index = expr_to_lit_or_expr(index, str_to_lit=False)._pyexpr
-        return wrap_expr(self._pyexpr.lst_take(index, null_on_oob))
-
-    def __getitem__(self, item: int) -> Expr:
-        return self.get(item)
+            index = pl.Series(index)
+        index = parse_as_expression(index)._pyexpr
+        return wrap_expr(self._pyexpr.list_take(index, null_on_oob))
 
     def first(self) -> Expr:
         """
         Get the first value of the sublists.
 
         Examples
         --------
         >>> df = pl.DataFrame({"foo": [[3, 2, 1], [], [1, 2]]})
-        >>> df.select(pl.col("foo").arr.first())
+        >>> df.select(pl.col("foo").list.first())
         shape: (3, 1)
         
          foo  
          ---  
          i64  
         
          3    
@@ -351,15 +351,15 @@
     def last(self) -> Expr:
         """
         Get the last value of the sublists.
 
         Examples
         --------
         >>> df = pl.DataFrame({"foo": [[3, 2, 1], [], [1, 2]]})
-        >>> df.select(pl.col("foo").arr.last())
+        >>> df.select(pl.col("foo").list.last())
         shape: (3, 1)
         
          foo  
          ---  
          i64  
         
          1    
@@ -384,28 +384,29 @@
         Returns
         -------
         Boolean mask
 
         Examples
         --------
         >>> df = pl.DataFrame({"foo": [[3, 2, 1], [], [1, 2]]})
-        >>> df.select(pl.col("foo").arr.contains(1))
+        >>> df.select(pl.col("foo").list.contains(1))
         shape: (3, 1)
         
          foo   
          ---   
          bool  
         
          true  
          false 
          true  
         
 
         """
-        return wrap_expr(self._pyexpr.arr_contains(expr_to_lit_or_expr(item)._pyexpr))
+        item = parse_as_expression(item, str_as_lit=True)._pyexpr
+        return wrap_expr(self._pyexpr.list_contains(item))
 
     def join(self, separator: str) -> Expr:
         """
         Join all string items in a sublist and place a separator between them.
 
         This errors if inner type of list `!= Utf8`.
 
@@ -417,27 +418,27 @@
         Returns
         -------
         Series of dtype Utf8
 
         Examples
         --------
         >>> df = pl.DataFrame({"s": [["a", "b", "c"], ["x", "y"]]})
-        >>> df.select(pl.col("s").arr.join(" "))
+        >>> df.select(pl.col("s").list.join(" "))
         shape: (2, 1)
         
          s     
          ---   
          str   
         
          a b c 
          x y   
         
 
         """
-        return wrap_expr(self._pyexpr.lst_join(separator))
+        return wrap_expr(self._pyexpr.list_join(separator))
 
     def arg_min(self) -> Expr:
         """
         Retrieve the index of the minimal value in every sublist.
 
         Returns
         -------
@@ -446,27 +447,27 @@
         Examples
         --------
         >>> df = pl.DataFrame(
         ...     {
         ...         "a": [[1, 2], [2, 1]],
         ...     }
         ... )
-        >>> df.select(pl.col("a").arr.arg_min())
+        >>> df.select(pl.col("a").list.arg_min())
         shape: (2, 1)
         
          a   
          --- 
          u32 
         
          0   
          1   
         
 
         """
-        return wrap_expr(self._pyexpr.lst_arg_min())
+        return wrap_expr(self._pyexpr.list_arg_min())
 
     def arg_max(self) -> Expr:
         """
         Retrieve the index of the maximum value in every sublist.
 
         Returns
         -------
@@ -475,27 +476,27 @@
         Examples
         --------
         >>> df = pl.DataFrame(
         ...     {
         ...         "a": [[1, 2], [2, 1]],
         ...     }
         ... )
-        >>> df.select(pl.col("a").arr.arg_max())
+        >>> df.select(pl.col("a").list.arg_max())
         shape: (2, 1)
         
          a   
          --- 
          u32 
         
          1   
          0   
         
 
         """
-        return wrap_expr(self._pyexpr.lst_arg_max())
+        return wrap_expr(self._pyexpr.list_arg_max())
 
     def diff(self, n: int = 1, null_behavior: NullBehavior = "ignore") -> Expr:
         """
         Calculate the n-th discrete difference of every sublist.
 
         Parameters
         ----------
@@ -503,72 +504,72 @@
             Number of slots to shift.
         null_behavior : {'ignore', 'drop'}
             How to handle null values.
 
         Examples
         --------
         >>> df = pl.DataFrame({"n": [[1, 2, 3, 4], [10, 2, 1]]})
-        >>> df.select(pl.col("n").arr.diff())
+        >>> df.select(pl.col("n").list.diff())
         shape: (2, 1)
         
          n              
          ---            
          list[i64]      
         
          [null, 1,  1] 
          [null, -8, -1] 
         
 
-        >>> df.select(pl.col("n").arr.diff(n=2))
+        >>> df.select(pl.col("n").list.diff(n=2))
         shape: (2, 1)
         
          n                 
          ---               
          list[i64]         
         
          [null, null,  2] 
          [null, null, -9]  
         
 
-        >>> df.select(pl.col("n").arr.diff(n=2, null_behavior="drop"))
+        >>> df.select(pl.col("n").list.diff(n=2, null_behavior="drop"))
         shape: (2, 1)
         
          n         
          ---       
          list[i64] 
         
          [2, 2]    
          [-9]      
         
 
         """
-        return wrap_expr(self._pyexpr.lst_diff(n, null_behavior))
+        return wrap_expr(self._pyexpr.list_diff(n, null_behavior))
 
     def shift(self, periods: int = 1) -> Expr:
         """
         Shift values by the given period.
 
         Parameters
         ----------
         periods
             Number of places to shift (may be negative).
 
         Examples
         --------
         >>> s = pl.Series("a", [[1, 2, 3, 4], [10, 2, 1]])
-        >>> s.arr.shift()
+        >>> s.list.shift()
         shape: (2,)
         Series: 'a' [list[i64]]
         [
             [null, 1,  3]
             [null, 10, 2]
         ]
 
         """
-        return wrap_expr(self._pyexpr.lst_shift(periods))
+        return wrap_expr(self._pyexpr.list_shift(periods))
 
     def slice(
         self, offset: int | str | Expr, length: int | str | Expr | None = None
     ) -> Expr:
         """
         Slice every sublist.
 
@@ -579,40 +580,40 @@
         length
             Length of the slice. If set to ``None`` (default), the slice is taken to the
             end of the list.
 
         Examples
         --------
         >>> s = pl.Series("a", [[1, 2, 3, 4], [10, 2, 1]])
-        >>> s.arr.slice(1, 2)
+        >>> s.list.slice(1, 2)
         shape: (2,)
         Series: 'a' [list[i64]]
         [
             [2, 3]
             [2, 1]
         ]
 
         """
-        offset = expr_to_lit_or_expr(offset, str_to_lit=False)._pyexpr
-        length = expr_to_lit_or_expr(length, str_to_lit=False)._pyexpr
-        return wrap_expr(self._pyexpr.lst_slice(offset, length))
+        offset = parse_as_expression(offset)._pyexpr
+        length = parse_as_expression(length)._pyexpr
+        return wrap_expr(self._pyexpr.list_slice(offset, length))
 
     def head(self, n: int | str | Expr = 5) -> Expr:
         """
         Slice the first `n` values of every sublist.
 
         Parameters
         ----------
         n
             Number of values to return for each sublist.
 
         Examples
         --------
         >>> s = pl.Series("a", [[1, 2, 3, 4], [10, 2, 1]])
-        >>> s.arr.head(2)
+        >>> s.list.head(2)
         shape: (2,)
         Series: 'a' [list[i64]]
         [
             [1, 2]
             [10, 2]
         ]
 
@@ -627,25 +628,25 @@
         ----------
         n
             Number of values to return for each sublist.
 
         Examples
         --------
         >>> s = pl.Series("a", [[1, 2, 3, 4], [10, 2, 1]])
-        >>> s.arr.tail(2)
+        >>> s.list.tail(2)
         shape: (2,)
         Series: 'a' [list[i64]]
         [
             [3, 4]
             [2, 1]
         ]
 
         """
-        offset = -expr_to_lit_or_expr(n, str_to_lit=False)
-        return self.slice(offset, n)
+        n = parse_as_expression(n)._pyexpr
+        return wrap_expr(self._pyexpr.list_tail(n))
 
     def explode(self) -> Expr:
         """
         Returns a column with a separate row for every list element.
 
         Returns
         -------
@@ -654,15 +655,15 @@
         See Also
         --------
         ExprNameSpace.reshape: Reshape this Expr to a flat Series or a Series of Lists.
 
         Examples
         --------
         >>> df = pl.DataFrame({"a": [[1, 2, 3], [4, 5, 6]]})
-        >>> df.select(pl.col("a").arr.explode())
+        >>> df.select(pl.col("a").list.explode())
         shape: (6, 1)
         
          a   
          --- 
          i64 
         
          1   
@@ -672,98 +673,119 @@
          5   
          6   
         
 
         """
         return wrap_expr(self._pyexpr.explode())
 
-    def count_match(
-        self, element: float | str | bool | int | date | datetime | time | Expr
-    ) -> Expr:
+    def count_match(self, element: IntoExpr) -> Expr:
         """
         Count how often the value produced by ``element`` occurs.
 
         Parameters
         ----------
         element
             An expression that produces a single value
 
         Examples
         --------
         >>> df = pl.DataFrame({"listcol": [[0], [1], [1, 2, 3, 2], [1, 2, 1], [4, 4]]})
-        >>> df.select(pl.col("listcol").arr.count_match(2).alias("number_of_twos"))
+        >>> df.select(pl.col("listcol").list.count_match(2).alias("number_of_twos"))
         shape: (5, 1)
         
          number_of_twos 
          ---            
          u32            
         
          0              
          0              
          2              
          1              
          0              
         
 
         """
-        return wrap_expr(
-            self._pyexpr.lst_count_match(expr_to_lit_or_expr(element)._pyexpr)
-        )
+        element = parse_as_expression(element, str_as_lit=True)._pyexpr
+        return wrap_expr(self._pyexpr.list_count_match(element))
 
+    @deprecated_alias(name_generator="fields")
     def to_struct(
         self,
         n_field_strategy: ToStructStrategy = "first_non_null",
-        name_generator: Callable[[int], str] | None = None,
+        fields: Sequence[str] | Callable[[int], str] | None = None,
         upper_bound: int = 0,
     ) -> Expr:
         """
         Convert the series of type ``List`` to a series of type ``Struct``.
 
         Parameters
         ----------
         n_field_strategy : {'first_non_null', 'max_width'}
             Strategy to determine the number of fields of the struct.
-        name_generator
-            A custom function that can be used to generate the field names.
-            Default field names are `field_0, field_1 .. field_n`
+
+            * "first_non_null": set number of fields equal to the length of the
+              first non zero-length sublist.
+            * "max_width": set number of fields as max length of all sublists.
+
+        fields
+            If the name and number of the desired fields is known in advance
+            a list of field names can be given, which will be assigned by index.
+            Otherwise, to dynamically assign field names, a custom function can be
+            used; if neither are set, fields will be `field_0, field_1 .. field_n`.
         upper_bound
-            A polars `LazyFrame` needs to know the schema at all time.
-            The caller therefore must provide an `upper_bound` of
-            struct fields that will be set.
-            If this is incorrectly downstream operation may fail.
-            For instance an `all().sum()` expression will look in
-            the current schema to determine which columns to select.
-            It is advised to set this value in a lazy query.
+            A polars ``LazyFrame`` needs to know the schema at all times, so the
+            caller must provide an upper bound of the number of struct fields that
+            will be created; if set incorrectly, subsequent operations may fail.
+            (For example, an ``all().sum()`` expression will look in the current
+            schema to determine which columns to select).
+
+            When operating on a ``DataFrame``, the schema does not need to be
+            tracked or pre-determined, as the result will be eagerly evaluated,
+            so you can leave this parameter unset.
 
         Examples
         --------
-        >>> df = pl.DataFrame({"a": [[1, 2, 3], [1, 2]]})
-        >>> df.select([pl.col("a").arr.to_struct()])
+        Convert list to struct with default field name assignment:
+
+        >>> df = pl.DataFrame({"n": [[0, 1, 2], [0, 1]]})
+        >>> df.select(pl.col("n").list.to_struct())
         shape: (2, 1)
         
-         a          
+         n          
          ---        
          struct[3]  
         
-         {1,2,3}    
-         {1,2,null} 
+         {0,1,2}    
+         {0,1,null} 
         
-        >>> df.select(
-        ...     [
-        ...         pl.col("a").arr.to_struct(
-        ...             name_generator=lambda idx: f"col_name_{idx}"
-        ...         )
-        ...     ]
-        ... ).to_series().to_list()
-        [{'col_name_0': 1, 'col_name_1': 2, 'col_name_2': 3},
-        {'col_name_0': 1, 'col_name_1': 2, 'col_name_2': None}]
+
+        Convert list to struct with field name assignment by function/index:
+
+        >>> df.select(pl.col("n").list.to_struct(fields=lambda idx: f"n{idx}")).rows(
+        ...     named=True
+        ... )
+        [{'n': {'n0': 0, 'n1': 1, 'n2': 2}}, {'n': {'n0': 0, 'n1': 1, 'n2': None}}]
+
+        Convert list to struct with field name assignment by index from a list of names:
+
+        >>> df.select(pl.col("n").list.to_struct(fields=["one", "two", "three"])).rows(
+        ...     named=True
+        ... )
+        [{'n': {'one': 0, 'two': 1, 'three': 2}},
+        {'n': {'one': 0, 'two': 1, 'three': None}}]
 
         """
+        if isinstance(fields, Sequence):
+            field_names = list(fields)
+
+            def fields(idx: int) -> str:
+                return field_names[idx]
+
         return wrap_expr(
-            self._pyexpr.lst_to_struct(n_field_strategy, name_generator, upper_bound)
+            self._pyexpr.list_to_struct(n_field_strategy, fields, upper_bound)
         )
 
     def eval(self, expr: Expr, *, parallel: bool = False) -> Expr:
         """
         Run any polars expression against the lists' elements.
 
         Parameters
@@ -778,22 +800,22 @@
             This likely should not be use in the groupby context, because we already
             parallel execution per group
 
         Examples
         --------
         >>> df = pl.DataFrame({"a": [1, 8, 3], "b": [4, 5, 2]})
         >>> df.with_columns(
-        ...     pl.concat_list(["a", "b"]).arr.eval(pl.element().rank()).alias("rank")
+        ...     pl.concat_list(["a", "b"]).list.eval(pl.element().rank()).alias("rank")
         ... )
         shape: (3, 3)
         
          a    b    rank       
          ---  ---  ---        
          i64  i64  list[f32]  
         
          1    4    [1.0, 2.0] 
          8    5    [2.0, 1.0] 
          3    2    [2.0, 1.0] 
         
 
         """
-        return wrap_expr(self._pyexpr.lst_eval(expr._pyexpr, parallel))
+        return wrap_expr(self._pyexpr.list_eval(expr._pyexpr, parallel))
```

### Comparing `polars_lts_cpu-0.17.9/polars/expr/meta.py` & `polars_lts_cpu-0.18.0/polars/expr/meta.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING
 
 from polars.utils._wrap import wrap_expr
 
 if TYPE_CHECKING:
-    from polars.expr.expr import Expr
+    from polars import Expr
 
 
 class ExprMetaNameSpace:
     """Namespace for expressions on a meta level."""
 
     _accessor = "meta"
 
@@ -18,14 +18,40 @@
 
     def __eq__(self, other: ExprMetaNameSpace | Expr) -> bool:  # type: ignore[override]
         return self._pyexpr.meta_eq(other._pyexpr)
 
     def __ne__(self, other: ExprMetaNameSpace | Expr) -> bool:  # type: ignore[override]
         return not self == other
 
+    def eq(self, other: ExprMetaNameSpace | Expr) -> bool:
+        """Indicate if this expression is the same as another expression."""
+        return self._pyexpr.meta_eq(other._pyexpr)
+
+    def ne(self, other: ExprMetaNameSpace | Expr) -> bool:
+        """Indicate if this expression is NOT the same as another expression."""
+        return not self.eq(other)
+
+    def has_multiple_outputs(self) -> bool:
+        """Whether this expression expands into multiple expressions."""
+        return self._pyexpr.meta_has_multiple_outputs()
+
+    def is_regex_projection(self) -> bool:
+        """Whether this expression expands to columns that match a regex pattern."""
+        return self._pyexpr.meta_is_regex_projection()
+
+    def output_name(self) -> str:
+        """
+        Get the column name that this expression would produce.
+
+        It may not always be possible to determine the output name, as that can depend
+        on the schema of the context; in that case this will raise ``ComputeError``.
+
+        """
+        return self._pyexpr.meta_output_name()
+
     def pop(self) -> list[Expr]:
         """
         Pop the latest expression and return the input(s) of the popped expression.
 
         Returns
         -------
         A list of expressions which in most cases will have a unit length.
@@ -33,31 +59,12 @@
         For instance in a ``fold`` expression.
 
         """
         return [wrap_expr(e) for e in self._pyexpr.meta_pop()]
 
     def root_names(self) -> list[str]:
         """Get a list with the root column name."""
-        return self._pyexpr.meta_roots()
-
-    def output_name(self) -> str:
-        """
-        Get the column name that this expression would produce.
-
-        It might not always be possible to determine the output name
-        as it might depend on the schema of the context. In that case
-        this will raise a ``pl.ComputeError``.
-
-        """
-        return self._pyexpr.meta_output_name()
+        return self._pyexpr.meta_root_names()
 
     def undo_aliases(self) -> Expr:
         """Undo any renaming operation like ``alias`` or ``keep_name``."""
         return wrap_expr(self._pyexpr.meta_undo_aliases())
-
-    def has_multiple_outputs(self) -> bool:
-        """Whether this expression expands into multiple expressions."""
-        return self._pyexpr.meta_has_multiple_outputs()
-
-    def is_regex_projection(self) -> bool:
-        """Whether this expression expands to columns that match a regex pattern."""
-        return self._pyexpr.meta_is_regex_projection()
```

### Comparing `polars_lts_cpu-0.17.9/polars/expr/string.py` & `polars_lts_cpu-0.18.0/polars/expr/string.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,159 +1,351 @@
 from __future__ import annotations
 
 import warnings
 from typing import TYPE_CHECKING
 
 from polars.datatypes import Date, Datetime, Time, py_type_to_dtype
 from polars.exceptions import ChronoFormatWarning
-from polars.utils import no_default
-from polars.utils._parse_expr_input import expr_to_lit_or_expr
+from polars.utils._parse_expr_input import parse_as_expression
 from polars.utils._wrap import wrap_expr
 from polars.utils.decorators import deprecated_alias
 from polars.utils.various import find_stacklevel
 
 if TYPE_CHECKING:
-    from polars.expr.expr import Expr
-    from polars.type_aliases import PolarsDataType, PolarsTemporalType, TransferEncoding
-    from polars.utils import NoDefault
+    from polars import Expr
+    from polars.type_aliases import (
+        PolarsDataType,
+        PolarsTemporalType,
+        TimeUnit,
+        TransferEncoding,
+    )
 
 
 class ExprStringNameSpace:
     """Namespace for string related expressions."""
 
     _accessor = "str"
 
     def __init__(self, expr: Expr):
         self._pyexpr = expr._pyexpr
 
+    def to_date(
+        self,
+        format: str | None = None,
+        *,
+        strict: bool = True,
+        exact: bool = True,
+        cache: bool = True,
+    ) -> Expr:
+        """
+        Convert a Utf8 column into a Date column.
+
+        Parameters
+        ----------
+        format
+            Format to use for conversion. Refer to the `chrono crate documentation
+            <https://docs.rs/chrono/latest/chrono/format/strftime/index.html>`_
+            for the full specification. Example: ``"%Y-%m-%d"``.
+            If set to None (default), the format is inferred from the data.
+        strict
+            Raise an error if any conversion fails.
+        exact
+            Require an exact format match. If False, allow the format to match anywhere
+            in the target string.
+        cache
+            Use a cache of unique, converted dates to apply the conversion.
+
+        Examples
+        --------
+        >>> s = pl.Series(["2020/01/01", "2020/02/01", "2020/03/01"])
+        >>> s.str.to_date()
+        shape: (3,)
+        Series: '' [date]
+        [
+                2020-01-01
+                2020-02-01
+                2020-03-01
+        ]
+
+        """
+        _validate_format_argument(format)
+        return wrap_expr(self._pyexpr.str_to_date(format, strict, exact, cache))
+
+    def to_datetime(
+        self,
+        format: str | None = None,
+        *,
+        time_unit: TimeUnit | None = None,
+        time_zone: str | None = None,
+        strict: bool = True,
+        exact: bool = True,
+        cache: bool = True,
+        utc: bool | None = None,
+    ) -> Expr:
+        """
+        Convert a Utf8 column into a Datetime column.
+
+        Parameters
+        ----------
+        format
+            Format to use for conversion. Refer to the `chrono crate documentation
+            <https://docs.rs/chrono/latest/chrono/format/strftime/index.html>`_
+            for the full specification. Example: ``"%Y-%m-%d %H:%M:%S"``.
+            If set to None (default), the format is inferred from the data.
+        time_unit : {None, 'us', 'ns', 'ms'}
+            Unit of time for the resulting Datetime column. If set to None (default),
+            the time unit is inferred from the format string if given, eg:
+            ``"%F %T%.3f"`` => ``Datetime("ms")``. If no fractional second component is
+            found, the default is ``"us"``.
+        time_zone
+            Time zone for the resulting Datetime column.
+        strict
+            Raise an error if any conversion fails.
+        exact
+            Require an exact format match. If False, allow the format to match anywhere
+            in the target string.
+        cache
+            Use a cache of unique, converted datetimes to apply the conversion.
+        utc
+            Parse time zone aware datetimes as UTC. This may be useful if you have data
+            with mixed offsets.
+
+            .. deprecated:: 0.18.0
+                This is now a no-op, you can safely remove it.
+                Offset-naive strings are parsed as ``pl.Datetime(time_unit)``,
+                and offset-aware strings are converted to
+                ``pl.Datetime(time_unit, "UTC")``.
+
+        Examples
+        --------
+        >>> s = pl.Series(["2020-01-01 01:00Z", "2020-01-01 02:00Z"])
+        >>> s.str.to_datetime("%Y-%m-%d %H:%M%#z")
+        shape: (2,)
+        Series: '' [datetime[s, UTC]]
+        [
+                2020-01-01 01:00:00 UTC
+                2020-01-01 02:00:00 UTC
+        ]
+        """
+        _validate_format_argument(format)
+        if utc is not None:
+            warnings.warn(
+                "The `utc` argument is now a no-op and has no effect. "
+                "You can safely remove it. "
+                "Offset-naive strings are parsed as ``pl.Datetime(time_unit)``, "
+                "and offset-aware strings are converted to "
+                '``pl.Datetime(time_unit, "UTC")``.',
+                DeprecationWarning,
+                stacklevel=find_stacklevel(),
+            )
+        return wrap_expr(
+            self._pyexpr.str_to_datetime(
+                format,
+                time_unit,
+                time_zone,
+                strict,
+                exact,
+                cache,
+            )
+        )
+
+    def to_time(
+        self,
+        format: str | None = None,
+        *,
+        strict: bool = True,
+        cache: bool = True,
+    ) -> Expr:
+        """
+        Convert a Utf8 column into a Time column.
+
+        Parameters
+        ----------
+        format
+            Format to use for conversion. Refer to the `chrono crate documentation
+            <https://docs.rs/chrono/latest/chrono/format/strftime/index.html>`_
+            for the full specification. Example: ``"%H:%M:%S"``.
+            If set to None (default), the format is inferred from the data.
+        strict
+            Raise an error if any conversion fails.
+        cache
+            Use a cache of unique, converted times to apply the conversion.
+
+        Examples
+        --------
+        >>> s = pl.Series(["01:00", "02:00", "03:00"])
+        >>> s.str.to_time("%H:%M")
+        shape: (3,)
+        Series: '' [time]
+        [
+                01:00:00
+                02:00:00
+                03:00:00
+        ]
+
+        """
+        _validate_format_argument(format)
+        return wrap_expr(self._pyexpr.str_to_time(format, strict, cache))
+
     @deprecated_alias(datatype="dtype", fmt="format")
     def strptime(
         self,
         dtype: PolarsTemporalType,
         format: str | None = None,
         *,
         strict: bool = True,
         exact: bool = True,
         cache: bool = True,
-        tz_aware: bool | NoDefault = no_default,
-        utc: bool = False,
+        utc: bool | None = None,
     ) -> Expr:
         """
-        Parse a Utf8 expression to a Date/Datetime/Time type.
+        Convert a Utf8 column into a Date/Datetime/Time column.
 
         Parameters
         ----------
         dtype
-            Date | Datetime | Time
+            The data type to convert into. Can be either Date, Datetime, or Time.
         format
-            Format to use, refer to the `chrono strftime documentation
+            Format to use for conversion. Refer to the `chrono crate documentation
             <https://docs.rs/chrono/latest/chrono/format/strftime/index.html>`_
-            for specification. Example: ``"%y-%m-%d"``.
+            for the full specification. Example: ``"%Y-%m-%d %H:%M:%S"``.
+            If set to None (default), the format is inferred from the data.
         strict
             Raise an error if any conversion fails.
         exact
-            - If True, require an exact format match.
-            - If False, allow the format to match anywhere in the target string.
+            Require an exact format match. If False, allow the format to match anywhere
+            in the target string. Conversion to the Time type is always exact.
         cache
             Use a cache of unique, converted dates to apply the datetime conversion.
-        tz_aware
-            Parse timezone aware datetimes. This may be automatically toggled by the
-            `format` given.
-
-            .. deprecated:: 0.16.17
-                This is now auto-inferred from the given `format`. You can safely drop
-                this argument, it will be removed in a future version.
         utc
-            Parse timezone aware datetimes as UTC. This may be useful if you have data
+            Parse time zone aware datetimes as UTC. This may be useful if you have data
             with mixed offsets.
 
+            .. deprecated:: 0.18.0
+                This is now a no-op, you can safely remove it.
+                Offset-naive strings are parsed as ``pl.Datetime(time_unit)``,
+                and offset-aware strings are converted to
+                ``pl.Datetime(time_unit, "UTC")``.
+
         Notes
         -----
-        When parsing a Datetime the column precision will be inferred from
-        the format string, if given, eg: "%F %T%.3f" => Datetime("ms"). If
-        no fractional second component is found then the default is "us".
+        When converting to a Datetime type, the time unit is inferred from the format
+        string if given, eg: ``"%F %T%.3f"`` => ``Datetime("ms")``. If no fractional
+        second component is found, the default is ``"us"``.
 
         Examples
         --------
         Dealing with a consistent format:
 
-        >>> ts = ["2020-01-01 01:00Z", "2020-01-01 02:00Z"]
-        >>> pl.Series(ts).str.strptime(pl.Datetime, "%Y-%m-%d %H:%M%#z")
+        >>> s = pl.Series(["2020-01-01 01:00Z", "2020-01-01 02:00Z"])
+        >>> s.str.strptime(pl.Datetime, "%Y-%m-%d %H:%M%#z")
         shape: (2,)
-        Series: '' [datetime[s, +00:00]]
+        Series: '' [datetime[s, UTC]]
         [
-                2020-01-01 01:00:00 +00:00
-                2020-01-01 02:00:00 +00:00
+                2020-01-01 01:00:00 UTC
+                2020-01-01 02:00:00 UTC
         ]
 
         Dealing with different formats.
 
         >>> s = pl.Series(
         ...     "date",
         ...     [
         ...         "2021-04-22",
         ...         "2022-01-04 00:00:00",
         ...         "01/31/22",
         ...         "Sun Jul  8 00:34:60 2001",
         ...     ],
         ... )
-        >>> s.to_frame().with_columns(
-        ...     pl.col("date")
-        ...     .str.strptime(pl.Date, "%F", strict=False)
-        ...     .fill_null(pl.col("date").str.strptime(pl.Date, "%F %T", strict=False))
-        ...     .fill_null(pl.col("date").str.strptime(pl.Date, "%D", strict=False))
-        ...     .fill_null(pl.col("date").str.strptime(pl.Date, "%c", strict=False))
-        ... )
-        shape: (4, 1)
-        
-         date       
-         ---        
-         date       
-        
-         2021-04-22 
-         2022-01-04 
-         2022-01-31 
-         2001-07-08 
-        
-
+        >>> s.to_frame().select(
+        ...     pl.coalesce(
+        ...         pl.col("date").str.strptime(pl.Date, "%F", strict=False),
+        ...         pl.col("date").str.strptime(pl.Date, "%F %T", strict=False),
+        ...         pl.col("date").str.strptime(pl.Date, "%D", strict=False),
+        ...         pl.col("date").str.strptime(pl.Date, "%c", strict=False),
+        ...     )
+        ... ).to_series()
+        shape: (4,)
+        Series: 'date' [date]
+        [
+                2021-04-22
+                2022-01-04
+                2022-01-31
+                2001-07-08
+        ]
         """
         _validate_format_argument(format)
 
-        if tz_aware is no_default:
-            tz_aware = False
-        else:
-            warnings.warn(
-                "`tz_aware` is now auto-inferred from `format` and will be removed "
-                "in a future version. You can safely drop this argument.",
-                category=DeprecationWarning,
-                stacklevel=find_stacklevel(),
-            )
-
         if dtype == Date:
-            return wrap_expr(self._pyexpr.str_to_date(format, strict, exact, cache))
+            return self.to_date(format, strict=strict, exact=exact, cache=cache)
         elif dtype == Datetime:
             time_unit = dtype.time_unit  # type: ignore[union-attr]
             time_zone = dtype.time_zone  # type: ignore[union-attr]
-            return wrap_expr(
-                self._pyexpr.str_to_datetime(
-                    format,
-                    time_unit,
-                    time_zone,
-                    strict,
-                    exact,
-                    cache,
-                    tz_aware,
-                    utc,
-                )
+            return self.to_datetime(
+                format,
+                time_unit=time_unit,
+                time_zone=time_zone,
+                strict=strict,
+                exact=exact,
+                cache=cache,
+                utc=utc,
             )
         elif dtype == Time:
-            return wrap_expr(self._pyexpr.str_to_time(format, strict, exact, cache))
+            return self.to_time(format, strict=strict, cache=cache)
         else:
             raise ValueError("dtype should be of type {Date, Datetime, Time}")
 
+    def to_decimal(
+        self,
+        inference_length: int = 100,
+    ) -> Expr:
+        """
+        Convert a Utf8 column into a Date column.
+
+        This method infers the needed parameters ``precision`` and ``scale``.
+
+        Parameters
+        ----------
+        inference_length
+            Number of elements to parse to determine the `precision` and `scale`
+
+        Examples
+        --------
+        >>> df = pl.DataFrame(
+        ...     {
+        ...         "numbers": [
+        ...             "40.12",
+        ...             "3420.13",
+        ...             "120134.19",
+        ...             "3212.98",
+        ...             "12.90",
+        ...             "143.09",
+        ...             "143.9",
+        ...         ]
+        ...     }
+        ... )
+        >>> df.select(pl.col("numbers").str.to_decimal())
+        shape: (7, 1)
+        
+         numbers      
+         ---          
+         decimal[8,2] 
+        
+         40.12        
+         3420.13      
+         120134.19    
+         3212.98      
+         12.9         
+         143.09       
+         143.9        
+        
+
+        """
+        return wrap_expr(self._pyexpr.str_to_decimal(inference_length))
+
     def lengths(self) -> Expr:
         """
         Get length of the strings as UInt32 (as number of bytes).
 
         Notes
         -----
         The returned lengths are equal to the number of bytes in the UTF8 string. If you
@@ -530,31 +722,53 @@
     ) -> Expr:
         """
         Check if string contains a substring that matches a regex.
 
         Parameters
         ----------
         pattern
-            A regex pattern compatible with the `regex crate
+            A valid regular expression pattern, compatible with the `regex crate
             <https://docs.rs/regex/latest/regex/>`_.
         literal
-            Treat pattern as a literal string.
+            Treat ``pattern`` as a literal string, not as a regular expression.
         strict
-            Raise an error if the underlying pattern is not a valid regex expression,
+            Raise an error if the underlying pattern is not a valid regex,
             otherwise mask out with a null value.
 
+        Notes
+        -----
+        To modify regular expression behaviour (such as case-sensitivity) with
+        flags, use the inline ``(?iLmsuxU)`` syntax. For example:
+
+        >>> pl.DataFrame({"s": ["AAA", "aAa", "aaa"]}).with_columns(
+        ...     default_match=pl.col("s").str.contains("AA"),
+        ...     insensitive_match=pl.col("s").str.contains("(?i)AA"),
+        ... )
+        shape: (3, 3)
+        
+         s    default_match  insensitive_match 
+         ---  ---            ---               
+         str  bool           bool              
+        
+         AAA  true           true              
+         aAa  false          true              
+         aaa  false          true              
+        
+
+        See the regex crate's section on `grouping and flags
+        <https://docs.rs/regex/latest/regex/#grouping-and-flags>`_ for
+        additional information about the use of inline expression modifiers.
+
         Examples
         --------
         >>> df = pl.DataFrame({"a": ["Crab", "cat and dog", "rab$bit", None]})
         >>> df.select(
-        ...     [
-        ...         pl.col("a"),
-        ...         pl.col("a").str.contains("cat|bit").alias("regex"),
-        ...         pl.col("a").str.contains("rab$", literal=True).alias("literal"),
-        ...     ]
+        ...     pl.col("a"),
+        ...     pl.col("a").str.contains("cat|bit").alias("regex"),
+        ...     pl.col("a").str.contains("rab$", literal=True).alias("literal"),
         ... )
         shape: (4, 3)
         
          a            regex  literal 
          ---          ---    ---     
          str          bool   bool    
         
@@ -566,15 +780,15 @@
 
         See Also
         --------
         starts_with : Check if string values start with a substring.
         ends_with : Check if string values end with a substring.
 
         """
-        pattern = expr_to_lit_or_expr(pattern, str_to_lit=True)._pyexpr
+        pattern = parse_as_expression(pattern, str_as_lit=True)._pyexpr
         return wrap_expr(self._pyexpr.str_contains(pattern, literal, strict))
 
     def ends_with(self, suffix: str | Expr) -> Expr:
         """
         Check if string values end with a substring.
 
         Parameters
@@ -613,15 +827,15 @@
 
         See Also
         --------
         contains : Check if string contains a substring that matches a regex.
         starts_with : Check if string values start with a substring.
 
         """
-        suffix = expr_to_lit_or_expr(suffix, str_to_lit=True)._pyexpr
+        suffix = parse_as_expression(suffix, str_as_lit=True)._pyexpr
         return wrap_expr(self._pyexpr.str_ends_with(suffix))
 
     def starts_with(self, prefix: str | Expr) -> Expr:
         """
         Check if string values start with a substring.
 
         Parameters
@@ -660,15 +874,15 @@
 
         See Also
         --------
         contains : Check if string contains a substring that matches a regex.
         ends_with : Check if string values end with a substring.
 
         """
-        prefix = expr_to_lit_or_expr(prefix, str_to_lit=True)._pyexpr
+        prefix = parse_as_expression(prefix, str_as_lit=True)._pyexpr
         return wrap_expr(self._pyexpr.str_starts_with(prefix))
 
     def json_extract(self, dtype: PolarsDataType | None = None) -> Expr:
         """
         Parse string values as JSON.
 
         Throw errors if encounter invalid JSON strings.
@@ -812,129 +1026,201 @@
     def extract(self, pattern: str, group_index: int = 1) -> Expr:
         r"""
         Extract the target capture group from provided patterns.
 
         Parameters
         ----------
         pattern
-            A regex pattern compatible with the `regex crate
+            A valid regular expression pattern, compatible with the `regex crate
             <https://docs.rs/regex/latest/regex/>`_.
         group_index
             Index of the targeted capture group.
             Group 0 mean the whole pattern, first group begin at index 1
             Default to the first capture group
 
+        Notes
+        -----
+        To modify regular expression behaviour (such as multi-line matching)
+        with flags, use the inline ``(?iLmsuxU)`` syntax. For example:
+
+        >>> df = pl.DataFrame(
+        ...     data={
+        ...         "lines": [
+        ...             "I Like\nThose\nOdds",
+        ...             "This is\nThe Way",
+        ...         ]
+        ...     }
+        ... )
+        >>> df.select(
+        ...     pl.col("lines").str.extract(r"(?m)^(T\w+)", 1).alias("matches"),
+        ... )
+        shape: (2, 1)
+        
+         matches 
+         ---     
+         str     
+        
+         Those   
+         This    
+        
+
+        See the regex crate's section on `grouping and flags
+        <https://docs.rs/regex/latest/regex/#grouping-and-flags>`_ for
+        additional information about the use of inline expression modifiers.
+
         Returns
         -------
         Utf8 array. Contain null if original value is null or regex capture nothing.
 
         Examples
         --------
         >>> df = pl.DataFrame(
         ...     {
-        ...         "a": [
-        ...             "http://vote.com/ballon_dor?candidate=messi&ref=polars",
-        ...             "http://vote.com/ballon_dor?candidat=jorginho&ref=polars",
+        ...         "url": [
+        ...             "http://vote.com/ballon_dor?error=404&ref=unknown",
+        ...             "http://vote.com/ballon_dor?ref=polars&candidate=messi",
         ...             "http://vote.com/ballon_dor?candidate=ronaldo&ref=polars",
         ...         ]
         ...     }
         ... )
         >>> df.select(
-        ...     [
-        ...         pl.col("a").str.extract(r"candidate=(\w+)", 1),
-        ...     ]
+        ...     pl.col("url").str.extract(r"candidate=(\w+)", 1).alias("candidate"),
+        ...     pl.col("url").str.extract(r"ref=(\w+)", 1).alias("referer"),
+        ...     pl.col("url").str.extract(r"error=(\w+)", 1).alias("error"),
         ... )
-        shape: (3, 1)
-        
-         a       
-         ---     
-         str     
-        
-         messi   
-         null    
-         ronaldo 
-        
+        shape: (3, 3)
+        
+         candidate  referer  error 
+         ---        ---      ---   
+         str        str      str   
+        
+         null       unknown  404   
+         messi      polars   null  
+         ronaldo    polars   null  
+        
 
         """
         return wrap_expr(self._pyexpr.str_extract(pattern, group_index))
 
     def extract_all(self, pattern: str | Expr) -> Expr:
-        r"""
-        Extracts all matches for the given regex pattern.
+        r'''
+        Extract all matches for the given regex pattern.
 
-        Extracts each successive non-overlapping regex match in an individual string as
-        an array.
+        Extract each successive non-overlapping regex match in an individual string
+        as a list. Extracted matches contain ``null`` if the original value is null
+        or the regex did not capture anything.
 
         Parameters
         ----------
         pattern
-            A regex pattern compatible with the `regex crate
+            A valid regular expression pattern, compatible with the `regex crate
             <https://docs.rs/regex/latest/regex/>`_.
 
+        Notes
+        -----
+        To modify regular expression behaviour (such as "verbose" mode and/or
+        case-sensitive matching) with flags, use the inline ``(?iLmsuxU)`` syntax.
+        For example:
+
+        >>> df = pl.DataFrame(
+        ...     data={
+        ...         "email": [
+        ...             "real.email@spam.com",
+        ...             "some_account@somewhere.net",
+        ...             "abc.def.ghi.jkl@uvw.xyz.co.uk",
+        ...         ]
+        ...     }
+        ... )
+        >>> # extract name/domain parts from the addresses, using verbose regex
+        >>> df.with_columns(
+        ...     pl.col("email")
+        ...     .str.extract_all(
+        ...         r"""(?xi)   # activate 'verbose' and 'case-insensitive' flags
+        ...         [           # (start character group)
+        ...           A-Z       # letters
+        ...           0-9       # digits
+        ...           ._%+\-    # special chars
+        ...         ]           # (end character group)
+        ...         +           # 'one or more' quantifier
+        ...         """
+        ...     )
+        ...     .list.to_struct(fields=["name", "domain"])
+        ...     .alias("email_parts")
+        ... ).unnest("email_parts")
+        shape: (3, 3)
+        
+         email                          name             domain        
+         ---                            ---              ---           
+         str                            str              str           
+        
+         real.email@spam.com            real.email       spam.com      
+         some_account@somewhere.net     some_account     somewhere.net 
+         abc.def.ghi.jkl@uvw.xyz.co.uk  abc.def.ghi.jkl  uvw.xyz.co.uk 
+        
+
+        See the regex crate's section on `grouping and flags
+        <https://docs.rs/regex/latest/regex/#grouping-and-flags>`_ for
+        additional information about the use of inline expression modifiers.
+
         Returns
         -------
-        List[Utf8] array. Contain null if original value is null or regex capture
-        nothing.
+        List[Utf8]
 
         Examples
         --------
         >>> df = pl.DataFrame({"foo": ["123 bla 45 asd", "xyz 678 910t"]})
         >>> df.select(
-        ...     [
-        ...         pl.col("foo").str.extract_all(r"(\d+)").alias("extracted_nrs"),
-        ...     ]
+        ...     pl.col("foo").str.extract_all(r"\d+").alias("extracted_nrs"),
         ... )
         shape: (2, 1)
         
          extracted_nrs  
          ---            
          list[str]      
         
          ["123", "45"]  
          ["678", "910"] 
         
 
-        """
-        pattern = expr_to_lit_or_expr(pattern, str_to_lit=True)
-        return wrap_expr(self._pyexpr.str_extract_all(pattern._pyexpr))
+        '''
+        pattern = parse_as_expression(pattern, str_as_lit=True)._pyexpr
+        return wrap_expr(self._pyexpr.str_extract_all(pattern))
 
     def count_match(self, pattern: str) -> Expr:
         r"""
         Count all successive non-overlapping regex matches.
 
         Parameters
         ----------
         pattern
-            A regex pattern compatible with the `regex crate
+            A valid regular expression pattern, compatible with the `regex crate
             <https://docs.rs/regex/latest/regex/>`_.
 
         Returns
         -------
         UInt32 array. Contain null if original value is null or regex capture nothing.
 
         Examples
         --------
         >>> df = pl.DataFrame({"foo": ["123 bla 45 asd", "xyz 678 910t"]})
         >>> df.select(
-        ...     [
-        ...         pl.col("foo").str.count_match(r"\d").alias("count_digits"),
-        ...     ]
+        ...     pl.col("foo").str.count_match(r"\d").alias("count_digits"),
         ... )
         shape: (2, 1)
         
          count_digits 
          ---          
          u32          
         
          5            
          6            
         
 
         """
-        return wrap_expr(self._pyexpr.count_match(pattern))
+        return wrap_expr(self._pyexpr.str_count_match(pattern))
 
     def split(self, by: str, *, inclusive: bool = False) -> Expr:
         """
         Split the string by a substring.
 
         Parameters
         ----------
@@ -984,17 +1270,15 @@
         inclusive
             If True, include the split character/string in the results.
 
         Examples
         --------
         >>> df = pl.DataFrame({"x": ["a_1", None, "c", "d_4"]})
         >>> df.select(
-        ...     [
-        ...         pl.col("x").str.split_exact("_", 1).alias("fields"),
-        ...     ]
+        ...     pl.col("x").str.split_exact("_", 1).alias("fields"),
         ... )
         shape: (4, 1)
         
          fields      
          ---         
          struct[2]   
         
@@ -1108,22 +1392,54 @@
     ) -> Expr:
         r"""
         Replace first matching regex/literal substring with a new string value.
 
         Parameters
         ----------
         pattern
-            A regex pattern compatible with the `regex crate
+            A valid regular expression pattern, compatible with the `regex crate
             <https://docs.rs/regex/latest/regex/>`_.
         value
-            Replacement string.
+            String that will replace the matched substring.
         literal
-             Treat pattern as a literal string.
+            Treat pattern as a literal string.
         n
-            Number of matches to replace
+            Number of matches to replace.
+
+        Notes
+        -----
+        To modify regular expression behaviour (such as case-sensitivity) with flags,
+        use the inline ``(?iLmsuxU)`` syntax. For example:
+
+        >>> df = pl.DataFrame(
+        ...     {
+        ...         "city": "Philadelphia",
+        ...         "season": ["Spring", "Summer", "Autumn", "Winter"],
+        ...         "weather": ["Rainy", "Sunny", "Cloudy", "Snowy"],
+        ...     }
+        ... )
+        >>> df.with_columns(
+        ...     # apply case-insensitive string replacement
+        ...     pl.col("weather").str.replace(r"(?i)foggy|rainy|cloudy|snowy", "Sunny")
+        ... )
+        shape: (4, 3)
+        
+         city          season  weather 
+         ---           ---     ---     
+         str           str     str     
+        
+         Philadelphia  Spring  Sunny   
+         Philadelphia  Summer  Sunny   
+         Philadelphia  Autumn  Sunny   
+         Philadelphia  Winter  Sunny   
+        
+
+        See the regex crate's section on `grouping and flags
+        <https://docs.rs/regex/latest/regex/#grouping-and-flags>`_ for
+        additional information about the use of inline expression modifiers.
 
         See Also
         --------
         replace_all : Replace all matching regex/literal substrings.
 
         Examples
         --------
@@ -1138,35 +1454,33 @@
          i64  str    
         
          1    123ABC 
          2    abc456 
         
 
         """
-        pattern = expr_to_lit_or_expr(pattern, str_to_lit=True)
-        value = expr_to_lit_or_expr(value, str_to_lit=True)
-        return wrap_expr(
-            self._pyexpr.str_replace_n(pattern._pyexpr, value._pyexpr, literal, n)
-        )
+        pattern = parse_as_expression(pattern, str_as_lit=True)._pyexpr
+        value = parse_as_expression(value, str_as_lit=True)._pyexpr
+        return wrap_expr(self._pyexpr.str_replace_n(pattern, value, literal, n))
 
     def replace_all(
         self, pattern: str | Expr, value: str | Expr, *, literal: bool = False
     ) -> Expr:
         """
         Replace all matching regex/literal substrings with a new string value.
 
         Parameters
         ----------
         pattern
-            A regex pattern compatible with the `regex crate
+            A valid regular expression pattern, compatible with the `regex crate
             <https://docs.rs/regex/latest/regex/>`_.
         value
             Replacement string.
         literal
-             Treat pattern as a literal string.
+            Treat pattern as a literal string.
 
         See Also
         --------
         replace : Replace first matching regex/literal substring.
 
         Examples
         --------
@@ -1179,19 +1493,17 @@
          i64  str     
         
          1    -bc-bc  
          2    123-123 
         
 
         """
-        pattern = expr_to_lit_or_expr(pattern, str_to_lit=True)
-        value = expr_to_lit_or_expr(value, str_to_lit=True)
-        return wrap_expr(
-            self._pyexpr.str_replace_all(pattern._pyexpr, value._pyexpr, literal)
-        )
+        pattern = parse_as_expression(pattern, str_as_lit=True)._pyexpr
+        value = parse_as_expression(value, str_as_lit=True)._pyexpr
+        return wrap_expr(self._pyexpr.str_replace_all(pattern, value, literal))
 
     def slice(self, offset: int, length: int | None = None) -> Expr:
         """
         Create subslices of the string values of a Utf8 Series.
 
         Parameters
         ----------
@@ -1267,15 +1579,15 @@
          o   
          b   
          a   
          r   
         
 
         """
-        return wrap_expr(self._pyexpr.explode())
+        return wrap_expr(self._pyexpr.str_explode())
 
     def parse_int(self, radix: int = 2, *, strict: bool = True) -> Expr:
         """
         Parse integers with base radix from strings.
 
         By default base 2. ParseError/Overflows become Nulls.
```

### Comparing `polars_lts_cpu-0.17.9/polars/expr/struct.py` & `polars_lts_cpu-0.18.0/polars/expr/struct.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Sequence
 
 from polars.utils._wrap import wrap_expr
 
 if TYPE_CHECKING:
-    from polars.expr.expr import Expr
+    from polars import Expr
 
 
 class ExprStructNameSpace:
     """Namespace for struct related expressions."""
 
     _accessor = "struct"
```

### Comparing `polars_lts_cpu-0.17.9/polars/functions/__init__.py` & `polars_lts_cpu-0.18.0/polars/functions/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,82 +1,76 @@
-from polars.functions.eager import (
-    align_frames,
-    concat,
-    cut,
-    date_range,
-    get_dummies,
-    ones,
-    zeros,
+from polars.functions.as_datatype import (
+    concat_list,
+    concat_str,
+    duration,
+    format,
+    struct,
 )
+from polars.functions.as_datatype import date_ as date
+from polars.functions.as_datatype import datetime_ as datetime
+from polars.functions.as_datatype import time_ as time
+from polars.functions.eager import align_frames, concat
 from polars.functions.lazy import (
     all,
     any,
     apply,
     approx_unique,
-    arange,
     arg_sort_by,
     arg_where,
     avg,
     coalesce,
     col,
     collect_all,
-    concat_list,
-    concat_str,
     corr,
     count,
     cov,
     cumfold,
     cumreduce,
     cumsum,
-    duration,
     element,
     exclude,
     first,
     fold,
-    format,
     from_epoch,
     groups,
     head,
+    implode,
     last,
     lit,
     map,
     max,
     mean,
     median,
     min,
     n_unique,
-    pearson_corr,
     quantile,
     reduce,
-    repeat,
+    rolling_corr,
+    rolling_cov,
     select,
-    spearman_rank_corr,
     std,
-    struct,
     sum,
     tail,
     var,
 )
-from polars.functions.lazy import date_ as date
-from polars.functions.lazy import datetime_ as datetime
-from polars.functions.lazy import list_ as list
+from polars.functions.range import arange, date_range, time_range
+from polars.functions.repeat import ones, repeat, zeros
 from polars.functions.whenthen import when
 
 __all__ = [
     # polars.functions.eager
     "align_frames",
     "approx_unique",
     "arg_where",
     "concat",
-    "cut",
     "date_range",
     "element",
-    "get_dummies",
     "ones",
     "repeat",
+    "time_range",
     "zeros",
     # polars.functions.lazy
     "all",
     "any",
     "apply",
     "arange",
     "arg_sort_by",
@@ -98,29 +92,30 @@
     "exclude",
     "first",
     "fold",
     "format",
     "from_epoch",
     "groups",
     "head",
+    "implode",
     "last",
-    "list",  # named list_, see import above
     "lit",
     "map",
     "max",
     "mean",
     "median",
     "min",
     "n_unique",
-    "pearson_corr",
     "quantile",
     "reduce",
+    "rolling_corr",
+    "rolling_cov",
     "select",
-    "spearman_rank_corr",
     "std",
     "struct",
     "sum",
     "tail",
+    "time",
     "var",
     # polars.functions.whenthen
     "when",
 ]
```

### Comparing `polars_lts_cpu-0.17.9/polars/functions/lazy.py` & `polars_lts_cpu-0.18.0/polars/functions/lazy.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,86 +1,53 @@
 from __future__ import annotations
 
 import contextlib
-import warnings
 from datetime import date, datetime, time, timedelta
 from typing import TYPE_CHECKING, Any, Callable, Iterable, Sequence, overload
 
-from polars import internals as pli
+import polars._reexport as pl
 from polars.datatypes import (
     DTYPE_TEMPORAL_UNITS,
     Date,
     Datetime,
     Duration,
-    Int32,
     Int64,
-    Struct,
     Time,
     UInt32,
     is_polars_dtype,
-    py_type_to_dtype,
 )
 from polars.dependencies import _check_for_numpy
 from polars.dependencies import numpy as np
-from polars.utils._parse_expr_input import expr_to_lit_or_expr, selection_to_pyexpr_list
+from polars.utils._parse_expr_input import (
+    parse_as_expression,
+    parse_as_list_of_expressions,
+)
 from polars.utils._wrap import wrap_df, wrap_expr
 from polars.utils.convert import (
     _datetime_to_pl_timestamp,
     _time_to_pl_time,
     _timedelta_to_pl_timedelta,
 )
 from polars.utils.decorators import deprecated_alias
-from polars.utils.various import find_stacklevel
 
 with contextlib.suppress(ImportError):  # Module not available when building docs
-    from polars.polars import arange as pyarange
-    from polars.polars import arg_sort_by as py_arg_sort_by
-    from polars.polars import arg_where as py_arg_where
-    from polars.polars import as_struct as _as_struct
-    from polars.polars import coalesce_exprs as _coalesce_exprs
-    from polars.polars import col as pycol
-    from polars.polars import collect_all as _collect_all
-    from polars.polars import cols as pycols
-    from polars.polars import concat_lst as _concat_lst
-    from polars.polars import concat_str as _concat_str
-    from polars.polars import count as _count
-    from polars.polars import cov as pycov
-    from polars.polars import cumfold as pycumfold
-    from polars.polars import cumreduce as pycumreduce
-    from polars.polars import dtype_cols as _dtype_cols
-    from polars.polars import first as _first
-    from polars.polars import fold as pyfold
-    from polars.polars import last as _last
-    from polars.polars import lit as pylit
-    from polars.polars import map_mul as _map_mul
-    from polars.polars import max_exprs as _max_exprs
-    from polars.polars import min_exprs as _min_exprs
-    from polars.polars import pearson_corr as pypearson_corr
-    from polars.polars import py_datetime, py_duration
-    from polars.polars import reduce as pyreduce
-    from polars.polars import repeat as _repeat
-    from polars.polars import spearman_rank_corr as pyspearman_rank_corr
-    from polars.polars import sum_exprs as _sum_exprs
+    import polars.polars as plr
 
 
 if TYPE_CHECKING:
     import sys
 
-    from polars.dataframe import DataFrame
-    from polars.expr.expr import Expr
-    from polars.lazyframe import LazyFrame
-    from polars.series import Series
+    from polars import DataFrame, Expr, LazyFrame, Series
     from polars.type_aliases import (
         CorrelationMethod,
         EpochTimeUnit,
         IntoExpr,
         PolarsDataType,
         PythonLiteral,
         RollingInterpolationMethod,
-        SchemaDict,
         TimeUnit,
     )
 
     if sys.version_info >= (3, 8):
         from typing import Literal
     else:
         from typing_extensions import Literal
@@ -219,38 +186,38 @@
     
 
     """
     if more_names:
         if isinstance(name, str):
             names_str = [name]
             names_str.extend(more_names)  # type: ignore[arg-type]
-            return wrap_expr(pycols(names_str))
+            return wrap_expr(plr.cols(names_str))
         elif is_polars_dtype(name):
             dtypes = [name]
             dtypes.extend(more_names)
-            return wrap_expr(_dtype_cols(dtypes))
+            return wrap_expr(plr.dtype_cols(dtypes))
         else:
             raise TypeError(
                 f"Invalid input for `col`. Expected `str` or `DataType`, got {type(name)!r}"
             )
 
     if isinstance(name, str):
-        return wrap_expr(pycol(name))
+        return wrap_expr(plr.col(name))
     elif is_polars_dtype(name):
-        return wrap_expr(_dtype_cols([name]))
+        return wrap_expr(plr.dtype_cols([name]))
     elif isinstance(name, Iterable):
         names = list(name)
         if not names:
-            return wrap_expr(pycols(names))
+            return wrap_expr(plr.cols(names))
 
         item = names[0]
         if isinstance(item, str):
-            return wrap_expr(pycols(names))
+            return wrap_expr(plr.cols(names))
         elif is_polars_dtype(item):
-            return wrap_expr(_dtype_cols(names))
+            return wrap_expr(plr.dtype_cols(names))
         else:
             raise TypeError(
                 "Invalid input for `col`. Expected iterable of type `str` or `DataType`,"
                 f" got iterable of type {type(item)!r}"
             )
     else:
         raise TypeError(
@@ -264,15 +231,15 @@
 
     Examples
     --------
     A horizontal rank computation by taking the elements of a list
 
     >>> df = pl.DataFrame({"a": [1, 8, 3], "b": [4, 5, 2]})
     >>> df.with_columns(
-    ...     pl.concat_list(["a", "b"]).arr.eval(pl.element().rank()).alias("rank")
+    ...     pl.concat_list(["a", "b"]).list.eval(pl.element().rank()).alias("rank")
     ... )
     shape: (3, 3)
     
      a    b    rank       
      ---  ---  ---        
      i64  i64  list[f32]  
     
@@ -281,15 +248,15 @@
      3    2    [2.0, 1.0] 
     
 
     A mathematical operation on array elements
 
     >>> df = pl.DataFrame({"a": [1, 8, 3], "b": [4, 5, 2]})
     >>> df.with_columns(
-    ...     pl.concat_list(["a", "b"]).arr.eval(pl.element() * 2).alias("a_b_doubled")
+    ...     pl.concat_list(["a", "b"]).list.eval(pl.element() * 2).alias("a_b_doubled")
     ... )
     shape: (3, 3)
     
      a    b    a_b_doubled 
      ---  ---  ---         
      i64  i64  list[i64]   
     
@@ -317,14 +284,17 @@
     ...
 
 
 def count(column: str | Series | None = None) -> Expr | int:
     """
     Count the number of values in this column/context.
 
+    .. warning::
+        `null` is deemed a value in this context.
+
     Parameters
     ----------
     column
         If dtype is:
 
         * ``pl.Series`` : count the values in the series.
         * ``str`` : count the values in this column.
@@ -351,17 +321,17 @@
     
      foo  2     
      bar  1     
     
 
     """
     if column is None:
-        return wrap_expr(_count())
+        return wrap_expr(plr.count())
 
-    if isinstance(column, pli.Series):
+    if isinstance(column, pl.Series):
         return column.len()
     return col(column).count()
 
 
 def implode(name: str) -> Expr:
     """
     Aggregate all column values into a list.
@@ -371,35 +341,14 @@
     name
         Name of the column that should be imploded.
 
     """
     return col(name).implode()
 
 
-def list_(name: str) -> Expr:
-    """
-    Aggregate to list.
-
-    .. deprecated:: 0.17.3
-        ``list`` will be removed in favor of ``implode``.
-
-    Parameters
-    ----------
-    name
-        Name of the column that should be aggregated into a list.
-
-    """
-    warnings.warn(
-        "`pl.list` is deprecated, please use `pl.implode` instead.",
-        DeprecationWarning,
-        stacklevel=find_stacklevel(),
-    )
-    return col(name).implode()
-
-
 @overload
 def std(column: str, ddof: int = 1) -> Expr:
     ...
 
 
 @overload
 def std(column: Series, ddof: int = 1) -> float | None:
@@ -431,15 +380,15 @@
     
      3.605551 
     
     >>> df["a"].std()
     3.605551275463989
 
     """
-    if isinstance(column, pli.Series):
+    if isinstance(column, pl.Series):
         return column.std(ddof)
     return col(column).std(ddof)
 
 
 @overload
 def var(column: str, ddof: int = 1) -> Expr:
     ...
@@ -475,15 +424,15 @@
     
      13.0 
     
     >>> df["a"].var()
     13.0
 
     """
-    if isinstance(column, pli.Series):
+    if isinstance(column, pl.Series):
         return column.var(ddof)
     return col(column).var(ddof)
 
 
 @overload
 def max(exprs: Series) -> PythonLiteral | None:  # type: ignore[misc]
     ...
@@ -494,90 +443,94 @@
     ...
 
 
 def max(exprs: IntoExpr | Iterable[IntoExpr], *more_exprs: IntoExpr) -> Expr | Any:
     """
     Get the maximum value.
 
-    If a single column is passed, get the maximum value of that column (vertical).
-    If multiple columns are passed, get the maximum value of each row (horizontal).
+    If a single string is passed, this is an alias for ``pl.col(name).max()``.
+    If a single Series is passed, this is an alias for ``Series.max()``.
+
+    Otherwise, this function computes the maximum value horizontally across multiple
+    columns.
 
     Parameters
     ----------
     exprs
         Column(s) to use in the aggregation. Accepts expression input. Strings are
         parsed as column names, other non-expression inputs are parsed as literals.
     *more_exprs
         Additional columns to use in the aggregation, specified as positional arguments.
 
     Examples
     --------
-    Get the maximum value by columns with a string column name.
+    Get the maximum value by row by passing multiple columns/expressions.
 
-    >>> df = pl.DataFrame({"a": [1, 8, 3], "b": [4, 5, 2], "c": ["foo", "bar", "foo"]})
-    >>> df.select(pl.max("a"))
-    shape: (1, 1)
+    >>> df = pl.DataFrame(
+    ...     {
+    ...         "a": [1, 8, 3],
+    ...         "b": [4, 5, 2],
+    ...         "c": ["foo", "bar", "foo"],
+    ...     }
+    ... )
+    >>> df.select(pl.max("a", "b"))
+    shape: (3, 1)
     
-     a   
+     max 
      --- 
      i64 
     
+     4   
      8   
+     3   
     
 
-    Get the maximum value by row with a list of columns/expressions.
+    Get the maximum value of a column by passing a single column name.
 
-    >>> df.select(pl.max(["a", "b"]))
-    shape: (3, 1)
+    >>> df.select(pl.max("a"))
+    shape: (1, 1)
     
-     max 
+     a   
      --- 
      i64 
     
-     4   
      8   
-     3   
     
 
-    To aggregate maximums for more than one column/expression use ``pl.col(list).max()``
-    or a regular expression selector like ``pl.sum(regex)``:
+    Get column-wise maximums for multiple columns by passing a regular expression,
+    or call ``.max()`` on a multi-column expression instead.
 
-    >>> df.select(pl.col(["a", "b"]).max())
+    >>> df.select(pl.max("^a|b$"))
     shape: (1, 2)
     
      a    b   
      ---  --- 
      i64  i64 
     
      8    5   
     
-
-    >>> df.select(pl.max("^.*[ab]$"))
+    >>> df.select(pl.col("a", "b").max())
     shape: (1, 2)
     
      a    b   
      ---  --- 
      i64  i64 
     
      8    5   
     
 
     """
     if not more_exprs:
-        if isinstance(exprs, pli.Series):
+        if isinstance(exprs, pl.Series):
             return exprs.max()
         elif isinstance(exprs, str):
             return col(exprs).max()
-        elif not isinstance(exprs, Iterable):
-            return lit(exprs).max()
 
-    exprs = selection_to_pyexpr_list(exprs)
-    if more_exprs:
-        exprs.extend(selection_to_pyexpr_list(more_exprs))
-    return wrap_expr(_max_exprs(exprs))
+    exprs = parse_as_list_of_expressions(exprs, *more_exprs)
+    return wrap_expr(plr.max_exprs(exprs))
 
 
 @overload
 def min(exprs: Series) -> PythonLiteral | None:  # type: ignore[misc]
     ...
 
 
@@ -588,130 +541,125 @@
 
 def min(
     exprs: IntoExpr | Iterable[IntoExpr], *more_exprs: IntoExpr
 ) -> Expr | PythonLiteral | None:
     """
     Get the minimum value.
 
-    If a single column is passed, get the minimum value of that column (vertical).
-    If multiple columns are passed, get the minimum value of each row (horizontal).
+    If a single string is passed, this is an alias for ``pl.col(name).min()``.
+    If a single Series is passed, this is an alias for ``Series.min()``.
+
+    Otherwise, this function computes the minimum value horizontally across multiple
+    columns.
 
     Parameters
     ----------
     exprs
         Column(s) to use in the aggregation. Accepts expression input. Strings are
         parsed as column names, other non-expression inputs are parsed as literals.
     *more_exprs
         Additional columns to use in the aggregation, specified as positional arguments.
 
     Examples
     --------
-    Get the minimum value by columns with a string column name.
+    Get the minimum value by row by passing multiple columns/expressions.
 
     >>> df = pl.DataFrame(
     ...     {
     ...         "a": [1, 8, 3],
     ...         "b": [4, 5, 2],
     ...         "c": ["foo", "bar", "foo"],
     ...     }
     ... )
-    >>> df.select(pl.min("a"))
-    shape: (1, 1)
+    >>> df.select(pl.min("a", "b"))
+    shape: (3, 1)
     
-     a   
+     min 
      --- 
      i64 
     
      1   
+     5   
+     2   
     
 
-    Get the minimum value by row with a list of columns/expressions.
+    Get the minimum value of a column by passing a single column name.
 
-    >>> df.select(pl.min(["a", "b"]))
-    shape: (3, 1)
+    >>> df.select(pl.min("a"))
+    shape: (1, 1)
     
-     min 
+     a   
      --- 
      i64 
     
      1   
-     5   
-     2   
     
 
-    To aggregate minimums for more than one column/expression use ``pl.col(list).min()``
-    or a regular expression selector like ``pl.sum(regex)``:
+    Get column-wise minimums for multiple columns by passing a regular expression,
+    or call ``.min()`` on a multi-column expression instead.
 
-    >>> df.select(pl.col(["a", "b"]).min())
+    >>> df.select(pl.min("^a|b$"))
     shape: (1, 2)
     
      a    b   
      ---  --- 
      i64  i64 
     
      1    2   
     
-
-    >>> df.select(pl.min("^.*[ab]$"))
+    >>> df.select(pl.col("a", "b").min())
     shape: (1, 2)
     
      a    b   
      ---  --- 
      i64  i64 
     
      1    2   
     
 
     """
     if not more_exprs:
-        if isinstance(exprs, pli.Series):
+        if isinstance(exprs, pl.Series):
             return exprs.min()
         elif isinstance(exprs, str):
             return col(exprs).min()
-        elif not isinstance(exprs, Iterable):
-            return lit(exprs).min()
 
-    exprs = selection_to_pyexpr_list(exprs)
-    if more_exprs:
-        exprs.extend(selection_to_pyexpr_list(more_exprs))
-    return wrap_expr(_min_exprs(exprs))
+    exprs = parse_as_list_of_expressions(exprs, *more_exprs)
+    return wrap_expr(plr.min_exprs(exprs))
 
 
 @overload
-def sum(column: str | Sequence[Expr | str] | Expr) -> Expr:
+def sum(exprs: Series) -> int | float:  # type: ignore[misc]
     ...
 
 
 @overload
-def sum(column: Series) -> int | float:
+def sum(exprs: IntoExpr | Iterable[IntoExpr], *more_exprs: IntoExpr) -> Expr:
     ...
 
 
+@deprecated_alias(column="exprs")
 def sum(
-    column: str | Sequence[Expr | str] | Series | Expr,
-) -> Expr | Any:
+    exprs: IntoExpr | Iterable[IntoExpr], *more_exprs: IntoExpr
+) -> Expr | int | float:
     """
-    Sum values in a column/Series, or horizontally across list of columns/expressions.
-
-    ``pl.sum(str)`` is syntactic sugar for:
-
-    >>> pl.col(str).sum()  # doctest: +SKIP
+    Sum all values.
 
-    ``pl.sum(list)`` is syntactic sugar for:
+    If a single string is passed, this is an alias for ``pl.col(name).sum()``.
+    If a single Series is passed, this is an alias for ``Series.sum()``.
 
-    >>> pl.fold(pl.lit(0), lambda x, y: x + y, list).alias("sum")  # doctest: +SKIP
+    Otherwise, this function computes the sum horizontally across multiple columns.
 
     Parameters
     ----------
-    column
-        Column(s) to be used in aggregation.
-        This can be:
-
-        - a column name, or Series -> aggregate the sum value of that column/Series.
-        - a List[Expr] -> aggregate the sum value horizontally across the Expr result.
+    exprs
+        Column(s) to use in the aggregation. Accepts expression input. Strings are
+        parsed as column names, other non-expression inputs are parsed as literals.
+    *more_exprs
+        Additional columns to use in the aggregation, specified as positional arguments.
 
     Examples
     --------
     >>> df = pl.DataFrame(
     ...     {
     ...         "a": [1, 2],
     ...         "b": [3, 4],
@@ -739,15 +687,15 @@
      i64 
     
      3   
     
 
     Sum a list of columns/expressions horizontally:
 
-    >>> df.with_columns(pl.sum(["a", "c"]))
+    >>> df.with_columns(pl.sum("a", "c"))
     shape: (2, 4)
     
      a    b    c    sum 
      ---  ---  ---  --- 
      i64  i64  i64  i64 
     
      1    3    5    6   
@@ -758,15 +706,15 @@
 
     >>> pl.sum(df.get_column("a"))
     3
 
     To aggregate the sums for more than one column/expression use ``pl.col(list).sum()``
     or a regular expression selector like ``pl.sum(regex)``:
 
-    >>> df.select(pl.col(["a", "c"]).sum())
+    >>> df.select(pl.col("a", "c").sum())
     shape: (1, 2)
     
      a    c   
      ---  --- 
      i64  i64 
     
      3    11  
@@ -779,24 +727,22 @@
      ---  --- 
      i64  i64 
     
      7    11  
     
 
     """
-    if isinstance(column, pli.Series):
-        return column.sum()
-    elif isinstance(column, str):
-        return col(column).sum()
-    elif isinstance(column, Sequence):
-        exprs = selection_to_pyexpr_list(column)
-        return wrap_expr(_sum_exprs(exprs))
-    else:
-        # (Expr): use u32 as that will not cast to float as eagerly
-        return fold(lit(0).cast(UInt32), lambda a, b: a + b, column).alias("sum")
+    if not more_exprs:
+        if isinstance(exprs, pl.Series):
+            return exprs.sum()
+        elif isinstance(exprs, str):
+            return col(exprs).sum()
+
+    exprs = parse_as_list_of_expressions(exprs, *more_exprs)
+    return wrap_expr(plr.sum_exprs(exprs))
 
 
 @overload
 def mean(column: str) -> Expr:
     ...
 
 
@@ -821,15 +767,15 @@
     
      4.0 
     
     >>> pl.mean(df["a"])
     4.0
 
     """
-    if isinstance(column, pli.Series):
+    if isinstance(column, pl.Series):
         return column.mean()
     return col(column).mean()
 
 
 @overload
 def avg(column: str) -> Expr:
     ...
@@ -889,15 +835,15 @@
     
      3.0 
     
     >>> pl.median(df["a"])
     3.0
 
     """
-    if isinstance(column, pli.Series):
+    if isinstance(column, pl.Series):
         return column.median()
     return col(column).median()
 
 
 @overload
 def n_unique(column: str) -> Expr:
     ...
@@ -924,15 +870,15 @@
     
      2   
     
     >>> pl.n_unique(df["a"])
     2
 
     """
-    if isinstance(column, pli.Series):
+    if isinstance(column, pl.Series):
         return column.n_unique()
     return col(column).n_unique()
 
 
 def approx_unique(column: str | Expr) -> Expr:
     """
     Approx count unique values.
@@ -954,15 +900,15 @@
      --- 
      u32 
     
      2   
     
 
     """
-    if isinstance(column, pli.Expr):
+    if isinstance(column, pl.Expr):
         return column.approx_unique()
     return col(column).approx_unique()
 
 
 @overload
 def first(column: str) -> Expr:
     ...
@@ -1014,17 +960,17 @@
      1   
     
     >>> pl.first(df["a"])
     1
 
     """
     if column is None:
-        return wrap_expr(_first())
+        return wrap_expr(plr.first())
 
-    if isinstance(column, pli.Series):
+    if isinstance(column, pl.Series):
         if column.len() > 0:
             return column[0]
         else:
             raise IndexError("The series is empty, so no first value can be returned.")
     return col(column).first()
 
 
@@ -1077,17 +1023,17 @@
      3   
     
     >>> pl.last(df["a"])
     3
 
     """
     if column is None:
-        return wrap_expr(_last())
+        return wrap_expr(plr.last())
 
-    if isinstance(column, pli.Series):
+    if isinstance(column, pl.Series):
         if column.len() > 0:
             return column[-1]
         else:
             raise IndexError("The series is empty, so no last value can be returned,")
     return col(column).last()
 
 
@@ -1141,15 +1087,15 @@
     Series: 'a' [i64]
     [
         1
         8
     ]
 
     """
-    if isinstance(column, pli.Series):
+    if isinstance(column, pl.Series):
         return column.head(n)
     return col(column).head(n)
 
 
 @overload
 def tail(column: str, n: int = ...) -> Expr:
     ...
@@ -1200,15 +1146,15 @@
     Series: 'a' [i64]
     [
         8
         3
     ]
 
     """
-    if isinstance(column, pli.Series):
+    if isinstance(column, pl.Series):
         return column.tail(n)
     return col(column).tail(n)
 
 
 def lit(
     value: Any, dtype: PolarsDataType | None = None, *, allow_object: bool = False
 ) -> Expr:
@@ -1284,29 +1230,29 @@
 
     elif isinstance(value, time):
         return lit(_time_to_pl_time(value)).cast(Time)
 
     elif isinstance(value, date):
         return lit(datetime(value.year, value.month, value.day)).cast(Date)
 
-    elif isinstance(value, pli.Series):
+    elif isinstance(value, pl.Series):
         name = value.name
         value = value._s
-        e = wrap_expr(pylit(value, allow_object))
+        e = wrap_expr(plr.lit(value, allow_object))
         if name == "":
             return e
         return e.alias(name)
 
     elif (_check_for_numpy(value) and isinstance(value, np.ndarray)) or isinstance(
         value, (list, tuple)
     ):
-        return lit(pli.Series("", value))
+        return lit(pl.Series("", value))
 
     elif dtype:
-        return wrap_expr(pylit(value, allow_object)).cast(dtype)
+        return wrap_expr(plr.lit(value, allow_object)).cast(dtype)
 
     try:
         # numpy literals like np.float32(0) have item/dtype
         item = value.item()
 
         # numpy item() is py-native datetime/timedelta when units < 'ns'
         if isinstance(item, (datetime, timedelta)):
@@ -1321,51 +1267,47 @@
                     Datetime(time_unit)
                     if dtype_name.startswith("date")
                     else Duration(time_unit)
                 )
 
     except AttributeError:
         item = value
-    return wrap_expr(pylit(item, allow_object))
+    return wrap_expr(plr.lit(item, allow_object))
 
 
 @overload
-def cumsum(column: str | Sequence[Expr | str] | Expr) -> Expr:
+def cumsum(exprs: Series) -> Series:  # type: ignore[misc]
     ...
 
 
 @overload
-def cumsum(column: Series) -> int | float:
+def cumsum(exprs: IntoExpr | Iterable[IntoExpr], *more_exprs: IntoExpr) -> Expr:
     ...
 
 
+@deprecated_alias(column="exprs")
 def cumsum(
-    column: str | Sequence[Expr | str] | Series | Expr,
-) -> Expr | Any:
+    exprs: IntoExpr | Iterable[IntoExpr], *more_exprs: IntoExpr
+) -> Expr | Series:
     """
-    Cumulatively sum values in a column/Series, or horizontally across list of columns/expressions.
+    Cumulatively sum all values.
 
-    ``pl.cumsum(str)`` is syntactic sugar for:
+    If a single string is passed, this is an alias for ``pl.col(name).cumsum()``.
+    If a single Series is passed, this is an alias for ``Series.cumsum()``.
 
-    >>> pl.col(str).cumsum()  # doctest: +SKIP
-
-    ``pl.cumsum(list)`` is syntactic sugar for:
-
-    >>> pl.cumfold(pl.lit(0), lambda x, y: x + y, list).alias(
-    ...     "cumsum"
-    ... )  # doctest: +SKIP
+    Otherwise, this function computes the cumulative sum horizontally across multiple
+    columns.
 
     Parameters
     ----------
-    column
-        Column(s) to be used in aggregation.
-        This can be:
-
-        - a column name, or Series -> aggregate the sum value of that column/Series.
-        - a List[Expr] -> aggregate the sum value horizontally across the Expr result.
+    exprs
+        Column(s) to use in the aggregation. Accepts expression input. Strings are
+        parsed as column names, other non-expression inputs are parsed as literals.
+    *more_exprs
+        Additional columns to use in the aggregation, specified as positional arguments.
 
     Examples
     --------
     >>> df = pl.DataFrame(
     ...     {
     ...         "a": [1, 2],
     ...         "b": [3, 4],
@@ -1394,156 +1336,60 @@
     
      1   
      3   
     
 
     Cumulatively sum a list of columns/expressions horizontally:
 
-    >>> df.with_columns(pl.cumsum(["a", "c"]))
+    >>> df.with_columns(pl.cumsum("a", "c"))
     shape: (2, 4)
     
      a    b    c    cumsum    
      ---  ---  ---  ---       
      i64  i64  i64  struct[2] 
     
      1    3    5    {1,6}     
      2    4    6    {2,8}     
     
 
-    """  # noqa: W505
-    if isinstance(column, pli.Series):
-        return column.cumsum()
-    elif isinstance(column, str):
-        return col(column).cumsum()
-    # (Expr): use u32 as that will not cast to float as eagerly
-    return cumfold(lit(0).cast(UInt32), lambda a, b: a + b, column).alias("cumsum")
-
-
-def spearman_rank_corr(
-    a: str | Expr, b: str | Expr, ddof: int = 1, *, propagate_nans: bool = False
-) -> Expr:
     """
-    Compute the spearman rank correlation between two columns.
-
-    Missing data will be excluded from the computation.
-
-    .. deprecated:: 0.16.10
-        ``spearman_rank_corr`` will be removed in favor of
-        ``corr(..., method="spearman")``.
-
-    Parameters
-    ----------
-    a
-        Column name or Expression.
-    b
-        Column name or Expression.
-    ddof
-        Delta Degrees of Freedom: the divisor used in the calculation is N - ddof,
-        where N represents the number of elements.
-        By default ddof is 1.
-    propagate_nans
-        If `True` any `NaN` encountered will lead to `NaN` in the output.
-        Defaults to `False` where `NaN` are regarded as larger than any finite number
-        and thus lead to the highest rank.
-
-    See Also
-    --------
-    corr
-
-    Examples
-    --------
-    >>> df = pl.DataFrame({"a": [1, 8, 3], "b": [4, 5, 2], "c": ["foo", "bar", "foo"]})
-    >>> df.select(pl.spearman_rank_corr("a", "b"))  # doctest: +SKIP
-    shape: (1, 1)
-    
-     a   
-     --- 
-     f64 
-    
-     0.5 
-    
-    """
-    warnings.warn(
-        "`spearman_rank_corr()` is deprecated in favor of `corr()`",
-        DeprecationWarning,
-        stacklevel=find_stacklevel(),
-    )
-    if isinstance(a, str):
-        a = col(a)
-    if isinstance(b, str):
-        b = col(b)
-    return wrap_expr(pyspearman_rank_corr(a._pyexpr, b._pyexpr, ddof, propagate_nans))
-
-
-def pearson_corr(a: str | Expr, b: str | Expr, ddof: int = 1) -> Expr:
-    """
-    Compute the pearson's correlation between two columns.
-
-    .. deprecated:: 0.16.10
-        ``pearson_corr`` will be removed in favor of ``corr(..., method="pearson")``.
-
-    Parameters
-    ----------
-    a
-        Column name or Expression.
-    b
-        Column name or Expression.
-    ddof
-        Delta Degrees of Freedom: the divisor used in the calculation is N - ddof,
-        where N represents the number of elements.
-        By default ddof is 1.
+    if not more_exprs:
+        if isinstance(exprs, pl.Series):
+            return exprs.cumsum()
+        elif isinstance(exprs, str):
+            return col(exprs).cumsum()
 
-    See Also
-    --------
-    corr
+    exprs = parse_as_list_of_expressions(exprs, *more_exprs)
 
-    Examples
-    --------
-    >>> df = pl.DataFrame({"a": [1, 8, 3], "b": [4, 5, 2], "c": ["foo", "bar", "foo"]})
-    >>> df.select(pl.pearson_corr("a", "b"))  # doctest: +SKIP
-    shape: (1, 1)
-    
-     a        
-     ---      
-     f64      
-    
-     0.544705 
-    
-    """
-    warnings.warn(
-        "`pearson_corr()` is deprecated in favor of `corr()`",
-        DeprecationWarning,
-        stacklevel=find_stacklevel(),
+    # (Expr): use u32 as that will not cast to float as eagerly
+    exprs_wrapped = [wrap_expr(e) for e in exprs]
+    return cumfold(lit(0).cast(UInt32), lambda a, b: a + b, exprs_wrapped).alias(
+        "cumsum"
     )
-    if isinstance(a, str):
-        a = col(a)
-    if isinstance(b, str):
-        b = col(b)
-    return wrap_expr(pypearson_corr(a._pyexpr, b._pyexpr, ddof))
 
 
 def corr(
     a: str | Expr,
     b: str | Expr,
     *,
     method: CorrelationMethod = "pearson",
     ddof: int = 1,
     propagate_nans: bool = False,
 ) -> Expr:
     """
-    Compute the pearson's or spearman rank correlation correlation between two columns.
+    Compute the Pearson's or Spearman rank correlation correlation between two columns.
 
     Parameters
     ----------
     a
         Column name or Expression.
     b
         Column name or Expression.
     ddof
-        Delta Degrees of Freedom: the divisor used in the calculation is N - ddof,
+        "Delta Degrees of Freedom": the divisor used in the calculation is N - ddof,
         where N represents the number of elements.
         By default ddof is 1.
     method : {'pearson', 'spearman'}
         Correlation method.
     propagate_nans
         If `True` any `NaN` encountered will lead to `NaN` in the output.
         Defaults to `False` where `NaN` are regarded as larger than any finite number
@@ -1579,18 +1425,18 @@
     """
     if isinstance(a, str):
         a = col(a)
     if isinstance(b, str):
         b = col(b)
 
     if method == "pearson":
-        return wrap_expr(pypearson_corr(a._pyexpr, b._pyexpr, ddof))
+        return wrap_expr(plr.pearson_corr(a._pyexpr, b._pyexpr, ddof))
     elif method == "spearman":
         return wrap_expr(
-            pyspearman_rank_corr(a._pyexpr, b._pyexpr, ddof, propagate_nans)
+            plr.spearman_rank_corr(a._pyexpr, b._pyexpr, ddof, propagate_nans)
         )
     else:
         raise ValueError(
             f"method must be one of {{'pearson', 'spearman'}}, got {method!r}"
         )
 
 
@@ -1619,15 +1465,15 @@
     
 
     """
     if isinstance(a, str):
         a = col(a)
     if isinstance(b, str):
         b = col(b)
-    return wrap_expr(pycov(a._pyexpr, b._pyexpr))
+    return wrap_expr(plr.cov(a._pyexpr, b._pyexpr))
 
 
 def map(
     exprs: Sequence[str] | Sequence[Expr],
     function: Callable[[Sequence[Series]], Series],
     return_dtype: PolarsDataType | None = None,
 ) -> Expr:
@@ -1676,17 +1522,17 @@
     
      1    4    6     
      2    5    8     
      3    6    10    
      4    7    12    
     
     """
-    exprs = selection_to_pyexpr_list(exprs)
+    exprs = parse_as_list_of_expressions(exprs)
     return wrap_expr(
-        _map_mul(
+        plr.map_mul(
             exprs, function, return_dtype, apply_groups=False, returns_scalar=False
         )
     )
 
 
 def apply(
     exprs: Sequence[str | Expr],
@@ -1753,17 +1599,17 @@
     
      7    2    49        
      2    5    4         
      3    6    9         
      4    7    16        
     
     """
-    exprs = selection_to_pyexpr_list(exprs)
+    exprs = parse_as_list_of_expressions(exprs)
     return wrap_expr(
-        _map_mul(
+        plr.map_mul(
             exprs,
             function,
             return_dtype,
             apply_groups=True,
             returns_scalar=returns_scalar,
         )
     )
@@ -1865,20 +1711,20 @@
      ---  --- 
      i64  i64 
     
      3    2   
     
     """
     # in case of pl.col("*")
-    acc = expr_to_lit_or_expr(acc, str_to_lit=True)
-    if isinstance(exprs, pli.Expr):
+    acc = parse_as_expression(acc, str_as_lit=True)._pyexpr
+    if isinstance(exprs, pl.Expr):
         exprs = [exprs]
 
-    exprs = selection_to_pyexpr_list(exprs)
-    return wrap_expr(pyfold(acc._pyexpr, function, exprs))
+    exprs = parse_as_list_of_expressions(exprs)
+    return wrap_expr(plr.fold(acc, function, exprs))
 
 
 def reduce(
     function: Callable[[Series, Series], Series],
     exprs: Sequence[Expr | str] | Expr,
 ) -> Expr:
     """
@@ -1930,19 +1776,19 @@
      1   
      3   
      5   
     
 
     """
     # in case of pl.col("*")
-    if isinstance(exprs, pli.Expr):
+    if isinstance(exprs, pl.Expr):
         exprs = [exprs]
 
-    exprs = selection_to_pyexpr_list(exprs)
-    return wrap_expr(pyreduce(function, exprs))
+    exprs = parse_as_list_of_expressions(exprs)
+    return wrap_expr(plr.reduce(function, exprs))
 
 
 def cumfold(
     acc: IntoExpr,
     function: Callable[[Series, Series], Series],
     exprs: Sequence[Expr | str] | Expr,
     *,
@@ -2006,20 +1852,20 @@
      {2,5,10}  
      {3,7,13}  
      {4,9,16}  
     
 
     """  # noqa: W505
     # in case of pl.col("*")
-    acc = expr_to_lit_or_expr(acc, str_to_lit=True)
-    if isinstance(exprs, pli.Expr):
+    acc = parse_as_expression(acc, str_as_lit=True)._pyexpr
+    if isinstance(exprs, pl.Expr):
         exprs = [exprs]
 
-    exprs = selection_to_pyexpr_list(exprs)
-    return wrap_expr(pycumfold(acc._pyexpr, function, exprs, include_init))
+    exprs = parse_as_list_of_expressions(exprs)
+    return wrap_expr(plr.cumfold(acc, function, exprs, include_init))
 
 
 def cumreduce(
     function: Callable[[Series, Series], Series],
     exprs: Sequence[Expr | str] | Expr,
 ) -> Expr:
     """
@@ -2069,29 +1915,49 @@
     
      {1,4,9}   
      {2,6,12}  
      {3,8,15}  
     
     """  # noqa: W505
     # in case of pl.col("*")
-    if isinstance(exprs, pli.Expr):
+    if isinstance(exprs, pl.Expr):
         exprs = [exprs]
 
-    exprs = selection_to_pyexpr_list(exprs)
-    return wrap_expr(pycumreduce(function, exprs))
+    exprs = parse_as_list_of_expressions(exprs)
+    return wrap_expr(plr.cumreduce(function, exprs))
+
+
+@overload
+def any(exprs: Series) -> bool:  # type: ignore[misc]
+    ...
+
 
+@overload
+def any(exprs: IntoExpr | Iterable[IntoExpr], *more_exprs: IntoExpr) -> Expr:
+    ...
 
-def any(columns: str | Sequence[str] | Sequence[Expr] | Expr) -> Expr:
+
+@deprecated_alias(columns="exprs")
+def any(exprs: IntoExpr | Iterable[IntoExpr], *more_exprs: IntoExpr) -> Expr | bool:
     """
-    Evaluate columnwise or elementwise with a bitwise OR operation.
+    Evaluate a bitwise OR operation.
+
+    If a single string is passed, this is an alias for ``pl.col(name).any()``.
+    If a single Series is passed, this is an alias for ``Series.any()``.
+
+    Otherwise, this function computes the bitwise OR horizontally across multiple
+    columns.
 
     Parameters
     ----------
-    columns
-        If given this function will apply a bitwise or on the columns.
+    exprs
+        Column(s) to use in the aggregation. Accepts expression input. Strings are
+        parsed as column names, other non-expression inputs are parsed as literals.
+    *more_exprs
+        Additional columns to use in the aggregation, specified as positional arguments.
 
     Examples
     --------
     >>> df = pl.DataFrame(
     ...     {
     ...         "a": [True, False, True],
     ...         "b": [False, False, False],
@@ -2119,61 +1985,130 @@
      a     b      c    
      ---   ---    ---  
      bool  bool   bool 
     
      true  false  true 
     
 
-    """
-    if isinstance(columns, str):
-        return col(columns).any()
-    else:
-        return fold(
-            lit(False), lambda a, b: a.cast(bool) | b.cast(bool), columns
-        ).alias("any")
+    Across multiple columns:
 
+    >>> df.select(pl.any("a", "b"))
+    shape: (3, 1)
+    
+     any   
+     ---   
+     bool  
+    
+     true  
+     false 
+     true  
+    
 
-def all(columns: str | Sequence[Expr] | Expr | None = None) -> Expr:
     """
-    Do one of two things.
+    if not more_exprs:
+        if isinstance(exprs, pl.Series):
+            return exprs.any()
+        elif isinstance(exprs, str):
+            return col(exprs).any()
+
+    exprs = parse_as_list_of_expressions(exprs, *more_exprs)
+
+    exprs_wrapped = [wrap_expr(e) for e in exprs]
+    return fold(
+        lit(False), lambda a, b: a.cast(bool) | b.cast(bool), exprs_wrapped
+    ).alias("any")
+
+
+@overload
+def all(exprs: Series) -> bool:  # type: ignore[misc]
+    ...
+
+
+@overload
+def all(
+    exprs: IntoExpr | Iterable[IntoExpr] | None = ..., *more_exprs: IntoExpr
+) -> Expr:
+    ...
 
-    * function can do a columnwise or elementwise AND operation
-    * a wildcard column selection
+
+@deprecated_alias(columns="exprs")
+def all(
+    exprs: IntoExpr | Iterable[IntoExpr] | None = None, *more_exprs: IntoExpr
+) -> Expr | bool:
+    """
+    Either return an expression representing all columns, or evaluate a bitwise AND operation.
+
+    If no arguments are passed, this is an alias for ``pl.col("*")``.
+    If a single string is passed, this is an alias for ``pl.col(name).any()``.
+    If a single Series is passed, this is an alias for ``Series.any()``.
+
+    Otherwise, this function computes the bitwise AND horizontally across multiple
+    columns.
 
     Parameters
     ----------
-    columns
-        If given this function will apply a bitwise & on the columns.
+    exprs
+        Column(s) to use in the aggregation. Accepts expression input. Strings are
+        parsed as column names, other non-expression inputs are parsed as literals.
+    *more_exprs
+        Additional columns to use in the aggregation, specified as positional arguments.
 
     Examples
     --------
-    Sum all columns
+    Selecting all columns and calculating the sum:
 
     >>> df = pl.DataFrame(
     ...     {"a": [1, 2, 3], "b": ["hello", "foo", "bar"], "c": [1, 1, 1]}
     ... )
     >>> df.select(pl.all().sum())
     shape: (1, 3)
     
      a    b     c   
      ---  ---   --- 
      i64  str   i64 
     
      6    null  3   
     
 
-    """
-    if columns is None:
-        return col("*")
-    elif isinstance(columns, str):
-        return col(columns).all()
-    else:
-        return fold(lit(True), lambda a, b: a.cast(bool) & b.cast(bool), columns).alias(
-            "all"
-        )
+    Bitwise AND across multiple columns:
+
+    >>> df = pl.DataFrame(
+    ...     {
+    ...         "a": [True, False, True],
+    ...         "b": [True, False, False],
+    ...         "c": [False, True, False],
+    ...     }
+    ... )
+    >>> df.select(pl.all("a", "b"))
+    shape: (3, 1)
+    
+     all   
+     ---   
+     bool  
+    
+     true  
+     false 
+     false 
+    
+
+    """  # noqa: W505
+    if not more_exprs:
+        if exprs is None:
+            return col("*")
+        elif isinstance(exprs, pl.Series):
+            return exprs.all()
+        elif isinstance(exprs, str):
+            return col(exprs).all()
+
+    exprs = parse_as_list_of_expressions(exprs, *more_exprs)
+
+    exprs_wrapped = [wrap_expr(e) for e in exprs]
+    return fold(
+        lit(True), lambda a, b: a.cast(bool) & b.cast(bool), exprs_wrapped
+    ).alias("all")
 
 
 def exclude(
     columns: str | PolarsDataType | Iterable[str] | Iterable[PolarsDataType],
     *more_columns: str | PolarsDataType,
 ) -> Expr:
     """
@@ -2267,100 +2202,14 @@
     interpolation : {'nearest', 'higher', 'lower', 'midpoint', 'linear'}
         Interpolation method.
 
     """
     return col(column).quantile(quantile, interpolation)
 
 
-@overload
-def arange(
-    start: int | Expr | Series,
-    end: int | Expr | Series,
-    step: int = ...,
-    *,
-    eager: Literal[False],
-) -> Expr:
-    ...
-
-
-@overload
-def arange(
-    start: int | Expr | Series,
-    end: int | Expr | Series,
-    step: int = ...,
-    *,
-    eager: Literal[True],
-    dtype: PolarsDataType | None = ...,
-) -> Series:
-    ...
-
-
-@overload
-def arange(
-    start: int | Expr | Series,
-    end: int | Expr | Series,
-    step: int = ...,
-    *,
-    eager: bool = ...,
-    dtype: PolarsDataType | None = ...,
-) -> Expr | Series:
-    ...
-
-
-@deprecated_alias(low="start", high="end")
-def arange(
-    start: int | Expr | Series,
-    end: int | Expr | Series,
-    step: int = 1,
-    *,
-    eager: bool = False,
-    dtype: PolarsDataType | None = None,
-) -> Expr | Series:
-    """
-    Create a range expression (or Series).
-
-    This can be used in a `select`, `with_column` etc. Be sure that the resulting
-    range size is equal to the length of the DataFrame you are collecting.
-
-    Examples
-    --------
-    >>> df.lazy().filter(pl.col("foo") < pl.arange(0, 100)).collect()  # doctest: +SKIP
-
-    Parameters
-    ----------
-    start
-        Lower bound of range.
-    end
-        Upper bound of range.
-    step
-        Step size of the range.
-    eager
-        Evaluate immediately and return a ``Series``. If set to ``False`` (default),
-        return an expression instead.
-    dtype
-        Apply an explicit integer dtype to the resulting expression (default is Int64).
-
-    """
-    start = expr_to_lit_or_expr(start, str_to_lit=False)
-    end = expr_to_lit_or_expr(end, str_to_lit=False)
-    range_expr = wrap_expr(pyarange(start._pyexpr, end._pyexpr, step))
-
-    if dtype is not None and dtype != Int64:
-        range_expr = range_expr.cast(dtype)
-    if not eager:
-        return range_expr
-    else:
-        return (
-            pli.DataFrame()
-            .select(range_expr)
-            .to_series()
-            .rename("arange", in_place=True)
-        )
-
-
 def arg_sort_by(
     exprs: IntoExpr | Iterable[IntoExpr],
     *more_exprs: IntoExpr,
     descending: bool | Sequence[bool] = False,
 ) -> Expr:
     """
     Return the row indices that would sort the columns.
@@ -2412,356 +2261,23 @@
      2   
      1   
      0   
      3   
     
 
     """
-    exprs = selection_to_pyexpr_list(exprs)
-    if more_exprs:
-        exprs.extend(selection_to_pyexpr_list(more_exprs))
+    exprs = parse_as_list_of_expressions(exprs, *more_exprs)
 
     if isinstance(descending, bool):
         descending = [descending] * len(exprs)
     elif len(exprs) != len(descending):
         raise ValueError(
             f"the length of `descending` ({len(descending)}) does not match the length of `exprs` ({len(exprs)})"
         )
-    return wrap_expr(py_arg_sort_by(exprs, descending))
-
-
-def duration(
-    *,
-    days: Expr | str | int | None = None,
-    seconds: Expr | str | int | None = None,
-    nanoseconds: Expr | str | int | None = None,
-    microseconds: Expr | str | int | None = None,
-    milliseconds: Expr | str | int | None = None,
-    minutes: Expr | str | int | None = None,
-    hours: Expr | str | int | None = None,
-    weeks: Expr | str | int | None = None,
-) -> Expr:
-    """
-    Create polars `Duration` from distinct time components.
-
-    Returns
-    -------
-    Expr of type `pl.Duration`
-
-    Examples
-    --------
-    >>> from datetime import datetime
-    >>> df = pl.DataFrame(
-    ...     {
-    ...         "dt": [datetime(2022, 1, 1), datetime(2022, 1, 2)],
-    ...         "add": [1, 2],
-    ...     }
-    ... )
-    >>> print(df)
-    shape: (2, 2)
-    
-     dt                   add 
-     ---                  --- 
-     datetime[s]         i64 
-    
-     2022-01-01 00:00:00  1   
-     2022-01-02 00:00:00  2   
-    
-    >>> with pl.Config(tbl_width_chars=120):
-    ...     df.select(
-    ...         (pl.col("dt") + pl.duration(weeks="add")).alias("add_weeks"),
-    ...         (pl.col("dt") + pl.duration(days="add")).alias("add_days"),
-    ...         (pl.col("dt") + pl.duration(seconds="add")).alias("add_seconds"),
-    ...         (pl.col("dt") + pl.duration(milliseconds="add")).alias("add_millis"),
-    ...         (pl.col("dt") + pl.duration(hours="add")).alias("add_hours"),
-    ...     )
-    ...
-    shape: (2, 5)
-    
-     add_weeks            add_days             add_seconds          add_millis               add_hours           
-     ---                  ---                  ---                  ---                      ---                 
-     datetime[s]         datetime[s]         datetime[s]         datetime[s]             datetime[s]        
-    
-     2022-01-08 00:00:00  2022-01-02 00:00:00  2022-01-01 00:00:01  2022-01-01 00:00:00.001  2022-01-01 01:00:00 
-     2022-01-16 00:00:00  2022-01-04 00:00:00  2022-01-02 00:00:02  2022-01-02 00:00:00.002  2022-01-02 02:00:00 
-    
-
-    """  # noqa: W505
-    if hours is not None:
-        hours = expr_to_lit_or_expr(hours, str_to_lit=False)._pyexpr
-    if minutes is not None:
-        minutes = expr_to_lit_or_expr(minutes, str_to_lit=False)._pyexpr
-    if seconds is not None:
-        seconds = expr_to_lit_or_expr(seconds, str_to_lit=False)._pyexpr
-    if milliseconds is not None:
-        milliseconds = expr_to_lit_or_expr(milliseconds, str_to_lit=False)._pyexpr
-    if microseconds is not None:
-        microseconds = expr_to_lit_or_expr(microseconds, str_to_lit=False)._pyexpr
-    if nanoseconds is not None:
-        nanoseconds = expr_to_lit_or_expr(nanoseconds, str_to_lit=False)._pyexpr
-    if days is not None:
-        days = expr_to_lit_or_expr(days, str_to_lit=False)._pyexpr
-    if weeks is not None:
-        weeks = expr_to_lit_or_expr(weeks, str_to_lit=False)._pyexpr
-
-    return wrap_expr(
-        py_duration(
-            days,
-            seconds,
-            nanoseconds,
-            microseconds,
-            milliseconds,
-            minutes,
-            hours,
-            weeks,
-        )
-    )
-
-
-def datetime_(
-    year: Expr | str | int,
-    month: Expr | str | int,
-    day: Expr | str | int,
-    hour: Expr | str | int | None = None,
-    minute: Expr | str | int | None = None,
-    second: Expr | str | int | None = None,
-    microsecond: Expr | str | int | None = None,
-) -> Expr:
-    """
-    Create a Polars literal expression of type Datetime.
-
-    Parameters
-    ----------
-    year
-        column or literal.
-    month
-        column or literal, ranging from 1-12.
-    day
-        column or literal, ranging from 1-31.
-    hour
-        column or literal, ranging from 1-23.
-    minute
-        column or literal, ranging from 1-59.
-    second
-        column or literal, ranging from 1-59.
-    microsecond
-        column or literal, ranging from 1-999999.
-
-    Returns
-    -------
-    Expr of type `pl.Datetime`
-
-    """
-    year_expr = expr_to_lit_or_expr(year, str_to_lit=False)
-    month_expr = expr_to_lit_or_expr(month, str_to_lit=False)
-    day_expr = expr_to_lit_or_expr(day, str_to_lit=False)
-
-    if hour is not None:
-        hour = expr_to_lit_or_expr(hour, str_to_lit=False)._pyexpr
-    if minute is not None:
-        minute = expr_to_lit_or_expr(minute, str_to_lit=False)._pyexpr
-    if second is not None:
-        second = expr_to_lit_or_expr(second, str_to_lit=False)._pyexpr
-    if microsecond is not None:
-        microsecond = expr_to_lit_or_expr(microsecond, str_to_lit=False)._pyexpr
-
-    return wrap_expr(
-        py_datetime(
-            year_expr._pyexpr,
-            month_expr._pyexpr,
-            day_expr._pyexpr,
-            hour,
-            minute,
-            second,
-            microsecond,
-        )
-    )
-
-
-def date_(
-    year: Expr | str | int,
-    month: Expr | str | int,
-    day: Expr | str | int,
-) -> Expr:
-    """
-    Create a Polars literal expression of type Date.
-
-    Parameters
-    ----------
-    year
-        column or literal.
-    month
-        column or literal, ranging from 1-12.
-    day
-        column or literal, ranging from 1-31.
-
-    Returns
-    -------
-    Expr of type pl.Date
-
-    """
-    return datetime_(year, month, day).cast(Date).alias("date")
-
-
-def concat_str(
-    exprs: IntoExpr | Iterable[IntoExpr],
-    *more_exprs: IntoExpr,
-    separator: str = "",
-) -> Expr:
-    """
-    Horizontally concatenate columns into a single string column.
-
-    Operates in linear time.
-
-    Parameters
-    ----------
-    exprs
-        Columns to concatenate into a single string column. Accepts expression input.
-        Strings are parsed as column names, other non-expression inputs are parsed as
-        literals. Non-``Utf8`` columns are cast to ``Utf8``.
-    *more_exprs
-        Additional columns to concatenate into a single string column, specified as
-        positional arguments.
-    separator
-        String that will be used to separate the values of each column.
-
-    Examples
-    --------
-    >>> df = pl.DataFrame(
-    ...     {
-    ...         "a": [1, 2, 3],
-    ...         "b": ["dogs", "cats", None],
-    ...         "c": ["play", "swim", "walk"],
-    ...     }
-    ... )
-    >>> df.with_columns(
-    ...     pl.concat_str(
-    ...         [
-    ...             pl.col("a") * 2,
-    ...             pl.col("b"),
-    ...             pl.col("c"),
-    ...         ],
-    ...         separator=" ",
-    ...     ).alias("full_sentence"),
-    ... )
-    shape: (3, 4)
-    
-     a    b     c     full_sentence 
-     ---  ---   ---   ---           
-     i64  str   str   str           
-    
-     1    dogs  play  2 dogs play   
-     2    cats  swim  4 cats swim   
-     3    null  walk  null          
-    
-
-    """
-    exprs = selection_to_pyexpr_list(exprs)
-    if more_exprs:
-        exprs.extend(selection_to_pyexpr_list(more_exprs))
-    return wrap_expr(_concat_str(exprs, separator))
-
-
-def format(f_string: str, *args: Expr | str) -> Expr:
-    """
-    Format expressions as a string.
-
-    Parameters
-    ----------
-    f_string
-        A string that with placeholders.
-        For example: "hello_{}" or "{}_world
-    args
-        Expression(s) that fill the placeholders
-
-    Examples
-    --------
-    >>> df = pl.DataFrame(
-    ...     {
-    ...         "a": ["a", "b", "c"],
-    ...         "b": [1, 2, 3],
-    ...     }
-    ... )
-    >>> df.select(
-    ...     [
-    ...         pl.format("foo_{}_bar_{}", pl.col("a"), "b").alias("fmt"),
-    ...     ]
-    ... )
-    shape: (3, 1)
-    
-     fmt         
-     ---         
-     str         
-    
-     foo_a_bar_1 
-     foo_b_bar_2 
-     foo_c_bar_3 
-    
-
-    """
-    if f_string.count("{}") != len(args):
-        raise ValueError("number of placeholders should equal the number of arguments")
-
-    exprs = []
-
-    arguments = iter(args)
-    for i, s in enumerate(f_string.split("{}")):
-        if i > 0:
-            e = expr_to_lit_or_expr(next(arguments), str_to_lit=False)
-            exprs.append(e)
-
-        if len(s) > 0:
-            exprs.append(lit(s))
-
-    return concat_str(exprs, separator="")
-
-
-def concat_list(exprs: IntoExpr | Iterable[IntoExpr], *more_exprs: IntoExpr) -> Expr:
-    """
-    Horizontally concatenate columns into a single list column.
-
-    Operates in linear time.
-
-    Parameters
-    ----------
-    exprs
-        Columns to concatenate into a single list column. Accepts expression input.
-        Strings are parsed as column names, other non-expression inputs are parsed as
-        literals.
-    *more_exprs
-        Additional columns to concatenate into a single list column, specified as
-        positional arguments.
-
-    Examples
-    --------
-    Create lagged columns and collect them into a list. This mimics a rolling window.
-
-    >>> df = pl.DataFrame({"A": [1.0, 2.0, 9.0, 2.0, 13.0]})
-    >>> df = df.select([pl.col("A").shift(i).alias(f"A_lag_{i}") for i in range(3)])
-    >>> df.select(
-    ...     pl.concat_list([f"A_lag_{i}" for i in range(3)][::-1]).alias("A_rolling")
-    ... )
-    shape: (5, 1)
-    
-     A_rolling         
-     ---               
-     list[f64]         
-    
-     [null, null, 1.0] 
-     [null, 1.0, 2.0]  
-     [1.0, 2.0, 9.0]   
-     [2.0, 9.0, 2.0]   
-     [9.0, 2.0, 13.0]  
-    
-
-    """
-    exprs = selection_to_pyexpr_list(exprs)
-    if more_exprs:
-        exprs.extend(selection_to_pyexpr_list(more_exprs))
-    return wrap_expr(_concat_lst(exprs))
+    return wrap_expr(plr.arg_sort_by(exprs, descending))
 
 
 def collect_all(
     lazy_frames: Sequence[LazyFrame],
     *,
     type_coercion: bool = True,
     predicate_pushdown: bool = True,
@@ -2819,15 +2335,15 @@
             simplify_expression,
             slice_pushdown,
             common_subplan_elimination,
             streaming,
         )
         prepared.append(ldf)
 
-    out = _collect_all(prepared)
+    out = plr.collect_all(prepared)
 
     # wrap the pydataframes into dataframe
     result = [wrap_df(pydf) for pydf in out]
 
     return result
 
 
@@ -2868,211 +2384,15 @@
     
      1   
      2   
      1   
     
 
     """
-    return pli.DataFrame().select(exprs, *more_exprs, **named_exprs)
-
-
-@overload
-def struct(
-    exprs: IntoExpr | Iterable[IntoExpr] = ...,
-    *more_exprs: IntoExpr,
-    eager: Literal[False] = ...,
-    schema: SchemaDict | None = ...,
-    **named_exprs: IntoExpr,
-) -> Expr:
-    ...
-
-
-@overload
-def struct(
-    exprs: IntoExpr | Iterable[IntoExpr] = ...,
-    *more_exprs: IntoExpr,
-    eager: Literal[True],
-    schema: SchemaDict | None = ...,
-    **named_exprs: IntoExpr,
-) -> Series:
-    ...
-
-
-@overload
-def struct(
-    exprs: IntoExpr | Iterable[IntoExpr] = ...,
-    *more_exprs: IntoExpr,
-    eager: bool,
-    schema: SchemaDict | None = ...,
-    **named_exprs: IntoExpr,
-) -> Expr | Series:
-    ...
-
-
-def struct(
-    exprs: IntoExpr | Iterable[IntoExpr] = None,
-    *more_exprs: IntoExpr,
-    eager: bool = False,
-    schema: SchemaDict | None = None,
-    **named_exprs: IntoExpr,
-) -> Expr | Series:
-    """
-    Collect columns into a struct column.
-
-    Parameters
-    ----------
-    exprs
-        Column(s) to collect into a struct column. Accepts expression input. Strings are
-        parsed as column names, other non-expression inputs are parsed as literals.
-    *more_exprs
-        Additional columns to collect into the struct column, specified as positional
-        arguments.
-    eager
-        Evaluate immediately and return a ``Series``. If set to ``False`` (default),
-        return an expression instead.
-    schema
-        Optional schema that explicitly defines the struct field dtypes.
-    **named_exprs
-        Additional columns to collect into the struct column, specified as keyword
-        arguments. The columns will be renamed to the keyword used.
-
-    Examples
-    --------
-    Collect all columns of a dataframe into a struct by passing ``pl.all()``.
-
-    >>> df = pl.DataFrame(
-    ...     {
-    ...         "int": [1, 2],
-    ...         "str": ["a", "b"],
-    ...         "bool": [True, None],
-    ...         "list": [[1, 2], [3]],
-    ...     }
-    ... )
-    >>> df.select(pl.struct(pl.all()).alias("my_struct"))
-    shape: (2, 1)
-    
-     my_struct           
-     ---                 
-     struct[4]           
-    
-     {1,"a",true,[1, 2]} 
-     {2,"b",null,[3]}    
-    
-
-    Collect selected columns into a struct by either passing a list of columns, or by
-    specifying each column as a positional argument.
-
-    >>> df.select(pl.struct("int", False).alias("my_struct"))
-    shape: (2, 1)
-    
-     my_struct 
-     ---       
-     struct[2] 
-    
-     {1,false} 
-     {2,false} 
-    
-
-    Use keyword arguments to easily name each struct field.
-
-    >>> df.select(pl.struct(p="int", q="bool").alias("my_struct")).schema
-    {'my_struct': Struct([Field('p', Int64), Field('q', Boolean)])}
-
-    """
-    exprs = selection_to_pyexpr_list(exprs)
-    if more_exprs:
-        exprs.extend(selection_to_pyexpr_list(more_exprs))
-    if named_exprs:
-        exprs.extend(
-            expr_to_lit_or_expr(expr, name=name, str_to_lit=False)._pyexpr
-            for name, expr in named_exprs.items()
-        )
-
-    expr = wrap_expr(_as_struct(exprs))
-    if schema:
-        expr = expr.cast(Struct(schema), strict=False)
-
-    if eager:
-        return select(expr).to_series()
-    else:
-        return expr
-
-
-@overload
-def repeat(
-    value: float | int | str | bool | None,
-    n: Expr | int,
-    *,
-    eager: Literal[False] = ...,
-    name: str | None = ...,
-) -> Expr:
-    ...
-
-
-@overload
-def repeat(
-    value: float | int | str | bool | None,
-    n: Expr | int,
-    *,
-    eager: Literal[True],
-    name: str | None = ...,
-) -> Series:
-    ...
-
-
-@overload
-def repeat(
-    value: float | int | str | bool | None,
-    n: Expr | int,
-    *,
-    eager: bool,
-    name: str | None,
-) -> Expr | Series:
-    ...
-
-
-def repeat(
-    value: float | int | str | bool | None,
-    n: Expr | int,
-    *,
-    eager: bool = False,
-    name: str | None = None,
-) -> Expr | Series:
-    """
-    Repeat a single value n times.
-
-    Parameters
-    ----------
-    value
-        Value to repeat.
-    n
-        repeat `n` times
-    eager
-        Evaluate immediately and return a ``Series``. If set to ``False`` (default),
-        return an expression instead.
-    name
-        Only used in `eager` mode. As expression, use `alias`
-
-    """
-    if eager:
-        if name is None:
-            name = ""
-        dtype = py_type_to_dtype(type(value))
-        if (
-            dtype == Int64
-            and isinstance(value, int)
-            and -(2**31) <= value <= 2**31 - 1
-        ):
-            dtype = Int32
-        s = pli.Series._repeat(name, value, n, dtype)  # type: ignore[arg-type]
-        return s
-    else:
-        if isinstance(n, int):
-            n = lit(n)
-        return wrap_expr(_repeat(value, n._pyexpr))
+    return pl.DataFrame().select(exprs, *more_exprs, **named_exprs)
 
 
 @overload
 def arg_where(condition: Expr | Series, *, eager: Literal[False] = ...) -> Expr:
     ...
 
 
@@ -3115,23 +2435,23 @@
 
     See Also
     --------
     Series.arg_true : Return indices where Series is True
 
     """
     if eager:
-        if not isinstance(condition, pli.Series):
+        if not isinstance(condition, pl.Series):
             raise ValueError(
                 "expected 'Series' in 'arg_where' if 'eager=True', got"
                 f" {type(condition)}"
             )
         return condition.to_frame().select(arg_where(col(condition.name))).to_series()
     else:
-        condition = expr_to_lit_or_expr(condition, str_to_lit=True)
-        return wrap_expr(py_arg_where(condition._pyexpr))
+        condition = parse_as_expression(condition)._pyexpr
+        return wrap_expr(plr.arg_where(condition))
 
 
 def coalesce(exprs: IntoExpr | Iterable[IntoExpr], *more_exprs: IntoExpr) -> Expr:
     """
     Folds the columns from left to right, keeping the first non-null value.
 
     Parameters
@@ -3173,18 +2493,16 @@
      1     1     5     1.0  
      null  2     null  2.0  
      null  null  3     3.0  
      null  null  null  10.0 
     
 
     """
-    exprs = selection_to_pyexpr_list(exprs)
-    if more_exprs:
-        exprs.extend(selection_to_pyexpr_list(more_exprs))
-    return wrap_expr(_coalesce_exprs(exprs))
+    exprs = parse_as_list_of_expressions(exprs, *more_exprs)
+    return wrap_expr(plr.coalesce(exprs))
 
 
 @overload
 def from_epoch(column: str | Expr, time_unit: EpochTimeUnit = ...) -> Expr:
     ...
 
 
@@ -3239,20 +2557,96 @@
             2003-10-20
             2003-10-21
     ]
 
     """
     if isinstance(column, str):
         column = col(column)
-    elif not isinstance(column, (pli.Series, pli.Expr)):
-        column = pli.Series(column)  # Sequence input handled by Series constructor
+    elif not isinstance(column, (pl.Series, pl.Expr)):
+        column = pl.Series(column)  # Sequence input handled by Series constructor
 
     if time_unit == "d":
         return column.cast(Date)
     elif time_unit == "s":
         return (column.cast(Int64) * 1_000_000).cast(Datetime("us"))
     elif time_unit in DTYPE_TEMPORAL_UNITS:
         return column.cast(Datetime(time_unit))
     else:
         raise ValueError(
             f"'time_unit' must be one of {{'ns', 'us', 'ms', 's', 'd'}}, got {time_unit!r}."
         )
+
+
+def rolling_cov(
+    a: str | Expr,
+    b: str | Expr,
+    *,
+    window_size: int,
+    min_periods: int | None = None,
+    ddof: int = 1,
+) -> Expr:
+    """
+    Compute the rolling covariance between two columns/ expressions.
+
+    Parameters
+    ----------
+    a
+        Column name or Expression.
+    b
+        Column name or Expression.
+    window_size
+        The length of the window.
+    min_periods
+        The number of values in the window that should be non-null before computing
+        a result. If None, it will be set equal to window size.
+    ddof
+        Delta degrees of freedom.  The divisor used in calculations
+        is ``N - ddof``, where ``N`` represents the number of elements.
+
+    """
+    if min_periods is None:
+        min_periods = window_size
+    if isinstance(a, str):
+        a = col(a)
+    if isinstance(b, str):
+        b = col(b)
+    return wrap_expr(
+        plr.rolling_cov(a._pyexpr, b._pyexpr, window_size, min_periods, ddof)
+    )
+
+
+def rolling_corr(
+    a: str | Expr,
+    b: str | Expr,
+    *,
+    window_size: int,
+    min_periods: int | None = None,
+    ddof: int = 1,
+) -> Expr:
+    """
+    Compute the rolling correlation between two columns/ expressions.
+
+    Parameters
+    ----------
+    a
+        Column name or Expression.
+    b
+        Column name or Expression.
+    window_size
+        The length of the window.
+    min_periods
+        The number of values in the window that should be non-null before computing
+        a result. If None, it will be set equal to window size.
+    ddof
+        Delta degrees of freedom.  The divisor used in calculations
+        is ``N - ddof``, where ``N`` represents the number of elements.
+
+    """
+    if min_periods is None:
+        min_periods = window_size
+    if isinstance(a, str):
+        a = col(a)
+    if isinstance(b, str):
+        b = col(b)
+    return wrap_expr(
+        plr.rolling_corr(a._pyexpr, b._pyexpr, window_size, min_periods, ddof)
+    )
```

### Comparing `polars_lts_cpu-0.17.9/polars/functions/whenthen.py` & `polars_lts_cpu-0.18.0/polars/functions/whenthen.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,25 +1,25 @@
 from __future__ import annotations
 
 import contextlib
 import typing
-from typing import TYPE_CHECKING, Any, Iterable
+from typing import TYPE_CHECKING, Any
 
-from polars.utils._parse_expr_input import expr_to_lit_or_expr
+from polars.utils._parse_expr_input import parse_as_expression
 from polars.utils._wrap import wrap_expr
 
 with contextlib.suppress(ImportError):  # Module not available when building docs
-    from polars.polars import when as pywhen
+    from polars.polars import when as _when
 
 if TYPE_CHECKING:
-    from polars.expr.expr import Expr
+    from polars import Expr
     from polars.type_aliases import IntoExpr
 
 
-def when(expr: IntoExpr | Iterable[IntoExpr]) -> When:
+def when(expr: IntoExpr) -> When:
     """
     Start a "when, then, otherwise" expression.
 
     Expression similar to an `if-else` statement in Python. Always initiated by a
     `pl.when(<condition>).then(<value if condition>)`. Optionally followed by chaining
     one or more `.when(<condition>).then(<value>)` statements. If none of the conditions
     are `True`, an optional `.otherwise(<value if all statements are false>)` can be
@@ -89,100 +89,100 @@
      1    3    null 
      3    4    1    
      4    0    1    
     
 
 
     """
-    expr = expr_to_lit_or_expr(expr)
-    pw = pywhen(expr._pyexpr)
-    return When(pw)
+    expr = parse_as_expression(expr)._pyexpr
+    pywhen = _when(expr)
+    return When(pywhen)
 
 
 class When:
     """Utility class. See the `when` function."""
 
-    def __init__(self, pywhen: pywhen):
+    def __init__(self, pywhen: Any):
         self._pywhen = pywhen
 
-    def then(self, expr: IntoExpr | Iterable[IntoExpr]) -> WhenThen:
+    def then(self, expr: IntoExpr) -> WhenThen:
         """
         Values to return in case of the predicate being `True`.
 
         See Also
         --------
         pl.when : Documentation for `when, then, otherwise`
 
         """
-        expr = expr_to_lit_or_expr(expr)
-        pywhenthen = self._pywhen.then(expr._pyexpr)
+        expr = parse_as_expression(expr, str_as_lit=True)._pyexpr
+        pywhenthen = self._pywhen.then(expr)
         return WhenThen(pywhenthen)
 
 
 class WhenThen:
     """Utility class. See the `when` function."""
 
     def __init__(self, pywhenthen: Any):
         self._pywhenthen = pywhenthen
 
-    def when(self, predicate: IntoExpr | Iterable[IntoExpr]) -> WhenThenThen:
+    def when(self, predicate: IntoExpr) -> WhenThenThen:
         """Start another "when, then, otherwise" layer."""
-        predicate = expr_to_lit_or_expr(predicate)
-        return WhenThenThen(self._pywhenthen.when(predicate._pyexpr))
+        predicate = parse_as_expression(predicate)._pyexpr
+        return WhenThenThen(self._pywhenthen.when(predicate))
 
-    def otherwise(self, expr: IntoExpr | Iterable[IntoExpr]) -> Expr:
+    def otherwise(self, expr: IntoExpr) -> Expr:
         """
         Values to return in case of the predicate being `False`.
 
         See Also
         --------
         pl.when : Documentation for `when, then, otherwise`
 
         """
-        expr = expr_to_lit_or_expr(expr)
-        return wrap_expr(self._pywhenthen.otherwise(expr._pyexpr))
+        expr = parse_as_expression(expr, str_as_lit=True)._pyexpr
+        return wrap_expr(self._pywhenthen.otherwise(expr))
 
     @typing.no_type_check
     def __getattr__(self, item) -> Expr:
         expr = self.otherwise(None)  # noqa: F841
         return eval(f"expr.{item}")
 
 
 class WhenThenThen:
     """Utility class. See the `when` function."""
 
     def __init__(self, pywhenthenthen: Any):
         self.pywhenthenthen = pywhenthenthen
 
-    def when(self, predicate: IntoExpr | Iterable[IntoExpr]) -> WhenThenThen:
+    def when(self, predicate: IntoExpr) -> WhenThenThen:
         """Start another "when, then, otherwise" layer."""
-        predicate = expr_to_lit_or_expr(predicate)
-        return WhenThenThen(self.pywhenthenthen.when(predicate._pyexpr))
+        predicate = parse_as_expression(predicate)._pyexpr
+        return WhenThenThen(self.pywhenthenthen.when(predicate))
 
-    def then(self, expr: IntoExpr | Iterable[IntoExpr]) -> WhenThenThen:
+    def then(self, expr: IntoExpr) -> WhenThenThen:
         """
         Values to return in case of the predicate being `True`.
 
         See Also
         --------
         pl.when : Documentation for `when, then, otherwise`
 
         """
-        expr_ = expr_to_lit_or_expr(expr)
-        return WhenThenThen(self.pywhenthenthen.then(expr_._pyexpr))
+        expr = parse_as_expression(expr, str_as_lit=True)._pyexpr
+        return WhenThenThen(self.pywhenthenthen.then(expr))
 
-    def otherwise(self, expr: IntoExpr | Iterable[IntoExpr]) -> Expr:
+    def otherwise(self, expr: IntoExpr) -> Expr:
         """
         Values to return in case of the predicate being `False`.
 
         See Also
         --------
         pl.when : Documentation for `when, then, otherwise`
 
         """
-        expr = expr_to_lit_or_expr(expr)
-        return wrap_expr(self.pywhenthenthen.otherwise(expr._pyexpr))
+        expr = parse_as_expression(expr, str_as_lit=True)._pyexpr
+        return wrap_expr(self.pywhenthenthen.otherwise(expr))
 
     @typing.no_type_check
     def __getattr__(self, item) -> Expr:
         expr = self.otherwise(None)  # noqa: F841
         return eval(f"expr.{item}")
```

### Comparing `polars_lts_cpu-0.17.9/polars/io/__init__.py` & `polars_lts_cpu-0.18.0/polars/io/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 """Functions for reading data."""
 
 from polars.io.avro import read_avro
 from polars.io.csv import read_csv, read_csv_batched, scan_csv
-from polars.io.database import read_database, read_sql
+from polars.io.database import read_database
 from polars.io.delta import read_delta, scan_delta
 from polars.io.excel import read_excel
 from polars.io.ipc import read_ipc, read_ipc_schema, scan_ipc
 from polars.io.json import read_json
 from polars.io.ndjson import read_ndjson, scan_ndjson
 from polars.io.parquet import read_parquet, read_parquet_schema, scan_parquet
 from polars.io.pyarrow_dataset import scan_ds, scan_pyarrow_dataset
@@ -20,15 +20,14 @@
     "read_excel",
     "read_ipc",
     "read_ipc_schema",
     "read_json",
     "read_ndjson",
     "read_parquet",
     "read_parquet_schema",
-    "read_sql",
     "scan_csv",
     "scan_delta",
     "scan_ds",
     "scan_ipc",
     "scan_ndjson",
     "scan_parquet",
     "scan_pyarrow_dataset",
```

### Comparing `polars_lts_cpu-0.17.9/polars/io/_utils.py` & `polars_lts_cpu-0.18.0/polars/io/_utils.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/polars/io/avro.py` & `polars_lts_cpu-0.18.0/polars/io/avro.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, BinaryIO
 
-from polars import internals as pli
+import polars._reexport as pl
 
 if TYPE_CHECKING:
     from io import BytesIO
     from pathlib import Path
 
-    from polars.dataframe import DataFrame
+    from polars import DataFrame
 
 
 def read_avro(
     source: str | Path | BytesIO | BinaryIO,
     *,
     columns: list[int] | list[str] | None = None,
     n_rows: int | None = None,
@@ -31,8 +31,8 @@
         Stop reading from Apache Avro file after reading ``n_rows``.
 
     Returns
     -------
     DataFrame
 
     """
-    return pli.DataFrame._read_avro(source, n_rows=n_rows, columns=columns)
+    return pl.DataFrame._read_avro(source, n_rows=n_rows, columns=columns)
```

### Comparing `polars_lts_cpu-0.17.9/polars/io/csv/_utils.py` & `polars_lts_cpu-0.18.0/polars/io/csv/_utils.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Sequence
 
 if TYPE_CHECKING:
-    from polars.dataframe import DataFrame
+    from polars import DataFrame
 
 
 def _check_arg_is_1byte(
     arg_name: str, arg: str | None, can_be_empty: bool = False
 ) -> None:
     if isinstance(arg, str):
         arg_byte_length = len(arg.encode("utf-8"))
```

### Comparing `polars_lts_cpu-0.17.9/polars/io/csv/batched_reader.py` & `polars_lts_cpu-0.18.0/polars/io/csv/batched_reader.py`

 * *Files 2% similar despite different names*

```diff
@@ -17,15 +17,15 @@
     normalise_filepath,
 )
 
 with contextlib.suppress(ImportError):  # Module not available when building docs
     from polars.polars import PyBatchedCsv
 
 if TYPE_CHECKING:
-    from polars.dataframe import DataFrame
+    from polars import DataFrame
     from polars.type_aliases import CsvEncoding, PolarsDataType, SchemaDict
 
 
 class BatchedCsvReader:
     def __init__(
         self,
         source: str | Path,
```

### Comparing `polars_lts_cpu-0.17.9/polars/io/csv/functions.py` & `polars_lts_cpu-0.18.0/polars/io/csv/functions.py`

 * *Files 1% similar despite different names*

```diff
@@ -7,26 +7,25 @@
     BinaryIO,
     Callable,
     Mapping,
     Sequence,
     TextIO,
 )
 
-from polars import internals as pli
+import polars._reexport as pl
 from polars.datatypes import N_INFER_DEFAULT, Utf8
 from polars.io._utils import _prepare_file_arg
 from polars.io.csv._utils import _check_arg_is_1byte, _update_columns
 from polars.io.csv.batched_reader import BatchedCsvReader
 from polars.utils.various import handle_projection_columns, normalise_filepath
 
 if TYPE_CHECKING:
     from io import BytesIO
 
-    from polars.dataframe import DataFrame
-    from polars.lazyframe import LazyFrame
+    from polars import DataFrame, LazyFrame
     from polars.type_aliases import CsvEncoding, PolarsDataType, SchemaDict
 
 
 def read_csv(
     source: str | TextIO | BytesIO | Path | BinaryIO | bytes,
     *,
     has_header: bool = True,
@@ -252,15 +251,15 @@
         if not has_header:
             # Rename 'f0', 'f1', ... columns names autogenerated by pyarrow
             # to 'column_1', 'column_2', ...
             tbl = tbl.rename_columns(
                 [f"column_{int(column[1:]) + 1}" for column in tbl.column_names]
             )
 
-        df = pli.DataFrame._from_arrow(tbl, rechunk=rechunk)
+        df = pl.DataFrame._from_arrow(tbl, rechunk=rechunk)
         if new_columns:
             return _update_columns(df, new_columns)
         return df
 
     if projection and dtypes and isinstance(dtypes, list):
         if len(projection) < len(dtypes):
             raise ValueError(
@@ -348,15 +347,15 @@
                 new_to_current.get(column_name, column_name): column_dtype
                 for column_name, column_dtype in dtypes.items()
             }
 
     with _prepare_file_arg(
         source, encoding=encoding, use_pyarrow=False, **storage_options
     ) as data:
-        df = pli.DataFrame._read_csv(
+        df = pl.DataFrame._read_csv(
             data,
             has_header=has_header,
             columns=columns if columns else projection,
             separator=separator,
             comment_char=comment_char,
             quote_char=quote_char,
             skip_rows=skip_rows,
@@ -869,15 +868,15 @@
     _check_arg_is_1byte("separator", separator, False)
     _check_arg_is_1byte("comment_char", comment_char, False)
     _check_arg_is_1byte("quote_char", quote_char, True)
 
     if isinstance(source, (str, Path)):
         source = normalise_filepath(source)
 
-    return pli.LazyFrame._scan_csv(
+    return pl.LazyFrame._scan_csv(
         source,
         has_header=has_header,
         separator=separator,
         comment_char=comment_char,
         quote_char=quote_char,
         skip_rows=skip_rows,
         dtypes=dtypes,  # type: ignore[arg-type]
```

### Comparing `polars_lts_cpu-0.17.9/polars/io/delta.py` & `polars_lts_cpu-0.18.0/polars/io/delta.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,16 +5,15 @@
 from urllib.parse import urlparse
 
 from polars.convert import from_arrow
 from polars.dependencies import _DELTALAKE_AVAILABLE, deltalake
 from polars.io.pyarrow_dataset import scan_pyarrow_dataset
 
 if TYPE_CHECKING:
-    from polars.dataframe import DataFrame
-    from polars.lazyframe import LazyFrame
+    from polars import DataFrame, LazyFrame
 
 
 def read_delta(
     source: str,
     *,
     version: int | None = None,
     columns: list[str] | None = None,
@@ -99,15 +98,15 @@
 
     Following type of table paths are supported,
 
     * az://<container>/<path>
     * adl://<container>/<path>
     * abfs://<container>/<path>
 
-    Supported options for Azure are available `here
+    See a list of supported storage options for Azure `here
     <https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html#variants>`__.
 
     >>> table_path = "az://container/path/to/delta-table/"
     >>> storage_options = {
     ...     "AZURE_STORAGE_ACCOUNT_NAME": "AZURE_STORAGE_ACCOUNT_NAME",
     ...     "AZURE_STORAGE_ACCOUNT_KEY": "AZURE_STORAGE_ACCOUNT_KEY",
     ... }
@@ -123,15 +122,15 @@
     ...     table_path, delta_table_options=delta_table_options
     ... )  # doctest: +SKIP
 
     """
     if pyarrow_options is None:
         pyarrow_options = {}
 
-    resolved_uri = _resolve_delta_lake_uri(source)
+    resolved_uri = resolve_delta_lake_uri(source)
 
     dl_tbl = _get_delta_lake_table(
         table_path=resolved_uri,
         version=version,
         storage_options=storage_options,
         delta_table_options=delta_table_options,
     )
@@ -251,31 +250,31 @@
     ...     table_path, delta_table_options=delta_table_options
     ... ).collect()  # doctest: +SKIP
 
     """
     if pyarrow_options is None:
         pyarrow_options = {}
 
-    resolved_uri = _resolve_delta_lake_uri(source)
+    resolved_uri = resolve_delta_lake_uri(source)
     dl_tbl = _get_delta_lake_table(
         table_path=resolved_uri,
         version=version,
         storage_options=storage_options,
         delta_table_options=delta_table_options,
     )
 
     pa_ds = dl_tbl.to_pyarrow_dataset(**pyarrow_options)
     return scan_pyarrow_dataset(pa_ds)
 
 
-def _resolve_delta_lake_uri(table_uri: str) -> str:
+def resolve_delta_lake_uri(table_uri: str, strict: bool = True) -> str:
     parsed_result = urlparse(table_uri)
 
     resolved_uri = str(
-        Path(table_uri).expanduser().resolve(True)
+        Path(table_uri).expanduser().resolve(strict)
         if parsed_result.scheme == ""
         else table_uri
     )
 
     return resolved_uri
 
 
@@ -294,23 +293,27 @@
     `here <https://delta-io.github.io/delta-rs/python/installation.html>`_.
 
     Returns
     -------
     DeltaTable
 
     """
-    if not _DELTALAKE_AVAILABLE:
-        raise ImportError(
-            "deltalake is not installed. Please run `pip install deltalake>=0.8.0`."
-        )
+    check_if_delta_available()
 
     if delta_table_options is None:
         delta_table_options = {}
 
     dl_tbl = deltalake.DeltaTable(
         table_path,
         version=version,
         storage_options=storage_options,
         **delta_table_options,
     )
 
     return dl_tbl
+
+
+def check_if_delta_available() -> None:
+    if not _DELTALAKE_AVAILABLE:
+        raise ImportError(
+            "deltalake is not installed. Please run `pip install deltalake>=0.9.0`."
+        )
```

### Comparing `polars_lts_cpu-0.17.9/polars/io/excel/_write_utils.py` & `polars_lts_cpu-0.18.0/polars/io/excel/_write_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -23,15 +23,15 @@
 if TYPE_CHECKING:
     import sys
 
     from xlsxwriter import Workbook
     from xlsxwriter.format import Format
     from xlsxwriter.worksheet import Worksheet
 
-    from polars.internals import DataFrame, Series
+    from polars import DataFrame, Series
     from polars.type_aliases import (
         ColumnTotalsDefinition,
         ConditionalFormatDict,
         OneOrMoreDataTypes,
         PolarsDataType,
         RowTotalsDefinition,
     )
```

### Comparing `polars_lts_cpu-0.17.9/polars/io/excel/functions.py` & `polars_lts_cpu-0.18.0/polars/io/excel/functions.py`

 * *Files 0% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 from polars.io.csv.functions import read_csv
 from polars.utils.various import normalise_filepath
 
 if TYPE_CHECKING:
     import sys
     from io import BytesIO
 
-    from polars.dataframe import DataFrame
+    from polars import DataFrame
 
     if sys.version_info >= (3, 8):
         from typing import Literal
     else:
         from typing_extensions import Literal
```

### Comparing `polars_lts_cpu-0.17.9/polars/io/ipc/anonymous_scan.py` & `polars_lts_cpu-0.18.0/polars/io/parquet/anonymous_scan.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,37 +1,36 @@
 from __future__ import annotations
 
 from functools import partial
 from typing import TYPE_CHECKING, Any
 
-import polars.io.ipc
-from polars import internals as pli
+import polars._reexport as pl
+import polars.io.parquet
 from polars.dependencies import pickle
 from polars.io._utils import _prepare_file_arg
 
 if TYPE_CHECKING:
-    from polars.dataframe import DataFrame
-    from polars.lazyframe import LazyFrame
+    from polars import DataFrame, LazyFrame
 
 
-def _scan_ipc_fsspec(
+def _scan_parquet_fsspec(
     source: str,
     storage_options: dict[str, object] | None = None,
 ) -> LazyFrame:
-    func = partial(_scan_ipc_impl, source, storage_options=storage_options)
+    func = partial(_scan_parquet_impl, source, storage_options=storage_options)
     func_serialized = pickle.dumps(func)
 
     storage_options = storage_options or {}
     with _prepare_file_arg(source, **storage_options) as data:
-        schema = polars.io.ipc.read_ipc_schema(data)
+        schema = polars.io.parquet.read_parquet_schema(data)
 
-    return pli.LazyFrame._scan_python_function(schema, func_serialized)
+    return pl.LazyFrame._scan_python_function(schema, func_serialized)
 
 
-def _scan_ipc_impl(  # noqa: D417
+def _scan_parquet_impl(  # noqa: D417
     source: str,
     columns: list[str] | None,
     predicate: str | None,
     n_rows: int | None,
     **kwargs: Any,
 ) -> DataFrame:
     """
@@ -41,10 +40,10 @@
     ----------
     source
         Source URI
     columns
         Columns that are projected
 
     """
-    import polars as pl
+    from polars import read_parquet
 
-    return pl.read_ipc(source, columns=columns, n_rows=n_rows, **kwargs)
+    return read_parquet(source, columns=columns, n_rows=n_rows, **kwargs)
```

### Comparing `polars_lts_cpu-0.17.9/polars/io/ipc/functions.py` & `polars_lts_cpu-0.18.0/polars/io/ipc/functions.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,26 +1,25 @@
 from __future__ import annotations
 
 import contextlib
 from pathlib import Path
 from typing import TYPE_CHECKING, Any, BinaryIO
 
-from polars import internals as pli
+import polars._reexport as pl
 from polars.dependencies import _PYARROW_AVAILABLE
 from polars.io._utils import _prepare_file_arg
 from polars.utils.various import normalise_filepath
 
 with contextlib.suppress(ImportError):
-    from polars.polars import ipc_schema as _ipc_schema
+    from polars.polars import read_ipc_schema as _read_ipc_schema
 
 if TYPE_CHECKING:
     from io import BytesIO
 
-    from polars.dataframe import DataFrame
-    from polars.lazyframe import LazyFrame
+    from polars import DataFrame, LazyFrame
     from polars.type_aliases import PolarsDataType
 
 
 def read_ipc(
     source: str | BinaryIO | BytesIO | Path | bytes,
     *,
     columns: list[int] | list[str] | None = None,
@@ -89,22 +88,22 @@
                     " 'read_ipc(..., use_pyarrow=True)'."
                 )
 
             import pyarrow as pa
             import pyarrow.feather
 
             tbl = pa.feather.read_table(data, memory_map=memory_map, columns=columns)
-            df = pli.DataFrame._from_arrow(tbl, rechunk=rechunk)
+            df = pl.DataFrame._from_arrow(tbl, rechunk=rechunk)
             if row_count_name is not None:
                 df = df.with_row_count(row_count_name, row_count_offset)
             if n_rows is not None:
                 df = df.slice(0, n_rows)
             return df
 
-        return pli.DataFrame._read_ipc(
+        return pl.DataFrame._read_ipc(
             data,
             columns=columns,
             n_rows=n_rows,
             row_count_name=row_count_name,
             row_count_offset=row_count_offset,
             rechunk=rechunk,
             memory_map=memory_map,
@@ -124,15 +123,15 @@
     -------
     Dictionary mapping column names to datatypes
 
     """
     if isinstance(source, (str, Path)):
         source = normalise_filepath(source)
 
-    return _ipc_schema(source)
+    return _read_ipc_schema(source)
 
 
 def scan_ipc(
     source: str | Path,
     *,
     n_rows: int | None = None,
     cache: bool = True,
@@ -169,15 +168,15 @@
         e.g. host, port, username, password, etc.
     memory_map
         Try to memory map the file. This can greatly improve performance on repeated
         queries as the OS may cache pages.
         Only uncompressed IPC files can be memory mapped.
 
     """
-    return pli.LazyFrame._scan_ipc(
+    return pl.LazyFrame._scan_ipc(
         source,
         n_rows=n_rows,
         cache=cache,
         rechunk=rechunk,
         row_count_name=row_count_name,
         row_count_offset=row_count_offset,
         storage_options=storage_options,
```

### Comparing `polars_lts_cpu-0.17.9/polars/io/ndjson.py` & `polars_lts_cpu-0.18.0/polars/io/ndjson.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,34 +1,33 @@
 from __future__ import annotations
 
 from pathlib import Path
 from typing import TYPE_CHECKING
 
-from polars import internals as pli
+import polars._reexport as pl
 from polars.datatypes import N_INFER_DEFAULT
 from polars.utils.various import normalise_filepath
 
 if TYPE_CHECKING:
     from io import IOBase
 
-    from polars.dataframe import DataFrame
-    from polars.lazyframe import LazyFrame
+    from polars import DataFrame, LazyFrame
 
 
 def read_ndjson(source: str | Path | IOBase) -> DataFrame:
     """
     Read into a DataFrame from a newline delimited JSON file.
 
     Parameters
     ----------
     source
         Path to a file or a file-like object.
 
     """
-    return pli.DataFrame._read_ndjson(source)
+    return pl.DataFrame._read_ndjson(source)
 
 
 def scan_ndjson(
     source: str | Path,
     *,
     infer_schema_length: int | None = N_INFER_DEFAULT,
     batch_size: int | None = 1024,
@@ -64,15 +63,15 @@
     row_count_offset
         Offset to start the row_count column (only use if the name is set)
 
     """
     if isinstance(source, (str, Path)):
         source = normalise_filepath(source)
 
-    return pli.LazyFrame._scan_ndjson(
+    return pl.LazyFrame._scan_ndjson(
         source,
         infer_schema_length=infer_schema_length,
         batch_size=batch_size,
         n_rows=n_rows,
         low_memory=low_memory,
         rechunk=rechunk,
         row_count_name=row_count_name,
```

### Comparing `polars_lts_cpu-0.17.9/polars/io/parquet/anonymous_scan.py` & `polars_lts_cpu-0.18.0/polars/io/ipc/anonymous_scan.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,37 +1,36 @@
 from __future__ import annotations
 
 from functools import partial
 from typing import TYPE_CHECKING, Any
 
-import polars.io.parquet
-from polars import internals as pli
+import polars._reexport as pl
+import polars.io.ipc
 from polars.dependencies import pickle
 from polars.io._utils import _prepare_file_arg
 
 if TYPE_CHECKING:
-    from polars.dataframe import DataFrame
-    from polars.lazyframe import LazyFrame
+    from polars import DataFrame, LazyFrame
 
 
-def _scan_parquet_fsspec(
+def _scan_ipc_fsspec(
     source: str,
     storage_options: dict[str, object] | None = None,
 ) -> LazyFrame:
-    func = partial(_scan_parquet_impl, source, storage_options=storage_options)
+    func = partial(_scan_ipc_impl, source, storage_options=storage_options)
     func_serialized = pickle.dumps(func)
 
     storage_options = storage_options or {}
     with _prepare_file_arg(source, **storage_options) as data:
-        schema = polars.io.parquet.read_parquet_schema(data)
+        schema = polars.io.ipc.read_ipc_schema(data)
 
-    return pli.LazyFrame._scan_python_function(schema, func_serialized)
+    return pl.LazyFrame._scan_python_function(schema, func_serialized)
 
 
-def _scan_parquet_impl(  # noqa: D417
+def _scan_ipc_impl(  # noqa: D417
     source: str,
     columns: list[str] | None,
     predicate: str | None,
     n_rows: int | None,
     **kwargs: Any,
 ) -> DataFrame:
     """
@@ -41,10 +40,10 @@
     ----------
     source
         Source URI
     columns
         Columns that are projected
 
     """
-    import polars as pl
+    from polars import read_ipc
 
-    return pl.read_parquet(source, columns=columns, n_rows=n_rows, **kwargs)
+    return read_ipc(source, columns=columns, n_rows=n_rows, **kwargs)
```

### Comparing `polars_lts_cpu-0.17.9/polars/io/parquet/functions.py` & `polars_lts_cpu-0.18.0/polars/io/parquet/functions.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,27 +1,26 @@
 from __future__ import annotations
 
 import contextlib
 from pathlib import Path
 from typing import TYPE_CHECKING, Any, BinaryIO
 
-from polars import internals as pli
+import polars._reexport as pl
 from polars.convert import from_arrow
 from polars.dependencies import _PYARROW_AVAILABLE
 from polars.io._utils import _prepare_file_arg
 from polars.utils.various import normalise_filepath
 
 with contextlib.suppress(ImportError):
-    from polars.polars import parquet_schema as _parquet_schema
+    from polars.polars import read_parquet_schema as _read_parquet_schema
 
 if TYPE_CHECKING:
     from io import BytesIO
 
-    from polars.dataframe import DataFrame
-    from polars.lazyframe import LazyFrame
+    from polars import DataFrame, LazyFrame
     from polars.type_aliases import ParallelStrategy, PolarsDataType
 
 
 def read_parquet(
     source: str | Path | BinaryIO | BytesIO | bytes,
     *,
     columns: list[int] | list[str] | None = None,
@@ -117,15 +116,15 @@
                     source_prep,
                     memory_map=memory_map,
                     columns=columns,
                     **pyarrow_options,
                 )
             )
 
-        return pli.DataFrame._read_parquet(
+        return pl.DataFrame._read_parquet(
             source_prep,
             columns=columns,
             n_rows=n_rows,
             parallel=parallel,
             row_count_name=row_count_name,
             row_count_offset=row_count_offset,
             low_memory=low_memory,
@@ -149,15 +148,15 @@
     -------
     Dictionary mapping column names to datatypes
 
     """
     if isinstance(source, (str, Path)):
         source = normalise_filepath(source)
 
-    return _parquet_schema(source)
+    return _read_parquet_schema(source)
 
 
 def scan_parquet(
     source: str | Path,
     *,
     n_rows: int | None = None,
     cache: bool = True,
@@ -204,15 +203,15 @@
         Use statistics in the parquet to determine if pages
         can be skipped from reading.
 
     """
     if isinstance(source, (str, Path)):
         source = normalise_filepath(source)
 
-    return pli.LazyFrame._scan_parquet(
+    return pl.LazyFrame._scan_parquet(
         source,
         n_rows=n_rows,
         cache=cache,
         parallel=parallel,
         rechunk=rechunk,
         row_count_name=row_count_name,
         row_count_offset=row_count_offset,
```

### Comparing `polars_lts_cpu-0.17.9/polars/io/pyarrow_dataset/anonymous_scan.py` & `polars_lts_cpu-0.18.0/polars/io/pyarrow_dataset/anonymous_scan.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,20 +1,18 @@
 from __future__ import annotations
 
 from functools import partial
 from typing import TYPE_CHECKING
 
-import polars as pl
-from polars import internals as pli
+import polars._reexport as pl
 from polars.dependencies import pickle
 from polars.dependencies import pyarrow as pa  # noqa: TCH001
 
 if TYPE_CHECKING:
-    from polars.dataframe import DataFrame
-    from polars.lazyframe import LazyFrame
+    from polars import DataFrame, LazyFrame
 
 
 def _scan_pyarrow_dataset(
     ds: pa.dataset.Dataset, allow_pyarrow_filter: bool = True
 ) -> LazyFrame:
     """
     Pickle the partially applied function `_scan_pyarrow_dataset_impl`.
@@ -30,15 +28,15 @@
         Allow predicates to be pushed down to pyarrow. This can lead to different
         results if comparisons are done with null values as pyarrow handles this
         different than polars does.
 
     """
     func = partial(_scan_pyarrow_dataset_impl, ds)
     func_serialized = pickle.dumps(func)
-    return pli.LazyFrame._scan_python_function(
+    return pl.LazyFrame._scan_python_function(
         ds.schema, func_serialized, allow_pyarrow_filter
     )
 
 
 def _scan_pyarrow_dataset_impl(
     ds: pa.dataset.Dataset,
     with_columns: list[str] | None,
@@ -60,22 +58,24 @@
         Materialize only n rows from the arrow dataset
 
     Returns
     -------
     DataFrame
 
     """
+    from polars import from_arrow
+
     _filter = None
     if predicate:
         # imports are used by inline python evaluated by `eval`
         from polars.datatypes import Date, Datetime, Duration  # noqa: F401
         from polars.utils.convert import (
             _to_python_datetime,  # noqa: F401
             _to_python_time,  # noqa: F401
             _to_python_timedelta,  # noqa: F401
         )
 
         _filter = eval(predicate)
     if n_rows:
-        return pl.from_arrow(ds.head(n_rows, columns=with_columns, filter=_filter))  # type: ignore[return-value]
+        return from_arrow(ds.head(n_rows, columns=with_columns, filter=_filter))  # type: ignore[return-value]
 
-    return pl.from_arrow(ds.to_table(columns=with_columns, filter=_filter))  # type: ignore[return-value]
+    return from_arrow(ds.to_table(columns=with_columns, filter=_filter))  # type: ignore[return-value]
```

### Comparing `polars_lts_cpu-0.17.9/polars/io/pyarrow_dataset/functions.py` & `polars_lts_cpu-0.18.0/polars/io/pyarrow_dataset/functions.py`

 * *Files 0% similar despite different names*

```diff
@@ -3,16 +3,16 @@
 import warnings
 from typing import TYPE_CHECKING
 
 from polars.io.pyarrow_dataset.anonymous_scan import _scan_pyarrow_dataset
 from polars.utils.various import find_stacklevel
 
 if TYPE_CHECKING:
+    from polars import LazyFrame
     from polars.dependencies import pyarrow as pa
-    from polars.lazyframe import LazyFrame
 
 
 def scan_pyarrow_dataset(
     source: pa.dataset.Dataset, *, allow_pyarrow_filter: bool = True
 ) -> LazyFrame:
     """
     Scan a pyarrow dataset.
```

### Comparing `polars_lts_cpu-0.17.9/polars/lazyframe/frame.py` & `polars_lts_cpu-0.18.0/polars/lazyframe/frame.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,29 +1,29 @@
 from __future__ import annotations
 
 import contextlib
 import os
 import typing
-import warnings
 from datetime import date, datetime, time, timedelta
 from io import BytesIO, StringIO
 from pathlib import Path
 from typing import (
     TYPE_CHECKING,
     Any,
     Callable,
+    Collection,
     Iterable,
     NoReturn,
     Sequence,
     TypeVar,
     overload,
 )
 
+import polars._reexport as pl
 from polars import functions as F
-from polars import internals as pli
 from polars.datatypes import (
     DTYPE_TEMPORAL_UNITS,
     N_INFER_DEFAULT,
     Boolean,
     Categorical,
     Date,
     Datetime,
@@ -44,51 +44,49 @@
 )
 from polars.dependencies import subprocess
 from polars.io._utils import _is_local_file
 from polars.io.ipc.anonymous_scan import _scan_ipc_fsspec
 from polars.io.parquet.anonymous_scan import _scan_parquet_fsspec
 from polars.lazyframe.groupby import LazyGroupBy
 from polars.slice import LazyPolarsSlice
-from polars.utils._parse_expr_input import expr_to_lit_or_expr, selection_to_pyexpr_list
+from polars.utils._parse_expr_input import (
+    parse_as_expression,
+    parse_as_list_of_expressions,
+)
 from polars.utils._wrap import wrap_df, wrap_expr
 from polars.utils.convert import _timedelta_to_pl_duration
 from polars.utils.various import (
     _in_notebook,
     _prepare_row_count_args,
     _process_null_values,
-    find_stacklevel,
     normalise_filepath,
 )
 
 with contextlib.suppress(ImportError):  # Module not available when building docs
     from polars.polars import PyLazyFrame
 
 
 if TYPE_CHECKING:
     import sys
     from io import IOBase
 
     import pyarrow as pa
 
-    from polars.dataframe import DataFrame
-    from polars.expr.expr import Expr
-    from polars.series import Series
+    from polars import DataFrame, Expr, Series
     from polars.type_aliases import (
         AsofJoinStrategy,
         ClosedInterval,
         CsvEncoding,
         FillNullStrategy,
         FrameInitTypes,
         IntoExpr,
         JoinStrategy,
         Orientation,
         ParallelStrategy,
         PolarsDataType,
-        PolarsExprType,
-        PythonLiteral,
         RollingInterpolationMethod,
         SchemaDefinition,
         SchemaDict,
         StartBy,
         UniqueKeepStrategy,
     )
 
@@ -653,21 +651,21 @@
 
     def __copy__(self) -> Self:
         return self.clone()
 
     def __deepcopy__(self, memo: None = None) -> Self:
         return self.clone()
 
-    def __getitem__(self, item: int | range | slice) -> Self:
+    def __getitem__(self, item: int | range | slice) -> LazyFrame:
         if not isinstance(item, slice):
             raise TypeError(
                 "'LazyFrame' object is not subscriptable (aside from slicing). Use"
                 " 'select()' or 'filter()' instead."
             )
-        return self._from_pyldf(LazyPolarsSlice(self).apply(item)._ldf)
+        return LazyPolarsSlice(self).apply(item)
 
     def __str__(self) -> str:
         return f"""\
 naive plan: (run LazyFrame.explain(optimized=True) to see the optimized plan)
 
 {self.explain(optimized=False)}\
 """
@@ -880,115 +878,14 @@
                 slice_pushdown,
                 common_subplan_elimination,
                 streaming,
             )
             return ldf.describe_optimized_plan()
         return self._ldf.describe_plan()
 
-    def describe_plan(
-        self,
-        *,
-        optimized: bool = False,
-        type_coercion: bool = True,
-        predicate_pushdown: bool = True,
-        projection_pushdown: bool = True,
-        simplify_expression: bool = True,
-        slice_pushdown: bool = True,
-        common_subplan_elimination: bool = True,
-        streaming: bool = False,
-    ) -> str:
-        """
-        Create a string representation of the unoptimized query plan.
-
-        Parameters
-        ----------
-        optimized
-            Return an optimized query plan. Defaults to ``False``.
-            If this is set to ``True`` the subsequent
-            optimization flags control which optimizations
-            run.
-        type_coercion
-            Do type coercion optimization.
-        predicate_pushdown
-            Do predicate pushdown optimization.
-        projection_pushdown
-            Do projection pushdown optimization.
-        simplify_expression
-            Run simplify expressions optimization.
-        slice_pushdown
-            Slice pushdown optimization.
-        common_subplan_elimination
-            Will try to cache branching subplans that occur on self-joins or unions.
-        streaming
-            Run parts of the query in a streaming fashion (this is in an alpha state)
-
-        .. deprecated:: 0.16.10
-            Use ``LazyFrame.explain``
-
-        Examples
-        --------
-        >>> lf = pl.LazyFrame(
-        ...     {
-        ...         "a": ["a", "b", "a", "b", "b", "c"],
-        ...         "b": [1, 2, 3, 4, 5, 6],
-        ...         "c": [6, 5, 4, 3, 2, 1],
-        ...     }
-        ... )
-        >>> lf.groupby("a", maintain_order=True).agg(pl.all().sum()).sort(
-        ...     "a"
-        ... ).describe_plan()  # doctest: +SKIP
-
-        """
-        warnings.warn(
-            "`LazyFrame.describe_plan` has been deprecated; Please use `LazyFrame.explain` instead",
-            category=DeprecationWarning,
-            stacklevel=find_stacklevel(),
-        )
-        if optimized:
-            ldf = self._ldf.optimization_toggle(
-                type_coercion,
-                predicate_pushdown,
-                projection_pushdown,
-                simplify_expression,
-                slice_pushdown,
-                common_subplan_elimination,
-                streaming,
-            )
-            return ldf.describe_optimized_plan()
-        return self._ldf.describe_plan()
-
-    def describe_optimized_plan(
-        self,
-        *,
-        type_coercion: bool = True,
-        predicate_pushdown: bool = True,
-        projection_pushdown: bool = True,
-        simplify_expression: bool = True,
-        slice_pushdown: bool = True,
-        common_subplan_elimination: bool = True,
-        streaming: bool = False,
-    ) -> str:
-        """Create a string representation of the optimized query plan."""
-        warnings.warn(
-            "`LazyFrame.describe_optimized_plan` has been deprecated; Please use `LazyFrame.explain` instead",
-            category=DeprecationWarning,
-            stacklevel=find_stacklevel(),
-        )
-        ldf = self._ldf.optimization_toggle(
-            type_coercion,
-            predicate_pushdown,
-            projection_pushdown,
-            simplify_expression,
-            slice_pushdown,
-            common_subplan_elimination,
-            streaming,
-        )
-
-        return ldf.describe_optimized_plan()
-
     def show_graph(
         self,
         *,
         optimized: bool = True,
         show: bool = True,
         output_path: str | None = None,
         raw_output: bool = False,
@@ -1209,17 +1106,15 @@
         
 
         """
         # Fast path for sorting by a single existing column
         if isinstance(by, str) and not more_by:
             return self._from_pyldf(self._ldf.sort(by, descending, nulls_last))
 
-        by = selection_to_pyexpr_list(by)
-        if more_by:
-            by.extend(selection_to_pyexpr_list(more_by))
+        by = parse_as_list_of_expressions(by, *more_by)
 
         if isinstance(descending, bool):
             descending = [descending]
         elif len(by) != len(descending):
             raise ValueError(
                 f"the length of `descending` ({len(descending)}) does not match the length of `by` ({len(by)})"
             )
@@ -1291,15 +1186,15 @@
          b    3   
          b    2   
          a    2   
          c    1   
         
 
         """
-        by = selection_to_pyexpr_list(by)
+        by = parse_as_list_of_expressions(by)
         if isinstance(descending, bool):
             descending = [descending]
         elif len(by) != len(descending):
             raise ValueError(
                 f"the length of `descending` ({len(descending)}) does not match the length of `by` ({len(by)})"
             )
         return self._from_pyldf(self._ldf.top_k(k, by, descending, nulls_last))
@@ -1370,15 +1265,15 @@
          a    1   
          a    2   
          b    1   
          b    2   
         
 
         """
-        by = selection_to_pyexpr_list(by)
+        by = parse_as_list_of_expressions(by)
         if isinstance(descending, bool):
             descending = [descending]
         return self._from_pyldf(self._ldf.bottom_k(k, by, descending, nulls_last))
 
     def profile(
         self,
         *,
@@ -1889,15 +1784,15 @@
         """
         return self
 
     def cache(self) -> Self:
         """Cache the result once the execution of the physical plan hits this node."""
         return self._from_pyldf(self._ldf.cache())
 
-    def clear(self, n: int = 0) -> Self:
+    def clear(self, n: int = 0) -> LazyFrame:
         """
         Create an empty copy of the current LazyFrame, with zero to 'n' rows.
 
         Returns a copy with an identical schema but no data.
 
         Parameters
         ----------
@@ -1934,15 +1829,15 @@
          i64   f64   bool 
         
          null  null  null 
          null  null  null 
         
 
         """
-        return self._from_pyldf(pli.DataFrame(schema=self.schema).clear(n).lazy()._ldf)
+        return pl.DataFrame(schema=self.schema).clear(n).lazy()
 
     def clone(self) -> Self:
         """
         Very cheap deepcopy/clone.
 
         See Also
         --------
@@ -2019,18 +1914,18 @@
         
          1    6    a   
          3    8    c   
         
 
         """
         if isinstance(predicate, list):
-            predicate = pli.Series(predicate)
+            predicate = pl.Series(predicate)
 
         return self._from_pyldf(
-            self._ldf.filter(expr_to_lit_or_expr(predicate, str_to_lit=False)._pyexpr)
+            self._ldf.filter(parse_as_expression(predicate)._pyexpr)
         )
 
     def select(
         self,
         exprs: IntoExpr | Iterable[IntoExpr] | None = None,
         *more_exprs: IntoExpr,
         **named_exprs: IntoExpr,
@@ -2138,33 +2033,26 @@
 
         """
         if exprs is None and not named_exprs:
             raise ValueError("Expected at least one of 'exprs' or '**named_exprs'")
 
         structify = bool(int(os.environ.get("POLARS_AUTO_STRUCTIFY", 0)))
 
-        exprs = selection_to_pyexpr_list(exprs, structify=structify)
-        if more_exprs:
-            exprs.extend(selection_to_pyexpr_list(more_exprs, structify=structify))
-        if named_exprs:
-            exprs.extend(
-                expr_to_lit_or_expr(
-                    expr, structify=structify, name=name, str_to_lit=False
-                )._pyexpr
-                for name, expr in named_exprs.items()
-            )
+        exprs = parse_as_list_of_expressions(
+            exprs, *more_exprs, **named_exprs, structify=structify
+        )
 
         return self._from_pyldf(self._ldf.select(exprs))
 
     def groupby(
         self,
         by: IntoExpr | Iterable[IntoExpr],
         *more_by: IntoExpr,
         maintain_order: bool = False,
-    ) -> LazyGroupBy[Self]:
+    ) -> LazyGroupBy:
         """
         Start a groupby operation.
 
         Parameters
         ----------
         by
             Column(s) to group by. Accepts expression input. Strings are parsed as
@@ -2245,29 +2133,28 @@
         
          a    0    4.0 
          b    1    3.0 
          c    1    1.0 
         
 
         """
-        exprs = selection_to_pyexpr_list(by)
-        if more_by:
-            exprs.extend(selection_to_pyexpr_list(more_by))
+        exprs = parse_as_list_of_expressions(by, *more_by)
         lgb = self._ldf.groupby(exprs, maintain_order)
-        return LazyGroupBy(lgb, lazyframe_class=self.__class__)
+        return LazyGroupBy(lgb)
 
     def groupby_rolling(
         self,
-        index_column: str,
+        index_column: IntoExpr,
         *,
         period: str | timedelta,
         offset: str | timedelta | None = None,
         closed: ClosedInterval = "right",
         by: IntoExpr | Iterable[IntoExpr] | None = None,
-    ) -> LazyGroupBy[Self]:
+        check_sorted: bool = True,
+    ) -> LazyGroupBy:
         """
         Create rolling groups based on a time column.
 
         Also works for index values of type Int32 or Int64.
 
         Different from a ``dynamic_groupby`` the windows are now determined by the
         individual values and are not of constant intervals. For constant intervals
@@ -2315,14 +2202,27 @@
             length of the window
         offset
             offset of the window. Default is -period
         closed : {'right', 'left', 'both', 'none'}
             Define which sides of the temporal interval are closed (inclusive).
         by
             Also group by this column/these columns
+        check_sorted
+            When the ``by`` argument is given, polars can not check sortedness
+            by the metadata and has to do a full scan on the index column to
+            verify data is sorted. This is expensive. If you are sure the
+            data within the by groups is sorted, you can set this to ``False``.
+            Doing so incorrectly will lead to incorrect output
+
+        Returns
+        -------
+        LazyGroupBy
+            Object you can call ``.agg`` on to aggregate by groups, the result
+            of which will be sorted by `index_column` (but note that if `by` columns are
+            passed, it will only be sorted within each `by` group).
 
         See Also
         --------
         groupby_dynamic
 
         Examples
         --------
@@ -2363,39 +2263,41 @@
          2020-01-01 16:45:09  15     3      7     
          2020-01-02 18:12:48  24     3      9     
          2020-01-03 19:45:32  11     2      9     
          2020-01-08 23:16:43  1      1      1     
         
 
         """
+        index_column = parse_as_expression(index_column)._pyexpr
         if offset is None:
             offset = f"-{_timedelta_to_pl_duration(period)}"
 
-        pyexprs_by = [] if by is None else selection_to_pyexpr_list(by)
+        pyexprs_by = parse_as_list_of_expressions(by)
         period = _timedelta_to_pl_duration(period)
         offset = _timedelta_to_pl_duration(offset)
 
         lgb = self._ldf.groupby_rolling(
-            index_column, period, offset, closed, pyexprs_by
+            index_column, period, offset, closed, pyexprs_by, check_sorted
         )
-        return LazyGroupBy(lgb, lazyframe_class=self.__class__)
+        return LazyGroupBy(lgb)
 
     def groupby_dynamic(
         self,
-        index_column: str,
+        index_column: IntoExpr,
         *,
         every: str | timedelta,
         period: str | timedelta | None = None,
         offset: str | timedelta | None = None,
         truncate: bool = True,
         include_boundaries: bool = False,
         closed: ClosedInterval = "left",
         by: IntoExpr | Iterable[IntoExpr] | None = None,
         start_by: StartBy = "window",
-    ) -> LazyGroupBy[Self]:
+        check_sorted: bool = True,
+    ) -> LazyGroupBy:
         """
         Group based on a time value (or index value of type Int32, Int64).
 
         Time windows are calculated and rows are assigned to windows. Different from a
         normal groupby is that a row can be member of multiple groups. The time/index
         window could be seen as a rolling window, with a window size determined by
         dates/times/values instead of slots in the DataFrame.
@@ -2460,20 +2362,36 @@
             Add the lower and upper bound of the window to the "_lower_bound" and
             "_upper_bound" columns. This will impact performance because it's harder to
             parallelize
         closed : {'right', 'left', 'both', 'none'}
             Define which sides of the temporal interval are closed (inclusive).
         by
             Also group by this column/these columns
-        start_by : {'window', 'datapoint', 'monday'}
+        start_by : {'window', 'datapoint', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday'}
             The strategy to determine the start of the first window by.
 
             * 'window': Truncate the start of the window with the 'every' argument.
             * 'datapoint': Start from the first encountered data point.
             * 'monday': Start the window on the monday before the first data point.
+            * 'tuesday': Start the window on the tuesday before the first data point.
+            * ...
+            * 'sunday': Start the window on the sunday before the first data point.
+        check_sorted
+            When the ``by`` argument is given, polars can not check sortedness
+            by the metadata and has to do a full scan on the index column to
+            verify data is sorted. This is expensive. If you are sure the
+            data within the by groups is sorted, you can set this to ``False``.
+            Doing so incorrectly will lead to incorrect output
+
+        Returns
+        -------
+        LazyGroupBy
+            Object you can call ``.agg`` on to aggregate by groups, the result
+            of which will be sorted by `index_column` (but note that if `by` columns are
+            passed, it will only be sorted within each `by` group).
 
         See Also
         --------
         groupby_rolling
 
         Examples
         --------
@@ -2481,14 +2399,15 @@
         >>> # create an example dataframe
         >>> lf = pl.LazyFrame(
         ...     {
         ...         "time": pl.date_range(
         ...             start=datetime(2021, 12, 16),
         ...             end=datetime(2021, 12, 16, 3),
         ...             interval="30m",
+        ...             eager=True,
         ...         ),
         ...         "n": range(7),
         ...     }
         ... )
         >>> lf.collect()
         shape: (7, 2)
         
@@ -2585,14 +2504,15 @@
 
         >>> lf = pl.LazyFrame(
         ...     {
         ...         "time": pl.date_range(
         ...             start=datetime(2021, 12, 16),
         ...             end=datetime(2021, 12, 16, 3),
         ...             interval="30m",
+        ...             eager=True,
         ...         ),
         ...         "groups": ["a", "a", "a", "b", "b", "a", "a"],
         ...     }
         ... )
         >>> lf.collect()
         shape: (7, 2)
         
@@ -2655,37 +2575,39 @@
         
          0                3                0    ["A", "B", "B"] 
          2                5                2    ["B", "B", "C"] 
          4                7                4    ["C"]           
         
 
         """  # noqa: W505
+        index_column = parse_as_expression(index_column)._pyexpr
         if offset is None:
             offset = f"-{_timedelta_to_pl_duration(every)}" if period is None else "0ns"
 
         if period is None:
             period = every
 
         period = _timedelta_to_pl_duration(period)
         offset = _timedelta_to_pl_duration(offset)
         every = _timedelta_to_pl_duration(every)
 
-        pyexprs_by = [] if by is None else selection_to_pyexpr_list(by)
+        pyexprs_by = parse_as_list_of_expressions(by)
         lgb = self._ldf.groupby_dynamic(
             index_column,
             every,
             period,
             offset,
             truncate,
             include_boundaries,
             closed,
             pyexprs_by,
             start_by,
+            check_sorted,
         )
-        return LazyGroupBy(lgb, lazyframe_class=self.__class__)
+        return LazyGroupBy(lgb)
 
     def join_asof(
         self,
         other: LazyFrame,
         *,
         left_on: str | None | Expr = None,
         right_on: str | None | Expr = None,
@@ -2711,14 +2633,17 @@
 
           - A "backward" search selects the last row in the right DataFrame whose
             'on' key is less than or equal to the left's key.
 
           - A "forward" search selects the first row in the right DataFrame whose
             'on' key is greater than or equal to the left's key.
 
+        - A "nearest" search selects the last row in the right DataFrame whose value
+          is nearest to the left's key.
+
         The default is "backward".
 
         Parameters
         ----------
         other
             Lazy DataFrame to join with.
         left_on
@@ -2730,15 +2655,15 @@
             None.
         by
             Join on these columns before doing asof join.
         by_left
             Join on these columns before doing asof join.
         by_right
             Join on these columns before doing asof join.
-        strategy : {'backward', 'forward'}
+        strategy : {'backward', 'forward', 'nearest'}
             Join strategy.
         suffix
             Suffix to append to columns with a duplicate name.
         tolerance
             Numeric tolerance. By setting this the join will only be done if the near
             keys are within this distance. If an asof join is done on columns of dtype
             "Date", "Datetime", "Duration" or "Time" you use the following string
@@ -2810,26 +2735,26 @@
 
         """
         if not isinstance(other, LazyFrame):
             raise TypeError(
                 f"Expected 'other' join table to be a LazyFrame, not a {type(other).__name__}"
             )
 
-        if isinstance(on, (str, pli.Expr)):
+        if isinstance(on, (str, pl.Expr)):
             left_on = on
             right_on = on
 
         if left_on is None or right_on is None:
             raise ValueError("You should pass the column to join on as an argument.")
 
         by_left_: Sequence[str] | None
         by_left_ = [by_left] if isinstance(by_left, str) else by_left
 
         by_right_: Sequence[str] | None
-        by_right_ = [by_right] if isinstance(by_right, (str, pli.Expr)) else by_right
+        by_right_ = [by_right] if isinstance(by_right, (str, pl.Expr)) else by_right
 
         if isinstance(by, str):
             by_left_ = [by]
             by_right_ = [by]
         elif isinstance(by, list):
             by_left_ = by
             by_right_ = by
@@ -2837,17 +2762,17 @@
         tolerance_str: str | None = None
         tolerance_num: float | int | None = None
         if isinstance(tolerance, str):
             tolerance_str = tolerance
         else:
             tolerance_num = tolerance
 
-        if not isinstance(left_on, pli.Expr):
+        if not isinstance(left_on, pl.Expr):
             left_on = F.col(left_on)
-        if not isinstance(right_on, pli.Expr):
+        if not isinstance(right_on, pl.Expr):
             right_on = F.col(right_on)
 
         return self._from_pyldf(
             self._ldf.join_asof(
                 other._ldf,
                 left_on._pyexpr,
                 right_on._pyexpr,
@@ -2981,20 +2906,20 @@
             return self._from_pyldf(
                 self._ldf.join(
                     other._ldf, [], [], allow_parallel, force_parallel, how, suffix
                 )
             )
 
         if on is not None:
-            pyexprs = selection_to_pyexpr_list(on)
+            pyexprs = parse_as_list_of_expressions(on)
             pyexprs_left = pyexprs
             pyexprs_right = pyexprs
         elif left_on is not None and right_on is not None:
-            pyexprs_left = selection_to_pyexpr_list(left_on)
-            pyexprs_right = selection_to_pyexpr_list(right_on)
+            pyexprs_left = parse_as_list_of_expressions(left_on)
+            pyexprs_right = parse_as_list_of_expressions(right_on)
         else:
             raise ValueError("must specify `on` OR `left_on` and `right_on`")
 
         return self._from_pyldf(
             self._ldf.join(
                 other._ldf,
                 pyexprs_left,
@@ -3004,24 +2929,17 @@
                 how,
                 suffix,
             )
         )
 
     def with_columns(
         self,
-        exprs: (
-            str
-            | PolarsExprType
-            | PythonLiteral
-            | Series
-            | Iterable[str | PolarsExprType | PythonLiteral | Series | None]
-            | None
-        ) = None,
-        *more_exprs: str | PolarsExprType | PythonLiteral | Series | None,
-        **named_exprs: str | PolarsExprType | PythonLiteral | Series | None,
+        exprs: IntoExpr | Iterable[IntoExpr] | None = None,
+        *more_exprs: IntoExpr,
+        **named_exprs: IntoExpr,
     ) -> Self:
         """
         Add columns to this DataFrame.
 
         Added columns will replace existing columns with the same name.
 
         Parameters
@@ -3163,24 +3081,17 @@
 
         """
         if exprs is None and not named_exprs:
             raise ValueError("Expected at least one of 'exprs' or '**named_exprs'")
 
         structify = bool(int(os.environ.get("POLARS_AUTO_STRUCTIFY", 0)))
 
-        exprs = selection_to_pyexpr_list(exprs, structify=structify)
-        if more_exprs:
-            exprs.extend(selection_to_pyexpr_list(more_exprs, structify=structify))
-        if named_exprs:
-            exprs.extend(
-                expr_to_lit_or_expr(
-                    expr, structify=structify, name=name, str_to_lit=False
-                )._pyexpr
-                for name, expr in named_exprs.items()
-            )
+        exprs = parse_as_list_of_expressions(
+            exprs, *more_exprs, **named_exprs, structify=structify
+        )
 
         return self._from_pyldf(self._ldf.with_columns(exprs))
 
     @typing.no_type_check
     def with_context(self, other: Self | list[Self]) -> Self:
         """
         Add an external context to the computation graph.
@@ -3235,15 +3146,15 @@
 
         """
         if not isinstance(other, list):
             other = [other]
 
         return self._from_pyldf(self._ldf.with_context([lf._ldf for lf in other]))
 
-    def drop(self, columns: str | Sequence[str], *more_columns: str) -> Self:
+    def drop(self, columns: str | Collection[str], *more_columns: str) -> Self:
         """
         Remove columns from the dataframe.
 
         Parameters
         ----------
         columns
             Name of the column(s) that should be removed from the dataframe.
@@ -3300,18 +3211,15 @@
          b   
          c   
         
 
         """
         if isinstance(columns, str):
             columns = [columns]
-        if more_columns:
-            columns = list(columns)
-            columns.extend(more_columns)
-        return self._from_pyldf(self._ldf.drop(columns))
+        return self._from_pyldf(self._ldf.drop([*columns, *more_columns]))
 
     def rename(self, mapping: dict[str, str]) -> Self:
         """
         Rename column names.
 
         Parameters
         ----------
@@ -3463,15 +3371,15 @@
         
          3    4   
          5    6   
          0    0   
         
 
         """
-        if not isinstance(fill_value, pli.Expr):
+        if not isinstance(fill_value, pl.Expr):
             fill_value = F.lit(fill_value)
         return self._from_pyldf(self._ldf.shift_and_fill(periods, fill_value._pyexpr))
 
     def slice(self, offset: int, length: int | None = None) -> Self:
         """
         Get a slice of this DataFrame.
 
@@ -3884,15 +3792,15 @@
         dtypes: Sequence[PolarsDataType]
 
         if value is not None:
 
             def infer_dtype(value: Any) -> PolarsDataType:
                 return next(iter(self.select(value).schema.values()))
 
-            if isinstance(value, pli.Expr):
+            if isinstance(value, pl.Expr):
                 dtypes = [infer_dtype(value)]
             elif isinstance(value, bool):
                 dtypes = [Boolean]
             elif matches_supertype and isinstance(value, (int, float)):
                 dtypes = [
                     Int8,
                     Int16,
@@ -3959,21 +3867,21 @@
          1.5   0.5  
          2.0   4.0  
          99.0  99.0 
          4.0   13.0 
         
 
         """
-        if not isinstance(value, pli.Expr):
+        if not isinstance(value, pl.Expr):
             value = F.lit(value)
         return self._from_pyldf(self._ldf.fill_nan(value._pyexpr))
 
     def std(self, ddof: int = 1) -> Self:
         """
-        Aggregate the columns in the DataFrame to their standard deviation value.
+        Aggregate the columns in the LazyFrame to their standard deviation value.
 
         Parameters
         ----------
         ddof
             Delta Degrees of Freedom: the divisor used in the calculation is N - ddof,
             where N represents the number of elements.
             By default ddof is 1.
@@ -4006,15 +3914,15 @@
         
 
         """
         return self._from_pyldf(self._ldf.std(ddof))
 
     def var(self, ddof: int = 1) -> Self:
         """
-        Aggregate the columns in the DataFrame to their variance value.
+        Aggregate the columns in the LazyFrame to their variance value.
 
         Parameters
         ----------
         ddof
             Delta Degrees of Freedom: the divisor used in the calculation is N - ddof,
             where N represents the number of elements.
             By default ddof is 1.
@@ -4047,15 +3955,15 @@
         
 
         """
         return self._from_pyldf(self._ldf.var(ddof))
 
     def max(self) -> Self:
         """
-        Aggregate the columns in the DataFrame to their maximum value.
+        Aggregate the columns in the LazyFrame to their maximum value.
 
         Examples
         --------
         >>> lf = pl.LazyFrame(
         ...     {
         ...         "a": [1, 2, 3, 4],
         ...         "b": [1, 2, 1, 1],
@@ -4072,15 +3980,15 @@
         
 
         """
         return self._from_pyldf(self._ldf.max())
 
     def min(self) -> Self:
         """
-        Aggregate the columns in the DataFrame to their minimum value.
+        Aggregate the columns in the LazyFrame to their minimum value.
 
         Examples
         --------
         >>> lf = pl.LazyFrame(
         ...     {
         ...         "a": [1, 2, 3, 4],
         ...         "b": [1, 2, 1, 1],
@@ -4097,15 +4005,15 @@
         
 
         """
         return self._from_pyldf(self._ldf.min())
 
     def sum(self) -> Self:
         """
-        Aggregate the columns in the DataFrame to their sum value.
+        Aggregate the columns in the LazyFrame to their sum value.
 
         Examples
         --------
         >>> lf = pl.LazyFrame(
         ...     {
         ...         "a": [1, 2, 3, 4],
         ...         "b": [1, 2, 1, 1],
@@ -4122,15 +4030,15 @@
         
 
         """
         return self._from_pyldf(self._ldf.sum())
 
     def mean(self) -> Self:
         """
-        Aggregate the columns in the DataFrame to their mean value.
+        Aggregate the columns in the LazyFrame to their mean value.
 
         Examples
         --------
         >>> lf = pl.LazyFrame(
         ...     {
         ...         "a": [1, 2, 3, 4],
         ...         "b": [1, 2, 1, 1],
@@ -4147,15 +4055,15 @@
         
 
         """
         return self._from_pyldf(self._ldf.mean())
 
     def median(self) -> Self:
         """
-        Aggregate the columns in the DataFrame to their median value.
+        Aggregate the columns in the LazyFrame to their median value.
 
         Examples
         --------
         >>> lf = pl.LazyFrame(
         ...     {
         ...         "a": [1, 2, 3, 4],
         ...         "b": [1, 2, 1, 1],
@@ -4170,21 +4078,47 @@
         
          2.5  1.0 
         
 
         """
         return self._from_pyldf(self._ldf.median())
 
+    def null_count(self) -> Self:
+        """
+        Aggregate the columns in the LazyFrame as the sum of their null value count.
+
+        Examples
+        --------
+        >>> lf = pl.LazyFrame(
+        ...     {
+        ...         "foo": [1, None, 3],
+        ...         "bar": [6, 7, None],
+        ...         "ham": ["a", "b", "c"],
+        ...     }
+        ... )
+        >>> lf.null_count().collect()
+        shape: (1, 3)
+        
+         foo  bar  ham 
+         ---  ---  --- 
+         u32  u32  u32 
+        
+         1    1    0   
+        
+
+        """
+        return self._from_pyldf(self._ldf.null_count())
+
     def quantile(
         self,
         quantile: float | Expr,
         interpolation: RollingInterpolationMethod = "nearest",
     ) -> Self:
         """
-        Aggregate the columns in the DataFrame to their quantile value.
+        Aggregate the columns in the LazyFrame to their quantile value.
 
         Parameters
         ----------
         quantile
             Quantile between 0.0 and 1.0.
         interpolation : {'nearest', 'higher', 'lower', 'midpoint', 'linear'}
             Interpolation method.
@@ -4204,16 +4138,16 @@
          ---  --- 
          f64  f64 
         
          3.0  1.0 
         
 
         """
-        quantile = expr_to_lit_or_expr(quantile, str_to_lit=False)
-        return self._from_pyldf(self._ldf.quantile(quantile._pyexpr, interpolation))
+        quantile = parse_as_expression(quantile)._pyexpr
+        return self._from_pyldf(self._ldf.quantile(quantile, interpolation))
 
     def explode(
         self,
         columns: str | Sequence[str] | Expr | Sequence[Expr],
         *more_columns: str | Expr,
     ) -> Self:
         """
@@ -4249,17 +4183,15 @@
          b        5       
          c        6       
          c        7       
          c        8       
         
 
         """
-        columns = selection_to_pyexpr_list(columns)
-        if more_columns:
-            columns.extend(selection_to_pyexpr_list(more_columns))
+        columns = parse_as_list_of_expressions(columns, *more_columns)
         return self._from_pyldf(self._ldf.explode(columns))
 
     def unique(
         self,
         subset: str | Sequence[str] | None = None,
         *,
         keep: UniqueKeepStrategy = "any",
@@ -4338,15 +4270,15 @@
         
 
         """
         if isinstance(subset, str):
             subset = [subset]
         return self._from_pyldf(self._ldf.unique(maintain_order, subset, keep))
 
-    def drop_nulls(self, subset: str | Sequence[str] | None = None) -> Self:
+    def drop_nulls(self, subset: str | Collection[str] | None = None) -> Self:
         """
         Drop all rows that contain null values.
 
         Returns a new LazyFrame.
 
         Parameters
         ----------
@@ -4410,16 +4342,18 @@
         
          null  1    1    
          null  2    null 
          null  1    1    
         
 
         """
-        if isinstance(subset, str):
-            subset = [subset]
+        if subset is not None:
+            if isinstance(subset, str):
+                subset = [subset]
+            subset = list(subset)
         return self._from_pyldf(self._ldf.drop_nulls(subset))
 
     def melt(
         self,
         id_vars: str | list[str] | None = None,
         value_vars: str | list[str] | None = None,
         variable_name: str | None = None,
@@ -4681,33 +4615,31 @@
             Key that is sorted.
 
         """
         return self._from_pyldf(self._ldf.merge_sorted(other._ldf, key))
 
     def set_sorted(
         self,
-        column: IntoExpr | Iterable[IntoExpr],
-        *more_columns: IntoExpr,
+        column: str | Iterable[str],
+        *more_columns: str,
         descending: bool = False,
     ) -> Self:
         """
         Indicate that one or multiple columns are sorted.
 
         Parameters
         ----------
         column
             Columns that are sorted
         more_columns
             Additional columns that are sorted, specified as positional arguments.
         descending
             Whether the columns are sorted in descending order.
         """
-        columns = selection_to_pyexpr_list(column)
-        if more_columns:
-            columns.extend(selection_to_pyexpr_list(more_columns))
+        columns = parse_as_list_of_expressions(column, *more_columns)
         return self.with_columns(
             [wrap_expr(e).set_sorted(descending=descending) for e in columns]
         )
 
     def update(
         self,
         other: LazyFrame,
@@ -4805,25 +4737,28 @@
             if name not in union_names:
                 raise ValueError(f"Join column {name} not found.")
 
         right_added_names = union_names - set(on)
 
         # no need to join if only join columns are in other
         if len(right_added_names) == 0:
+            if row_count_used:
+                return self.drop(row_count_name)
             return self
-        tmp_name = "__POLARS_RIGHT"
 
+        tmp_name = "__POLARS_RIGHT"
         result = (
             self.join(other.select(list(union_names)), on=on, how=how, suffix=tmp_name)
             .with_columns(
                 [
                     F.coalesce([column_name + tmp_name, F.col(column_name)]).alias(
                         column_name
                     )
                     for column_name in right_added_names
                 ]
             )
             .drop([name + tmp_name for name in right_added_names])
         )
         if row_count_used:
             result = result.drop(row_count_name)
+
         return self._from_pyldf(result._ldf)
```

### Comparing `polars_lts_cpu-0.17.9/polars/lazyframe/groupby.py` & `polars_lts_cpu-0.18.0/polars/lazyframe/groupby.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,40 +1,37 @@
 from __future__ import annotations
 
-from typing import TYPE_CHECKING, Callable, Generic, Iterable, TypeVar
+from typing import TYPE_CHECKING, Callable, Iterable
 
 from polars import functions as F
-from polars.utils._parse_expr_input import expr_to_lit_or_expr, selection_to_pyexpr_list
+from polars.utils._parse_expr_input import parse_as_list_of_expressions
+from polars.utils._wrap import wrap_ldf
 
 if TYPE_CHECKING:
-    from polars.dataframe import DataFrame
-    from polars.lazyframe import LazyFrame
+    from polars import DataFrame, LazyFrame
     from polars.polars import PyLazyGroupBy
     from polars.type_aliases import IntoExpr, RollingInterpolationMethod, SchemaDict
 
-LDF = TypeVar("LDF", bound="LazyFrame")
 
-
-class LazyGroupBy(Generic[LDF]):
+class LazyGroupBy:
     """
     Utility class for performing a groupby operation over a lazy dataframe.
 
     Generated by calling ``df.lazy().groupby(...)``.
     """
 
-    def __init__(self, lgb: PyLazyGroupBy, lazyframe_class: type[LDF]) -> None:
+    def __init__(self, lgb: PyLazyGroupBy) -> None:
         self.lgb = lgb
-        self._lazyframe_class = lazyframe_class
 
     def agg(
         self,
         aggs: IntoExpr | Iterable[IntoExpr] | None = None,
         *more_aggs: IntoExpr,
         **named_aggs: IntoExpr,
-    ) -> LDF:
+    ) -> LazyFrame:
         """
         Compute aggregations for each group of a groupby operation.
 
         Parameters
         ----------
         aggs
             Aggregations to compute for each group of the groupby operation.
@@ -122,44 +119,36 @@
         if isinstance(aggs, dict):
             raise ValueError(
                 f"'aggs' argument should be one or multiple expressions, got: '{aggs}'."
             )
         if aggs is None and not named_aggs:
             raise ValueError("Expected at least one of 'aggs' or '**named_aggs'")
 
-        exprs = selection_to_pyexpr_list(aggs)
-        if more_aggs:
-            exprs.extend(selection_to_pyexpr_list(more_aggs))
-        if named_aggs:
-            exprs.extend(
-                expr_to_lit_or_expr(expr, name=name, str_to_lit=False)._pyexpr
-                for name, expr in named_aggs.items()
-            )
+        exprs = parse_as_list_of_expressions(aggs, *more_aggs, **named_aggs)  # type: ignore[arg-type]
 
-        return self._lazyframe_class._from_pyldf(self.lgb.agg(exprs))
+        return wrap_ldf(self.lgb.agg(exprs))
 
     def apply(
         self,
         function: Callable[[DataFrame], DataFrame],
         schema: SchemaDict | None,
-    ) -> LDF:
+    ) -> LazyFrame:
         """
         Apply a custom/user-defined function (UDF) over the groups as a new DataFrame.
 
-        Implementing logic using a Python function is almost always _significantly_
-        slower and more memory intensive than implementing the same logic using
-        the native expression API because:
-
-        - The native expression engine runs in Rust; UDFs run in Python.
-        - Use of Python UDFs forces the DataFrame to be materialized in memory.
-        - Polars-native expressions can be parallelised (UDFs cannot).
-        - Polars-native expressions can be logically optimised (UDFs cannot).
+        Using this is considered an anti-pattern. This will be very slow because:
+
+        - it forces the engine to materialize the whole `DataFrames` for the groups.
+        - it is not parallelized
+        - it blocks optimizations as the passed python function is opaque to the
+          optimizer
 
-        Wherever possible you should strongly prefer the native expression API
-        to achieve the best performance.
+        The idiomatic way to apply custom functions over multiple columns is using:
+
+        `pl.struct([my_columns]).apply(lambda struct_series: ..)`
 
         Parameters
         ----------
         function
             Function to apply over each group of the `LazyFrame`.
         schema
             Schema of the output function. This has to be known statically. If the
@@ -215,17 +204,17 @@
         >>> (
         ...     df.lazy()
         ...     .filter(pl.arange(0, pl.count()).shuffle().over("color") < 2)
         ...     .collect()
         ... )  # doctest: +IGNORE_RESULT
 
         """
-        return self._lazyframe_class._from_pyldf(self.lgb.apply(function, schema))
+        return wrap_ldf(self.lgb.apply(function, schema))
 
-    def head(self, n: int = 5) -> LDF:
+    def head(self, n: int = 5) -> LazyFrame:
         """
         Get the first `n` rows of each group.
 
         Parameters
         ----------
         n
             Number of rows to return.
@@ -263,17 +252,17 @@
          a        5   
          b        6   
          c        1   
          c        2   
         
 
         """
-        return self._lazyframe_class._from_pyldf(self.lgb.head(n))
+        return wrap_ldf(self.lgb.head(n))
 
-    def tail(self, n: int = 5) -> LDF:
+    def tail(self, n: int = 5) -> LazyFrame:
         """
         Get the last `n` rows of each group.
 
         Parameters
         ----------
         n
             Number of rows to return.
@@ -311,17 +300,17 @@
          a        5   
          b        6   
          c        2   
          c        4   
         
 
         """
-        return self._lazyframe_class._from_pyldf(self.lgb.tail(n))
+        return wrap_ldf(self.lgb.tail(n))
 
-    def all(self) -> LDF:
+    def all(self) -> LazyFrame:
         """
         Aggregate the groups into Series.
 
         Examples
         --------
         >>> ldf = pl.DataFrame(
         ...     {
@@ -339,18 +328,21 @@
          one  [1, 3]    
          two  [2, 4]    
         
 
         """
         return self.agg(F.all())
 
-    def count(self) -> LDF:
+    def count(self) -> LazyFrame:
         """
         Count the number of values in each group.
 
+        .. warning::
+            `null` is deemed a value in this context.
+
         Examples
         --------
         >>> ldf = pl.DataFrame(
         ...     {
         ...         "a": [1, 2, 2, 3, 4, 5],
         ...         "b": [0.5, 0.5, 4, 10, 13, 14],
         ...         "c": [True, True, True, False, False, True],
@@ -368,15 +360,15 @@
          Orange  1     
          Banana  2     
         
 
         """
         return self.agg(F.count())
 
-    def first(self) -> LDF:
+    def first(self) -> LazyFrame:
         """
         Aggregate the first values in the group.
 
         Examples
         --------
         >>> ldf = pl.DataFrame(
         ...     {
@@ -397,15 +389,15 @@
          Orange  2    0.5   true  
          Banana  4    13.0  false 
         
 
         """
         return self.agg(F.all().first())
 
-    def last(self) -> LDF:
+    def last(self) -> LazyFrame:
         """
         Aggregate the last values in the group.
 
         Examples
         --------
         >>> ldf = pl.DataFrame(
         ...     {
@@ -426,15 +418,15 @@
          Orange  2    0.5   true  
          Banana  5    14.0  true  
         
 
         """
         return self.agg(F.all().last())
 
-    def max(self) -> LDF:
+    def max(self) -> LazyFrame:
         """
         Reduce the groups to the maximal value.
 
         Examples
         --------
         >>> ldf = pl.DataFrame(
         ...     {
@@ -455,15 +447,15 @@
          Orange  2    0.5   true 
          Banana  5    14.0  true 
         
 
         """
         return self.agg(F.all().max())
 
-    def mean(self) -> LDF:
+    def mean(self) -> LazyFrame:
         """
         Reduce the groups to the mean values.
 
         Examples
         --------
         >>> ldf = pl.DataFrame(
         ...     {
@@ -484,15 +476,15 @@
          Orange  2.0  0.5       1.0      
          Banana  4.5  13.5      0.5      
         
 
         """
         return self.agg(F.all().mean())
 
-    def median(self) -> LDF:
+    def median(self) -> LazyFrame:
         """
         Return the median per group.
 
         Examples
         --------
         >>> ldf = pl.DataFrame(
         ...     {
@@ -511,15 +503,15 @@
          Apple   2.0  4.0  
          Banana  4.0  13.0 
         
 
         """
         return self.agg(F.all().median())
 
-    def min(self) -> LDF:
+    def min(self) -> LazyFrame:
         """
         Reduce the groups to the minimal value.
 
         Examples
         --------
         >>> ldf = pl.DataFrame(
         ...     {
@@ -540,15 +532,15 @@
          Orange  2    0.5   true  
          Banana  4    13.0  false 
         
 
         """
         return self.agg(F.all().min())
 
-    def n_unique(self) -> LDF:
+    def n_unique(self) -> LazyFrame:
         """
         Count the unique values per group.
 
         Examples
         --------
         >>> ldf = pl.DataFrame(
         ...     {
@@ -569,15 +561,15 @@
         
 
         """
         return self.agg(F.all().n_unique())
 
     def quantile(
         self, quantile: float, interpolation: RollingInterpolationMethod = "nearest"
-    ) -> LDF:
+    ) -> LazyFrame:
         """
         Compute the quantile per group.
 
         Parameters
         ----------
         quantile
             Quantile between 0.0 and 1.0.
@@ -604,15 +596,15 @@
          Orange  2.0  0.5  
          Banana  5.0  14.0 
         
 
         """
         return self.agg(F.all().quantile(quantile, interpolation=interpolation))
 
-    def sum(self) -> LDF:
+    def sum(self) -> LazyFrame:
         """
         Reduce the groups to the sum.
 
         Examples
         --------
         >>> ldf = pl.DataFrame(
         ...     {
```

### Comparing `polars_lts_cpu-0.17.9/polars/series/_numpy.py` & `polars_lts_cpu-0.18.0/polars/series/_numpy.py`

 * *Files 1% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 
 import ctypes
 from typing import TYPE_CHECKING, Any
 
 import numpy as np
 
 if TYPE_CHECKING:
-    from polars.series import Series
+    from polars import Series
 
 
 # https://numpy.org/doc/stable/user/basics.subclassing.html#slightly-more-realistic-example-attribute-added-to-existing-array
 class SeriesView(np.ndarray):  # type: ignore[type-arg]
     def __new__(
         cls, input_array: np.ndarray[Any, Any], owned_series: Series
     ) -> SeriesView:
```

### Comparing `polars_lts_cpu-0.17.9/polars/series/binary.py` & `polars_lts_cpu-0.18.0/polars/series/binary.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING
 
 from polars.series.utils import expr_dispatch
 
 if TYPE_CHECKING:
+    from polars import Series
     from polars.polars import PySeries
-    from polars.series import Series
     from polars.type_aliases import TransferEncoding
 
 
 @expr_dispatch
 class BinaryNameSpace:
     """Series.bin namespace."""
```

### Comparing `polars_lts_cpu-0.17.9/polars/series/categorical.py` & `polars_lts_cpu-0.18.0/polars/series/categorical.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING
 
 from polars.series.utils import expr_dispatch
 
 if TYPE_CHECKING:
+    from polars import Series
     from polars.polars import PySeries
-    from polars.series import Series
     from polars.type_aliases import CategoricalOrdering
 
 
 @expr_dispatch
 class CatNameSpace:
     """Namespace for categorical related series."""
```

### Comparing `polars_lts_cpu-0.17.9/polars/series/datetime.py` & `polars_lts_cpu-0.18.0/polars/series/datetime.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,24 +1,22 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING
 
-from polars import functions as F
 from polars.datatypes import Date
 from polars.series.utils import expr_dispatch
 from polars.utils._wrap import wrap_s
 from polars.utils.convert import _to_python_date, _to_python_datetime
 from polars.utils.decorators import deprecated_alias
 
 if TYPE_CHECKING:
     import datetime as dt
 
-    from polars.expr.expr import Expr
+    from polars import Expr, Series
     from polars.polars import PySeries
-    from polars.series import Series
     from polars.type_aliases import EpochTimeUnit, TimeUnit
 
 
 @expr_dispatch
 class DateTimeNameSpace:
     """Series.dt namespace."""
 
@@ -34,18 +32,20 @@
     def min(self) -> dt.date | dt.datetime | dt.timedelta | None:
         """
         Return minimum as python DateTime.
 
         Examples
         --------
         >>> from datetime import datetime
-        >>> date = pl.date_range(datetime(2001, 1, 1), datetime(2001, 1, 3), "1d")
+        >>> date = pl.date_range(
+        ...     datetime(2001, 1, 1), datetime(2001, 1, 3), "1d", eager=True
+        ... )
         >>> date
         shape: (3,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2001-01-01 00:00:00
                 2001-01-02 00:00:00
                 2001-01-03 00:00:00
         ]
         >>> date.dt.min()
         datetime.datetime(2001, 1, 1, 0, 0)
@@ -56,18 +56,20 @@
     def max(self) -> dt.date | dt.datetime | dt.timedelta | None:
         """
         Return maximum as python DateTime.
 
         Examples
         --------
         >>> from datetime import datetime
-        >>> date = pl.date_range(datetime(2001, 1, 1), datetime(2001, 1, 3), "1d")
+        >>> date = pl.date_range(
+        ...     datetime(2001, 1, 1), datetime(2001, 1, 3), "1d", eager=True
+        ... )
         >>> date
         shape: (3,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2001-01-01 00:00:00
                 2001-01-02 00:00:00
                 2001-01-03 00:00:00
         ]
         >>> date.dt.max()
         datetime.datetime(2001, 1, 3, 0, 0)
@@ -78,18 +80,20 @@
     def median(self) -> dt.date | dt.datetime | dt.timedelta | None:
         """
         Return median as python DateTime.
 
         Examples
         --------
         >>> from datetime import datetime
-        >>> date = pl.date_range(datetime(2001, 1, 1), datetime(2001, 1, 3), "1d")
+        >>> date = pl.date_range(
+        ...     datetime(2001, 1, 1), datetime(2001, 1, 3), "1d", eager=True
+        ... )
         >>> date
         shape: (3,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2001-01-01 00:00:00
                 2001-01-02 00:00:00
                 2001-01-03 00:00:00
         ]
         >>> date.dt.median()
         datetime.datetime(2001, 1, 2, 0, 0)
@@ -97,28 +101,30 @@
         """
         s = wrap_s(self._s)
         out = s.median()
         if out is not None:
             if s.dtype == Date:
                 return _to_python_date(int(out))
             else:
-                return _to_python_datetime(int(out), s.time_unit)
+                return _to_python_datetime(int(out), s._s.time_unit())
         return None
 
     def mean(self) -> dt.date | dt.datetime | None:
         """
         Return mean as python DateTime.
 
         Examples
         --------
         >>> from datetime import datetime
-        >>> date = pl.date_range(datetime(2001, 1, 1), datetime(2001, 1, 3), "1d")
+        >>> date = pl.date_range(
+        ...     datetime(2001, 1, 1), datetime(2001, 1, 3), "1d", eager=True
+        ... )
         >>> date
         shape: (3,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2001-01-01 00:00:00
                 2001-01-02 00:00:00
                 2001-01-03 00:00:00
         ]
         >>> date.dt.mean()
         datetime.datetime(2001, 1, 2, 0, 0)
@@ -126,61 +132,88 @@
         """
         s = wrap_s(self._s)
         out = s.mean()
         if out is not None:
             if s.dtype == Date:
                 return _to_python_date(int(out))
             else:
-                return _to_python_datetime(int(out), s.time_unit)
+                return _to_python_datetime(int(out), s._s.time_unit())
         return None
 
-    @deprecated_alias(fmt="format")
-    def strftime(self, format: str) -> Series:
+    def to_string(self, format: str) -> Series:
         """
-        Format Date/datetime with a formatting rule.
+        Convert a Date/Time/Datetime column into a Utf8 column with the given format.
+
+        Similar to ``cast(pl.Utf8)``, but this method allows you to customize the
+        formatting of the resulting string.
 
         Parameters
         ----------
         format
             Format to use, refer to the `chrono strftime documentation
             <https://docs.rs/chrono/latest/chrono/format/strftime/index.html>`_
             for specification. Example: ``"%y-%m-%d"``.
 
-        Returns
-        -------
-        Utf8 Series
-
         Examples
         --------
         >>> from datetime import datetime
-        >>> start = datetime(2001, 1, 1)
-        >>> stop = datetime(2001, 1, 4)
-        >>> date = pl.date_range(start, stop, interval="1d")
-        >>> date
-        shape: (4,)
-        Series: '' [datetime[s]]
+        >>> s = pl.Series(
+        ...     "datetime",
+        ...     [datetime(2020, 3, 1), datetime(2020, 4, 1), datetime(2020, 5, 1)],
+        ... )
+        >>> s.dt.to_string("%Y/%m/%d")
+        shape: (3,)
+        Series: 'datetime' [str]
         [
-                2001-01-01 00:00:00
-                2001-01-02 00:00:00
-                2001-01-03 00:00:00
-                2001-01-04 00:00:00
+                "2020/03/01"
+                "2020/04/01"
+                "2020/05/01"
         ]
-        >>> date.dt.strftime(format="%Y-%m-%d")
-        shape: (4,)
-        Series: '' [str]
+
+        """
+
+    @deprecated_alias(fmt="format")
+    def strftime(self, format: str) -> Series:
+        """
+        Convert a Date/Time/Datetime column into a Utf8 column with the given format.
+
+        Similar to ``cast(pl.Utf8)``, but this method allows you to customize the
+        formatting of the resulting string.
+
+        Alias for :func:`to_string`.
+
+        Parameters
+        ----------
+        format
+            Format to use, refer to the `chrono strftime documentation
+            <https://docs.rs/chrono/latest/chrono/format/strftime/index.html>`_
+            for specification. Example: ``"%y-%m-%d"``.
+
+        Examples
+        --------
+        >>> from datetime import datetime
+        >>> s = pl.Series(
+        ...     "datetime",
+        ...     [datetime(2020, 3, 1), datetime(2020, 4, 1), datetime(2020, 5, 1)],
+        ... )
+        >>> s.dt.strftime("%Y/%m/%d")
+        shape: (3,)
+        Series: 'datetime' [str]
         [
-                "2001-01-01"
-                "2001-01-02"
-                "2001-01-03"
-                "2001-01-04"
+                "2020/03/01"
+                "2020/04/01"
+                "2020/05/01"
         ]
 
+        See Also
+        --------
+        to_string : The identical Series method for which ``strftime`` is an alias.
+
         """
-        s = wrap_s(self._s)
-        return s.to_frame().select(F.col(s.name).dt.strftime(format)).to_series()
+        return self.to_string(format)
 
     def year(self) -> Series:
         """
         Extract the year from the underlying date representation.
 
         Applies to Date and Datetime columns.
 
@@ -191,25 +224,25 @@
         Year part as Int32
 
         Examples
         --------
         >>> from datetime import datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2002, 1, 1)
-        >>> date = pl.date_range(start, stop, interval="1y")
+        >>> date = pl.date_range(start, stop, interval="1y", eager=True)
         >>> date
         shape: (2,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2001-01-01 00:00:00
                 2002-01-01 00:00:00
         ]
         >>> date.dt.year()
         shape: (2,)
-        Series: '' [i32]
+        Series: 'date' [i32]
         [
                 2001
                 2002
         ]
 
         """
 
@@ -224,26 +257,26 @@
         Leap year info as Boolean
 
         Examples
         --------
         >>> from datetime import datetime
         >>> start = datetime(2000, 1, 1)
         >>> stop = datetime(2002, 1, 1)
-        >>> date = pl.date_range(start, stop, interval="1y")
+        >>> date = pl.date_range(start, stop, interval="1y", eager=True)
         >>> date
         shape: (3,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2000-01-01 00:00:00
                 2001-01-01 00:00:00
                 2002-01-01 00:00:00
         ]
         >>> date.dt.is_leap_year()
         shape: (3,)
-        Series: '' [bool]
+        Series: 'date' [bool]
         [
                 true
                 false
                 false
         ]
 
         """
@@ -287,27 +320,27 @@
         Quarter as UInt32
 
         Examples
         --------
         >>> from datetime import datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 4, 1)
-        >>> date = pl.date_range(start, stop, interval="1mo")
+        >>> date = pl.date_range(start, stop, interval="1mo", eager=True)
         >>> date
         shape: (4,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2001-01-01 00:00:00
                 2001-02-01 00:00:00
                 2001-03-01 00:00:00
                 2001-04-01 00:00:00
         ]
         >>> date.dt.quarter()
         shape: (4,)
-        Series: '' [u32]
+        Series: 'date' [u32]
         [
                 1
                 1
                 1
                 2
         ]
 
@@ -327,27 +360,27 @@
         Month part as UInt32
 
         Examples
         --------
         >>> from datetime import datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 4, 1)
-        >>> date = pl.date_range(start, stop, interval="1mo")
+        >>> date = pl.date_range(start, stop, interval="1mo", eager=True)
         >>> date
         shape: (4,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2001-01-01 00:00:00
                 2001-02-01 00:00:00
                 2001-03-01 00:00:00
                 2001-04-01 00:00:00
         ]
         >>> date.dt.month()
         shape: (4,)
-        Series: '' [u32]
+        Series: 'date' [u32]
         [
                 1
                 2
                 3
                 4
         ]
 
@@ -367,27 +400,27 @@
         Week number as UInt32
 
         Examples
         --------
         >>> from datetime import datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 4, 1)
-        >>> date = pl.date_range(start, stop, interval="1mo")
+        >>> date = pl.date_range(start, stop, interval="1mo", eager=True)
         >>> date
         shape: (4,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2001-01-01 00:00:00
                 2001-02-01 00:00:00
                 2001-03-01 00:00:00
                 2001-04-01 00:00:00
         ]
         >>> date.dt.week()
         shape: (4,)
-        Series: '' [u32]
+        Series: 'date' [u32]
         [
                 1
                 5
                 9
                 13
         ]
 
@@ -406,30 +439,30 @@
         Weekday as UInt32
 
         Examples
         --------
         >>> from datetime import datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 7)
-        >>> date = pl.date_range(start, stop, interval="1d")
+        >>> date = pl.date_range(start, stop, interval="1d", eager=True)
         >>> date
         shape: (7,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2001-01-01 00:00:00
                 2001-01-02 00:00:00
                 2001-01-03 00:00:00
                 2001-01-04 00:00:00
                 2001-01-05 00:00:00
                 2001-01-06 00:00:00
                 2001-01-07 00:00:00
         ]
         >>> date.dt.weekday()
         shape: (7,)
-        Series: '' [u32]
+        Series: 'date' [u32]
         [
                 1
                 2
                 3
                 4
                 5
                 6
@@ -452,28 +485,28 @@
         Day part as UInt32
 
         Examples
         --------
         >>> from datetime import datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 9)
-        >>> date = pl.date_range(start, stop, interval="2d")
+        >>> date = pl.date_range(start, stop, interval="2d", eager=True)
         >>> date
         shape: (5,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2001-01-01 00:00:00
                 2001-01-03 00:00:00
                 2001-01-05 00:00:00
                 2001-01-07 00:00:00
                 2001-01-09 00:00:00
         ]
         >>> date.dt.day()
         shape: (5,)
-        Series: '' [u32]
+        Series: 'date' [u32]
         [
                 1
                 3
                 5
                 7
                 9
         ]
@@ -494,26 +527,26 @@
         Ordinal day as UInt32
 
         Examples
         --------
         >>> from datetime import datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 3, 1)
-        >>> date = pl.date_range(start, stop, interval="1mo")
+        >>> date = pl.date_range(start, stop, interval="1mo", eager=True)
         >>> date
         shape: (3,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2001-01-01 00:00:00
                 2001-02-01 00:00:00
                 2001-03-01 00:00:00
         ]
         >>> date.dt.ordinal_day()
         shape: (3,)
-        Series: '' [u32]
+        Series: 'date' [u32]
         [
                 1
                 32
                 60
         ]
 
         """
@@ -621,27 +654,27 @@
         Hour part as UInt32
 
         Examples
         --------
         >>> from datetime import datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 1, 3)
-        >>> date = pl.date_range(start, stop, interval="1h")
+        >>> date = pl.date_range(start, stop, interval="1h", eager=True)
         >>> date
         shape: (4,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2001-01-01 00:00:00
                 2001-01-01 01:00:00
                 2001-01-01 02:00:00
                 2001-01-01 03:00:00
         ]
         >>> date.dt.hour()
         shape: (4,)
-        Series: '' [u32]
+        Series: 'date' [u32]
         [
                 0
                 1
                 2
                 3
         ]
 
@@ -660,26 +693,26 @@
         Minute part as UInt32
 
         Examples
         --------
         >>> from datetime import datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 1, 0, 4, 0)
-        >>> date = pl.date_range(start, stop, interval="2m")
+        >>> date = pl.date_range(start, stop, interval="2m", eager=True)
         >>> date
         shape: (3,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2001-01-01 00:00:00
                 2001-01-01 00:02:00
                 2001-01-01 00:04:00
         ]
         >>> date.dt.minute()
         shape: (3,)
-        Series: '' [u32]
+        Series: 'date' [u32]
         [
                 0
                 2
                 4
         ]
 
         """
@@ -704,46 +737,46 @@
         Second part as UInt32 (or Float64)
 
         Examples
         --------
         >>> from datetime import datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 1, 0, 0, 4)
-        >>> date = pl.date_range(start, stop, interval="500ms")
+        >>> date = pl.date_range(start, stop, interval="500ms", eager=True)
         >>> date
         shape: (9,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2001-01-01 00:00:00
                 2001-01-01 00:00:00.500
                 2001-01-01 00:00:01
                 2001-01-01 00:00:01.500
                 2001-01-01 00:00:02
                 2001-01-01 00:00:02.500
                 2001-01-01 00:00:03
                 2001-01-01 00:00:03.500
                 2001-01-01 00:00:04
         ]
         >>> date.dt.second()
         shape: (9,)
-        Series: '' [u32]
+        Series: 'date' [u32]
         [
                 0
                 0
                 1
                 1
                 2
                 2
                 3
                 3
                 4
         ]
         >>> date.dt.second(fractional=True)
         shape: (9,)
-        Series: '' [f64]
+        Series: 'date' [f64]
         [
                 0.0
                 0.5
                 1.0
                 1.5
                 2.0
                 2.5
@@ -765,32 +798,32 @@
         Millisecond part as UInt32
 
         Examples
         --------
         >>> from datetime import datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 1, 0, 0, 4)
-        >>> date = pl.date_range(start, stop, interval="500ms")
+        >>> date = pl.date_range(start, stop, interval="500ms", eager=True)
         >>> date
         shape: (9,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2001-01-01 00:00:00
                 2001-01-01 00:00:00.500
                 2001-01-01 00:00:01
                 2001-01-01 00:00:01.500
                 2001-01-01 00:00:02
                 2001-01-01 00:00:02.500
                 2001-01-01 00:00:03
                 2001-01-01 00:00:03.500
                 2001-01-01 00:00:04
         ]
         >>> date.dt.millisecond()
         shape: (9,)
-        Series: '' [u32]
+        Series: 'date' [u32]
         [
                 0
                 500
                 0
                 500
                 0
                 500
@@ -812,32 +845,32 @@
         Microsecond part as UInt32
 
         Examples
         --------
         >>> from datetime import datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 1, 0, 0, 4)
-        >>> date = pl.date_range(start, stop, interval="500ms")
+        >>> date = pl.date_range(start, stop, interval="500ms", eager=True)
         >>> date
         shape: (9,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2001-01-01 00:00:00
                 2001-01-01 00:00:00.500
                 2001-01-01 00:00:01
                 2001-01-01 00:00:01.500
                 2001-01-01 00:00:02
                 2001-01-01 00:00:02.500
                 2001-01-01 00:00:03
                 2001-01-01 00:00:03.500
                 2001-01-01 00:00:04
         ]
         >>> date.dt.microsecond()
         shape: (9,)
-        Series: '' [u32]
+        Series: 'date' [u32]
         [
                 0
                 500000
                 0
                 500000
                 0
                 500000
@@ -859,32 +892,32 @@
         Nanosecond part as UInt32
 
         Examples
         --------
         >>> from datetime import datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 1, 0, 0, 4)
-        >>> date = pl.date_range(start, stop, interval="500ms")
+        >>> date = pl.date_range(start, stop, interval="500ms", eager=True)
         >>> date
         shape: (9,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2001-01-01 00:00:00
                 2001-01-01 00:00:00.500
                 2001-01-01 00:00:01
                 2001-01-01 00:00:01.500
                 2001-01-01 00:00:02
                 2001-01-01 00:00:02.500
                 2001-01-01 00:00:03
                 2001-01-01 00:00:03.500
                 2001-01-01 00:00:04
         ]
         >>> date.dt.nanosecond()
         shape: (9,)
-        Series: '' [u32]
+        Series: 'date' [u32]
         [
                 0
                 500000000
                 0
                 500000000
                 0
                 500000000
@@ -905,18 +938,18 @@
             Time unit.
 
         Examples
         --------
         >>> from datetime import datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 3)
-        >>> date = pl.date_range(start, stop, interval="1d")
+        >>> date = pl.date_range(start, stop, interval="1d", eager=True)
         >>> date
         shape: (3,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2001-01-01 00:00:00
                 2001-01-02 00:00:00
                 2001-01-03 00:00:00
         ]
         >>> date.dt.timestamp().alias("timestamp_us")
         shape: (3,)
@@ -947,18 +980,18 @@
             Unit of time.
 
         Examples
         --------
         >>> from datetime import datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 3)
-        >>> date = pl.date_range(start, stop, interval="1d")
+        >>> date = pl.date_range(start, stop, interval="1d", eager=True)
         >>> date
         shape: (3,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2001-01-01 00:00:00
                 2001-01-02 00:00:00
                 2001-01-03 00:00:00
         ]
         >>> date.dt.epoch().alias("epoch_ns")
         shape: (3,)
@@ -992,18 +1025,18 @@
             Unit of time for the ``Datetime`` Series.
 
         Examples
         --------
         >>> from datetime import datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 3)
-        >>> date = pl.date_range(start, stop, "1d", time_unit="ns")
+        >>> date = pl.date_range(start, stop, "1d", time_unit="ns", eager=True)
         >>> date
         shape: (3,)
-        Series: '' [datetime[ns]]
+        Series: 'date' [datetime[ns]]
         [
                 2001-01-01 00:00:00
                 2001-01-02 00:00:00
                 2001-01-03 00:00:00
         ]
         >>> date.dt.with_time_unit("us").alias("time_unit_us")
         shape: (3,)
@@ -1026,18 +1059,18 @@
             Unit of time for the ``Datetime`` Series.
 
         Examples
         --------
         >>> from datetime import datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 3)
-        >>> date = pl.date_range(start, stop, "1d")
+        >>> date = pl.date_range(start, stop, "1d", eager=True)
         >>> date
         shape: (3,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2001-01-01 00:00:00
                 2001-01-02 00:00:00
                 2001-01-03 00:00:00
         ]
         >>> date.dt.cast_time_unit("ms").alias("time_unit_ms")
         shape: (3,)
@@ -1068,18 +1101,18 @@
             Time zone for the `Datetime` Series.
 
         Examples
         --------
         >>> from datetime import datetime
         >>> start = datetime(2020, 3, 1)
         >>> stop = datetime(2020, 5, 1)
-        >>> date = pl.date_range(start, stop, "1mo", time_zone="UTC")
+        >>> date = pl.date_range(start, stop, "1mo", time_zone="UTC", eager=True)
         >>> date
         shape: (3,)
-        Series: '' [datetime[s, UTC]]
+        Series: 'date' [datetime[s, UTC]]
         [
                 2020-03-01 00:00:00 UTC
                 2020-04-01 00:00:00 UTC
                 2020-05-01 00:00:00 UTC
         ]
         >>> date = date.dt.convert_time_zone("Europe/London").alias("London")
         >>> date
@@ -1116,14 +1149,15 @@
         >>> df = pl.DataFrame(
         ...     {
         ...         "london_timezone": pl.date_range(
         ...             datetime(2020, 3, 1),
         ...             datetime(2020, 7, 1),
         ...             "1mo",
         ...             time_zone="UTC",
+        ...             eager=True,
         ...         ).dt.convert_time_zone(time_zone="Europe/London"),
         ...     }
         ... )
         >>> df.select(
         ...     [
         ...         pl.col("london_timezone"),
         ...         pl.col("london_timezone")
@@ -1194,26 +1228,28 @@
         Returns
         -------
         A series of dtype Int64
 
         Examples
         --------
         >>> from datetime import datetime
-        >>> date = pl.date_range(datetime(2020, 3, 1), datetime(2020, 5, 1), "1mo")
+        >>> date = pl.date_range(
+        ...     datetime(2020, 3, 1), datetime(2020, 5, 1), "1mo", eager=True
+        ... )
         >>> date
         shape: (3,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2020-03-01 00:00:00
                 2020-04-01 00:00:00
                 2020-05-01 00:00:00
         ]
         >>> date.diff().dt.days()
         shape: (3,)
-        Series: '' [i64]
+        Series: 'date' [i64]
         [
                 null
                 31
                 30
         ]
 
         """
@@ -1225,27 +1261,29 @@
         Returns
         -------
         A series of dtype Int64
 
         Examples
         --------
         >>> from datetime import datetime
-        >>> date = pl.date_range(datetime(2020, 1, 1), datetime(2020, 1, 4), "1d")
+        >>> date = pl.date_range(
+        ...     datetime(2020, 1, 1), datetime(2020, 1, 4), "1d", eager=True
+        ... )
         >>> date
         shape: (4,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2020-01-01 00:00:00
                 2020-01-02 00:00:00
                 2020-01-03 00:00:00
                 2020-01-04 00:00:00
         ]
         >>> date.diff().dt.hours()
         shape: (4,)
-        Series: '' [i64]
+        Series: 'date' [i64]
         [
                 null
                 24
                 24
                 24
         ]
 
@@ -1258,27 +1296,29 @@
         Returns
         -------
         A series of dtype Int64
 
         Examples
         --------
         >>> from datetime import datetime
-        >>> date = pl.date_range(datetime(2020, 1, 1), datetime(2020, 1, 4), "1d")
+        >>> date = pl.date_range(
+        ...     datetime(2020, 1, 1), datetime(2020, 1, 4), "1d", eager=True
+        ... )
         >>> date
         shape: (4,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2020-01-01 00:00:00
                 2020-01-02 00:00:00
                 2020-01-03 00:00:00
                 2020-01-04 00:00:00
         ]
         >>> date.diff().dt.minutes()
         shape: (4,)
-        Series: '' [i64]
+        Series: 'date' [i64]
         [
                 null
                 1440
                 1440
                 1440
         ]
 
@@ -1292,29 +1332,29 @@
         -------
         A series of dtype Int64
 
         Examples
         --------
         >>> from datetime import datetime
         >>> date = pl.date_range(
-        ...     datetime(2020, 1, 1), datetime(2020, 1, 1, 0, 4, 0), "1m"
+        ...     datetime(2020, 1, 1), datetime(2020, 1, 1, 0, 4, 0), "1m", eager=True
         ... )
         >>> date
         shape: (5,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2020-01-01 00:00:00
                 2020-01-01 00:01:00
                 2020-01-01 00:02:00
                 2020-01-01 00:03:00
                 2020-01-01 00:04:00
         ]
         >>> date.diff().dt.seconds()
         shape: (5,)
-        Series: '' [i64]
+        Series: 'date' [i64]
         [
                 null
                 60
                 60
                 60
                 60
         ]
@@ -1329,27 +1369,30 @@
         -------
         A series of dtype Int64
 
         Examples
         --------
         >>> from datetime import datetime
         >>> date = pl.date_range(
-        ...     datetime(2020, 1, 1), datetime(2020, 1, 1, 0, 0, 1, 0), "1ms"
+        ...     datetime(2020, 1, 1),
+        ...     datetime(2020, 1, 1, 0, 0, 1, 0),
+        ...     "1ms",
+        ...     eager=True,
         ... )[:3]
         >>> date
         shape: (3,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2020-01-01 00:00:00
                 2020-01-01 00:00:00.001
                 2020-01-01 00:00:00.002
         ]
         >>> date.diff().dt.milliseconds()
         shape: (3,)
-        Series: '' [i64]
+        Series: 'date' [i64]
         [
                 null
                 1
                 1
         ]
 
         """
@@ -1362,27 +1405,30 @@
         -------
         A series of dtype Int64
 
         Examples
         --------
         >>> from datetime import datetime
         >>> date = pl.date_range(
-        ...     datetime(2020, 1, 1), datetime(2020, 1, 1, 0, 0, 1, 0), "1ms"
+        ...     datetime(2020, 1, 1),
+        ...     datetime(2020, 1, 1, 0, 0, 1, 0),
+        ...     "1ms",
+        ...     eager=True,
         ... )[:3]
         >>> date
         shape: (3,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2020-01-01 00:00:00
                 2020-01-01 00:00:00.001
                 2020-01-01 00:00:00.002
         ]
         >>> date.diff().dt.microseconds()
         shape: (3,)
-        Series: '' [i64]
+        Series: 'date' [i64]
         [
                 null
                 1000
                 1000
         ]
 
         """
@@ -1395,27 +1441,30 @@
         -------
         A series of dtype Int64
 
         Examples
         --------
         >>> from datetime import datetime
         >>> date = pl.date_range(
-        ...     datetime(2020, 1, 1), datetime(2020, 1, 1, 0, 0, 1, 0), "1ms"
+        ...     datetime(2020, 1, 1),
+        ...     datetime(2020, 1, 1, 0, 0, 1, 0),
+        ...     "1ms",
+        ...     eager=True,
         ... )[:3]
         >>> date
         shape: (3,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2020-01-01 00:00:00
                 2020-01-01 00:00:00.001
                 2020-01-01 00:00:00.002
         ]
         >>> date.diff().dt.nanoseconds()
         shape: (3,)
-        Series: '' [i64]
+        Series: 'date' [i64]
         [
                 null
                 1000000
                 1000000
         ]
 
         """
@@ -1452,18 +1501,20 @@
         Returns
         -------
         Date/Datetime expression
 
         Examples
         --------
         >>> from datetime import datetime
-        >>> dates = pl.date_range(datetime(2000, 1, 1), datetime(2005, 1, 1), "1y")
+        >>> dates = pl.date_range(
+        ...     datetime(2000, 1, 1), datetime(2005, 1, 1), "1y", eager=True
+        ... )
         >>> dates
         shape: (6,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2000-01-01 00:00:00
                 2001-01-01 00:00:00
                 2002-01-01 00:00:00
                 2003-01-01 00:00:00
                 2004-01-01 00:00:00
                 2005-01-01 00:00:00
@@ -1491,15 +1542,15 @@
                 2003-11-01 00:00:00
         ]
 
         To get to the end of each month, combine with `truncate`:
 
         >>> dates.dt.truncate("1mo").dt.offset_by("1mo").dt.offset_by("-1d")
         shape: (6,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2000-01-31 00:00:00
                 2001-01-31 00:00:00
                 2002-01-31 00:00:00
                 2003-01-31 00:00:00
                 2004-01-31 00:00:00
                 2005-01-31 00:00:00
@@ -1552,32 +1603,32 @@
         Date/Datetime series
 
         Examples
         --------
         >>> from datetime import timedelta, datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 2)
-        >>> s = pl.date_range(start, stop, timedelta(minutes=165), name="dates")
+        >>> s = pl.date_range(start, stop, timedelta(minutes=165), eager=True)
         >>> s
         shape: (9,)
-        Series: 'dates' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
             2001-01-01 00:00:00
             2001-01-01 02:45:00
             2001-01-01 05:30:00
             2001-01-01 08:15:00
             2001-01-01 11:00:00
             2001-01-01 13:45:00
             2001-01-01 16:30:00
             2001-01-01 19:15:00
             2001-01-01 22:00:00
         ]
         >>> s.dt.truncate("1h")
         shape: (9,)
-        Series: 'dates' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
             2001-01-01 00:00:00
             2001-01-01 02:00:00
             2001-01-01 05:00:00
             2001-01-01 08:00:00
             2001-01-01 11:00:00
             2001-01-01 13:00:00
@@ -1586,30 +1637,30 @@
             2001-01-01 22:00:00
         ]
         >>> s.dt.truncate("1h").series_equal(s.dt.truncate(timedelta(hours=1)))
         True
 
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 1, 1)
-        >>> s = pl.date_range(start, stop, "10m", name="dates")
+        >>> s = pl.date_range(start, stop, "10m", eager=True)
         >>> s
         shape: (7,)
-        Series: 'dates' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2001-01-01 00:00:00
                 2001-01-01 00:10:00
                 2001-01-01 00:20:00
                 2001-01-01 00:30:00
                 2001-01-01 00:40:00
                 2001-01-01 00:50:00
                 2001-01-01 01:00:00
         ]
         >>> s.dt.truncate("30m")
         shape: (7,)
-        Series: 'dates' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2001-01-01 00:00:00
                 2001-01-01 00:00:00
                 2001-01-01 00:00:00
                 2001-01-01 00:30:00
                 2001-01-01 00:30:00
                 2001-01-01 00:30:00
@@ -1668,32 +1719,32 @@
         change without it being considered a breaking change.
 
         Examples
         --------
         >>> from datetime import timedelta, datetime
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 2)
-        >>> s = pl.date_range(start, stop, timedelta(minutes=165), name="dates")
+        >>> s = pl.date_range(start, stop, timedelta(minutes=165), eager=True)
         >>> s
         shape: (9,)
-        Series: 'dates' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
             2001-01-01 00:00:00
             2001-01-01 02:45:00
             2001-01-01 05:30:00
             2001-01-01 08:15:00
             2001-01-01 11:00:00
             2001-01-01 13:45:00
             2001-01-01 16:30:00
             2001-01-01 19:15:00
             2001-01-01 22:00:00
         ]
         >>> s.dt.round("1h")
         shape: (9,)
-        Series: 'dates' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
             2001-01-01 00:00:00
             2001-01-01 03:00:00
             2001-01-01 06:00:00
             2001-01-01 08:00:00
             2001-01-01 11:00:00
             2001-01-01 14:00:00
@@ -1702,18 +1753,18 @@
             2001-01-01 22:00:00
         ]
         >>> s.dt.round("1h").series_equal(s.dt.round(timedelta(hours=1)))
         True
 
         >>> start = datetime(2001, 1, 1)
         >>> stop = datetime(2001, 1, 1, 1)
-        >>> s = pl.date_range(start, stop, "10m", name="dates")
+        >>> s = pl.date_range(start, stop, "10m", eager=True)
         >>> s.dt.round("30m")
         shape: (7,)
-        Series: 'dates' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2001-01-01 00:00:00
                 2001-01-01 00:00:00
                 2001-01-01 00:30:00
                 2001-01-01 00:30:00
                 2001-01-01 00:30:00
                 2001-01-01 01:00:00
@@ -1765,18 +1816,20 @@
         -----
         If you're coming from pandas, you can think of this as a vectorised version
         of ``pandas.tseries.offsets.MonthBegin().rollback(datetime)``.
 
         Examples
         --------
         >>> from datetime import datetime
-        >>> s = pl.date_range(datetime(2000, 1, 2, 2), datetime(2000, 4, 2, 2), "1mo")
+        >>> s = pl.date_range(
+        ...     datetime(2000, 1, 2, 2), datetime(2000, 4, 2, 2), "1mo", eager=True
+        ... )
         >>> s.dt.month_start()
         shape: (4,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2000-01-01 02:00:00
                 2000-02-01 02:00:00
                 2000-03-01 02:00:00
                 2000-04-01 02:00:00
         ]
         """
@@ -1793,18 +1846,20 @@
         -----
         If you're coming from pandas, you can think of this as a vectorised version
         of ``pandas.tseries.offsets.MonthEnd().rollforward(datetime)``.
 
         Examples
         --------
         >>> from datetime import datetime
-        >>> s = pl.date_range(datetime(2000, 1, 2, 2), datetime(2000, 4, 2, 2), "1mo")
+        >>> s = pl.date_range(
+        ...     datetime(2000, 1, 2, 2), datetime(2000, 4, 2, 2), "1mo", eager=True
+        ... )
         >>> s.dt.month_end()
         shape: (4,)
-        Series: '' [datetime[s]]
+        Series: 'date' [datetime[s]]
         [
                 2000-01-31 02:00:00
                 2000-02-29 02:00:00
                 2000-03-31 02:00:00
                 2000-04-30 02:00:00
         ]
         """
```

### Comparing `polars_lts_cpu-0.17.9/polars/series/list.py` & `polars_lts_cpu-0.18.0/polars/series/list.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,41 +1,41 @@
 from __future__ import annotations
 
-from typing import TYPE_CHECKING, Any, Callable
+from typing import TYPE_CHECKING, Any, Callable, Sequence
 
 from polars import functions as F
 from polars.series.utils import expr_dispatch
 from polars.utils._wrap import wrap_s
+from polars.utils.decorators import deprecated_alias
 
 if TYPE_CHECKING:
     from datetime import date, datetime, time
 
-    from polars.expr.expr import Expr
+    from polars import Expr, Series
     from polars.polars import PySeries
-    from polars.series import Series
     from polars.type_aliases import NullBehavior, ToStructStrategy
 
 
 @expr_dispatch
 class ListNameSpace:
-    """Series.arr namespace."""
+    """Namespace for list related methods."""
 
-    _accessor = "arr"
+    _accessor = "list"
 
     def __init__(self, series: Series):
         self._s: PySeries = series._s
 
     def lengths(self) -> Series:
         """
         Get the length of the arrays as UInt32.
 
         Examples
         --------
         >>> s = pl.Series([[1, 2, 3], [5]])
-        >>> s.arr.lengths()
+        >>> s.list.lengths()
         shape: (2,)
         Series: '' [u32]
         [
             3
             1
         ]
 
@@ -61,22 +61,22 @@
         ----------
         descending
             Sort in descending order.
 
         Examples
         --------
         >>> s = pl.Series("a", [[3, 2, 1], [9, 1, 2]])
-        >>> s.arr.sort()
+        >>> s.list.sort()
         shape: (2,)
         Series: 'a' [list[i64]]
         [
                 [1, 2, 3]
                 [1, 2, 9]
         ]
-        >>> s.arr.sort(descending=True)
+        >>> s.list.sort(descending=True)
         shape: (2,)
         Series: 'a' [list[i64]]
         [
                 [3, 2, 1]
                 [9, 2, 1]
         ]
 
@@ -160,15 +160,15 @@
         Returns
         -------
         Series of dtype Utf8
 
         Examples
         --------
         >>> s = pl.Series([["foo", "bar"], ["hello", "world"]])
-        >>> s.arr.join(separator="-")
+        >>> s.list.join(separator="-")
         shape: (2,)
         Series: '' [str]
         [
             "foo-bar"
             "hello-world"
         ]
 
@@ -225,31 +225,31 @@
             Number of slots to shift.
         null_behavior : {'ignore', 'drop'}
             How to handle null values.
 
         Examples
         --------
         >>> s = pl.Series("a", [[1, 2, 3, 4], [10, 2, 1]])
-        >>> s.arr.diff()
+        >>> s.list.diff()
         shape: (2,)
         Series: 'a' [list[i64]]
         [
             [null, 1,  1]
             [null, -8, -1]
         ]
 
-        >>> s.arr.diff(n=2)
+        >>> s.list.diff(n=2)
         shape: (2,)
         Series: 'a' [list[i64]]
         [
             [null, null,  2]
             [null, null, -9]
         ]
 
-        >>> s.arr.diff(n=2, null_behavior="drop")
+        >>> s.list.diff(n=2, null_behavior="drop")
         shape: (2,)
         Series: 'a' [list[i64]]
         [
             [2, 2]
             [-9]
         ]
 
@@ -263,84 +263,84 @@
         ----------
         periods
             Number of places to shift (may be negative).
 
         Examples
         --------
         >>> s = pl.Series("a", [[1, 2, 3, 4], [10, 2, 1]])
-        >>> s.arr.shift()
+        >>> s.list.shift()
         shape: (2,)
         Series: 'a' [list[i64]]
         [
             [null, 1,  3]
             [null, 10, 2]
         ]
 
         """
 
-    def slice(self, offset: int, length: int | None = None) -> Series:
+    def slice(self, offset: int | Expr, length: int | Expr | None = None) -> Series:
         """
         Slice every sublist.
 
         Parameters
         ----------
         offset
             Start index. Negative indexing is supported.
         length
             Length of the slice. If set to ``None`` (default), the slice is taken to the
             end of the list.
 
         Examples
         --------
         >>> s = pl.Series("a", [[1, 2, 3, 4], [10, 2, 1]])
-        >>> s.arr.slice(1, 2)
+        >>> s.list.slice(1, 2)
         shape: (2,)
         Series: 'a' [list[i64]]
         [
             [2, 3]
             [2, 1]
         ]
 
         """
 
-    def head(self, n: int = 5) -> Series:
+    def head(self, n: int | Expr = 5) -> Series:
         """
         Slice the first `n` values of every sublist.
 
         Parameters
         ----------
         n
             Number of values to return for each sublist.
 
         Examples
         --------
         >>> s = pl.Series("a", [[1, 2, 3, 4], [10, 2, 1]])
-        >>> s.arr.head(2)
+        >>> s.list.head(2)
         shape: (2,)
         Series: 'a' [list[i64]]
         [
             [1, 2]
             [10, 2]
         ]
 
         """
 
-    def tail(self, n: int = 5) -> Series:
+    def tail(self, n: int | Expr = 5) -> Series:
         """
         Slice the last `n` values of every sublist.
 
         Parameters
         ----------
         n
             Number of values to return for each sublist.
 
         Examples
         --------
         >>> s = pl.Series("a", [[1, 2, 3, 4], [10, 2, 1]])
-        >>> s.arr.tail(2)
+        >>> s.list.tail(2)
         shape: (2,)
         Series: 'a' [list[i64]]
         [
             [3, 4]
             [2, 1]
         ]
 
@@ -357,15 +357,15 @@
         See Also
         --------
         Series.reshape : Reshape this Series to a flat Series or a Series of Lists.
 
         Examples
         --------
         >>> s = pl.Series("a", [[1, 2, 3], [4, 5, 6]])
-        >>> s.arr.explode()
+        >>> s.list.explode()
         shape: (6,)
         Series: 'a' [i64]
         [
             1
             2
             3
             4
@@ -384,65 +384,84 @@
         Parameters
         ----------
         element
             An expression that produces a single value
 
         """
 
+    @deprecated_alias(name_generator="fields")
     def to_struct(
         self,
         n_field_strategy: ToStructStrategy = "first_non_null",
-        name_generator: Callable[[int], str] | None = None,
+        fields: Callable[[int], str] | Sequence[str] | None = None,
     ) -> Series:
         """
         Convert the series of type ``List`` to a series of type ``Struct``.
 
         Parameters
         ----------
         n_field_strategy : {'first_non_null', 'max_width'}
             Strategy to determine the number of fields of the struct.
-            'first_non_null': set number of fields to the length of the first
-            non-zero-length sublist.
-            'max_width': set number of fields as max length of all sublists.
-        name_generator
-            A custom function that can be used to generate the field names.
-            Default field names are `field_0, field_1 .. field_n`
+
+            * "first_non_null": set number of fields equal to the length of the
+              first non zero-length sublist.
+            * "max_width": set number of fields as max length of all sublists.
+
+        fields
+            If the name and number of the desired fields is known in advance
+            a list of field names can be given, which will be assigned by index.
+            Otherwise, to dynamically assign field names, a custom function can be
+            used; if neither are set, fields will be `field_0, field_1 .. field_n`.
 
         Examples
         --------
-        >>> df = pl.DataFrame({"a": [[1, 2, 3], [1, 2]]})
-        >>> df.select([pl.col("a").arr.to_struct()])
-        shape: (2, 1)
-        
-         a          
-         ---        
-         struct[3]  
-        
-         {1,2,3}    
-         {1,2,null} 
-        
-        >>> df.select(
-        ...     [
-        ...         pl.col("a").arr.to_struct(
-        ...             name_generator=lambda idx: f"col_name_{idx}"
-        ...         )
-        ...     ]
-        ... ).to_series().to_list()
-        [{'col_name_0': 1, 'col_name_1': 2, 'col_name_2': 3},
-        {'col_name_0': 1, 'col_name_1': 2, 'col_name_2': None}]
-
-        """
-        # We set the upper bound to 0.
-        # No need to create the proper schema in eager mode.
-        s = wrap_s(self)
+        Convert list to struct with default field name assignment:
+
+        >>> s1 = pl.Series("n", [[0, 1, 2], [0, 1]])
+        >>> s2 = s1.list.to_struct()
+        >>> s2
+        shape: (2,)
+        Series: 'n' [struct[3]]
+        [
+            {0,1,2}
+            {0,1,null}
+        ]
+        >>> s2.struct.fields
+        ['field_0', 'field_1', 'field_2']
+
+        Convert list to struct with field name assignment by function/index:
+
+        >>> s3 = s1.list.to_struct(fields=lambda idx: f"n{idx:02}")
+        >>> s3.struct.fields
+        ['n00', 'n01', 'n02']
+
+        Convert list to struct with field name assignment by index from a list of names:
+
+        >>> s1.list.to_struct(fields=["one", "two", "three"]).struct.unnest()
+        shape: (2, 3)
+        
+         one  two  three 
+         ---  ---  ---   
+         i64  i64  i64   
+        
+         0    1    2     
+         0    1    null  
+        
+
+        """
+        s = wrap_s(self._s)
         return (
             s.to_frame()
             .select(
-                F.col(s.name).arr.to_struct(
-                    n_field_strategy, name_generator, upper_bound=0
+                F.col(s.name).list.to_struct(
+                    # note: in eager mode, 'upper_bound' is always zero, as (unlike
+                    # in lazy mode) there is no need to determine/track the schema.
+                    n_field_strategy,
+                    fields,
+                    upper_bound=0,
                 )
             )
             .to_series()
         )
 
     def eval(self, expr: Expr, *, parallel: bool = False) -> Series:
         """
@@ -460,15 +479,15 @@
             This likely should not be use in the groupby context, because we already
             parallel execution per group
 
         Examples
         --------
         >>> df = pl.DataFrame({"a": [1, 8, 3], "b": [4, 5, 2]})
         >>> df.with_columns(
-        ...     pl.concat_list(["a", "b"]).arr.eval(pl.element().rank()).alias("rank")
+        ...     pl.concat_list(["a", "b"]).list.eval(pl.element().rank()).alias("rank")
         ... )
         shape: (3, 3)
         
          a    b    rank       
          ---  ---  ---        
          i64  i64  list[f32]  
         
```

### Comparing `polars_lts_cpu-0.17.9/polars/series/series.py` & `polars_lts_cpu-0.18.0/polars/series/series.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,39 +1,42 @@
 from __future__ import annotations
 
 import contextlib
 import math
 import os
 import typing
+import warnings
 from datetime import date, datetime, time, timedelta
 from typing import (
     TYPE_CHECKING,
     Any,
     Callable,
     Collection,
+    Generator,
     Iterable,
     NoReturn,
     Sequence,
     Union,
     overload,
 )
 
+import polars._reexport as pl
 from polars import functions as F
-from polars import internals as pli
 from polars.datatypes import (
     FLOAT_DTYPES,
     INTEGER_DTYPES,
     NUMERIC_DTYPES,
     SIGNED_INTEGER_DTYPES,
     TEMPORAL_DTYPES,
     UNSIGNED_INTEGER_DTYPES,
     Boolean,
     Categorical,
     Date,
     Datetime,
+    Decimal,
     Duration,
     Float32,
     Float64,
     Int8,
     Int16,
     Int32,
     Int64,
@@ -59,14 +62,15 @@
     _check_for_pandas,
     _check_for_pyarrow,
 )
 from polars.dependencies import numpy as np
 from polars.dependencies import pandas as pd
 from polars.dependencies import pyarrow as pa
 from polars.exceptions import ShapeError
+from polars.series.array import ArrayNameSpace
 from polars.series.binary import BinaryNameSpace
 from polars.series.categorical import CatNameSpace
 from polars.series.datetime import DateTimeNameSpace
 from polars.series.list import ListNameSpace
 from polars.series.string import StringNameSpace
 from polars.series.struct import StructNameSpace
 from polars.series.utils import expr_dispatch, get_ffi_func
@@ -85,14 +89,15 @@
     _datetime_to_pl_timestamp,
     _time_to_pl_time,
 )
 from polars.utils.decorators import deprecated_alias
 from polars.utils.meta import get_index_type
 from polars.utils.various import (
     _is_generator,
+    find_stacklevel,
     is_int_sequence,
     parse_version,
     range_to_series,
     range_to_slice,
     scale_bytes,
     sphinx_accessor,
 )
@@ -100,16 +105,15 @@
 with contextlib.suppress(ImportError):  # Module not available when building docs
     from polars.polars import PyDataFrame, PySeries
 
 
 if TYPE_CHECKING:
     import sys
 
-    from polars.dataframe import DataFrame
-    from polars.expr.expr import Expr
+    from polars import DataFrame, Expr
     from polars.series._numpy import SeriesView
     from polars.type_aliases import (
         ClosedInterval,
         ComparisonOperator,
         FillNullStrategy,
         InterpolationMethod,
         IntoExpr,
@@ -211,15 +215,15 @@
             2
             3
     ]
 
     """
 
     _s: PySeries = None
-    _accessors: set[str] = {"arr", "cat", "dt", "str", "bin", "struct"}
+    _accessors: set[str] = {"arr", "cat", "dt", "list", "str", "bin", "struct"}
 
     def __init__(
         self,
         name: str | ArrayLike | None = None,
         values: ArrayLike | None = None,
         dtype: PolarsDataType | None = None,
         *,
@@ -329,20 +333,14 @@
         nan_to_null: bool = True,
     ) -> Self:
         """Construct a Series from a pandas Series or DatetimeIndex."""
         return cls._from_pyseries(
             pandas_to_pyseries(name, values, nan_to_null=nan_to_null)
         )
 
-    @classmethod
-    def _repeat(
-        cls, name: str, val: int | float | str | bool, n: int, dtype: PolarsDataType
-    ) -> Self:
-        return cls._from_pyseries(PySeries.repeat(name, val, n, dtype))
-
     def _get_ptr(self) -> int:
         """
         Get a pointer to the start of the values buffer of a numeric Series.
 
         This will raise an error if the
         ``Series`` contains multiple chunks
 
@@ -402,14 +400,20 @@
     def shape(self) -> tuple[int]:
         """Shape of this Series."""
         return (self._s.len(),)
 
     @property
     def time_unit(self) -> TimeUnit | None:
         """Get the time unit of underlying Datetime Series as {"ns", "us", "ms"}."""
+        warnings.warn(
+            "`Series.time_unit` is deprecated and will be removed in a future version,"
+            " please use `Series.dtype.time_unit` instead",
+            category=DeprecationWarning,
+            stacklevel=find_stacklevel(),
+        )
         return self._s.time_unit()
 
     def __bool__(self) -> NoReturn:
         raise ValueError(
             "The truth value of a Series is ambiguous. Hint: use '&' or '|' to chain "
             "Series boolean results together, not and/or; to check if a Series "
             "contains any values, use 'is_empty()'"
@@ -461,15 +465,15 @@
         if self.dtype == Boolean and isinstance(other, bool) and op in ("eq", "neq"):
             if (other is True and op == "eq") or (other is False and op == "neq"):
                 return self.clone()
             elif (other is False and op == "eq") or (other is True and op == "neq"):
                 return ~self
 
         if isinstance(other, datetime) and self.dtype == Datetime:
-            ts = _datetime_to_pl_timestamp(other, self.time_unit)
+            ts = _datetime_to_pl_timestamp(other, self._s.time_unit())
             f = get_ffi_func(op + "_<>", Int64, self._s)
             assert f is not None
             return self._from_pyseries(f(ts))
         elif isinstance(other, time) and self.dtype == Time:
             d = _time_to_pl_time(other)
             f = get_ffi_func(op + "_<>", Int64, self._s)
             assert f is not None
@@ -484,65 +488,167 @@
 
         if isinstance(other, Sequence) and not isinstance(other, str):
             other = Series("", other, dtype_if_empty=self.dtype)
         if isinstance(other, Series):
             return self._from_pyseries(getattr(self._s, op)(other._s))
 
         if other is not None:
-            other = maybe_cast(other, self.dtype, self.time_unit)
+            other = maybe_cast(other, self.dtype, self._s.time_unit())
         f = get_ffi_func(op + "_<>", self.dtype, self._s)
         if f is None:
             return NotImplemented
 
         return self._from_pyseries(f(other))
 
-    def __eq__(self, other: Any) -> Self:  # type: ignore[override]
+    @overload  # type: ignore[override]
+    def __eq__(self, other: Expr) -> Expr:  # type: ignore[misc]
+        ...
+
+    @overload
+    def __eq__(self, other: Any) -> Self:
+        ...
+
+    def __eq__(self, other: Any) -> Self | Expr:
+        if isinstance(other, pl.Expr):
+            return F.lit(self).__eq__(other)
         return self._comp(other, "eq")
 
-    def __ne__(self, other: Any) -> Self:  # type: ignore[override]
+    @overload  # type: ignore[override]
+    def __ne__(self, other: Expr) -> Expr:  # type: ignore[misc]
+        ...
+
+    @overload
+    def __ne__(self, other: Any) -> Self:
+        ...
+
+    def __ne__(self, other: Any) -> Self | Expr:
+        if isinstance(other, pl.Expr):
+            return F.lit(self).__ne__(other)
         return self._comp(other, "neq")
 
+    @overload
+    def __gt__(self, other: Expr) -> Expr:  # type: ignore[misc]
+        ...
+
+    @overload
     def __gt__(self, other: Any) -> Self:
+        ...
+
+    def __gt__(self, other: Any) -> Self | Expr:
+        if isinstance(other, pl.Expr):
+            return F.lit(self).__gt__(other)
         return self._comp(other, "gt")
 
+    @overload
+    def __lt__(self, other: Expr) -> Expr:  # type: ignore[misc]
+        ...
+
+    @overload
     def __lt__(self, other: Any) -> Self:
+        ...
+
+    def __lt__(self, other: Any) -> Self | Expr:
+        if isinstance(other, pl.Expr):
+            return F.lit(self).__lt__(other)
         return self._comp(other, "lt")
 
+    @overload
+    def __ge__(self, other: Expr) -> Expr:  # type: ignore[misc]
+        ...
+
+    @overload
     def __ge__(self, other: Any) -> Self:
+        ...
+
+    def __ge__(self, other: Any) -> Self | Expr:
+        if isinstance(other, pl.Expr):
+            return F.lit(self).__ge__(other)
         return self._comp(other, "gt_eq")
 
+    @overload
+    def __le__(self, other: Expr) -> Expr:  # type: ignore[misc]
+        ...
+
+    @overload
     def __le__(self, other: Any) -> Self:
+        ...
+
+    def __le__(self, other: Any) -> Self | Expr:
+        if isinstance(other, pl.Expr):
+            return F.lit(self).__le__(other)
         return self._comp(other, "lt_eq")
 
-    def le(self, other: Any) -> Self:
+    def le(self, other: Any) -> Self | Expr:
         """Method equivalent of operator expression ``series <= other``."""
         return self.__le__(other)
 
-    def lt(self, other: Any) -> Self:
+    def lt(self, other: Any) -> Self | Expr:
         """Method equivalent of operator expression ``series < other``."""
         return self.__lt__(other)
 
-    def eq(self, other: Any) -> Self:
+    def eq(self, other: Any) -> Self | Expr:
         """Method equivalent of operator expression ``series == other``."""
         return self.__eq__(other)
 
-    def ne(self, other: Any) -> Self:
+    @overload
+    def eq_missing(self, other: Any) -> Self:
+        ...
+
+    @overload
+    def eq_missing(self, other: Expr) -> Expr:  # type: ignore[misc]
+        ...
+
+    def eq_missing(self, other: Any) -> Self | Expr:
+        """
+        Method equivalent of equality operator ``expr == other`` where `None` == None`.
+
+        This differs from default ``ne`` where null values are propagated.
+
+        Parameters
+        ----------
+        other
+            A literal or expression value to compare with.
+
+        """
+
+    def ne(self, other: Any) -> Self | Expr:
         """Method equivalent of operator expression ``series != other``."""
         return self.__ne__(other)
 
-    def ge(self, other: Any) -> Self:
+    @overload
+    def ne_missing(self, other: Expr) -> Expr:  # type: ignore[misc]
+        ...
+
+    @overload
+    def ne_missing(self, other: Any) -> Self:
+        ...
+
+    def ne_missing(self, other: Any) -> Self | Expr:
+        """
+        Method equivalent of equality operator ``expr != other`` where `None` == None`.
+
+        This differs from default ``ne`` where null values are propagated.
+
+        Parameters
+        ----------
+        other
+            A literal or expression value to compare with.
+
+        """
+
+    def ge(self, other: Any) -> Self | Expr:
         """Method equivalent of operator expression ``series >= other``."""
         return self.__ge__(other)
 
-    def gt(self, other: Any) -> Self:
+    def gt(self, other: Any) -> Self | Expr:
         """Method equivalent of operator expression ``series > other``."""
         return self.__gt__(other)
 
     def _arithmetic(self, other: Any, op_s: str, op_ffi: str) -> Self:
-        if isinstance(other, pli.Expr):
+        if isinstance(other, pl.Expr):
             # expand pl.lit, pl.datetime, pl.duration Exprs to compatible Series
             other = self.to_frame().select(other).to_series()
         if isinstance(other, Series):
             return self._from_pyseries(getattr(self._s, op_s)(other._s))
         if _check_for_numpy(other) and isinstance(other, np.ndarray):
             return self._from_pyseries(getattr(self._s, op_s)(Series(other)._s))
         if (
@@ -551,15 +657,15 @@
         ):
             _s = sequence_to_pyseries(self.name, [other])
             if "rhs" in op_ffi:
                 return self._from_pyseries(getattr(_s, op_s)(self._s))
             else:
                 return self._from_pyseries(getattr(self._s, op_s)(_s))
         else:
-            other = maybe_cast(other, self.dtype, self.time_unit)
+            other = maybe_cast(other, self.dtype, self._s.time_unit())
             f = get_ffi_func(op_ffi, self.dtype, self._s)
         if f is None:
             raise ValueError(
                 f"cannot do arithmetic with series of dtype: {self.dtype} and argument"
                 f" of type: {type(other)}"
             )
         return self._from_pyseries(f(other))
@@ -575,61 +681,61 @@
     @overload
     def __add__(self, other: Any) -> Self:
         ...
 
     def __add__(self, other: Any) -> Self | DataFrame | Expr:
         if isinstance(other, str):
             other = Series("", [other])
-        elif isinstance(other, pli.DataFrame):
+        elif isinstance(other, pl.DataFrame):
             return other + self
-        elif isinstance(other, pli.Expr):
+        elif isinstance(other, pl.Expr):
             return F.lit(self) + other
         return self._arithmetic(other, "add", "add_<>")
 
     @overload
     def __sub__(self, other: Expr) -> Expr:  # type: ignore[misc]
         ...
 
     @overload
     def __sub__(self, other: Any) -> Self:
         ...
 
     def __sub__(self, other: Any) -> Self | Expr:
-        if isinstance(other, pli.Expr):
+        if isinstance(other, pl.Expr):
             return F.lit(self) - other
         return self._arithmetic(other, "sub", "sub_<>")
 
     @overload
     def __truediv__(self, other: Expr) -> Expr:  # type: ignore[misc]
         ...
 
     @overload
     def __truediv__(self, other: Any) -> Series:
         ...
 
     def __truediv__(self, other: Any) -> Series | Expr:
-        if isinstance(other, pli.Expr):
+        if isinstance(other, pl.Expr):
             return F.lit(self) / other
         if self.is_temporal():
             raise ValueError("first cast to integer before dividing datelike dtypes")
 
         # this branch is exactly the floordiv function without rounding the floats
-        if self.is_float():
+        if self.is_float() or self.dtype == Decimal:
             return self._arithmetic(other, "div", "div_<>")
 
         return self.cast(Float64) / other
 
     # python 3.7 is not happy. Remove this when we finally ditch that
     @typing.no_type_check
     def __floordiv__(self, other: Any) -> Series:
-        if isinstance(other, pli.Expr):
+        if isinstance(other, pl.Expr):
             return F.lit(self).__floordiv__(other)
         if self.is_temporal():
             raise ValueError("first cast to integer before dividing datelike dtypes")
-        if not isinstance(other, pli.Expr):
+        if not isinstance(other, pl.Expr):
             other = F.lit(other)
         return self.to_frame().select(F.col(self.name) // other).to_series()
 
     def __invert__(self) -> Self:
         if self.dtype == Boolean:
             return self._from_pyseries(self._s._not())
         return NotImplemented
@@ -643,33 +749,33 @@
         ...
 
     @overload
     def __mul__(self, other: Any) -> Series:
         ...
 
     def __mul__(self, other: Any) -> Series | DataFrame | Expr:
-        if isinstance(other, pli.Expr):
+        if isinstance(other, pl.Expr):
             return F.lit(self) * other
         if self.is_temporal():
             raise ValueError("first cast to integer before multiplying datelike dtypes")
-        elif isinstance(other, pli.DataFrame):
+        elif isinstance(other, pl.DataFrame):
             return other * self
         else:
             return self._arithmetic(other, "mul", "mul_<>")
 
     @overload
     def __mod__(self, other: Expr) -> Expr:  # type: ignore[misc]
         ...
 
     @overload
     def __mod__(self, other: Any) -> Series:
         ...
 
     def __mod__(self, other: Any) -> Series | Expr:
-        if isinstance(other, pli.Expr):
+        if isinstance(other, pl.Expr):
             return F.lit(self).__mod__(other)
         if self.is_temporal():
             raise ValueError(
                 "first cast to integer before applying modulo on datelike dtypes"
             )
         return self._arithmetic(other, "rem", "rem_<>")
 
@@ -719,15 +825,15 @@
         return self.to_frame().select(other ** F.col(self.name)).to_series()
 
     def __matmul__(self, other: Any) -> float | Series | None:
         if isinstance(other, Sequence) or (
             _check_for_numpy(other) and isinstance(other, np.ndarray)
         ):
             other = Series(other)
-        # elif isinstance(other, pli.DataFrame):
+        # elif isinstance(other, pl.DataFrame):
         #     return other.__rmatmul__(self)  # type: ignore[return-value]
         return self.dot(other)
 
     def __rmatmul__(self, other: Any) -> float | Series | None:
         if isinstance(other, Sequence) or (
             _check_for_numpy(other) and isinstance(other, np.ndarray)
         ):
@@ -745,16 +851,26 @@
 
     def __copy__(self) -> Self:
         return self.clone()
 
     def __deepcopy__(self, memo: None = None) -> Self:
         return self.clone()
 
-    def __iter__(self) -> SeriesIter:
-        return SeriesIter(self.len(), self)
+    def __iter__(self) -> Generator[Any, None, None]:
+        if self.dtype == List:
+            # TODO: either make a change and return py-native list data here, or find
+            #  a faster way to return nested/List series; sequential 'get_idx' calls
+            #  make this path a lot slower (~10x) than it needs to be.
+            get_idx = self._s.get_idx
+            for idx in range(0, self.len()):
+                yield get_idx(idx)
+        else:
+            buffer_size = 25_000
+            for offset in range(0, self.len(), buffer_size):
+                yield from self.slice(offset, buffer_size).to_list()
 
     def _pos_idxs(self, idxs: np.ndarray[Any, Any] | Series) -> Series:
         # pl.UInt32 (polars) or pl.UInt64 (polars_u64_idx).
         idx_type = get_index_type()
 
         if isinstance(idxs, Series):
             if idxs.dtype == idx_type:
@@ -951,14 +1067,16 @@
         elif isinstance(key, (list, tuple)):
             s = self._from_pyseries(sequence_to_pyseries("", key, dtype=UInt32))
             self.__setitem__(s, value)
         else:
             raise ValueError(f'cannot use "{key}" for indexing')
 
     def __array__(self, dtype: Any = None) -> np.ndarray[Any, Any]:
+        if not dtype and self.dtype == Utf8 and not self.has_validity():
+            dtype = np.dtype("U")
         if dtype:
             return self.to_numpy().__array__(dtype)
         else:
             return self.to_numpy().__array__()
 
     def __array_ufunc__(
         self, ufunc: np.ufunc, method: str, *inputs: Any, **kwargs: Any
@@ -1290,15 +1408,15 @@
                 "min": str(self.dt.min()),
                 "max": str(self.dt.max()),
                 "median": str(self.dt.median()),
             }
         else:
             raise TypeError("This type is not supported")
 
-        return pli.DataFrame({"statistic": stats.keys(), "value": stats.values()})
+        return pl.DataFrame({"statistic": stats.keys(), "value": stats.values()})
 
     def sum(self) -> int | float:
         """
         Reduce this Series to the sum value.
 
         Notes
         -----
@@ -1325,15 +1443,15 @@
         2.0
 
         """
         return self._s.mean()
 
     def product(self) -> int | float:
         """Reduce this Series to the product value."""
-        return self.to_frame().select(F.col(self.name).product()).to_series()[0]
+        return self.to_frame().select(F.col(self.name).product()).to_series().item()
 
     def pow(self, exponent: int | float | Series) -> Series:
         """
         Raise to the power of the given exponent.
 
         Parameters
         ----------
@@ -1822,15 +1940,15 @@
         >>> new_aliased_srs = srs.alias("y")
 
         """
         s = self.clone()
         s._s.rename(name)
         return s
 
-    def rename(self, name: str, *, in_place: bool = False) -> Series:
+    def rename(self, name: str, *, in_place: bool | None = None) -> Series:
         """
         Rename this Series.
 
         Parameters
         ----------
         name
             New name.
@@ -1846,14 +1964,25 @@
         [
                 1
                 2
                 3
         ]
 
         """
+        if in_place is not None:
+            # if 'in_place' is not None, this indicates that the parameter was
+            # explicitly set by the caller, and we should warn against it (use
+            # of NoDefault only applies when one of the valid values is None).
+            warnings.warn(
+                "the `in_place` parameter is deprecated and will be removed in a future"
+                " version; note that renaming is a shallow-copy operation with"
+                " essentially zero cost.",
+                category=DeprecationWarning,
+                stacklevel=find_stacklevel(),
+            )
         if in_place:
             self._s.rename(name)
             return self
         else:
             return self.alias(name)
 
     def chunk_lengths(self) -> list[int]:
@@ -2809,15 +2938,15 @@
                 false
         ]
 
         """
 
     def explode(self) -> Series:
         """
-        Explode a list or utf8 Series.
+        Explode a list Series.
 
         This means that every item is expanded to a new row.
 
         Returns
         -------
         Exploded Series of same dtype
 
@@ -2916,14 +3045,15 @@
         Cast to physical representation of the logical dtype.
 
         - :func:`polars.datatypes.Date` -> :func:`polars.datatypes.Int32`
         - :func:`polars.datatypes.Datetime` -> :func:`polars.datatypes.Int64`
         - :func:`polars.datatypes.Time` -> :func:`polars.datatypes.Int64`
         - :func:`polars.datatypes.Duration` -> :func:`polars.datatypes.Int64`
         - :func:`polars.datatypes.Categorical` -> :func:`polars.datatypes.UInt32`
+        - ``List(inner)`` -> ``List(physical of inner)``
         - Other data types will be left unchanged.
 
         Examples
         --------
         Replicating the pandas
         `pd.Series.factorize
         <https://pandas.pydata.org/docs/reference/api/pandas.Series.factorize.html>`_
@@ -3231,15 +3361,18 @@
             of nulls, or for non-primitive types).
         writable
             For numpy arrays created with zero copy (view on the Arrow data),
             the resulting array is not writable (Arrow data is immutable).
             By setting this to True, a copy of the array is made to ensure
             it is writable.
         use_pyarrow
-            Use pyarrow for the conversion to numpy.
+            Use `pyarrow.Array.to_numpy
+            <https://arrow.apache.org/docs/python/generated/pyarrow.Array.html#pyarrow.Array.to_numpy>`_
+
+            for the conversion to numpy.
 
         Examples
         --------
         >>> s = pl.Series("a", [1, 2, 3])
         >>> arr = s.to_numpy()
         >>> arr  # doctest: +IGNORE_RESULT
         array([1, 2, 3], dtype=int64)
@@ -3248,17 +3381,17 @@
 
         """
 
         def convert_to_date(arr: np.ndarray[Any, Any]) -> np.ndarray[Any, Any]:
             if self.dtype == Date:
                 tp = "datetime64[D]"
             elif self.dtype == Duration:
-                tp = f"timedelta64[{self.time_unit}]"
+                tp = f"timedelta64[{self._s.time_unit()}]"
             else:
-                tp = f"datetime64[{self.time_unit}]"
+                tp = f"datetime64[{self._s.time_unit()}]"
             return arr.astype(tp)
 
         def raise_no_zero_copy() -> None:
             if zero_copy_only:
                 raise ValueError("Cannot return a zero-copy array")
 
         if (
@@ -3745,15 +3878,15 @@
                 2.0
                 3.0
                 4.0
         ]
 
         """
 
-    def round(self, decimals: int) -> Series:
+    def round(self, decimals: int = 0) -> Series:
         """
         Round underlying floating point data by `decimals` digits.
 
         Examples
         --------
         >>> s = pl.Series("a", [1.12345, 2.56789, 3.901234])
         >>> s.round(2)
@@ -4163,15 +4296,15 @@
         periods
             Number of places to shift (may be negative).
 
         """
 
     def shift_and_fill(
         self,
-        fill_value: int | pli.Expr,
+        fill_value: int | Expr,
         *,
         periods: int = 1,
     ) -> Series:
         """
         Shift the values by a given period and fill the resulting null values.
 
         Parameters
@@ -5348,39 +5481,39 @@
         ...     "JPN": "Japan",
         ...     "TUR": "Trkiye",
         ...     "NLD": "Netherlands",
         ... }
 
         Remap, setting a default for unrecognised values...
 
-        >>> s.map_dict(country_lookup, default="Unspecified").rename("country_name")
+        >>> s.map_dict(country_lookup, default="Unspecified").alias("country_name")
         shape: (4,)
         Series: 'country_name' [str]
         [
             "Trkiye"
             "Unspecified"
             "Japan"
             "Netherlands"
         ]
 
         ...or keep the original value, by making use of ``pl.first()``:
 
-        >>> s.map_dict(country_lookup, default=pl.first()).rename("country_name")
+        >>> s.map_dict(country_lookup, default=pl.first()).alias("country_name")
         shape: (4,)
         Series: 'country_name' [str]
         [
             "Trkiye"
             "???"
             "Japan"
             "Netherlands"
         ]
 
         ...or keep the original value, by assigning the input series:
 
-        >>> s.map_dict(country_lookup, default=s).rename("country_name")
+        >>> s.map_dict(country_lookup, default=s).alias("country_name")
         shape: (4,)
         Series: 'country_name' [str]
         [
             "Trkiye"
             "???"
             "Japan"
             "Netherlands"
@@ -5766,21 +5899,16 @@
     def get_chunks(self) -> list[Series]:
         """Get the chunks of this Series as a list of Series."""
         return self._s.get_chunks()
 
     def implode(self) -> Self:
         """Aggregate values into a list."""
 
-    # Below are the namespaces defined. Do not move these up in the definition of
-    # Series, as it confuses mypy between the type annotation `str` and the
-    # namespace `str`
-    @property
-    def arr(self) -> ListNameSpace:
-        """Create an object namespace of all list related methods."""
-        return ListNameSpace(self)
+    # Keep the `list` and `str` properties below at the end of the definition of Series,
+    # as to not confuse mypy with the type annotation `str` and `list`
 
     @property
     def bin(self) -> BinaryNameSpace:
         """Create an object namespace of all binary related methods."""
         return BinaryNameSpace(self)
 
     @property
@@ -5790,44 +5918,34 @@
 
     @property
     def dt(self) -> DateTimeNameSpace:
         """Create an object namespace of all datetime related methods."""
         return DateTimeNameSpace(self)
 
     @property
+    def list(self) -> ListNameSpace:
+        """Create an object namespace of all list related methods."""
+        return ListNameSpace(self)
+
+    @property
+    def arr(self) -> ArrayNameSpace:
+        """Create an object namespace of all array related methods."""
+        return ArrayNameSpace(self)
+
+    @property
     def str(self) -> StringNameSpace:
         """Create an object namespace of all string related methods."""
         return StringNameSpace(self)
 
     @property
     def struct(self) -> StructNameSpace:
         """Create an object namespace of all struct related methods."""
         return StructNameSpace(self)
 
 
-class SeriesIter:
-    """Utility class that allows slow iteration over a `Series`."""
-
-    def __init__(self, length: int, s: Series):
-        self.len = length
-        self.i = 0
-        self.s = s
-
-    def __iter__(self) -> SeriesIter:
-        return self
-
-    def __next__(self) -> Any:
-        if self.i < self.len:
-            i = self.i
-            self.i += 1
-            return self.s[i]
-        else:
-            raise StopIteration
-
-
 def _resolve_datetime_dtype(
     dtype: PolarsDataType | None, ndtype: np.datetime64
 ) -> PolarsDataType | None:
     """Given polars/numpy datetime dtypes, resolve to an explicit unit."""
     if dtype is None or (dtype == Datetime and not getattr(dtype, "time_unit", None)):
         time_unit = getattr(dtype, "time_unit", None) or np.datetime_data(ndtype)[0]
         # explicit formulation is verbose, but keeps mypy happy
```

### Comparing `polars_lts_cpu-0.17.9/polars/series/string.py` & `polars_lts_cpu-0.18.0/polars/series/string.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,145 +1,317 @@
 from __future__ import annotations
 
+import warnings
 from typing import TYPE_CHECKING
 
 from polars import functions as F
 from polars.series.utils import expr_dispatch
-from polars.utils import no_default
 from polars.utils._wrap import wrap_s
 from polars.utils.decorators import deprecated_alias
+from polars.utils.various import find_stacklevel
 
 if TYPE_CHECKING:
-    from polars.expr import Expr
+    from polars import Expr, Series
     from polars.polars import PySeries
-    from polars.series import Series
-    from polars.type_aliases import PolarsDataType, PolarsTemporalType, TransferEncoding
-    from polars.utils import NoDefault
+    from polars.type_aliases import (
+        PolarsDataType,
+        PolarsTemporalType,
+        TimeUnit,
+        TransferEncoding,
+    )
 
 
 @expr_dispatch
 class StringNameSpace:
     """Series.str namespace."""
 
     _accessor = "str"
 
     def __init__(self, series: Series):
         self._s: PySeries = series._s
 
+    def to_date(
+        self,
+        format: str | None = None,
+        *,
+        strict: bool = True,
+        exact: bool = True,
+        cache: bool = True,
+    ) -> Series:
+        """
+        Convert a Utf8 column into a Date column.
+
+        Parameters
+        ----------
+        format
+            Format to use for conversion. Refer to the `chrono crate documentation
+            <https://docs.rs/chrono/latest/chrono/format/strftime/index.html>`_
+            for the full specification. Example: ``"%Y-%m-%d"``.
+            If set to None (default), the format is inferred from the data.
+        strict
+            Raise an error if any conversion fails.
+        exact
+            Require an exact format match. If False, allow the format to match anywhere
+            in the target string.
+        cache
+            Use a cache of unique, converted dates to apply the conversion.
+
+        Examples
+        --------
+        >>> s = pl.Series(["2020/01/01", "2020/02/01", "2020/03/01"])
+        >>> s.str.to_date()
+        shape: (3,)
+        Series: '' [date]
+        [
+                2020-01-01
+                2020-02-01
+                2020-03-01
+        ]
+
+        """
+
+    def to_datetime(
+        self,
+        format: str | None = None,
+        *,
+        time_unit: TimeUnit | None = None,
+        time_zone: str | None = None,
+        strict: bool = True,
+        exact: bool = True,
+        cache: bool = True,
+        utc: bool | None = None,
+    ) -> Series:
+        """
+        Convert a Utf8 column into a Datetime column.
+
+        Parameters
+        ----------
+        format
+            Format to use for conversion. Refer to the `chrono crate documentation
+            <https://docs.rs/chrono/latest/chrono/format/strftime/index.html>`_
+            for the full specification. Example: ``"%Y-%m-%d %H:%M:%S"``.
+            If set to None (default), the format is inferred from the data.
+        time_unit : {None, 'us', 'ns', 'ms'}
+            Unit of time for the resulting Datetime column. If set to None (default),
+            the time unit is inferred from the format string if given, eg:
+            ``"%F %T%.3f"`` => ``Datetime("ms")``. If no fractional second component is
+            found, the default is ``"us"``.
+        time_zone
+            Time zone for the resulting Datetime column.
+        strict
+            Raise an error if any conversion fails.
+        exact
+            Require an exact format match. If False, allow the format to match anywhere
+            in the target string.
+        cache
+            Use a cache of unique, converted datetimes to apply the conversion.
+        utc
+            Parse time zone aware datetimes as UTC. This may be useful if you have data
+            with mixed offsets.
+
+            .. deprecated:: 0.18.0
+                This is now a no-op, you can safely remove it.
+                Offset-naive strings are parsed as ``pl.Datetime(time_unit)``,
+                and offset-aware strings are converted to
+                ``pl.Datetime(time_unit, "UTC")``.
+
+        Examples
+        --------
+        >>> s = pl.Series(["2020-01-01 01:00Z", "2020-01-01 02:00Z"])
+        >>> s.str.to_datetime("%Y-%m-%d %H:%M%#z")
+        shape: (2,)
+        Series: '' [datetime[s, UTC]]
+        [
+                2020-01-01 01:00:00 UTC
+                2020-01-01 02:00:00 UTC
+        ]
+        """
+
+    def to_time(
+        self,
+        format: str | None = None,
+        *,
+        strict: bool = True,
+        cache: bool = True,
+    ) -> Series:
+        """
+        Convert a Utf8 column into a Time column.
+
+        Parameters
+        ----------
+        format
+            Format to use for conversion. Refer to the `chrono crate documentation
+            <https://docs.rs/chrono/latest/chrono/format/strftime/index.html>`_
+            for the full specification. Example: ``"%H:%M:%S"``.
+            If set to None (default), the format is inferred from the data.
+        strict
+            Raise an error if any conversion fails.
+        cache
+            Use a cache of unique, converted times to apply the conversion.
+
+        Examples
+        --------
+        >>> s = pl.Series(["01:00", "02:00", "03:00"])
+        >>> s.str.to_time("%H:%M")
+        shape: (3,)
+        Series: '' [time]
+        [
+                01:00:00
+                02:00:00
+                03:00:00
+        ]
+
+        """
+
     @deprecated_alias(datatype="dtype", fmt="format")
     def strptime(
         self,
         dtype: PolarsTemporalType,
         format: str | None = None,
         *,
         strict: bool = True,
         exact: bool = True,
         cache: bool = True,
-        tz_aware: bool | NoDefault = no_default,
-        utc: bool = False,
+        utc: bool | None = None,
     ) -> Series:
         """
-        Parse a Series of dtype Utf8 to a Date/Datetime Series.
+        Convert a Utf8 column into a Date/Datetime/Time column.
 
         Parameters
         ----------
         dtype
-            Date, Datetime or Time.
+            The data type to convert to. Can be either Date, Datetime, or Time.
         format
-            Format to use, refer to the
-            `chrono strftime documentation
+            Format to use for conversion. Refer to the `chrono crate documentation
             <https://docs.rs/chrono/latest/chrono/format/strftime/index.html>`_
-            for specification. Example: ``"%y-%m-%d"``.
+            for the full specification. Example: ``"%Y-%m-%d %H:%M:%S"``.
+            If set to None (default), the format is inferred from the data.
         strict
             Raise an error if any conversion fails.
         exact
-            - If True, require an exact format match.
-            - If False, allow the format to match anywhere in the target string.
+            Require an exact format match. If False, allow the format to match anywhere
+            in the target string. Conversion to the Time type is always exact.
         cache
             Use a cache of unique, converted dates to apply the datetime conversion.
-        tz_aware
-            Parse timezone aware datetimes. This may be automatically toggled by the
-            `fmt` given.
-
-            .. deprecated:: 0.16.17
-                This is now auto-inferred from the given `fmt`. You can safely drop
-                this argument, it will be removed in a future version.
         utc
-            Parse timezone aware datetimes as UTC. This may be useful if you have data
+            Parse time zone aware datetimes as UTC. This may be useful if you have data
             with mixed offsets.
 
-        Returns
-        -------
-        A Date / Datetime / Time Series
+            .. deprecated:: 0.18.0
+                This is now a no-op, you can safely remove it.
+                Offset-naive strings are parsed as ``pl.Datetime(time_unit)``,
+                and offset-aware strings are converted to
+                ``pl.Datetime(time_unit, "UTC")``.
+
+        Notes
+        -----
+        When converting to a Datetime type, the time unit is inferred from the format
+        string if given, eg: ``"%F %T%.3f"`` => ``Datetime("ms")``. If no fractional
+        second component is found, the default is ``"us"``.
 
         Examples
         --------
         Dealing with a consistent format:
 
-        >>> ts = ["2020-01-01 01:00Z", "2020-01-01 02:00Z"]
-        >>> pl.Series(ts).str.strptime(pl.Datetime, "%Y-%m-%d %H:%M%#z")
+        >>> s = pl.Series(["2020-01-01 01:00Z", "2020-01-01 02:00Z"])
+        >>> s.str.strptime(pl.Datetime, "%Y-%m-%d %H:%M%#z")
         shape: (2,)
-        Series: '' [datetime[s, +00:00]]
+        Series: '' [datetime[s, UTC]]
         [
-                2020-01-01 01:00:00 +00:00
-                2020-01-01 02:00:00 +00:00
+                2020-01-01 01:00:00 UTC
+                2020-01-01 02:00:00 UTC
         ]
 
         Dealing with different formats.
 
         >>> s = pl.Series(
         ...     "date",
         ...     [
         ...         "2021-04-22",
         ...         "2022-01-04 00:00:00",
         ...         "01/31/22",
         ...         "Sun Jul  8 00:34:60 2001",
         ...     ],
         ... )
-        >>> (
-        ...     s.to_frame().with_columns(
-        ...         pl.col("date")
-        ...         .str.strptime(pl.Date, "%F", strict=False)
-        ...         .fill_null(
-        ...             pl.col("date").str.strptime(pl.Date, "%F %T", strict=False)
-        ...         )
-        ...         .fill_null(pl.col("date").str.strptime(pl.Date, "%D", strict=False))
-        ...         .fill_null(pl.col("date").str.strptime(pl.Date, "%c", strict=False))
+        >>> s.to_frame().select(
+        ...     pl.coalesce(
+        ...         pl.col("date").str.strptime(pl.Date, "%F", strict=False),
+        ...         pl.col("date").str.strptime(pl.Date, "%F %T", strict=False),
+        ...         pl.col("date").str.strptime(pl.Date, "%D", strict=False),
+        ...         pl.col("date").str.strptime(pl.Date, "%c", strict=False),
         ...     )
-        ... )
-        shape: (4, 1)
-        
-         date       
-         ---        
-         date       
-        
-         2021-04-22 
-         2022-01-04 
-         2022-01-31 
-         2001-07-08 
-        
-
-        """
+        ... ).to_series()
+        shape: (4,)
+        Series: 'date' [date]
+        [
+                2021-04-22
+                2022-01-04
+                2022-01-31
+                2001-07-08
+        ]
+        """
+        if utc is not None:
+            warnings.warn(
+                "The `utc` argument is now a no-op and has no effect. "
+                "You can safely remove it. "
+                "Offset-naive strings are parsed as ``pl.Datetime(time_unit)``, "
+                "and offset-aware strings are converted to "
+                '``pl.Datetime(time_unit, "UTC")``.',
+                DeprecationWarning,
+                stacklevel=find_stacklevel(),
+            )
         s = wrap_s(self._s)
         return (
             s.to_frame()
             .select(
                 F.col(s.name).str.strptime(
                     dtype,
                     format,
                     strict=strict,
                     exact=exact,
                     cache=cache,
-                    tz_aware=tz_aware,
-                    utc=utc,
                 )
             )
             .to_series()
         )
 
+    def to_decimal(
+        self,
+        inference_length: int = 100,
+    ) -> Series:
+        """
+        Convert a Utf8 column into a Decimal column.
+
+        This method infers the needed parameters ``precision`` and ``scale``.
+
+        Parameters
+        ----------
+        inference_length
+            Number of elements to parse to determine the `precision` and `scale`
+
+        Examples
+        --------
+        >>> s = pl.Series(
+        ...     ["40.12", "3420.13", "120134.19", "3212.98", "12.90", "143.09", "143.9"]
+        ... )
+        >>> s.str.to_decimal()
+        shape: (7,)
+        Series: '' [decimal[8,2]]
+        [
+            40.12
+            3420.13
+            120134.19
+            3212.98
+            12.9
+            143.09
+            143.9
+        ]
+
+        """
+
     def lengths(self) -> Series:
         """
         Get length of the string values in the Series (as number of bytes).
 
         Notes
         -----
         The returned lengths are equal to the number of bytes in the UTF8 string. If you
@@ -217,21 +389,43 @@
     ) -> Series:
         """
         Check if strings in Series contain a substring that matches a regex.
 
         Parameters
         ----------
         pattern
-            A valid regex pattern.
+            A valid regular expression pattern, compatible with the `regex crate
+            <https://docs.rs/regex/latest/regex/>`_.
         literal
-            Treat pattern as a literal string.
+            Treat ``pattern`` as a literal string, not as a regular expression.
         strict
-            Raise an error if the underlying pattern is not a valid regex expression,
+            Raise an error if the underlying pattern is not a valid regex,
             otherwise mask out with a null value.
 
+        Notes
+        -----
+        To modify regular expression behaviour (such as case-sensitivity) with
+        flags, use the inline ``(?iLmsuxU)`` syntax. For example:
+
+        Default (case-sensitive) match:
+
+        >>> s = pl.Series("s", ["AAA", "aAa", "aaa"])
+        >>> s.str.contains("AA").to_list()
+        [True, False, False]
+
+        Case-insensitive match, using an inline flag:
+
+        >>> s = pl.Series("s", ["AAA", "aAa", "aaa"])
+        >>> s.str.contains("(?i)AA").to_list()
+        [True, True, True]
+
+        See the regex crate's section on `grouping and flags
+        <https://docs.rs/regex/latest/regex/#grouping-and-flags>`_ for
+        additional information about the use of inline expression modifiers.
+
         Returns
         -------
         Boolean mask
 
         Examples
         --------
         >>> s = pl.Series(["Crab", "cat and dog", "rab$bit", None])
@@ -425,87 +619,147 @@
     def extract(self, pattern: str, group_index: int = 1) -> Series:
         r"""
         Extract the target capture group from provided patterns.
 
         Parameters
         ----------
         pattern
-            A valid regex pattern
+            A valid regular expression pattern, compatible with the `regex crate
+            <https://docs.rs/regex/latest/regex/>`_.
         group_index
             Index of the targeted capture group.
             Group 0 mean the whole pattern, first group begin at index 1
             Default to the first capture group
 
+        Notes
+        -----
+        To modify regular expression behaviour (such as multi-line matching)
+        with flags, use the inline ``(?iLmsuxU)`` syntax. For example:
+
+        >>> s = pl.Series(
+        ...     name="lines",
+        ...     values=[
+        ...         "I Like\nThose\nOdds",
+        ...         "This is\nThe Way",
+        ...     ],
+        ... )
+        >>> s.str.extract(r"(?m)^(T\w+)", 1).alias("matches")
+        shape: (2,)
+        Series: 'matches' [str]
+        [
+            "Those"
+            "This"
+        ]
+
+        See the regex crate's section on `grouping and flags
+        <https://docs.rs/regex/latest/regex/#grouping-and-flags>`_ for
+        additional information about the use of inline expression modifiers.
+
         Returns
         -------
         Utf8 array. Contain null if original value is null or regex capture nothing.
 
         Examples
         --------
-        >>> df = pl.DataFrame(
-        ...     {
-        ...         "a": [
-        ...             "http://vote.com/ballon_dor?candidate=messi&ref=polars",
-        ...             "http://vote.com/ballon_dor?candidat=jorginho&ref=polars",
-        ...             "http://vote.com/ballon_dor?candidate=ronaldo&ref=polars",
-        ...         ]
-        ...     }
+        >>> s = pl.Series(
+        ...     name="url",
+        ...     values=[
+        ...         "http://vote.com/ballon_dor?ref=polars&candidate=messi",
+        ...         "http://vote.com/ballon_dor?candidate=ronaldo&ref=polars",
+        ...         "http://vote.com/ballon_dor?error=404&ref=unknown",
+        ...     ],
         ... )
-        >>> df.select([pl.col("a").str.extract(r"candidate=(\w+)", 1)])
-        shape: (3, 1)
-        
-         a       
-         ---     
-         str     
-        
-         messi   
-         null    
-         ronaldo 
-        
+        >>> s.str.extract(r"candidate=(\w+)", 1).alias("candidate")
+        shape: (3,)
+        Series: 'candidate' [str]
+        [
+            "messi"
+            "ronaldo"
+            null
+        ]
 
         """
 
     def extract_all(self, pattern: str | Series) -> Series:
-        r"""
-        Extracts all matches for the given regex pattern.
+        r'''
+        Extract all matches for the given regex pattern.
 
-        Extract each successive non-overlapping regex match in an individual string as
-        an array
+        Extract each successive non-overlapping regex match in an individual string
+        as a list. Extracted matches contain ``null`` if the original value is null
+        or the regex did not capture anything.
 
         Parameters
         ----------
         pattern
-            A valid regex pattern
+            A valid regular expression pattern, compatible with the `regex crate
+            <https://docs.rs/regex/latest/regex/>`_.
+
+        Notes
+        -----
+        To modify regular expression behaviour (such as "verbose" mode and/or
+        case-sensitive matching) with flags, use the inline ``(?iLmsuxU)`` syntax.
+        For example:
+
+        >>> s = pl.Series(
+        ...     name="email",
+        ...     values=[
+        ...         "real.email@spam.com",
+        ...         "some_account@somewhere.net",
+        ...         "abc.def.ghi.jkl@uvw.xyz.co.uk",
+        ...     ],
+        ... )
+        >>> # extract name/domain parts from email, using verbose regex
+        >>> s.str.extract_all(
+        ...     r"""(?xi)   # activate 'verbose' and 'case-insensitive' flags
+        ...       [         # (start character group)
+        ...         A-Z     # letters
+        ...         0-9     # digits
+        ...         ._%+\-  # special chars
+        ...       ]         # (end character group)
+        ...       +         # 'one or more' quantifier
+        ...     """
+        ... ).alias("email_parts")
+        shape: (3,)
+        Series: 'email_parts' [list[str]]
+        [
+            ["real.email", "spam.com"]
+            ["some_account", "somewhere.net"]
+            ["abc.def.ghi.jkl", "uvw.xyz.co.uk"]
+        ]
+
+        See the regex crate's section on `grouping and flags
+        <https://docs.rs/regex/latest/regex/#grouping-and-flags>`_ for
+        additional information about the use of inline expression modifiers.
 
         Returns
         -------
-        List[Utf8] array. Contain null if original value is null or regex capture
-        nothing.
+        List[Utf8]
 
         Examples
         --------
         >>> s = pl.Series("foo", ["123 bla 45 asd", "xyz 678 910t"])
-        >>> s.str.extract_all(r"(\d+)")
+        >>> s.str.extract_all(r"\d+")
         shape: (2,)
         Series: 'foo' [list[str]]
         [
             ["123", "45"]
             ["678", "910"]
         ]
 
-        """
+        '''
 
     def count_match(self, pattern: str) -> Series:
         r"""
         Count all successive non-overlapping regex matches.
 
         Parameters
         ----------
         pattern
-            A valid regex pattern
+            A valid regular expression pattern, compatible with the `regex crate
+            <https://docs.rs/regex/latest/regex/>`_.
 
         Returns
         -------
         UInt32 array. Contain null if original value is null or regex capture nothing.
 
         Examples
         --------
@@ -661,21 +915,49 @@
     ) -> Series:
         r"""
         Replace first matching regex/literal substring with a new string value.
 
         Parameters
         ----------
         pattern
-            A valid regex pattern.
+            A valid regular expression pattern, compatible with the `regex crate
+            <https://docs.rs/regex/latest/regex/>`_.
         value
-            Substring to replace.
+            String that will replace the matched substring.
         literal
-             Treat pattern as a literal string.
+            Treat pattern as a literal string.
         n
-            Number of matches to replace
+            Number of matches to replace.
+
+        Notes
+        -----
+        To modify regular expression behaviour (such as case-sensitivity) with flags,
+        use the inline ``(?iLmsuxU)`` syntax. For example:
+
+        >>> s = pl.Series(
+        ...     name="weather",
+        ...     values=[
+        ...         "Foggy",
+        ...         "Rainy",
+        ...         "Sunny",
+        ...     ],
+        ... )
+        >>> # apply case-insensitive string replacement
+        >>> s.str.replace(r"(?i)foggy|rainy", "Sunny")
+        shape: (3,)
+        Series: 'weather' [str]
+        [
+            "Sunny"
+            "Sunny"
+            "Sunny"
+        ]
+
+        See the regex crate's section on `grouping and flags
+        <https://docs.rs/regex/latest/regex/#grouping-and-flags>`_ for
+        additional information about the use of inline expression modifiers.
 
         See Also
         --------
         replace_all : Replace all matching regex/literal substrings.
 
         Examples
         --------
@@ -693,19 +975,20 @@
     def replace_all(self, pattern: str, value: str, *, literal: bool = False) -> Series:
         """
         Replace all matching regex/literal substrings with a new string value.
 
         Parameters
         ----------
         pattern
-            A valid regex pattern.
+            A valid regular expression pattern, compatible with the `regex crate
+            <https://docs.rs/regex/latest/regex/>`_.
         value
-            Substring to replace.
+            String that will replace the matches.
         literal
-             Treat pattern as a literal string.
+            Treat pattern as a literal string.
 
         See Also
         --------
         replace : Replace first matching regex/literal substring.
 
         Examples
         --------
```

### Comparing `polars_lts_cpu-0.17.9/polars/series/struct.py` & `polars_lts_cpu-0.18.0/polars/series/struct.py`

 * *Files 15% similar despite different names*

```diff
@@ -4,17 +4,17 @@
 from typing import TYPE_CHECKING, Sequence
 
 from polars.series.utils import expr_dispatch
 from polars.utils._wrap import wrap_df
 from polars.utils.various import sphinx_accessor
 
 if TYPE_CHECKING:
-    from polars.dataframe import DataFrame
+    from polars import DataFrame, Series
+    from polars.datatypes import PolarsDataType
     from polars.polars import PySeries
-    from polars.series import Series
 elif os.getenv("BUILDING_SPHINX_DOCS"):
     property = sphinx_accessor
 
 
 @expr_dispatch
 class StructNameSpace:
     """Series.struct namespace."""
@@ -60,14 +60,21 @@
         Parameters
         ----------
         names
             New names in the order of the struct's fields
 
         """
 
+    @property
+    def schema(self) -> dict[str, PolarsDataType]:
+        """Get the struct definition as a name/dtype schema dict."""
+        if getattr(self, "_s", None) is None:
+            return {}
+        return self._s.dtype().to_schema()
+
     def unnest(self) -> DataFrame:
         """
         Convert this struct Series to a DataFrame with a separate column for each field.
 
         Examples
         --------
         >>> s = pl.Series([{"a": 1, "b": 2}, {"a": 3, "b": 4}])
```

### Comparing `polars_lts_cpu-0.17.9/polars/series/utils.py` & `polars_lts_cpu-0.18.0/polars/series/utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,22 +1,22 @@
 from __future__ import annotations
 
 import inspect
 import sys
 from functools import wraps
 from typing import TYPE_CHECKING, Any, Callable, TypeVar
 
+import polars._reexport as pl
 from polars import functions as F
-from polars import internals as pli
 from polars.datatypes import dtype_to_ffiname
 from polars.utils._wrap import wrap_s
 
 if TYPE_CHECKING:
+    from polars import Series
     from polars.polars import PySeries
-    from polars.series import Series
     from polars.type_aliases import PolarsDataType
 
     if sys.version_info >= (3, 10):
         from typing import ParamSpec
     else:
         from typing_extensions import ParamSpec
 
@@ -62,15 +62,15 @@
         or (sys.flags.optimize == 2 and fc.co_consts == (None,))
     )
 
 
 def _expr_lookup(namespace: str | None) -> set[tuple[str | None, str, tuple[str, ...]]]:
     """Create lookup of potential Expr methods (in the given namespace)."""
     # dummy Expr object that we can introspect
-    expr = pli.Expr()
+    expr = pl.Expr()
     expr._pyexpr = None
 
     # optional indirection to "expr.str", "expr.dt", etc
     if namespace is not None:
         expr = getattr(expr, namespace)
 
     lookup = set()
```

### Comparing `polars_lts_cpu-0.17.9/polars/slice.py` & `polars_lts_cpu-0.18.0/polars/slice.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,17 +1,15 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Union
 
-from polars import internals as pli
+import polars._reexport as pl
 
 if TYPE_CHECKING:
-    from polars.dataframe import DataFrame
-    from polars.lazyframe import LazyFrame
-    from polars.series import Series
+    from polars import DataFrame, LazyFrame, Series
 
     FrameOrSeries = Union["DataFrame", "Series"]
 
 
 class PolarsSlice:
     """
     Apply Python slice object to Polars DataFrame or Series.
@@ -30,20 +28,20 @@
     def __init__(self, obj: FrameOrSeries):
         self.obj = obj
 
     @staticmethod
     def _as_original(lazy: LazyFrame, original: FrameOrSeries) -> FrameOrSeries:
         """Return lazy variant back to its original type."""
         frame = lazy.collect()
-        return frame if isinstance(original, pli.DataFrame) else frame.to_series()
+        return frame if isinstance(original, pl.DataFrame) else frame.to_series()
 
     @staticmethod
     def _lazify(obj: FrameOrSeries) -> LazyFrame:
         """Make lazy to ensure efficient/consistent handling."""
-        return obj.to_frame().lazy() if isinstance(obj, pli.Series) else obj.lazy()
+        return obj.to_frame().lazy() if isinstance(obj, pl.Series) else obj.lazy()
 
     def _slice_positive(self, obj: LazyFrame) -> LazyFrame:
         """Logic for slices with positive stride."""
         # note: at this point stride is guaranteed to be > 1
         return obj.slice(self.start, self.slice_length).take_every(self.stride)
 
     def _slice_negative(self, obj: LazyFrame) -> LazyFrame:
```

### Comparing `polars_lts_cpu-0.17.9/polars/string_cache.py` & `polars_lts_cpu-0.18.0/polars/string_cache.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/polars/testing/_tempdir.py` & `polars_lts_cpu-0.18.0/polars/testing/_tempdir.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/polars/testing/asserts.py` & `polars_lts_cpu-0.18.0/polars/testing/asserts.py`

 * *Files 20% similar despite different names*

```diff
@@ -2,20 +2,22 @@
 
 import textwrap
 from typing import Any
 
 from polars import functions as F
 from polars.dataframe import DataFrame
 from polars.datatypes import (
+    FLOAT_DTYPES,
     UNSIGNED_INTEGER_DTYPES,
     Categorical,
     DataTypeClass,
-    Float32,
-    Float64,
+    List,
+    Struct,
     dtype_to_py_type,
+    unpack_dtypes,
 )
 from polars.exceptions import ComputeError, InvalidAssert
 from polars.lazyframe import LazyFrame
 from polars.series import Series
 
 
 def assert_frame_equal(
@@ -320,30 +322,51 @@
 
     check_exact = (
         check_exact or not can_be_subtracted or left.is_boolean() or left.is_temporal()
     )
     if check_dtype and left.dtype != right.dtype:
         raise_assert_detail("Series", "Dtype mismatch", left.dtype, right.dtype)
 
-    # confirm that we can call 'is_nan' on both sides
-    left_is_float = left.dtype in (Float32, Float64)
-    right_is_float = right.dtype in (Float32, Float64)
-    comparing_float_dtypes = left_is_float and right_is_float
+    if left.null_count() != right.null_count():
+        raise_assert_detail(
+            "Series", "null_count is not equal", left.null_count(), right.null_count()
+        )
 
     # create mask of which (if any) values are unequal
-    unequal = left != right
-    if unequal.any() and nans_compare_equal and comparing_float_dtypes:
-        # handle NaN values (which compare unequal to themselves)
-        unequal = unequal & ~((left.is_nan() & right.is_nan()).fill_null(F.lit(False)))
+    unequal = left.ne_missing(right)
+
+    # handle NaN values (which compare unequal to themselves)
+    comparing_float_dtypes = left.dtype in FLOAT_DTYPES and right.dtype in FLOAT_DTYPES
+    if unequal.any() and nans_compare_equal:
+        # when both dtypes are scalar floats
+        if comparing_float_dtypes:
+            unequal = unequal & ~(
+                (left.is_nan() & right.is_nan()).fill_null(F.lit(False))
+            )
+
+        # account for float values in nested dtypes
+        elif left.dtype.is_nested or right.dtype.is_nested:
+            if _assert_series_nested_nans_equal(
+                left=left.filter(unequal),
+                right=right.filter(unequal),
+                check_dtype=check_dtype,
+                check_exact=check_exact,
+                atol=atol,
+                rtol=rtol,
+            ):
+                return
 
     # assert exact, or with tolerance
     if unequal.any():
         if check_exact:
             raise_assert_detail(
-                "Series", "Exact value mismatch", left=list(left), right=list(right)
+                "Series",
+                "Exact value mismatch",
+                left=list(left),
+                right=list(right),
             )
         else:
             # apply check with tolerance (to the known-unequal matches).
             left, right = left.filter(unequal), right.filter(unequal)
 
             if all(tp in UNSIGNED_INTEGER_DTYPES for tp in (left.dtype, right.dtype)):
                 # avoid potential "subtract-with-overflow" panic on uint math
@@ -370,14 +393,79 @@
                     "Series",
                     f"Value mismatch{nan_info}",
                     left=list(left),
                     right=list(right),
                 )
 
 
+def _assert_series_nested_nans_equal(
+    left: Series,
+    right: Series,
+    check_dtype: bool,
+    check_exact: bool,
+    atol: float,
+    rtol: float,
+) -> bool:
+    # check that float values exist at _some_ level of nesting
+    if not any(tp in FLOAT_DTYPES for tp in unpack_dtypes(left.dtype, right.dtype)):
+        return False
+
+    # compare nested lists element-wise
+    elif left.dtype == List == right.dtype:
+        for s1, s2 in zip(left, right):
+            if s1 is None and s2 is None:
+                continue
+            elif (s1 is None and s2 is not None) or (s2 is None and s1 is not None):
+                raise_assert_detail("Series", "Nested value mismatch", s1, s2)
+            elif len(s1) != len(s2):
+                raise_assert_detail(
+                    "Series", "Nested list length mismatch", len(s1), len(s2)
+                )
+            _assert_series_inner(
+                s1,
+                s2,
+                check_dtype=check_dtype,
+                check_exact=check_exact,
+                nans_compare_equal=True,
+                atol=atol,
+                rtol=rtol,
+            )
+        return True
+
+    # unnest structs as series and compare
+    elif left.dtype == Struct == right.dtype:
+        ls, rs = left.struct.unnest(), right.struct.unnest()
+        if len(ls.columns) != len(rs.columns):
+            raise_assert_detail(
+                "Series",
+                "Nested struct fields mismatch",
+                len(ls.columns),
+                len(rs.columns),
+            )
+        elif len(ls) != len(rs):
+            raise_assert_detail(
+                "Series", "Nested struct length mismatch", len(ls), len(rs)
+            )
+        for s1, s2 in zip(ls, rs):
+            _assert_series_inner(
+                s1,
+                s2,
+                check_dtype=check_dtype,
+                check_exact=check_exact,
+                nans_compare_equal=True,
+                atol=atol,
+                rtol=rtol,
+            )
+        return True
+    else:
+        # fall-back to outer codepath (if mismatched dtypes we would expect
+        # the equality check to fail - unless ALL series values are null)
+        return False
+
+
 def raise_assert_detail(
     obj: str,
     detail: str,
     left: Any,
     right: Any,
     exc: AssertionError | None = None,
 ) -> None:
```

### Comparing `polars_lts_cpu-0.17.9/polars/testing/parametric/__init__.py` & `polars_lts_cpu-0.18.0/polars/testing/parametric/__init__.py`

 * *Files 13% similar despite different names*

```diff
@@ -7,28 +7,32 @@
         column,
         columns,
         dataframes,
         series,
     )
     from polars.testing.parametric.profiles import load_profile, set_profile
     from polars.testing.parametric.strategies import (
+        all_strategies,
         create_list_strategy,
+        nested_strategies,
         scalar_strategies,
     )
 else:
 
     def __getattr__(*args: Any, **kwargs: Any) -> Any:
         raise ModuleNotFoundError(
             f"polars.testing.parametric.{args[0]} requires the 'hypothesis' module"
         ) from None
 
 
 __all__ = [
+    "all_strategies",
     "column",
     "columns",
     "create_list_strategy",
     "dataframes",
     "load_profile",
+    "nested_strategies",
     "scalar_strategies",
     "series",
     "set_profile",
 ]
```

### Comparing `polars_lts_cpu-0.17.9/polars/testing/parametric/primitives.py` & `polars_lts_cpu-0.18.0/polars/testing/parametric/primitives.py`

 * *Files 1% similar despite different names*

```diff
@@ -29,24 +29,25 @@
     is_polars_dtype,
     py_type_to_dtype,
 )
 from polars.series import Series
 from polars.string_cache import StringCache
 from polars.testing.asserts import is_categorical_dtype
 from polars.testing.parametric.strategies import (
-    _hash,
+    _flexhash,
+    all_strategies,
     between,
     create_list_strategy,
     scalar_strategies,
 )
 
 if TYPE_CHECKING:
     from hypothesis.strategies import DrawFn, SearchStrategy
 
-    from polars.lazyframe import LazyFrame
+    from polars import LazyFrame
     from polars.type_aliases import OneOrMoreDataTypes, PolarsDataType
 
 
 _time_units = list(DTYPE_TEMPORAL_UNITS)
 
 
 def empty_list(value: Any, nested: bool) -> bool:
@@ -59,15 +60,19 @@
 # ====================================================================
 # Polars 'hypothesis' primitives for Series, DataFrame, and LazyFrame
 # See: https://hypothesis.readthedocs.io/
 # ====================================================================
 MAX_DATA_SIZE = 10  # max generated frame/series length
 MAX_COLS = 8  # max number of generated cols
 
-strategy_dtypes = list({dtype.base_type() for dtype in scalar_strategies})
+# note: there is a rare 'list' dtype failure that needs to be tracked
+# down before re-enabling selection from "all_strategies" ...
+strategy_dtypes = list(
+    {dtype.base_type() for dtype in scalar_strategies}  # all_strategies}
+)
 
 
 @dataclass
 class column:
     """
     Define a column for use with the @dataframes strategy.
 
@@ -96,15 +101,15 @@
     >>> column(name="ccy", strategy=sampled_from(["GBP", "EUR", "JPY"]))
     column(name='ccy', dtype=Utf8, strategy=sampled_from(['GBP', 'EUR', 'JPY']), null_probability=None, unique=False)
 
     """  # noqa: W505
 
     name: str
     dtype: PolarsDataType | None = None
-    strategy: SearchStrategy[Series | int] | None = None
+    strategy: SearchStrategy[Any] | None = None
     null_probability: float | None = None
     unique: bool = False
 
     def __post_init__(self) -> None:
         if (self.null_probability is not None) and (
             self.null_probability < 0 or self.null_probability > 1
         ):
@@ -122,15 +127,18 @@
             self.dtype = random.choice(strategy_dtypes)
 
         elif self.dtype == List:
             if self.strategy is not None:
                 self.dtype = getattr(self.strategy, "_dtype", self.dtype)
             else:
                 self.strategy = create_list_strategy(getattr(self.dtype, "inner", None))
-                self.dtype = self.strategy._dtype  # type: ignore[union-attr]
+                self.dtype = self.strategy._dtype  # type: ignore[attr-defined]
+
+        # elif self.dtype == Struct:
+        #     ...
 
         elif self.dtype not in scalar_strategies:
             if self.dtype is not None:
                 raise InvalidArgument(
                     f"No strategy (currently) available for {self.dtype} type"
                 )
             else:
@@ -349,37 +357,37 @@
         excluded_dtypes = [excluded_dtypes]
 
     selectable_dtypes = [
         dtype
         for dtype in (allowed_dtypes or strategy_dtypes)
         if dtype not in (excluded_dtypes or ())
     ]
-    if null_probability and (null_probability < 0 or null_probability > 1):
+    if null_probability and not (0 <= null_probability <= 1):
         raise InvalidArgument(
             "null_probability should be between 0.0 and 1.0; found"
             f" {null_probability}"
         )
     null_probability = float(null_probability or 0.0)
 
     @composite
     def draw_series(draw: DrawFn) -> Series:
         with StringCache():
             # create/assign series dtype and retrieve matching strategy
-            series_dtype = (
-                draw(sampled_from(selectable_dtypes))
+            series_dtype: PolarsDataType = (
+                draw(sampled_from(selectable_dtypes))  # type: ignore[assignment]
                 if dtype is None and strategy is None
                 else dtype
             )
             if strategy is None:
                 if series_dtype is Datetime or series_dtype is Duration:
                     series_dtype = series_dtype(random.choice(_time_units))  # type: ignore[operator]
-                dtype_strategy = scalar_strategies[
+                dtype_strategy = all_strategies[
                     series_dtype
-                    if series_dtype in scalar_strategies
-                    else series_dtype.base_type()  # type: ignore[union-attr]
+                    if series_dtype in all_strategies
+                    else series_dtype.base_type()
                 ]
             else:
                 dtype_strategy = strategy
 
             if series_dtype in FLOAT_DTYPES and not allow_infinities:
                 dtype_strategy = dtype_strategy.filter(
                     lambda x: not isinstance(x, float) or isfinite(x)
@@ -403,15 +411,15 @@
                 series_values = [None] * series_size
             else:
                 series_values = draw(
                     lists(
                         dtype_strategy,
                         min_size=series_size,
                         max_size=series_size,
-                        unique_by=(_hash if unique else None),
+                        unique_by=(_flexhash if unique else None),
                     )
                 )
 
             # apply null values (custom frequency)
             if null_probability and null_probability != 1:
                 for idx in range(series_size):
                     if random.random() < null_probability:
@@ -443,15 +451,15 @@
     *,
     min_cols: int | None = 0,
     max_cols: int | None = MAX_COLS,
     size: int | None = None,
     min_size: int | None = 0,
     max_size: int | None = MAX_DATA_SIZE,
     chunked: bool | None = None,
-    include_cols: Sequence[column] | None = None,
+    include_cols: Sequence[column] | column | None = None,
     null_probability: float | dict[str, float] = 0.0,
     allow_infinities: bool = True,
     allowed_dtypes: Collection[PolarsDataType] | PolarsDataType | None = None,
     excluded_dtypes: Collection[PolarsDataType] | PolarsDataType | None = None,
 ) -> SearchStrategy[DataFrame | LazyFrame]:
     """
     Hypothesis strategy for producing polars DataFrames or LazyFrames.
@@ -566,14 +574,16 @@
 
     if isinstance(min_size, int) and min_cols in (0, None):
         min_cols = 1
     if isinstance(allowed_dtypes, (DataType, DataTypeClass)):
         allowed_dtypes = [allowed_dtypes]
     if isinstance(excluded_dtypes, (DataType, DataTypeClass)):
         excluded_dtypes = [excluded_dtypes]
+    if isinstance(include_cols, column):
+        include_cols = [include_cols]
 
     selectable_dtypes = [
         dtype
         for dtype in (allowed_dtypes or strategy_dtypes)
         if dtype in strategy_dtypes and dtype not in (excluded_dtypes or ())
     ]
 
@@ -590,15 +600,15 @@
                 coldefs = columns(cols=n, dtype=dtypes_)
             elif isinstance(cols, column):
                 coldefs = [cols]
             else:
                 coldefs = list(cols)
 
             # append any explicitly provided cols
-            coldefs.extend(include_cols or ())
+            coldefs.extend(include_cols or ())  # type: ignore[arg-type]
 
             # assign dataframe/series size
             series_size = (
                 between(
                     draw, int, min_=(min_size or 0), max_=(max_size or MAX_DATA_SIZE)
                 )
                 if size is None
```

### Comparing `polars_lts_cpu-0.17.9/polars/testing/parametric/profiles.py` & `polars_lts_cpu-0.18.0/polars/testing/parametric/profiles.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/polars/type_aliases.py` & `polars_lts_cpu-0.18.0/polars/type_aliases.py`

 * *Files 7% similar despite different names*

```diff
@@ -9,30 +9,30 @@
     Collection,
     Iterable,
     List,
     Mapping,
     Sequence,
     Tuple,
     Type,
+    TypeVar,
     Union,
 )
 
 if sys.version_info >= (3, 8):
     from typing import Literal
 else:
     from typing_extensions import Literal
 
 if TYPE_CHECKING:
+    from polars import DataFrame, Expr, LazyFrame, Series
     from polars.datatypes import DataType, DataTypeClass, TemporalType
     from polars.dependencies import numpy as np
     from polars.dependencies import pandas as pd
     from polars.dependencies import pyarrow as pa
-    from polars.expr import Expr
     from polars.functions.whenthen import WhenThen, WhenThenThen
-    from polars.series import Series
 
     if sys.version_info >= (3, 10):
         from typing import TypeAlias
     else:
         from typing_extensions import TypeAlias
 
 # Data types
@@ -63,15 +63,15 @@
 SchemaDict: TypeAlias = Mapping[str, PolarsDataType]
 
 # Types that qualify as expressions (eg: for use in 'select', 'with_columns'...)
 PolarsExprType: TypeAlias = Union["Expr", "WhenThen", "WhenThenThen"]
 
 # literal types that are allowed in expressions (auto-converted to pl.lit)
 PythonLiteral: TypeAlias = Union[
-    str, int, float, bool, date, time, datetime, timedelta, bytes, Decimal
+    str, int, float, bool, date, time, datetime, timedelta, bytes, Decimal, List[Any]
 ]
 
 IntoExpr: TypeAlias = Union[PolarsExprType, PythonLiteral, "Series", None]
 ComparisonOperator: TypeAlias = Literal["eq", "neq", "gt", "lt", "gt_eq", "lt_eq"]
 
 # User-facing string literal types
 # The following all have an equivalent Rust enum with the same name
@@ -101,44 +101,55 @@
     "tb",
     "bytes",
     "kilobytes",
     "megabytes",
     "gigabytes",
     "terabytes",
 ]
-StartBy: TypeAlias = Literal["window", "datapoint", "monday"]
+StartBy: TypeAlias = Literal[
+    "window",
+    "datapoint",
+    "monday",
+    "tuesday",
+    "wednesday",
+    "thursday",
+    "friday",
+    "saturday",
+    "sunday",
+]
 TimeUnit: TypeAlias = Literal["ns", "us", "ms"]
 UniqueKeepStrategy: TypeAlias = Literal["first", "last", "any", "none"]
 UnstackDirection: TypeAlias = Literal["vertical", "horizontal"]
 ApplyStrategy: TypeAlias = Literal["thread_local", "threading"]
 
 # The following have a Rust enum equivalent with a different name
-AsofJoinStrategy: TypeAlias = Literal["backward", "forward"]  # AsofStrategy
+AsofJoinStrategy: TypeAlias = Literal["backward", "forward", "nearest"]  # AsofStrategy
 ClosedInterval: TypeAlias = Literal["left", "right", "both", "none"]  # ClosedWindow
 InterpolationMethod: TypeAlias = Literal["linear", "nearest"]
 JoinStrategy: TypeAlias = Literal[
     "inner", "left", "outer", "semi", "anti", "cross"
 ]  # JoinType
 RollingInterpolationMethod: TypeAlias = Literal[
     "nearest", "higher", "lower", "midpoint", "linear"
 ]  # QuantileInterpolOptions
 ToStructStrategy: TypeAlias = Literal[
     "first_non_null", "max_width"
 ]  # ListToStructWidthStrategy
 
 # The following have no equivalent on the Rust side
-ConcatMethod = Literal["vertical", "diagonal", "horizontal"]
+ConcatMethod = Literal["vertical", "diagonal", "horizontal", "align"]
 EpochTimeUnit = Literal["ns", "us", "ms", "s", "d"]
 Orientation: TypeAlias = Literal["col", "row"]
 SearchSortedSide: TypeAlias = Literal["any", "left", "right"]
 TransferEncoding: TypeAlias = Literal["hex", "base64"]
 CorrelationMethod: TypeAlias = Literal["pearson", "spearman"]
 DbReadEngine: TypeAlias = Literal["adbc", "connectorx"]
 DbWriteEngine: TypeAlias = Literal["sqlalchemy", "adbc"]
 DbWriteMode: TypeAlias = Literal["replace", "append", "fail"]
+WindowMappingStrategy: TypeAlias = Literal["group_to_rows", "join", "explode"]
 
 # type signature for allowed frame init
 FrameInitTypes: TypeAlias = Union[
     Mapping[str, Union[Sequence[object], Mapping[str, Sequence[object]], "Series"]],
     Sequence[Any],
     "np.ndarray[Any, Any]",
     "pa.Table",
@@ -162,7 +173,11 @@
     Mapping[str, Union[str, Collection[str]]],
     Collection[str],
     bool,
 ]
 
 # standard/named hypothesis profiles used for parametric testing
 ParametricProfileNames: TypeAlias = Literal["fast", "balanced", "expensive"]
+
+# typevars for core polars types
+PolarsType = TypeVar("PolarsType", "DataFrame", "LazyFrame", "Series", "Expr")
+FrameType = TypeVar("FrameType", "DataFrame", "LazyFrame")
```

### Comparing `polars_lts_cpu-0.17.9/polars/utils/__init__.py` & `polars_lts_cpu-0.18.0/polars/utils/__init__.py`

 * *Files 3% similar despite different names*

```diff
@@ -12,15 +12,14 @@
     _time_to_pl_time,
     _timedelta_to_pl_timedelta,
     _to_python_date,
     _to_python_datetime,
     _to_python_decimal,
     _to_python_time,
     _to_python_timedelta,
-    _tzinfo_to_str,
 )
 from polars.utils.meta import get_idx_type, get_index_type, threadpool_size
 from polars.utils.show_versions import show_versions
 from polars.utils.various import NoDefault, no_default
 
 __all__ = [
     "NoDefault",
@@ -38,9 +37,8 @@
     "_to_python_date",
     "_to_python_datetime",
     "_to_python_decimal",
     "_to_python_time",
     "_to_python_timedelta",
     "_datetime_for_anyvalue",
     "_datetime_for_anyvalue_windows",
-    "_tzinfo_to_str",
 ]
```

### Comparing `polars_lts_cpu-0.17.9/polars/utils/_construction.py` & `polars_lts_cpu-0.18.0/polars/utils/_construction.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 from __future__ import annotations
 
 import contextlib
+import warnings
 from datetime import date, datetime, time, timedelta
 from decimal import Decimal as PyDecimal
 from functools import lru_cache, partial, singledispatch
 from itertools import islice, zip_longest
 from sys import version_info
 from typing import (
     TYPE_CHECKING,
@@ -14,18 +15,21 @@
     Iterable,
     Mapping,
     MutableMapping,
     Sequence,
     get_type_hints,
 )
 
+import polars._reexport as pl
 from polars import functions as F
-from polars import internals as pli
 from polars.datatypes import (
+    FLOAT_DTYPES,
+    INTEGER_DTYPES,
     N_INFER_DEFAULT,
+    TEMPORAL_DTYPES,
     Boolean,
     Categorical,
     Date,
     Datetime,
     Duration,
     Float32,
     List,
@@ -51,42 +55,50 @@
     _check_for_pydantic,
     dataclasses,
     pydantic,
 )
 from polars.dependencies import numpy as np
 from polars.dependencies import pandas as pd
 from polars.dependencies import pyarrow as pa
-from polars.exceptions import ComputeError, ShapeError
+from polars.exceptions import ComputeError, ShapeError, TimeZoneAwareConstructorWarning
 from polars.utils._wrap import wrap_df, wrap_s
-from polars.utils.convert import _tzinfo_to_str
 from polars.utils.meta import threadpool_size
-from polars.utils.various import _is_generator, arrlen, range_to_series
+from polars.utils.various import _is_generator, arrlen, find_stacklevel, range_to_series
 
 with contextlib.suppress(ImportError):  # Module not available when building docs
     from polars.polars import PyDataFrame, PySeries
 
 if TYPE_CHECKING:
-    from polars.dataframe import DataFrame
-    from polars.series import Series
+    from polars import DataFrame, Series
     from polars.type_aliases import (
         Orientation,
         PolarsDataType,
         SchemaDefinition,
         SchemaDict,
     )
 
+
+def _get_annotations(obj: type) -> dict[str, Any]:
+    return getattr(obj, "__annotations__", {})
+
+
 if version_info >= (3, 10):
 
     def type_hints(obj: type) -> dict[str, Any]:
-        return get_type_hints(obj)
+        try:
+            # often the same as obj.__annotations__, but handles forward references
+            # encoded as string literals, adds Optional[t] if a default value equal
+            # to None is set and recursively replaces 'Annotated[T, ...]' with 'T'.
+            return get_type_hints(obj)
+        except TypeError:
+            # fallback on edge-cases (eg: InitVar inference on python 3.10).
+            return _get_annotations(obj)
 
 else:
-
-    def type_hints(obj: type) -> dict[str, Any]:
-        return getattr(obj, "__annotations__", {})
+    type_hints = _get_annotations
 
 
 @lru_cache(64)
 def is_namedtuple(cls: Any, annotated: bool = False) -> bool:
     """Check whether given class derives from NamedTuple."""
     if all(hasattr(cls, attr) for attr in ("_fields", "_field_defaults", "_replace")):
         if len(cls.__annotations__) == len(cls._fields) if annotated else True:
@@ -122,31 +134,32 @@
 
 def nt_unpack(obj: Any) -> Any:
     """Recursively unpack a nested NamedTuple."""
     if isinstance(obj, dict):
         return {key: nt_unpack(value) for key, value in obj.items()}
     elif isinstance(obj, list):
         return [nt_unpack(value) for value in obj]
-    elif is_namedtuple(obj):
+    elif is_namedtuple(obj.__class__):
         return {key: nt_unpack(value) for key, value in obj._asdict().items()}
     elif isinstance(obj, tuple):
         return tuple(nt_unpack(value) for value in obj)
     else:
         return obj
 
 
 ################################
 # Series constructor interface #
 ################################
 
 
 def series_to_pyseries(name: str, values: Series) -> PySeries:
     """Construct a PySeries from a Polars Series."""
-    values.rename(name, in_place=True)
-    return values._s
+    py_s = values._s
+    py_s.rename(name)
+    return py_s
 
 
 def arrow_to_pyseries(name: str, values: pa.Array, rechunk: bool = True) -> PySeries:
     """Construct a PySeries from an Arrow array."""
     array = coerce_arrow(values)
 
     # special handling of empty categorical arrays
@@ -155,15 +168,15 @@
         and isinstance(array.type, pa.DictionaryType)
         and array.type.value_type
         in (
             pa.utf8(),
             pa.large_utf8(),
         )
     ):
-        pys = pli.Series(name, [], dtype=Categorical)._s
+        pys = pl.Series(name, [], dtype=Categorical)._s
 
     elif not hasattr(array, "num_chunks"):
         pys = PySeries.from_arrow(name, array)
     else:
         if array.num_chunks > 1:
             # somehow going through ffi with a structarray
             # returns the first chunk everytime
@@ -250,15 +263,15 @@
     chunk_size: int = 1_000_000,
 ) -> PySeries:
     """Construct a PySeries from an iterable/generator."""
     if not isinstance(values, Generator):
         values = iter(values)
 
     def to_series_chunk(values: list[Any], dtype: PolarsDataType | None) -> Series:
-        return pli.Series(
+        return pl.Series(
             name=name,
             values=values,
             dtype=dtype,
             strict=strict,
             dtype_if_empty=dtype_if_empty,
         )
 
@@ -284,43 +297,49 @@
     return series._s
 
 
 def _construct_series_with_fallbacks(
     constructor: Callable[[str, Sequence[Any], bool], PySeries],
     name: str,
     values: Sequence[Any],
+    target_dtype: PolarsDataType | None,
     strict: bool,
 ) -> PySeries:
     """Construct Series, with fallbacks for basic type mismatch (eg: bool/int)."""
     while True:
         try:
             return constructor(name, values, strict)
         except TypeError as exc:
-            str_val = str(exc)
+            str_exc = str(exc)
 
             # from x to float
             # error message can be:
             #   - integers: "'float' object cannot be interpreted as an integer"
-            if "'float'" in str_val:
+            if "'float'" in str_exc and (
+                # we do not accept float values as int/temporal, as it causes silent
+                # information loss; the caller should explicitly cast in this case.
+                target_dtype
+                not in (INTEGER_DTYPES | TEMPORAL_DTYPES)
+            ):
                 constructor = py_type_to_constructor(float)
 
             # from x to string
             # error message can be:
             #   - integers: "'str' object cannot be interpreted as an integer"
             #   - floats: "must be real number, not str"
-            elif "'str'" in str_val or str_val == "must be real number, not str":
+            elif "'str'" in str_exc or str_exc == "must be real number, not str":
                 constructor = py_type_to_constructor(str)
 
             # from x to int
             # error message can be:
             #   - bools: "'int' object cannot be converted to 'PyBool'"
-            elif str_val == "'int' object cannot be converted to 'PyBool'":
+            elif str_exc == "'int' object cannot be converted to 'PyBool'":
                 constructor = py_type_to_constructor(int)
 
-            elif "decimal.Decimal" in str_val:
+            elif "decimal.Decimal" in str_exc:
                 constructor = py_type_to_constructor(PyDecimal)
             else:
                 raise exc
 
 
 def sequence_to_pyseries(
     name: str,
@@ -351,22 +370,23 @@
     value = _get_first_non_none(values)
     if value is not None:
         if (
             dataclasses.is_dataclass(value)
             or is_pydantic_model(value)
             or is_namedtuple(value.__class__, annotated=True)
         ):
-            return pli.DataFrame(values).to_struct(name)._s
+            return pl.DataFrame(values).to_struct(name)._s
         elif isinstance(value, range):
             values = [range_to_series("", v) for v in values]
         else:
             # for temporal dtypes:
             # * if the values are integer, we take the physical branch.
             # * if the values are python types, take the temporal branch.
             # * if the values are ISO-8601 strings, init then convert via strptime.
+            # * if the values are floats/other dtypes, this is an error.
             if dtype in py_temporal_types and isinstance(value, int):
                 dtype = py_type_to_dtype(dtype)  # construct from integer
             elif (
                 dtype in pl_temporal_types or type(dtype) in pl_temporal_types
             ) and not isinstance(value, int):
                 python_dtype = dtype_to_py_type(dtype)  # type: ignore[arg-type]
 
@@ -375,16 +395,17 @@
     if (
         dtype is not None
         and dtype not in (List, Struct, Unknown)
         and is_polars_dtype(dtype)
         and (python_dtype is None)
     ):
         constructor = polars_type_to_constructor(dtype)
-        pyseries = _construct_series_with_fallbacks(constructor, name, values, strict)
-
+        pyseries = _construct_series_with_fallbacks(
+            constructor, name, values, dtype, strict
+        )
         if dtype in (Date, Datetime, Duration, Time, Categorical, Boolean):
             if pyseries.dtype() != dtype:
                 pyseries = pyseries.cast(dtype, True)
         return pyseries
 
     elif dtype == Struct:
         struct_schema = dtype.to_schema() if isinstance(dtype, Struct) else None
@@ -399,44 +420,65 @@
             if value is None:
                 # Create a series with a dtype_if_empty dtype (if set) or Float32
                 # (if not set) for a sequence which contains only None values.
                 constructor = polars_type_to_constructor(
                     dtype_if_empty if dtype_if_empty else Float32
                 )
                 return _construct_series_with_fallbacks(
-                    constructor, name, values, strict
+                    constructor, name, values, dtype, strict
                 )
 
             # generic default dtype
             python_dtype = type(value)
 
         # temporal branch
         if python_dtype in py_temporal_types:
             if dtype is None:
                 dtype = py_type_to_dtype(python_dtype)  # construct from integer
             elif dtype in py_temporal_types:
                 dtype = py_type_to_dtype(dtype)
-            time_unit = getattr(dtype, "time_unit", None)
+
+            values_dtype = (
+                None
+                if value is None
+                else py_type_to_dtype(type(value), raise_unmatched=False)
+            )
+            if values_dtype in FLOAT_DTYPES:
+                raise TypeError(
+                    # we do not accept float values as temporal; if this is
+                    # required, the caller should explicitly cast to int first.
+                    f"'float' object cannot be interpreted as a {python_dtype.__name__}"
+                )
 
             # we use anyvalue builder to create the datetime array
             # we store the values internally as UTC and set the timezone
             py_series = PySeries.new_from_anyvalues(name, values, strict)
+            time_unit = getattr(dtype, "time_unit", None)
             if time_unit is None:
                 s = wrap_s(py_series)
             else:
                 s = wrap_s(py_series).dt.cast_time_unit(time_unit)
             if dtype == Datetime and value.tzinfo is not None:
-                tz = _tzinfo_to_str(value.tzinfo)
+                tz = str(value.tzinfo)
                 dtype_tz = dtype.time_zone  # type: ignore[union-attr]
                 if dtype_tz is not None and tz != dtype_tz:
                     raise ValueError(
                         "Given time_zone is different from that of timezone aware datetimes."
                         f" Given: '{dtype_tz}', got: '{tz}'."
                     )
-                return s.dt.replace_time_zone("UTC").dt.convert_time_zone(tz)._s
+                if tz != "UTC":
+                    warnings.warn(
+                        "Constructing a Series with time-zone-aware "
+                        "datetimes results in a Series with UTC time zone. "
+                        "To silence this warning, you can filter "
+                        "warnings of class TimeZoneAwareConstructorWarning.",
+                        TimeZoneAwareConstructorWarning,
+                        stacklevel=find_stacklevel(),
+                    )
+                return s.dt.replace_time_zone("UTC")._s
             return s._s
 
         elif (
             _check_for_numpy(value)
             and isinstance(value, np.ndarray)
             and len(value.shape) == 1
         ):
@@ -446,34 +488,37 @@
                 strict,
             )
 
         elif python_dtype in (list, tuple):
             if isinstance(dtype, Object):
                 return PySeries.new_object(name, values, strict)
             if dtype:
-                return sequence_from_anyvalue_or_object(name, values).cast(
-                    dtype, strict=False
-                )
+                srs = sequence_from_anyvalue_or_object(name, values)
+                if dtype.is_not(srs.dtype()):
+                    srs = srs.cast(dtype, strict=False)
+                return srs
             return sequence_from_anyvalue_or_object(name, values)
 
-        elif python_dtype == pli.Series:
+        elif python_dtype == pl.Series:
             return PySeries.new_series_list(name, [v._s for v in values], strict)
 
         elif python_dtype == PySeries:
             return PySeries.new_series_list(name, values, strict)
         else:
             constructor = py_type_to_constructor(python_dtype)
             if constructor == PySeries.new_object:
                 try:
                     return PySeries.new_from_anyvalues(name, values, strict)
                 # raised if we cannot convert to Wrap<AnyValue>
                 except RuntimeError:
                     return sequence_from_anyvalue_or_object(name, values)
 
-            return _construct_series_with_fallbacks(constructor, name, values, strict)
+            return _construct_series_with_fallbacks(
+                constructor, name, values, dtype, strict
+            )
 
 
 def _pandas_series_to_arrow(
     values: pd.Series | pd.DatetimeIndex,
     nan_to_null: bool = True,
     length: int | None = None,
 ) -> pa.Array:
@@ -535,23 +580,24 @@
     data: list[PySeries], columns: Sequence[str] | None = None, from_dict: bool = False
 ) -> list[PySeries]:
     """Rename data according to columns argument."""
     if not columns:
         return data
     else:
         if not data:
-            return [pli.Series(c, None)._s for c in columns]
+            return [pl.Series(c, None)._s for c in columns]
         elif len(data) == len(columns):
             if from_dict:
                 series_map = {s.name(): s for s in data}
                 if all((col in series_map) for col in columns):
                     return [series_map[col] for col in columns]
-
             for i, c in enumerate(columns):
-                data[i].rename(c)
+                if c != data[i].name():
+                    data[i] = data[i].clone()
+                    data[i].rename(c)
             return data
         else:
             raise ValueError("Dimensions of columns arg must match data dimensions.")
 
 
 def _post_apply_columns(
     pydf: PyDataFrame,
@@ -647,40 +693,44 @@
     if data:
         dtypes = schema_overrides or {}
         array_len = max((arrlen(val) or 0) for val in data.values())
         if array_len > 0:
             for name, val in data.items():
                 dtype = dtypes.get(name)
                 if isinstance(val, dict) and dtype != Struct:
-                    updated_data[name] = pli.DataFrame(val).to_struct(name)
+                    updated_data[name] = pl.DataFrame(val).to_struct(name)
+
+                elif isinstance(val, pl.Series):
+                    s = val.rename(name) if name != val.name else val
+                    if dtype and dtype != s.dtype:
+                        s = s.cast(dtype)
+                    updated_data[name] = s
 
                 elif arrlen(val) is not None or _is_generator(val):
-                    updated_data[name] = pli.Series(
+                    updated_data[name] = pl.Series(
                         name=name, values=val, dtype=dtype, nan_to_null=nan_to_null
                     )
                 elif val is None or isinstance(  # type: ignore[redundant-expr]
                     val, (int, float, str, bool, date, datetime, time, timedelta)
                 ):
-                    updated_data[name] = pli.Series(
+                    updated_data[name] = pl.Series(
                         name=name, values=[val], dtype=dtype
                     ).extend_constant(val, array_len - 1)
                 else:
-                    updated_data[name] = pli.Series(
+                    updated_data[name] = pl.Series(
                         name=name, values=[val] * array_len, dtype=dtype
                     )
 
         elif all((arrlen(val) == 0) for val in data.values()):
             for name, val in data.items():
-                updated_data[name] = pli.Series(
-                    name, values=val, dtype=dtypes.get(name)
-                )
+                updated_data[name] = pl.Series(name, values=val, dtype=dtypes.get(name))
 
         elif all((arrlen(val) is None) for val in data.values()):
             for name, val in data.items():
-                updated_data[name] = pli.Series(
+                updated_data[name] = pl.Series(
                     name,
                     values=(val if _is_generator(val) else [val]),
                     dtype=dtypes.get(name),
                 )
     if order and list(updated_data) != order:
         return {col: updated_data.pop(col) for col in order}
     return updated_data
@@ -725,25 +775,25 @@
 
             pool_size = threadpool_size()
             with multiprocessing.dummy.Pool(pool_size) as pool:
                 data = dict(
                     zip(
                         column_names,
                         pool.map(
-                            lambda t: pli.Series(t[0], t[1])
+                            lambda t: pl.Series(t[0], t[1])
                             if isinstance(t[1], np.ndarray)
                             else t[1],
                             [(k, v) for k, v in data.items()],
                         ),
                     )
                 )
 
     if not data and schema_overrides:
         data_series = [
-            pli.Series(
+            pl.Series(
                 name, [], dtype=schema_overrides.get(name), nan_to_null=nan_to_null
             )._s
             for name in column_names
         ]
     else:
         data_series = [
             s._s
@@ -789,15 +839,15 @@
         schema or series_names,
         schema_overrides=schema_overrides,
         n_expected=len(data),
     )
     data_series: list[PySeries] = []
     for i, s in enumerate(data):
         if not s.name:
-            s.rename(column_names[i], in_place=True)
+            s = s.alias(column_names[i])
         new_dtype = schema_overrides.get(column_names[i])
         if new_dtype and new_dtype != s.dtype:
             s = s.cast(new_dtype)
         data_series.append(s._s)
 
     data_series = _handle_columns_arg(data_series, columns=column_names)
     return PyDataFrame(data_series)
@@ -830,15 +880,15 @@
 
     if isinstance(first_element, Generator):
         to_pydf = _sequence_of_sequence_to_pydf
         data = [list(row) for row in data]
         first_element = data[0]
         register_with_singledispatch = False
 
-    elif isinstance(first_element, pli.Series):
+    elif isinstance(first_element, pl.Series):
         to_pydf = _sequence_of_series_to_pydf
 
     elif _check_for_numpy(first_element) and isinstance(first_element, np.ndarray):
         to_pydf = _sequence_of_numpy_to_pydf
 
     elif _check_for_pandas(first_element) and isinstance(
         first_element, (pd.Series, pd.DatetimeIndex)
@@ -895,15 +945,15 @@
 
         unpack_nested = False
         for col, tp in local_schema_override.items():
             if tp == Categorical:
                 local_schema_override[col] = Utf8
             elif not unpack_nested and (tp.base_type() in (Unknown, Struct)):
                 unpack_nested = contains_nested(
-                    getattr(first_element, col, None), is_namedtuple
+                    getattr(first_element, col, None).__class__, is_namedtuple
                 )
 
         if unpack_nested:
             dicts = [nt_unpack(d) for d in data]
             pydf = PyDataFrame.read_dicts(dicts, infer_schema_length)
         else:
             pydf = PyDataFrame.read_rows(
@@ -918,15 +968,15 @@
         return pydf
 
     if orient == "col" or orient is None:
         column_names, schema_overrides = _unpack_schema(
             schema, schema_overrides=schema_overrides, n_expected=len(data)
         )
         data_series: list[PySeries] = [
-            pli.Series(
+            pl.Series(
                 column_names[i], element, schema_overrides.get(column_names[i])
             )._s
             for i, element in enumerate(data)
         ]
         return PyDataFrame(data_series)
 
     raise ValueError(
@@ -1004,15 +1054,15 @@
     schema_overrides: SchemaDict | None,
     **kwargs: Any,
 ) -> PyDataFrame:
     column_names, schema_overrides = _unpack_schema(
         schema, schema_overrides=schema_overrides, n_expected=1
     )
     data_series: list[PySeries] = [
-        pli.Series(column_names[0], data, schema_overrides.get(column_names[0]))._s
+        pl.Series(column_names[0], data, schema_overrides.get(column_names[0]))._s
     ]
     data_series = _handle_columns_arg(data_series, columns=column_names)
     return PyDataFrame(data_series)
 
 
 def _sequence_of_numpy_to_pydf(
     first_element: np.ndarray[Any, Any],
@@ -1074,15 +1124,23 @@
     else:
         column_names = []
         schema_override = {
             col: (py_type_to_dtype(tp, raise_unmatched=False) or Unknown)
             for col, tp in type_hints(first_element.__class__).items()
             if col != "__slots__"
         }
-        schema_override.update(schema_overrides or {})
+        if schema_overrides:
+            schema_override.update(schema_overrides)
+        elif not from_model:
+            dc_fields = set(asdict(first_element))
+            schema_overrides = schema_override = {
+                nm: tp for nm, tp in schema_override.items() if nm in dc_fields
+            }
+        else:
+            schema_overrides = schema_override
 
     for col, tp in schema_override.items():
         if tp == Categorical:
             schema_override[col] = Utf8
         elif not unpack_nested and (tp.base_type() in (Unknown, Struct)):
             unpack_nested = contains_nested(
                 getattr(first_element, col, None),
@@ -1117,87 +1175,110 @@
 def numpy_to_pydf(
     data: np.ndarray[Any, Any],
     schema: SchemaDefinition | None = None,
     schema_overrides: SchemaDict | None = None,
     orient: Orientation | None = None,
     nan_to_null: bool = False,
 ) -> PyDataFrame:
-    """Construct a PyDataFrame from a numpy ndarray."""
+    """Construct a PyDataFrame from a numpy ndarray (including structured ndarrays)."""
     shape = data.shape
 
-    # Unpack columns
-    if shape == (0,):
-        n_columns = 0
-
-    elif len(shape) == 1:
-        n_columns = 1
+    if data.dtype.names is not None:
+        structured_array, orient = True, "col"
+        record_names = list(data.dtype.names)
+        n_columns = len(record_names)
+        for nm in record_names:
+            shape = data[nm].shape
+            if len(data[nm].shape) > 2:
+                raise ValueError(
+                    f"Cannot create DataFrame from structured array with elements > 2D; shape[{nm!r}] = {shape}"
+                )
+        if not schema:
+            schema = record_names
+    else:
+        # Unpack columns
+        structured_array, record_names = False, []
+        if shape == (0,):
+            n_columns = 0
+
+        elif len(shape) == 1:
+            n_columns = 1
+
+        elif len(shape) == 2:
+            if orient is None and schema is None:
+                # default convention; first axis is rows, second axis is columns
+                n_columns = shape[1]
+                orient = "row"
 
-    elif len(shape) == 2:
-        # default convention
-        # first axis is rows, second axis is columns
-        if orient is None and schema is None:
-            n_columns = shape[1]
-            orient = "row"
+            elif orient is None and schema is not None:
+                # infer orientation from 'schema' param
+                if len(schema) == shape[0]:
+                    orient = "col"
+                    n_columns = shape[0]
+                else:
+                    orient = "row"
+                    n_columns = shape[1]
 
-        # Infer orientation if columns argument is given
-        elif orient is None and schema is not None:
-            if len(schema) == shape[0]:
-                orient = "col"
+            elif orient == "row":
+                n_columns = shape[1]
+            elif orient == "col":
                 n_columns = shape[0]
             else:
-                orient = "row"
-                n_columns = shape[1]
-
-        elif orient == "row":
-            n_columns = shape[1]
-        elif orient == "col":
-            n_columns = shape[0]
+                raise ValueError(
+                    f"orient must be one of {{'col', 'row', None}}; found {orient!r} instead."
+                )
         else:
             raise ValueError(
-                f"orient must be one of {{'col', 'row', None}}, got {orient} instead."
+                f"Cannot create DataFrame from array with more than two dimensions; shape = {shape}"
             )
-    else:
-        raise ValueError(
-            "Cannot create DataFrame from numpy array with more than two dimensions."
-        )
 
     if schema is not None and len(schema) != n_columns:
-        raise ValueError("Dimensions of columns arg must match data dimensions.")
+        raise ValueError("Dimensions of 'schema' arg must match data dimensions.")
 
     column_names, schema_overrides = _unpack_schema(
         schema, schema_overrides=schema_overrides, n_expected=n_columns
     )
 
     # Convert data to series
-    if shape == (0,):
+    if structured_array:
+        data_series = [
+            pl.Series(
+                name=series_name,
+                values=data[record_name],
+                dtype=schema_overrides.get(record_name),
+                nan_to_null=nan_to_null,
+            )._s
+            for series_name, record_name in zip(column_names, record_names)
+        ]
+    elif shape == (0,):
         data_series = []
 
     elif len(shape) == 1:
         data_series = [
-            pli.Series(
+            pl.Series(
                 name=column_names[0],
                 values=data,
                 dtype=schema_overrides.get(column_names[0]),
                 nan_to_null=nan_to_null,
             )._s
         ]
     else:
         if orient == "row":
             data_series = [
-                pli.Series(
+                pl.Series(
                     name=column_names[i],
                     values=data[:, i],
                     dtype=schema_overrides.get(column_names[i]),
                     nan_to_null=nan_to_null,
                 )._s
                 for i in range(n_columns)
             ]
         else:
             data_series = [
-                pli.Series(
+                pl.Series(
                     name=column_names[i],
                     values=data[i],
                     dtype=schema_overrides.get(column_names[i]),
                     nan_to_null=nan_to_null,
                 )._s
                 for i in range(n_columns)
             ]
@@ -1246,24 +1327,21 @@
             data_dict[name] = column
 
     if len(data_dict) > 0:
         tbl = pa.table(data_dict)
 
         # path for table without rows that keeps datatype
         if tbl.shape[0] == 0:
-            pydf = pli.DataFrame(
-                [
-                    pli.Series(name, c)
-                    for (name, c) in zip(tbl.column_names, tbl.columns)
-                ]
+            pydf = pl.DataFrame(
+                [pl.Series(name, c) for (name, c) in zip(tbl.column_names, tbl.columns)]
             )._df
         else:
             pydf = PyDataFrame.from_arrow_record_batches(tbl.to_batches())
     else:
-        pydf = pli.DataFrame([])._df
+        pydf = pl.DataFrame([])._df
     if rechunk:
         pydf = pydf.rechunk()
 
     reset_order = False
     if len(dictionary_cols) > 0:
         df = wrap_df(pydf)
         df = df.with_columns([F.lit(s).alias(s.name) for s in dictionary_cols.values()])
@@ -1339,25 +1417,25 @@
     if orient == "col":
         if column_names and schema_overrides:
             dtypes_by_idx = {
                 idx: schema_overrides.get(col, Unknown)
                 for idx, col in enumerate(column_names)
             }
 
-        return pli.DataFrame(
+        return pl.DataFrame(
             {
-                (column_names[idx] if column_names else f"column_{idx}"): pli.Series(
+                (column_names[idx] if column_names else f"column_{idx}"): pl.Series(
                     coldata, dtype=dtypes_by_idx.get(idx)
                 )
                 for idx, coldata in enumerate(data)
             }
         )._df
 
     def to_frame_chunk(values: list[Any], schema: SchemaDefinition | None) -> DataFrame:
-        return pli.DataFrame(
+        return pl.DataFrame(
             data=values,
             schema=schema,
             orient="row",
             infer_schema_length=infer_schema_length,
         )
 
     n_chunks = 0
```

### Comparing `polars_lts_cpu-0.17.9/polars/utils/_scan.py` & `polars_lts_cpu-0.18.0/polars/utils/_scan.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Any
 
 from polars.dependencies import pickle
 
 if TYPE_CHECKING:
-    from polars.dataframe import DataFrame
+    from polars import DataFrame
 
 
 def _deserialize_and_execute(
     buf: bytes, with_columns: list[str] | None, *args: Any
 ) -> DataFrame:
     """
     Deserialize and execute the given function for the projected columns.
```

### Comparing `polars_lts_cpu-0.17.9/polars/utils/_wrap.py` & `polars_lts_cpu-0.18.0/polars/utils/_wrap.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,28 +1,25 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING
 
-from polars import internals as pli
+import polars._reexport as pl
 
 if TYPE_CHECKING:
-    from polars.dataframe import DataFrame
-    from polars.expr import Expr
-    from polars.lazyframe import LazyFrame
+    from polars import DataFrame, Expr, LazyFrame, Series
     from polars.polars import PyDataFrame, PyExpr, PyLazyFrame, PySeries
-    from polars.series import Series
 
 
 def wrap_df(df: PyDataFrame) -> DataFrame:
-    return pli.DataFrame._from_pydf(df)
+    return pl.DataFrame._from_pydf(df)
 
 
 def wrap_ldf(ldf: PyLazyFrame) -> LazyFrame:
-    return pli.LazyFrame._from_pyldf(ldf)
+    return pl.LazyFrame._from_pyldf(ldf)
 
 
 def wrap_s(s: PySeries) -> Series:
-    return pli.Series._from_pyseries(s)
+    return pl.Series._from_pyseries(s)
 
 
 def wrap_expr(pyexpr: PyExpr) -> Expr:
-    return pli.Expr._from_pyexpr(pyexpr)
+    return pl.Expr._from_pyexpr(pyexpr)
```

### Comparing `polars_lts_cpu-0.17.9/polars/utils/build_info.py` & `polars_lts_cpu-0.18.0/polars/utils/build_info.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/polars/utils/convert.py` & `polars_lts_cpu-0.18.0/polars/utils/convert.py`

 * *Files 1% similar despite different names*

```diff
@@ -254,15 +254,7 @@
 
 @lru_cache(None)
 def _create_decimal_with_prec(
     precision: int,
 ) -> Callable[[tuple[int, Sequence[int], int]], Decimal]:
     # pre-cache contexts so we don't have to spend time on recreating them every time
     return Context(prec=precision).create_decimal
-
-
-def _tzinfo_to_str(tzinfo: tzinfo) -> str:
-    if tzinfo == timezone.utc:
-        return "UTC"
-    if isinstance(tzinfo, timezone):
-        return str(tzinfo).replace("UTC", "")
-    return str(tzinfo)
```

### Comparing `polars_lts_cpu-0.17.9/polars/utils/decorators.py` & `polars_lts_cpu-0.18.0/polars/utils/decorators.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from __future__ import annotations
 
-import functools
 import inspect
 import warnings
+from functools import partial, wraps
 from typing import TYPE_CHECKING, Any, Callable, TypeVar
 
 from polars.utils.various import find_stacklevel
 
 if TYPE_CHECKING:
     import sys
 
@@ -27,15 +27,15 @@
 
     @deprecated_alias(old_arg='new_arg')
     def myfunc(new_arg):
         ...
     """
 
     def deco(function: Callable[P, T]) -> Callable[P, T]:
-        @functools.wraps(function)
+        @wraps(function)
         def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:
             _rename_kwargs(function.__name__, kwargs, aliases)
             return function(*args, **kwargs)
 
         return wrapper
 
     return deco
@@ -119,15 +119,15 @@
                 f"All arguments of {function.__qualname__}{{except_args}} will be keyword-only in the next breaking release."
                 " Use keyword arguments to silence this warning."
             )
             msg = msg_format.format(except_args=_format_argument_list(allow_args))
         else:
             msg = message
 
-        @functools.wraps(function)
+        @wraps(function)
         def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:
             if len(args) > num_allowed_args:
                 warnings.warn(msg, DeprecationWarning, stacklevel=find_stacklevel())
             return function(*args, **kwargs)
 
         wrapper.__signature__ = new_sig  # type: ignore[attr-defined]
         return wrapper
@@ -148,33 +148,41 @@
         return f" except for {allowed_args[0]!r}"
     else:
         last = allowed_args[-1]
         args = ", ".join([f"{x!r}" for x in allowed_args[:-1]])
         return f" except for {args} and {last!r}"
 
 
-def redirect(from_to: dict[str, str]) -> Callable[[type[T]], type[T]]:
+def redirect(
+    from_to: dict[str, str | tuple[str, dict[str, Any]]]
+) -> Callable[[type[T]], type[T]]:
     """
     Class decorator allowing deprecation/transition from one method name to another.
 
-    The parameters must be the same (unless they are being renamed, in
-    which case you can use this in conjunction with @deprecated_alias).
+    The parameters must be the same (unless they are being renamed, in which case
+    you can use this in conjunction with @deprecated_alias). If you need to redirect
+    with custom kwargs, can redirect to a method name and associated kwargs dict.
     """
 
     def _redirecting_getattr_(obj: T, item: Any) -> Any:
         if isinstance(item, str) and item in from_to:
             new_item = from_to[item]
+            new_item_name = new_item if isinstance(new_item, str) else new_item[0]
             warnings.warn(
                 f"`{type(obj).__name__}.{item}` has been renamed; this"
-                f" redirect is temporary, please use `.{new_item}` instead",
+                f" redirect is temporary, please use `.{new_item_name}` instead",
                 category=DeprecationWarning,
                 stacklevel=find_stacklevel(),
             )
-            item = new_item
-        return obj.__getattribute__(item)
+            item = new_item_name
+
+        attr = obj.__getattribute__(item)
+        if isinstance(new_item, tuple):
+            attr = partial(attr, **new_item[1])
+        return attr
 
     def _cls_(cls: type[T]) -> type[T]:
         # note: __getattr__ is only invoked if item isn't found on the class
         cls.__getattr__ = _redirecting_getattr_  # type: ignore[attr-defined]
         return cls
 
     return _cls_
```

### Comparing `polars_lts_cpu-0.17.9/polars/utils/meta.py` & `polars_lts_cpu-0.18.0/polars/utils/meta.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/polars/utils/polars_version.py` & `polars_lts_cpu-0.18.0/polars/utils/polars_version.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/polars/utils/show_versions.py` & `polars_lts_cpu-0.18.0/polars/utils/show_versions.py`

 * *Files 10% similar despite different names*

```diff
@@ -9,44 +9,54 @@
 def show_versions() -> None:
     """
     Print out version of Polars and dependencies to stdout.
 
     Examples
     --------
     >>> pl.show_versions()  # doctest: +SKIP
-    ---Version info---
-    Polars: 0.16.13
-    Index type: UInt32
-    Platform: macOS-13.2.1-arm64-arm-64bit
-    Python: 3.11.2 (main, Feb 16 2023, 02:55:59) [Clang 14.0.0 (clang-1400.0.29.202)]
-    ---Optional dependencies---
-    numpy: 1.24.2
-    pandas: 1.5.3
-    pyarrow: 11.0.0
-    connectorx: 0.3.2_alpha.2
-    deltalake: <version not detected>
-    fsspec: <not installed>
-    matplotlib: <not installed>
-    xlsx2csv: 0.8.1
-    xlsxwriter: 3.0.8
-
+    --------Version info---------
+    Polars:      0.17.11
+    Index type:  UInt32
+    Platform:    Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
+    Python:      3.11.3 (main, Apr 15 2023, 14:44:51) [GCC 11.3.0]
+
+    ----Optional dependencies----
+    numpy:       1.24.2
+    pandas:      2.0.0
+    pyarrow:     11.0.0
+    connectorx:  <not installed>
+    deltalake:   0.8.1
+    fsspec:      2023.4.0
+    matplotlib:  3.7.1
+    xlsx2csv:    0.8.1
+    xlsxwriter:  3.1.0
     """
     # note: we import 'platform' here as a micro-optimisation for initial import
     import platform
 
-    print("---Version info---")
-    print(f"Polars: {get_polars_version()}")
-    print(f"Index type: {get_index_type()}")
-    print(f"Platform: {platform.platform()}")
-    print(f"Python: {sys.version}")
-
-    print("---Optional dependencies---")
+    # optional dependencies
     deps = _get_dependency_info()
+
+    # determine key length for alignment
+    keylen = (
+        max(
+            len(x) for x in [*deps.keys(), "Polars", "Index type", "Platform", "Python"]
+        )
+        + 1
+    )
+
+    print("--------Version info---------")
+    print(f"{'Polars:':{keylen}s} {get_polars_version()}")
+    print(f"{'Index type:':{keylen}s} {get_index_type()}")
+    print(f"{'Platform:':{keylen}s} {platform.platform()}")
+    print(f"{'Python:':{keylen}s} {sys.version}")
+
+    print("\n----Optional dependencies----")
     for name, v in deps.items():
-        print(f"{name}: {v}")
+        print(f"{name:{keylen}s} {v}")
 
 
 def _get_dependency_info() -> dict[str, str]:
     # see the list of dependencies in pyproject.toml
     opt_deps = [
         "numpy",
         "pandas",
@@ -54,15 +64,15 @@
         "connectorx",
         "deltalake",
         "fsspec",
         "matplotlib",
         "xlsx2csv",
         "xlsxwriter",
     ]
-    return {name: _get_dependency_version(name) for name in opt_deps}
+    return {f"{name}:": _get_dependency_version(name) for name in opt_deps}
 
 
 def _get_dependency_version(dep_name: str) -> str:
     # note: we import 'importlib' here as a significiant optimisation for initial import
     import importlib
 
     if sys.version_info >= (3, 8):
```

### Comparing `polars_lts_cpu-0.17.9/polars/utils/various.py` & `polars_lts_cpu-0.18.0/polars/utils/various.py`

 * *Files 3% similar despite different names*

```diff
@@ -5,56 +5,51 @@
 import re
 import sys
 from collections.abc import MappingView, Sized
 from enum import Enum
 from typing import TYPE_CHECKING, Any, Generator, Iterable, Sequence, TypeVar
 
 import polars as pl
-
-if sys.version_info >= (3, 8):
-    from typing import Literal
-else:
-    from typing_extensions import Literal
-
 from polars import functions as F
 from polars.datatypes import (
     Boolean,
     Date,
     Datetime,
     Duration,
     Int64,
     Time,
     Utf8,
     is_polars_dtype,
 )
 
-if TYPE_CHECKING:
-    from polars.internals import DataFrame
-    from polars.series import Series
-
-
-# note: reversed views don't match as instances of MappingView
-if sys.version_info >= (3, 11):
-    _views: list[Reversible[Any]] = [{}.keys(), {}.values(), {}.items()]
-    _reverse_mapping_views = tuple(type(reversed(view)) for view in _views)
+if sys.version_info >= (3, 8):
+    from typing import Literal
+else:
+    from typing_extensions import Literal
 
 if TYPE_CHECKING:
     from collections.abc import Reversible
     from pathlib import Path
 
+    from polars import DataFrame, Series
     from polars.type_aliases import PolarsDataType, SizeUnit
 
     if sys.version_info >= (3, 10):
         from typing import ParamSpec, TypeGuard
     else:
         from typing_extensions import ParamSpec, TypeGuard
 
     P = ParamSpec("P")
     T = TypeVar("T")
 
+# note: reversed views don't match as instances of MappingView
+if sys.version_info >= (3, 11):
+    _views: list[Reversible[Any]] = [{}.keys(), {}.values(), {}.items()]
+    _reverse_mapping_views = tuple(type(reversed(view)) for view in _views)
+
 
 def _process_null_values(
     null_values: None | str | Sequence[str] | dict[str, str] = None,
 ) -> None | str | Sequence[str] | list[tuple[str, str]]:
     if isinstance(null_values, dict):
         return list(null_values.items())
     else:
@@ -100,24 +95,24 @@
     """
     if allow_str is False and isinstance(val, str):
         return False
     return isinstance(val, Sequence) and _is_iterable_of(val, str)
 
 
 def range_to_series(
-    name: str, rng: range, dtype: PolarsDataType | None = Int64
+    name: str, rng: range, dtype: PolarsDataType | None = None
 ) -> Series:
     """Fast conversion of the given range to a Series."""
     return F.arange(
         start=rng.start,
         end=rng.stop,
         step=rng.step,
-        eager=True,
         dtype=dtype,
-    ).rename(name, in_place=True)
+        eager=True,
+    ).alias(name)
 
 
 def range_to_slice(rng: range) -> slice:
     """Return the given range as an equivalent slice."""
     return slice(rng.start, rng.stop, rng.step)
 
 
@@ -192,14 +187,21 @@
 def parse_version(version: Sequence[str | int]) -> tuple[int, ...]:
     """Simple version parser; split into a tuple of ints for comparison."""
     if isinstance(version, str):
         version = version.split(".")
     return tuple(int(re.sub(r"\D", "", str(v))) for v in version)
 
 
+def ordered_unique(values: Sequence[Any]) -> list[Any]:
+    """Return unique list of sequence values, maintaining their order of appearance."""
+    seen: set[Any] = set()
+    add_ = seen.add
+    return [v for v in values if not (v in seen or add_(v))]
+
+
 def scale_bytes(sz: int, unit: SizeUnit) -> int | float:
     """Scale size in bytes to other size units (eg: "kb", "mb", "gb", "tb")."""
     if unit in {"b", "bytes"}:
         return sz
     elif unit in {"kb", "kilobytes"}:
         return sz / 1024
     elif unit in {"mb", "megabytes"}:
@@ -211,15 +213,15 @@
     else:
         raise ValueError(
             f"unit must be one of {{'b', 'kb', 'mb', 'gb', 'tb'}}, got {unit!r}"
         )
 
 
 def _cast_repr_strings_with_schema(
-    df: DataFrame, schema: dict[str, PolarsDataType]
+    df: DataFrame, schema: dict[str, PolarsDataType | None]
 ) -> DataFrame:
     """
     Utility function to cast table repr/string values into frame-native types.
 
     Parameters
     ----------
     df
@@ -229,14 +231,15 @@
 
     Notes
     -----
     Table repr strings are less strict (or different) than equivalent CSV data, so need
     special handling; as this function is only used for reprs, parsing is flexible.
 
     """
+    tp: PolarsDataType | None
     if not df.is_empty():
         for tp in df.schema.values():
             if tp != Utf8:
                 raise TypeError(
                     f"DataFrame should contain only Utf8 string repr data; found {tp}"
                 )
 
@@ -332,15 +335,18 @@
     # https://github.com/pandas-dev/pandas/blob/e7859983a814b1823cf26e3b491ae2fa3be47c53/pandas/_libs/lib.pyx#L2736-L2748
     no_default = "NO_DEFAULT"
 
     def __repr__(self) -> str:
         return "<no_default>"
 
 
-no_default = _NoDefault.no_default  # Sentinel indicating the default value.
+# 'NoDefault' is a sentinel indicating that no default value has been set; note that
+# this should typically be used only when one of the valid parameter values is also
+# None, as otherwise we cannot determine if the caller has explicitly set that value.
+no_default = _NoDefault.no_default
 NoDefault = Literal[_NoDefault.no_default]
 
 
 def find_stacklevel() -> int:
     """
     Find the first place in the stack that is not inside polars (tests notwithstanding).
 
@@ -357,7 +363,34 @@
         fname = inspect.getfile(frame)
         if fname.startswith(pkg_dir) and not fname.startswith(test_dir):
             frame = frame.f_back
             n += 1
         else:
             break
     return n
+
+
+def _get_stack_locals(
+    of_type: type | tuple[type, ...] | None = None, n_objects: int | None = None
+) -> dict[str, Any]:
+    """
+    Retrieve f_locals from all stack frames (starting from the current frame).
+
+    Parameters
+    ----------
+    of_type
+        Only return objects of this type.
+    n_objects
+        If specified, return only the most recent ``n`` matching objects.
+
+    """
+    objects = {}
+    stack_frame = getattr(inspect.currentframe(), "f_back", None)
+    while stack_frame:
+        local_items = list(stack_frame.f_locals.items())
+        for nm, obj in reversed(local_items):
+            if nm not in objects and (not of_type or isinstance(obj, of_type)):
+                objects[nm] = obj
+                if n_objects is not None and len(objects) >= n_objects:
+                    return objects
+        stack_frame = stack_frame.f_back
+    return objects
```

### Comparing `polars_lts_cpu-0.17.9/pyproject.toml` & `polars_lts_cpu-0.18.0/pyproject.toml`

 * *Files 1% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 readme = "README.md"
 authors = [
   { name = "Ritchie Vink", email = "ritchie46@gmail.com" },
 ]
 license = { file = "LICENSE" }
 requires-python = ">=3.7"
 dependencies = [
-  "typing_extensions >= 4.0.1; python_version < '3.11'",
+  "typing_extensions >= 4.0.1; python_version < '3.8'",
 ]
 keywords = ["dataframe", "arrow", "out-of-core"]
 classifiers = [
   "Development Status :: 5 - Production/Stable",
   "Environment :: Console",
   "Intended Audience :: Science/Research",
   "License :: OSI Approved :: MIT License",
```

### Comparing `polars_lts_cpu-0.17.9/requirements-dev.txt` & `polars_lts_cpu-0.18.0/requirements-dev.txt`

 * *Files 2% similar despite different names*

```diff
@@ -11,18 +11,18 @@
 pyarrow
 pydantic
 backports.zoneinfo; python_version < '3.9'
 tzdata; platform_system == 'Windows'
 xlsx2csv
 XlsxWriter
 adbc_driver_sqlite; python_version >= '3.9' and platform_system != 'Windows'
-connectorx==0.3.2a2; python_version >= '3.8'  # Latest full release is broken - unpin when 0.3.2 released
+connectorx==0.3.2a5; python_version >= '3.8'  # Latest full release is broken - unpin when 0.3.2 released
 
 # Tooling
-hypothesis==6.72.1
+hypothesis==6.75.1
 maturin==0.14.10
 pytest==7.3.0
 pytest-cov==4.0.0
 pytest-xdist==3.2.0
 
 # Stub files
 pandas-stubs==1.2.0.62
```

### Comparing `polars_lts_cpu-0.17.9/scripts/check_stacklevels.py` & `polars_lts_cpu-0.18.0/scripts/check_stacklevels.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/src/apply/dataframe.rs` & `polars_lts_cpu-0.18.0/src/apply/dataframe.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/src/apply/mod.rs` & `polars_lts_cpu-0.18.0/src/apply/mod.rs`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 pub mod dataframe;
+pub mod lazy;
 pub mod series;
 
 use std::collections::BTreeMap;
 
 use polars::chunked_array::builder::get_list_builder;
 use polars::prelude::*;
 use polars_core::export::rayon::prelude::*;
```

### Comparing `polars_lts_cpu-0.17.9/src/apply/series.rs` & `polars_lts_cpu-0.18.0/src/apply/series.rs`

 * *Files 15% similar despite different names*

```diff
@@ -1638,14 +1638,493 @@
                 self.name(),
                 self.len(),
             ))
         }
     }
 }
 
+#[cfg(feature = "dtype-array")]
+impl<'a> ApplyLambda<'a> for ArrayChunked {
+    fn apply_lambda_unknown(&'a self, py: Python, lambda: &'a PyAny) -> PyResult<PySeries> {
+        let pypolars = PyModule::import(py, "polars")?;
+        let mut null_count = 0;
+        for opt_v in self.into_iter() {
+            if let Some(v) = opt_v {
+                // create a PySeries struct/object for Python
+                let pyseries = PySeries::new(v);
+                // Wrap this PySeries object in the python side Series wrapper
+                let python_series_wrapper = pypolars
+                    .getattr("wrap_s")
+                    .unwrap()
+                    .call1((pyseries,))
+                    .unwrap();
+
+                let out = lambda.call1((python_series_wrapper,))?;
+                if out.is_none() {
+                    null_count += 1;
+                    continue;
+                }
+                return infer_and_finish(self, py, lambda, out, null_count);
+            } else {
+                null_count += 1
+            }
+        }
+        Ok(Self::full_null(self.name(), self.len())
+            .into_series()
+            .into())
+    }
+
+    fn apply_lambda(&'a self, py: Python, lambda: &'a PyAny) -> PyResult<PySeries> {
+        // get the pypolars module
+        let pypolars = PyModule::import(py, "polars")?;
+
+        match self.dtype() {
+            DataType::List(dt) => {
+                let mut builder = get_list_builder(dt, self.len() * 5, self.len(), self.name())
+                    .map_err(PyPolarsErr::from)?;
+                if !self.has_validity() {
+                    let mut it = self.into_no_null_iter();
+                    // use first value to get dtype and replace default builder
+                    if let Some(series) = it.next() {
+                        let out_series = call_series_lambda(pypolars, lambda, series)
+                            .expect("Cannot determine dtype because lambda failed; Make sure that your udf returns a Series");
+                        let dt = out_series.dtype();
+                        builder = get_list_builder(dt, self.len() * 5, self.len(), self.name())
+                            .map_err(PyPolarsErr::from)?;
+                        builder.append_opt_series(Some(&out_series));
+                    } else {
+                        let mut builder =
+                            get_list_builder(dt, 0, 1, self.name()).map_err(PyPolarsErr::from)?;
+                        let ca = builder.finish();
+                        return Ok(PySeries::new(ca.into_series()));
+                    }
+                    for series in it {
+                        append_series(pypolars, &mut *builder, lambda, series)?;
+                    }
+                } else {
+                    let mut it = self.into_iter();
+                    let mut nulls = 0;
+
+                    // use first values to get dtype and replace default builders
+                    // continue until no null is found
+                    for opt_series in &mut it {
+                        if let Some(series) = opt_series {
+                            let out_series = call_series_lambda(pypolars, lambda, series)
+                                .expect("Cannot determine dtype because lambda failed; Make sure that your udf returns a Series");
+                            let dt = out_series.dtype();
+                            builder = get_list_builder(dt, self.len() * 5, self.len(), self.name())
+                                .map_err(PyPolarsErr::from)?;
+                            builder.append_opt_series(Some(&out_series));
+                            break;
+                        } else {
+                            nulls += 1;
+                        }
+                    }
+                    for _ in 0..nulls {
+                        builder.append_opt_series(None);
+                    }
+                    for opt_series in it {
+                        if let Some(series) = opt_series {
+                            append_series(pypolars, &mut *builder, lambda, series)?;
+                        } else {
+                            builder.append_opt_series(None)
+                        }
+                    }
+                };
+                let ca = builder.finish();
+                Ok(PySeries::new(ca.into_series()))
+            }
+            _ => unimplemented!(),
+        }
+    }
+
+    fn apply_to_struct(
+        &'a self,
+        py: Python,
+        lambda: &'a PyAny,
+        init_null_count: usize,
+        first_value: AnyValue<'a>,
+    ) -> PyResult<PySeries> {
+        let skip = 1;
+        // get the pypolars module
+        let pypolars = PyModule::import(py, "polars")?;
+        if !self.has_validity() {
+            let it = self
+                .into_no_null_iter()
+                .skip(init_null_count + skip)
+                .map(|val| {
+                    // create a PySeries struct/object for Python
+                    let pyseries = PySeries::new(val);
+                    // Wrap this PySeries object in the python side Series wrapper
+                    let python_series_wrapper = pypolars
+                        .getattr("wrap_s")
+                        .unwrap()
+                        .call1((pyseries,))
+                        .unwrap();
+                    call_lambda(py, lambda, python_series_wrapper).ok()
+                });
+            iterator_to_struct(it, init_null_count, first_value, self.name(), self.len())
+        } else {
+            let it = self
+                .into_iter()
+                .skip(init_null_count + skip)
+                .map(|opt_val| {
+                    opt_val.and_then(|val| {
+                        // create a PySeries struct/object for Python
+                        let pyseries = PySeries::new(val);
+                        // Wrap this PySeries object in the python side Series wrapper
+                        let python_series_wrapper = pypolars
+                            .getattr("wrap_s")
+                            .unwrap()
+                            .call1((pyseries,))
+                            .unwrap();
+                        call_lambda(py, lambda, python_series_wrapper).ok()
+                    })
+                });
+            iterator_to_struct(it, init_null_count, first_value, self.name(), self.len())
+        }
+    }
+
+    fn apply_lambda_with_primitive_out_type<D>(
+        &'a self,
+        py: Python,
+        lambda: &'a PyAny,
+        init_null_count: usize,
+        first_value: Option<D::Native>,
+    ) -> PyResult<ChunkedArray<D>>
+    where
+        D: PyArrowPrimitiveType,
+        D::Native: ToPyObject + FromPyObject<'a>,
+    {
+        let skip = usize::from(first_value.is_some());
+        let pypolars = PyModule::import(py, "polars")?;
+        if init_null_count == self.len() {
+            Ok(ChunkedArray::full_null(self.name(), self.len()))
+        } else if !self.has_validity() {
+            let it = self
+                .into_no_null_iter()
+                .skip(init_null_count + skip)
+                .map(|val| {
+                    // create a PySeries struct/object for Python
+                    let pyseries = PySeries::new(val);
+                    // Wrap this PySeries object in the python side Series wrapper
+                    let python_series_wrapper = pypolars
+                        .getattr("wrap_s")
+                        .unwrap()
+                        .call1((pyseries,))
+                        .unwrap();
+                    call_lambda_and_extract(py, lambda, python_series_wrapper).ok()
+                });
+            Ok(iterator_to_primitive(
+                it,
+                init_null_count,
+                first_value,
+                self.name(),
+                self.len(),
+            ))
+        } else {
+            let it = self
+                .into_iter()
+                .skip(init_null_count + skip)
+                .map(|opt_val| {
+                    opt_val.and_then(|val| {
+                        // create a PySeries struct/object for Python
+                        let pyseries = PySeries::new(val);
+                        // Wrap this PySeries object in the python side Series wrapper
+                        let python_series_wrapper = pypolars
+                            .getattr("wrap_s")
+                            .unwrap()
+                            .call1((pyseries,))
+                            .unwrap();
+                        call_lambda_and_extract(py, lambda, python_series_wrapper).ok()
+                    })
+                });
+            Ok(iterator_to_primitive(
+                it,
+                init_null_count,
+                first_value,
+                self.name(),
+                self.len(),
+            ))
+        }
+    }
+
+    fn apply_lambda_with_bool_out_type(
+        &'a self,
+        py: Python,
+        lambda: &'a PyAny,
+        init_null_count: usize,
+        first_value: Option<bool>,
+    ) -> PyResult<BooleanChunked> {
+        let skip = usize::from(first_value.is_some());
+        let pypolars = PyModule::import(py, "polars")?;
+        if init_null_count == self.len() {
+            Ok(ChunkedArray::full_null(self.name(), self.len()))
+        } else if !self.has_validity() {
+            let it = self
+                .into_no_null_iter()
+                .skip(init_null_count + skip)
+                .map(|val| {
+                    // create a PySeries struct/object for Python
+                    let pyseries = PySeries::new(val);
+                    // Wrap this PySeries object in the python side Series wrapper
+                    let python_series_wrapper = pypolars
+                        .getattr("wrap_s")
+                        .unwrap()
+                        .call1((pyseries,))
+                        .unwrap();
+                    call_lambda_and_extract(py, lambda, python_series_wrapper).ok()
+                });
+            Ok(iterator_to_bool(
+                it,
+                init_null_count,
+                first_value,
+                self.name(),
+                self.len(),
+            ))
+        } else {
+            let it = self
+                .into_iter()
+                .skip(init_null_count + skip)
+                .map(|opt_val| {
+                    opt_val.and_then(|val| {
+                        // create a PySeries struct/object for Python
+                        let pyseries = PySeries::new(val);
+                        // Wrap this PySeries object in the python side Series wrapper
+                        let python_series_wrapper = pypolars
+                            .getattr("wrap_s")
+                            .unwrap()
+                            .call1((pyseries,))
+                            .unwrap();
+                        call_lambda_and_extract(py, lambda, python_series_wrapper).ok()
+                    })
+                });
+            Ok(iterator_to_bool(
+                it,
+                init_null_count,
+                first_value,
+                self.name(),
+                self.len(),
+            ))
+        }
+    }
+
+    fn apply_lambda_with_utf8_out_type(
+        &'a self,
+        py: Python,
+        lambda: &'a PyAny,
+        init_null_count: usize,
+        first_value: Option<&str>,
+    ) -> PyResult<Utf8Chunked> {
+        let skip = usize::from(first_value.is_some());
+        // get the pypolars module
+        let pypolars = PyModule::import(py, "polars")?;
+
+        if init_null_count == self.len() {
+            Ok(ChunkedArray::full_null(self.name(), self.len()))
+        } else if !self.has_validity() {
+            let it = self
+                .into_no_null_iter()
+                .skip(init_null_count + skip)
+                .map(|val| {
+                    // create a PySeries struct/object for Python
+                    let pyseries = PySeries::new(val);
+                    // Wrap this PySeries object in the python side Series wrapper
+                    let python_series_wrapper = pypolars
+                        .getattr("wrap_s")
+                        .unwrap()
+                        .call1((pyseries,))
+                        .unwrap();
+                    call_lambda_and_extract(py, lambda, python_series_wrapper).ok()
+                });
+
+            Ok(iterator_to_utf8(
+                it,
+                init_null_count,
+                first_value,
+                self.name(),
+                self.len(),
+            ))
+        } else {
+            let it = self
+                .into_iter()
+                .skip(init_null_count + skip)
+                .map(|opt_val| {
+                    opt_val.and_then(|val| {
+                        // create a PySeries struct/object for Python
+                        let pyseries = PySeries::new(val);
+                        // Wrap this PySeries object in the python side Series wrapper
+                        let python_series_wrapper = pypolars
+                            .getattr("wrap_s")
+                            .unwrap()
+                            .call1((pyseries,))
+                            .unwrap();
+                        call_lambda_and_extract(py, lambda, python_series_wrapper).ok()
+                    })
+                });
+            Ok(iterator_to_utf8(
+                it,
+                init_null_count,
+                first_value,
+                self.name(),
+                self.len(),
+            ))
+        }
+    }
+    fn apply_lambda_with_list_out_type(
+        &'a self,
+        py: Python,
+        lambda: PyObject,
+        init_null_count: usize,
+        first_value: &Series,
+        dt: &DataType,
+    ) -> PyResult<ListChunked> {
+        let skip = 1;
+        let pypolars = PyModule::import(py, "polars")?;
+        let lambda = lambda.as_ref(py);
+        if init_null_count == self.len() {
+            Ok(ChunkedArray::full_null(self.name(), self.len()))
+        } else if !self.has_validity() {
+            let it = self
+                .into_no_null_iter()
+                .skip(init_null_count + skip)
+                .map(|val| call_series_lambda(pypolars, lambda, val));
+
+            iterator_to_list(
+                dt,
+                it,
+                init_null_count,
+                Some(first_value),
+                self.name(),
+                self.len(),
+            )
+        } else {
+            let it = self
+                .into_iter()
+                .skip(init_null_count + skip)
+                .map(|opt_val| opt_val.and_then(|val| call_series_lambda(pypolars, lambda, val)));
+            iterator_to_list(
+                dt,
+                it,
+                init_null_count,
+                Some(first_value),
+                self.name(),
+                self.len(),
+            )
+        }
+    }
+
+    fn apply_extract_any_values(
+        &'a self,
+        py: Python,
+        lambda: &'a PyAny,
+        init_null_count: usize,
+        first_value: AnyValue<'a>,
+    ) -> PyResult<Series> {
+        let pypolars = PyModule::import(py, "polars")?;
+        let mut avs = Vec::with_capacity(self.len());
+        avs.extend(std::iter::repeat(AnyValue::Null).take(init_null_count));
+        avs.push(first_value);
+
+        let call_with_value = |val: Series| {
+            // create a PySeries struct/object for Python
+            let pyseries = PySeries::new(val);
+            // Wrap this PySeries object in the python side Series wrapper
+            let python_series_wrapper = pypolars
+                .getattr("wrap_s")
+                .unwrap()
+                .call1((pyseries,))
+                .unwrap();
+            call_lambda_and_extract::<_, Wrap<AnyValue>>(py, lambda, python_series_wrapper)
+                .unwrap()
+                .0
+        };
+
+        if self.null_count() > 0 {
+            let iter = self
+                .into_iter()
+                .skip(init_null_count + 1)
+                .map(|opt_val| match opt_val {
+                    None => AnyValue::Null,
+                    Some(val) => call_with_value(val),
+                });
+            avs.extend(iter);
+        } else {
+            let iter = self
+                .into_no_null_iter()
+                .skip(init_null_count + 1)
+                .map(call_with_value);
+            avs.extend(iter);
+        }
+        Ok(Series::new(self.name(), &avs))
+    }
+
+    #[cfg(feature = "object")]
+    fn apply_lambda_with_object_out_type(
+        &'a self,
+        py: Python,
+        lambda: &'a PyAny,
+        init_null_count: usize,
+        first_value: Option<ObjectValue>,
+    ) -> PyResult<ObjectChunked<ObjectValue>> {
+        let skip = usize::from(first_value.is_some());
+        let pypolars = PyModule::import(py, "polars")?;
+        if init_null_count == self.len() {
+            Ok(ChunkedArray::full_null(self.name(), self.len()))
+        } else if !self.has_validity() {
+            let it = self
+                .into_no_null_iter()
+                .skip(init_null_count + skip)
+                .map(|val| {
+                    // create a PySeries struct/object for Python
+                    let pyseries = PySeries::new(val);
+                    // Wrap this PySeries object in the python side Series wrapper
+                    let python_series_wrapper = pypolars
+                        .getattr("wrap_s")
+                        .unwrap()
+                        .call1((pyseries,))
+                        .unwrap();
+                    call_lambda_and_extract(py, lambda, python_series_wrapper).ok()
+                });
+
+            Ok(iterator_to_object(
+                it,
+                init_null_count,
+                first_value,
+                self.name(),
+                self.len(),
+            ))
+        } else {
+            let it = self
+                .into_iter()
+                .skip(init_null_count + skip)
+                .map(|opt_val| {
+                    opt_val.and_then(|val| {
+                        // create a PySeries struct/object for Python
+                        let pyseries = PySeries::new(val);
+                        // Wrap this PySeries object in the python side Series wrapper
+                        let python_series_wrapper = pypolars
+                            .getattr("wrap_s")
+                            .unwrap()
+                            .call1((pyseries,))
+                            .unwrap();
+                        call_lambda_and_extract(py, lambda, python_series_wrapper).ok()
+                    })
+                });
+            Ok(iterator_to_object(
+                it,
+                init_null_count,
+                first_value,
+                self.name(),
+                self.len(),
+            ))
+        }
+    }
+}
+
 #[cfg(feature = "object")]
 impl<'a> ApplyLambda<'a> for ObjectChunked<ObjectValue> {
     fn apply_lambda_unknown(&'a self, py: Python, lambda: &'a PyAny) -> PyResult<PySeries> {
         let mut null_count = 0;
         for opt_v in self.into_iter() {
             if let Some(v) = opt_v {
                 let arg = PyTuple::new(py, [v]);
```

### Comparing `polars_lts_cpu-0.17.9/src/arrow_interop/to_py.rs` & `polars_lts_cpu-0.18.0/src/arrow_interop/to_py.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/src/arrow_interop/to_rust.rs` & `polars_lts_cpu-0.18.0/src/arrow_interop/to_rust.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/src/batched_csv.rs` & `polars_lts_cpu-0.18.0/src/batched_csv.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/src/conversion.rs` & `polars_lts_cpu-0.18.0/src/conversion.rs`

 * *Files 2% similar despite different names*

```diff
@@ -10,30 +10,30 @@
 #[cfg(feature = "ipc")]
 use polars::io::ipc::IpcCompression;
 use polars::prelude::AnyValue;
 use polars::series::ops::NullBehavior;
 use polars_core::frame::row::any_values_to_dtype;
 use polars_core::prelude::QuantileInterpolOptions;
 use polars_core::utils::arrow::types::NativeType;
+use polars_lazy::prelude::*;
 use pyo3::basic::CompareOp;
 use pyo3::conversion::{FromPyObject, IntoPy};
 use pyo3::exceptions::{PyTypeError, PyValueError};
 use pyo3::prelude::*;
 use pyo3::types::{PyBool, PyBytes, PyDict, PyFloat, PyList, PySequence, PyString, PyTuple};
 use pyo3::{PyAny, PyResult};
 use smartstring::alias::String as SmartString;
 
-use crate::dataframe::PyDataFrame;
 use crate::error::PyPolarsErr;
-use crate::lazy::dataframe::PyLazyFrame;
 #[cfg(feature = "object")]
 use crate::object::OBJECT_NAME;
 use crate::prelude::*;
 use crate::py_modules::{POLARS, UTILS};
 use crate::series::PySeries;
+use crate::{PyDataFrame, PyLazyFrame};
 
 pub(crate) fn slice_to_wrapped<T>(slice: &[T]) -> &[Wrap<T>] {
     // Safety:
     // Wrap is transparent.
     unsafe { std::mem::transmute(slice) }
 }
 
@@ -239,15 +239,15 @@
                 let time_unit = time_unit.to_ascii();
                 convert.call1((v, time_unit)).unwrap().into_py(py)
             }
             AnyValue::Time(v) => {
                 let convert = utils.getattr("_to_python_time").unwrap();
                 convert.call1((v,)).unwrap().into_py(py)
             }
-            AnyValue::List(v) => PySeries::new(v).to_list(),
+            AnyValue::Array(v, _) | AnyValue::List(v) => PySeries::new(v).to_list(),
             ref av @ AnyValue::Struct(_, _, flds) => struct_dict(py, av._iter_struct_av(), flds),
             AnyValue::StructOwned(payload) => struct_dict(py, payload.0.into_iter(), &payload.1),
             #[cfg(feature = "object")]
             AnyValue::Object(v) => {
                 let object = v.as_any().downcast_ref::<ObjectValue>().unwrap();
                 object.inner.clone()
             }
@@ -299,14 +299,19 @@
                 .unwrap()
                 .call1((*precision, *scale))
                 .unwrap()
                 .into(),
             DataType::Boolean => pl.getattr("Boolean").unwrap().into(),
             DataType::Utf8 => pl.getattr("Utf8").unwrap().into(),
             DataType::Binary => pl.getattr("Binary").unwrap().into(),
+            DataType::Array(inner, size) => {
+                let inner = Wrap(*inner.clone()).to_object(py);
+                let list_class = pl.getattr("Array").unwrap();
+                list_class.call1((*size, inner)).unwrap().into()
+            }
             DataType::List(inner) => {
                 let inner = Wrap(*inner.clone()).to_object(py);
                 let list_class = pl.getattr("List").unwrap();
                 list_class.call1((inner,)).unwrap().into()
             }
             DataType::Date => pl.getattr("Date").unwrap().into(),
             DataType::Datetime(tu, tz) => {
@@ -407,14 +412,21 @@
                 DataType::Decimal(precision, Some(scale))
             }
             "List" => {
                 let inner = ob.getattr("inner").unwrap();
                 let inner = inner.extract::<Wrap<DataType>>()?;
                 DataType::List(Box::new(inner.0))
             }
+            "Array" => {
+                let inner = ob.getattr("inner").unwrap();
+                let width = ob.getattr("width").unwrap();
+                let inner = inner.extract::<Wrap<DataType>>()?;
+                let width = width.extract::<usize>()?;
+                DataType::Array(Box::new(inner.0), width)
+            }
             "Struct" => {
                 let fields = ob.getattr("fields")?;
                 let fields = fields
                     .extract::<Vec<Wrap<Field>>>()?
                     .into_iter()
                     .map(|f| f.0)
                     .collect::<Vec<Field>>();
@@ -534,15 +546,15 @@
     }
 }
 
 impl ToPyObject for Wrap<&DecimalChunked> {
     fn to_object(&self, py: Python) -> PyObject {
         let utils = UTILS.as_ref(py);
         let convert = utils.getattr("_to_python_decimal").unwrap();
-        let py_scale = self.0.scale().to_object(py);
+        let py_scale = (-(self.0.scale() as i32)).to_object(py);
         // if we don't know precision, the only safe bet is to set it to 39
         let py_precision = self.0.precision().unwrap_or(39).to_object(py);
         let iter = self.0.into_iter().map(|opt_v| {
             opt_v.map(|v| {
                 // TODO! use anyvalue so that we have a single impl.
                 const N: usize = 3;
                 let mut buf = [0_u128; N];
@@ -761,26 +773,14 @@
     fn extract(ob: &'s PyAny) -> PyResult<Self> {
         let vals = ob.extract::<Vec<Wrap<AnyValue<'s>>>>()?;
         let vals: Vec<AnyValue> = unsafe { std::mem::transmute(vals) };
         Ok(Wrap(Row(vals)))
     }
 }
 
-pub(crate) trait ToSeries {
-    fn to_series(self) -> Vec<Series>;
-}
-
-impl ToSeries for Vec<PySeries> {
-    fn to_series(self) -> Vec<Series> {
-        // Safety:
-        // transparent repr
-        unsafe { std::mem::transmute(self) }
-    }
-}
-
 impl FromPyObject<'_> for Wrap<Schema> {
     fn extract(ob: &PyAny) -> PyResult<Self> {
         let dict = ob.extract::<&PyDict>()?;
 
         Ok(Wrap(
             dict.iter()
                 .map(|(key, val)| {
@@ -932,17 +932,18 @@
 
 #[cfg(feature = "asof_join")]
 impl FromPyObject<'_> for Wrap<AsofStrategy> {
     fn extract(ob: &PyAny) -> PyResult<Self> {
         let parsed = match ob.extract::<&str>()? {
             "backward" => AsofStrategy::Backward,
             "forward" => AsofStrategy::Forward,
+            "nearest" => AsofStrategy::Nearest,
             v => {
                 return Err(PyValueError::new_err(format!(
-                    "strategy must be one of {{'backward', 'forward'}}, got {v}",
+                    "strategy must be one of {{'backward', 'forward', 'nearest'}}, got {v}",
                 )))
             }
         };
         Ok(Wrap(parsed))
     }
 }
 
@@ -995,17 +996,23 @@
 
 impl FromPyObject<'_> for Wrap<StartBy> {
     fn extract(ob: &PyAny) -> PyResult<Self> {
         let parsed = match ob.extract::<&str>()? {
             "window" => StartBy::WindowBound,
             "datapoint" => StartBy::DataPoint,
             "monday" => StartBy::Monday,
+            "tuesday" => StartBy::Tuesday,
+            "wednesday" => StartBy::Wednesday,
+            "thursday" => StartBy::Thursday,
+            "friday" => StartBy::Friday,
+            "saturday" => StartBy::Saturday,
+            "sunday" => StartBy::Sunday,
             v => {
                 return Err(PyValueError::new_err(format!(
-                    "closed must be one of {{'window', 'datapoint', 'monday'}}, got {v}",
+                    "closed must be one of {{'window', 'datapoint', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday'}}, got {v}",
                 )))
             }
         };
         Ok(Wrap(parsed))
     }
 }
 
@@ -1238,14 +1245,30 @@
                 )))
             }
         };
         Ok(Wrap(parsed))
     }
 }
 
+impl FromPyObject<'_> for Wrap<WindowMapping> {
+    fn extract(ob: &PyAny) -> PyResult<Self> {
+        let parsed = match ob.extract::<&str>()? {
+            "group_to_rows" => WindowMapping::GroupsToRows,
+            "join" => WindowMapping::Join,
+            "explode" => WindowMapping::Explode,
+            v => {
+                return Err(PyValueError::new_err(format!(
+                    "side must be one of {{'group_to_rows', 'join', 'explode'}}, got {v}",
+                )))
+            }
+        };
+        Ok(Wrap(parsed))
+    }
+}
+
 pub(crate) fn parse_fill_null_strategy(
     strategy: &str,
     limit: FillNullLimit,
 ) -> PyResult<FillNullStrategy> {
     let parsed = match strategy {
         "forward" => FillNullStrategy::Forward(limit),
         "backward" => FillNullStrategy::Backward(limit),
```

### Comparing `polars_lts_cpu-0.17.9/src/dataframe.rs` & `polars_lts_cpu-0.18.0/src/dataframe.rs`

 * *Files 2% similar despite different names*

```diff
@@ -26,18 +26,17 @@
     apply_lambda_with_utf8_out_type,
 };
 #[cfg(feature = "parquet")]
 use crate::conversion::parse_parquet_compression;
 use crate::conversion::{ObjectValue, Wrap};
 use crate::error::PyPolarsErr;
 use crate::file::{get_either_file, get_file_like, get_mmap_bytes_reader, EitherRustPythonFile};
-use crate::lazy::dataframe::PyLazyFrame;
 use crate::prelude::{dicts_to_rows, strings_to_smartstrings};
-use crate::series::{to_pyseries_collection, to_series_collection, PySeries};
-use crate::{arrow_interop, py_modules, PyExpr};
+use crate::series::{PySeries, ToPySeries, ToSeries};
+use crate::{arrow_interop, py_modules, PyExpr, PyLazyFrame};
 
 #[pyclass]
 #[repr(transparent)]
 #[derive(Clone)]
 pub struct PyDataFrame {
     pub df: DataFrame,
 }
@@ -110,15 +109,15 @@
         let cols = unsafe { std::mem::take(df.get_columns_mut()) };
         let (ptr, len, cap) = cols.into_raw_parts();
         (ptr as usize, len, cap)
     }
 
     #[new]
     pub fn __init__(columns: Vec<PySeries>) -> PyResult<Self> {
-        let columns = to_series_collection(columns);
+        let columns = columns.to_series();
         let df = DataFrame::new(columns).map_err(PyPolarsErr::from)?;
         Ok(PyDataFrame::new(df))
     }
 
     pub fn estimated_size(&self) -> usize {
         self.df.estimated_size()
     }
@@ -366,22 +365,31 @@
             let mmap_read: ReaderBytes = (&mmap_bytes_r).into();
             let bytes = mmap_read.deref();
 
             // Happy path is our column oriented json as that is most performant
             // on failure we try
             match serde_json::from_slice::<DataFrame>(bytes) {
                 Ok(df) => Ok(df.into()),
-                // try arrow json reader instead
-                // this is row oriented
-                Err(_) => {
-                    let out = JsonReader::new(mmap_bytes_r)
-                        .with_json_format(JsonFormat::Json)
-                        .finish()
-                        .map_err(|e| PyPolarsErr::Other(format!("{e}")))?;
-                    Ok(out.into())
+                Err(e) => {
+                    let msg = format!("{e}");
+                    // parsing succeeded, but the dataframe was invalid
+                    if msg.contains("successful parse invalid data") {
+                        let e = PyPolarsErr::from(PolarsError::ComputeError(msg.into()));
+                        Err(PyErr::from(e))
+                    }
+                    // parsing error
+                    // try arrow json reader instead
+                    // this is row oriented
+                    else {
+                        let out = JsonReader::new(mmap_bytes_r)
+                            .with_json_format(JsonFormat::Json)
+                            .finish()
+                            .map_err(|e| PyPolarsErr::Other(format!("{e}")))?;
+                        Ok(out.into())
+                    }
                 }
             }
         }
     }
 
     #[staticmethod]
     #[cfg(feature = "json")]
@@ -721,15 +729,15 @@
                 .map_err(PyPolarsErr::from)?;
         }
 
         Ok(())
     }
 
     pub fn to_arrow(&mut self) -> PyResult<Vec<PyObject>> {
-        self.df.rechunk();
+        self.df.align_chunks();
         Python::with_gil(|py| {
             let pyarrow = py.import("pyarrow")?;
             let names = self.df.get_column_names();
 
             let rbs = self
                 .df
                 .iter_chunks()
@@ -866,15 +874,15 @@
     /// Format `DataFrame` as String
     pub fn as_str(&self) -> String {
         format!("{:?}", self.df)
     }
 
     pub fn get_columns(&self) -> Vec<PySeries> {
         let cols = self.df.get_columns().to_vec();
-        to_pyseries_collection(cols)
+        cols.to_pyseries()
     }
 
     /// Get column names
     pub fn columns(&self) -> Vec<&str> {
         self.df.get_column_names()
     }
 
@@ -908,21 +916,21 @@
     }
 
     pub fn width(&self) -> usize {
         self.df.width()
     }
 
     pub fn hstack_mut(&mut self, columns: Vec<PySeries>) -> PyResult<()> {
-        let columns = to_series_collection(columns);
+        let columns = columns.to_series();
         self.df.hstack_mut(&columns).map_err(PyPolarsErr::from)?;
         Ok(())
     }
 
     pub fn hstack(&self, columns: Vec<PySeries>) -> PyResult<Self> {
-        let columns = to_series_collection(columns);
+        let columns = columns.to_series();
         let df = self.df.hstack(&columns).map_err(PyPolarsErr::from)?;
         Ok(df.into())
     }
 
     pub fn extend(&mut self, df: &PyDataFrame) -> PyResult<()> {
         self.df.extend(&df.df).map_err(PyPolarsErr::from)?;
         Ok(())
```

### Comparing `polars_lts_cpu-0.17.9/src/datatypes.rs` & `polars_lts_cpu-0.18.0/src/datatypes.rs`

 * *Files 12% similar despite different names*

```diff
@@ -27,14 +27,15 @@
     Time,
     #[cfg(feature = "object")]
     Object,
     Categorical,
     Struct,
     Binary,
     Decimal(Option<usize>, usize),
+    Array(usize),
 }
 
 impl From<&DataType> for PyDataType {
     fn from(dt: &DataType) -> Self {
         use PyDataType::*;
         match dt {
             DataType::Int8 => Int8,
@@ -47,14 +48,15 @@
             DataType::UInt64 => UInt64,
             DataType::Float32 => Float32,
             DataType::Float64 => Float64,
             DataType::Decimal(p, s) => Decimal(*p, s.expect("unexpected null decimal scale")),
             DataType::Boolean => Bool,
             DataType::Utf8 => Utf8,
             DataType::Binary => Binary,
+            DataType::Array(_, width) => Array(*width),
             DataType::List(_) => List,
             DataType::Date => Date,
             DataType::Datetime(tu, tz) => Datetime(*tu, tz.clone()),
             DataType::Duration(tu) => Duration(*tu),
             DataType::Time => Time,
             #[cfg(feature = "object")]
             DataType::Object(_) => Object,
@@ -96,14 +98,15 @@
             PyDataType::Duration(tu) => Duration(tu),
             PyDataType::Time => Time,
             #[cfg(feature = "object")]
             PyDataType::Object => Object(OBJECT_NAME),
             PyDataType::Categorical => Categorical(None),
             PyDataType::Struct => Struct(vec![]),
             PyDataType::Decimal(p, s) => Decimal(p, Some(s)),
+            PyDataType::Array(width) => Array(DataType::Null.into(), width),
         }
     }
 }
 
 impl FromPyObject<'_> for PyDataType {
     fn extract(ob: &PyAny) -> PyResult<Self> {
         let dt = ob.extract::<Wrap<DataType>>()?;
```

### Comparing `polars_lts_cpu-0.17.9/src/error.rs` & `polars_lts_cpu-0.18.0/src/error.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/src/file.rs` & `polars_lts_cpu-0.18.0/src/file.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/src/lazy/apply.rs` & `polars_lts_cpu-0.18.0/src/apply/lazy.rs`

 * *Files 1% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 use polars::prelude::*;
 use pyo3::prelude::*;
 use pyo3::types::PyList;
 
-use crate::lazy::dsl::PyExpr;
 use crate::py_modules::POLARS;
 use crate::series::PySeries;
-use crate::Wrap;
+use crate::{PyExpr, Wrap};
 
 trait ToSeries {
     fn to_series(&self, py: Python, py_polars_module: &PyObject, name: &str) -> Series;
 }
 
 impl ToSeries for PyObject {
     fn to_series(&self, py: Python, py_polars_module: &PyObject, name: &str) -> Series {
```

### Comparing `polars_lts_cpu-0.17.9/src/lazy/dataframe.rs` & `polars_lts_cpu-0.18.0/src/lazyframe.rs`

 * *Files 11% similar despite different names*

```diff
@@ -4,120 +4,49 @@
 use std::path::PathBuf;
 
 use polars::io::RowCount;
 #[cfg(feature = "csv")]
 use polars::lazy::frame::LazyCsvReader;
 #[cfg(feature = "json")]
 use polars::lazy::frame::LazyJsonLineReader;
-use polars::lazy::frame::{AllowedOptimizations, LazyFrame, LazyGroupBy};
+use polars::lazy::frame::{AllowedOptimizations, LazyFrame};
 use polars::lazy::prelude::col;
 use polars::prelude::{ClosedWindow, CsvEncoding, DataFrame, Field, JoinType, Schema};
 use polars::time::*;
 use polars_core::cloud;
 use polars_core::frame::explode::MeltArgs;
 use polars_core::frame::UniqueKeepStrategy;
 use polars_core::prelude::*;
 use pyo3::exceptions::PyValueError;
 use pyo3::prelude::*;
 use pyo3::types::{PyBytes, PyDict, PyList};
 
 use crate::arrow_interop::to_rust::pyarrow_schema_to_rust;
 use crate::conversion::Wrap;
-use crate::dataframe::PyDataFrame;
 use crate::error::PyPolarsErr;
+use crate::expr::ToExprs;
 use crate::file::get_file_like;
-use crate::lazy::dsl::PyExpr;
-use crate::lazy::utils::py_exprs_to_exprs;
 use crate::prelude::*;
 use crate::py_modules::POLARS;
+use crate::{PyDataFrame, PyExpr, PyLazyGroupBy};
 
-#[pyclass]
-#[repr(transparent)]
-pub struct PyLazyGroupBy {
-    // option because we cannot get a self by value in pyo3
-    pub lgb: Option<LazyGroupBy>,
-}
 /// Extract CloudOptions from a Python object.
 fn extract_cloud_options(url: &str, py_object: PyObject) -> PyResult<cloud::CloudOptions> {
     let untyped_options = Python::with_gil(|py| py_object.extract::<HashMap<String, String>>(py))
         .expect("Expected a dictionary for cloud_options");
     Ok(
         cloud::CloudOptions::from_untyped_config(url, untyped_options)
             .map_err(PyPolarsErr::from)?,
     )
 }
 
-#[pymethods]
-impl PyLazyGroupBy {
-    pub fn agg(&mut self, aggs: Vec<PyExpr>) -> PyLazyFrame {
-        let lgb = self.lgb.take().unwrap();
-        let aggs = py_exprs_to_exprs(aggs);
-        lgb.agg(aggs).into()
-    }
-
-    pub fn head(&mut self, n: usize) -> PyLazyFrame {
-        let lgb = self.lgb.take().unwrap();
-        lgb.head(Some(n)).into()
-    }
-
-    pub fn tail(&mut self, n: usize) -> PyLazyFrame {
-        let lgb = self.lgb.take().unwrap();
-        lgb.tail(Some(n)).into()
-    }
-
-    pub fn apply(
-        &mut self,
-        lambda: PyObject,
-        schema: Option<Wrap<Schema>>,
-    ) -> PyResult<PyLazyFrame> {
-        let lgb = self.lgb.take().unwrap();
-        let schema = match schema {
-            Some(schema) => Arc::new(schema.0),
-            None => LazyFrame::from(lgb.logical_plan.clone())
-                .schema()
-                .map_err(PyPolarsErr::from)?,
-        };
-
-        let function = move |df: DataFrame| {
-            Python::with_gil(|py| {
-                // get the pypolars module
-                let pypolars = PyModule::import(py, "polars").unwrap();
-
-                // create a PyDataFrame struct/object for Python
-                let pydf = PyDataFrame::new(df);
-
-                // Wrap this PySeries object in the python side DataFrame wrapper
-                let python_df_wrapper =
-                    pypolars.getattr("wrap_df").unwrap().call1((pydf,)).unwrap();
-
-                // call the lambda and get a python side DataFrame wrapper
-                let result_df_wrapper = lambda.call1(py, (python_df_wrapper,)).map_err(|e| {
-                    PolarsError::ComputeError(
-                        format!("User provided python function failed: {e}").into(),
-                    )
-                })?;
-                // unpack the wrapper in a PyDataFrame
-                let py_pydf = result_df_wrapper.getattr(py, "_df").expect(
-                "Could net get DataFrame attribute '_df'. Make sure that you return a DataFrame object.",
-            );
-                // Downcast to Rust
-                let pydf = py_pydf.extract::<PyDataFrame>(py).unwrap();
-                // Finally get the actual DataFrame
-                Ok(pydf.df)
-            })
-        };
-        Ok(lgb.apply(function, schema).into())
-    }
-}
-
 #[pyclass]
 #[repr(transparent)]
 #[derive(Clone)]
 pub struct PyLazyFrame {
-    // option because we cannot get a self by value in pyo3
     pub ldf: LazyFrame,
 }
 
 impl PyLazyFrame {
     fn get_schema(&self) -> PyResult<SchemaRef> {
         let schema = self.ldf.schema().map_err(PyPolarsErr::from)?;
         Ok(schema)
@@ -129,47 +58,47 @@
         PyLazyFrame { ldf }
     }
 }
 
 #[pymethods]
 #[allow(clippy::should_implement_trait)]
 impl PyLazyFrame {
-    pub fn __getstate__(&self, py: Python) -> PyResult<PyObject> {
+    fn __getstate__(&self, py: Python) -> PyResult<PyObject> {
         // Used in pickle/pickling
         let mut writer: Vec<u8> = vec![];
         ciborium::ser::into_writer(&self.ldf.logical_plan, &mut writer)
             .map_err(|e| PyPolarsErr::Other(format!("{}", e)))?;
 
         Ok(PyBytes::new(py, &writer).to_object(py))
     }
 
-    pub fn __setstate__(&mut self, py: Python, state: PyObject) -> PyResult<()> {
+    fn __setstate__(&mut self, py: Python, state: PyObject) -> PyResult<()> {
         // Used in pickle/pickling
         match state.extract::<&PyBytes>(py) {
             Ok(s) => {
                 let lp: LogicalPlan = ciborium::de::from_reader(s.as_bytes())
                     .map_err(|e| PyPolarsErr::Other(format!("{}", e)))?;
                 self.ldf = LazyFrame::from(lp);
                 Ok(())
             }
             Err(e) => Err(e),
         }
     }
 
     #[cfg(all(feature = "json", feature = "serde_json"))]
-    pub fn write_json(&self, py_f: PyObject) -> PyResult<()> {
+    fn write_json(&self, py_f: PyObject) -> PyResult<()> {
         let file = BufWriter::new(get_file_like(py_f, true)?);
         serde_json::to_writer(file, &self.ldf.logical_plan)
             .map_err(|err| PyValueError::new_err(format!("{err:?}")))?;
         Ok(())
     }
 
     #[staticmethod]
     #[cfg(feature = "json")]
-    pub fn read_json(py_f: PyObject) -> PyResult<Self> {
+    fn read_json(py_f: PyObject) -> PyResult<Self> {
         // it is faster to first read to memory and then parse: https://github.com/serde-rs/json/issues/160
         // so don't bother with files.
         let mut json = String::new();
         let _ = get_file_like(py_f, false)?
             .read_to_string(&mut json)
             .unwrap();
 
@@ -186,15 +115,15 @@
         Ok(LazyFrame::from(lp).into())
     }
 
     #[staticmethod]
     #[cfg(feature = "json")]
     #[allow(clippy::too_many_arguments)]
     #[pyo3(signature = (path, infer_schema_length, batch_size, n_rows, low_memory, rechunk, row_count))]
-    pub fn new_from_ndjson(
+    fn new_from_ndjson(
         path: String,
         infer_schema_length: Option<usize>,
         batch_size: Option<usize>,
         n_rows: Option<usize>,
         low_memory: bool,
         rechunk: bool,
         row_count: Option<(String, IdxSize)>,
@@ -218,15 +147,15 @@
     #[cfg(feature = "csv")]
     #[pyo3(signature = (path, separator, has_header, ignore_errors, skip_rows, n_rows, cache, overwrite_dtype,
         low_memory, comment_char, quote_char, null_values, missing_utf8_is_empty_string,
         infer_schema_length, with_schema_modify, rechunk, skip_rows_after_header,
         encoding, row_count, try_parse_dates, eol_char,
     )
     )]
-    pub fn new_from_csv(
+    fn new_from_csv(
         path: String,
         separator: &str,
         has_header: bool,
         ignore_errors: bool,
         skip_rows: usize,
         n_rows: Option<usize>,
         cache: bool,
@@ -306,15 +235,15 @@
 
     #[cfg(feature = "parquet")]
     #[staticmethod]
     #[allow(clippy::too_many_arguments)]
     #[pyo3(signature = (path, n_rows, cache, parallel, rechunk, row_count,
         low_memory, cloud_options, use_statistics)
     )]
-    pub fn new_from_parquet(
+    fn new_from_parquet(
         path: String,
         n_rows: Option<usize>,
         cache: bool,
         parallel: Wrap<ParallelStrategy>,
         rechunk: bool,
         row_count: Option<(String, IdxSize)>,
         low_memory: bool,
@@ -338,15 +267,15 @@
         let lf = LazyFrame::scan_parquet(path, args).map_err(PyPolarsErr::from)?;
         Ok(lf.into())
     }
 
     #[cfg(feature = "ipc")]
     #[staticmethod]
     #[pyo3(signature = (path, n_rows, cache, rechunk, row_count, memory_map))]
-    pub fn new_from_ipc(
+    fn new_from_ipc(
         path: String,
         n_rows: Option<usize>,
         cache: bool,
         rechunk: bool,
         row_count: Option<(String, IdxSize)>,
         memory_map: bool,
     ) -> PyResult<Self> {
@@ -359,60 +288,60 @@
             memmap: memory_map,
         };
         let lf = LazyFrame::scan_ipc(path, args).map_err(PyPolarsErr::from)?;
         Ok(lf.into())
     }
 
     #[staticmethod]
-    pub fn scan_from_python_function_arrow_schema(
+    fn scan_from_python_function_arrow_schema(
         schema: &PyList,
         scan_fn: Vec<u8>,
         pyarrow: bool,
     ) -> PyResult<Self> {
         let schema = pyarrow_schema_to_rust(schema)?;
         Ok(LazyFrame::scan_from_python_function(schema, scan_fn, pyarrow).into())
     }
 
     #[staticmethod]
-    pub fn scan_from_python_function_pl_schema(
+    fn scan_from_python_function_pl_schema(
         schema: Vec<(&str, Wrap<DataType>)>,
         scan_fn: Vec<u8>,
         pyarrow: bool,
     ) -> PyResult<Self> {
         let schema = Schema::from_iter(schema.into_iter().map(|(name, dt)| Field::new(name, dt.0)));
         Ok(LazyFrame::scan_from_python_function(schema, scan_fn, pyarrow).into())
     }
 
-    pub fn describe_plan(&self) -> String {
+    fn describe_plan(&self) -> String {
         self.ldf.describe_plan()
     }
 
-    pub fn describe_optimized_plan(&self) -> PyResult<String> {
+    fn describe_optimized_plan(&self) -> PyResult<String> {
         let result = self
             .ldf
             .describe_optimized_plan()
             .map_err(PyPolarsErr::from)?;
         Ok(result)
     }
-    pub fn to_dot(&self, optimized: bool) -> PyResult<String> {
+    fn to_dot(&self, optimized: bool) -> PyResult<String> {
         let result = self.ldf.to_dot(optimized).map_err(PyPolarsErr::from)?;
         Ok(result)
     }
 
     #[allow(clippy::too_many_arguments)]
-    pub fn optimization_toggle(
+    fn optimization_toggle(
         &self,
         type_coercion: bool,
         predicate_pushdown: bool,
         projection_pushdown: bool,
         simplify_expr: bool,
         slice_pushdown: bool,
         cse: bool,
         streaming: bool,
-    ) -> PyLazyFrame {
+    ) -> Self {
         let ldf = self.ldf.clone();
         let mut ldf = ldf
             .with_type_coercion(type_coercion)
             .with_predicate_pushdown(predicate_pushdown)
             .with_simplify_expr(simplify_expr)
             .with_slice_pushdown(slice_pushdown)
             .with_streaming(streaming)
@@ -422,91 +351,80 @@
         {
             ldf = ldf.with_common_subplan_elimination(cse);
         }
 
         ldf.into()
     }
 
-    pub fn sort(&self, by_column: &str, descending: bool, nulls_last: bool) -> PyLazyFrame {
+    fn sort(&self, by_column: &str, descending: bool, nulls_last: bool) -> Self {
         let ldf = self.ldf.clone();
         ldf.sort(
             by_column,
             SortOptions {
                 descending,
                 nulls_last,
                 multithreaded: true,
             },
         )
         .into()
     }
 
-    pub fn sort_by_exprs(
-        &self,
-        by: Vec<PyExpr>,
-        descending: Vec<bool>,
-        nulls_last: bool,
-    ) -> PyLazyFrame {
+    fn sort_by_exprs(&self, by: Vec<PyExpr>, descending: Vec<bool>, nulls_last: bool) -> Self {
         let ldf = self.ldf.clone();
-        let exprs = py_exprs_to_exprs(by);
+        let exprs = by.to_exprs();
         ldf.sort_by_exprs(exprs, descending, nulls_last).into()
     }
 
-    pub fn top_k(
-        &self,
-        k: IdxSize,
-        by: Vec<PyExpr>,
-        descending: Vec<bool>,
-        nulls_last: bool,
-    ) -> PyLazyFrame {
+    fn top_k(&self, k: IdxSize, by: Vec<PyExpr>, descending: Vec<bool>, nulls_last: bool) -> Self {
         let ldf = self.ldf.clone();
-        let exprs = py_exprs_to_exprs(by);
+        let exprs = by.to_exprs();
         ldf.top_k(k, exprs, descending, nulls_last).into()
     }
 
-    pub fn bottom_k(
+    fn bottom_k(
         &self,
         k: IdxSize,
         by: Vec<PyExpr>,
         descending: Vec<bool>,
         nulls_last: bool,
-    ) -> PyLazyFrame {
+    ) -> Self {
         let ldf = self.ldf.clone();
-        let exprs = py_exprs_to_exprs(by);
+        let exprs = by.to_exprs();
         ldf.bottom_k(k, exprs, descending, nulls_last).into()
     }
 
-    pub fn cache(&self) -> PyLazyFrame {
+    fn cache(&self) -> Self {
         let ldf = self.ldf.clone();
         ldf.cache().into()
     }
 
-    pub fn profile(&self, py: Python) -> PyResult<(PyDataFrame, PyDataFrame)> {
+    fn profile(&self, py: Python) -> PyResult<(PyDataFrame, PyDataFrame)> {
         // if we don't allow threads and we have udfs trying to acquire the gil from different
         // threads we deadlock.
         let (df, time_df) = py.allow_threads(|| {
             let ldf = self.ldf.clone();
             ldf.profile().map_err(PyPolarsErr::from)
         })?;
         Ok((df.into(), time_df.into()))
     }
 
-    pub fn collect(&self, py: Python) -> PyResult<PyDataFrame> {
+    fn collect(&self, py: Python) -> PyResult<PyDataFrame> {
         // if we don't allow threads and we have udfs trying to acquire the gil from different
         // threads we deadlock.
         let df = py.allow_threads(|| {
             let ldf = self.ldf.clone();
             ldf.collect().map_err(PyPolarsErr::from)
         })?;
         Ok(df.into())
     }
 
     #[allow(clippy::too_many_arguments)]
     #[cfg(all(feature = "streaming", feature = "parquet"))]
     #[pyo3(signature = (path, compression, compression_level, statistics, row_group_size, data_pagesize_limit, maintain_order))]
-    pub fn sink_parquet(
+    fn sink_parquet(
         &self,
         py: Python,
         path: PathBuf,
         compression: &str,
         compression_level: Option<i32>,
         statistics: bool,
         row_group_size: Option<usize>,
@@ -531,15 +449,15 @@
         })?;
         Ok(())
     }
 
     #[allow(clippy::too_many_arguments)]
     #[cfg(all(feature = "streaming", feature = "ipc"))]
     #[pyo3(signature = (path, compression, maintain_order))]
-    pub fn sink_ipc(
+    fn sink_ipc(
         &self,
         py: Python,
         path: PathBuf,
         compression: Option<Wrap<IpcCompression>>,
         maintain_order: bool,
     ) -> PyResult<()> {
         let options = IpcWriterOptions {
@@ -552,117 +470,123 @@
         py.allow_threads(|| {
             let ldf = self.ldf.clone();
             ldf.sink_ipc(path, options).map_err(PyPolarsErr::from)
         })?;
         Ok(())
     }
 
-    pub fn fetch(&self, py: Python, n_rows: usize) -> PyResult<PyDataFrame> {
+    fn fetch(&self, py: Python, n_rows: usize) -> PyResult<PyDataFrame> {
         let ldf = self.ldf.clone();
         let df = py.allow_threads(|| ldf.fetch(n_rows).map_err(PyPolarsErr::from))?;
         Ok(df.into())
     }
 
-    pub fn filter(&mut self, predicate: PyExpr) -> PyLazyFrame {
+    fn filter(&mut self, predicate: PyExpr) -> Self {
         let ldf = self.ldf.clone();
         ldf.filter(predicate.inner).into()
     }
 
-    pub fn select(&mut self, exprs: Vec<PyExpr>) -> PyLazyFrame {
+    fn select(&mut self, exprs: Vec<PyExpr>) -> Self {
         let ldf = self.ldf.clone();
-        let exprs = py_exprs_to_exprs(exprs);
+        let exprs = exprs.to_exprs();
         ldf.select(exprs).into()
     }
 
-    pub fn groupby(&mut self, by: Vec<PyExpr>, maintain_order: bool) -> PyLazyGroupBy {
+    fn groupby(&mut self, by: Vec<PyExpr>, maintain_order: bool) -> PyLazyGroupBy {
         let ldf = self.ldf.clone();
-        let by = py_exprs_to_exprs(by);
+        let by = by.to_exprs();
         let lazy_gb = if maintain_order {
             ldf.groupby_stable(by)
         } else {
             ldf.groupby(by)
         };
 
         PyLazyGroupBy { lgb: Some(lazy_gb) }
     }
 
-    pub fn groupby_rolling(
+    fn groupby_rolling(
         &mut self,
-        index_column: &str,
+        index_column: PyExpr,
         period: &str,
         offset: &str,
         closed: Wrap<ClosedWindow>,
         by: Vec<PyExpr>,
+        check_sorted: bool,
     ) -> PyLazyGroupBy {
         let closed_window = closed.0;
         let ldf = self.ldf.clone();
         let by = by
             .into_iter()
             .map(|pyexpr| pyexpr.inner)
             .collect::<Vec<_>>();
         let lazy_gb = ldf.groupby_rolling(
+            index_column.inner,
             by,
             RollingGroupOptions {
-                index_column: index_column.into(),
+                index_column: "".into(),
                 period: Duration::parse(period),
                 offset: Duration::parse(offset),
                 closed_window,
+                check_sorted,
             },
         );
 
         PyLazyGroupBy { lgb: Some(lazy_gb) }
     }
 
     #[allow(clippy::too_many_arguments)]
-    pub fn groupby_dynamic(
+    fn groupby_dynamic(
         &mut self,
-        index_column: &str,
+        index_column: PyExpr,
         every: &str,
         period: &str,
         offset: &str,
         truncate: bool,
         include_boundaries: bool,
         closed: Wrap<ClosedWindow>,
         by: Vec<PyExpr>,
         start_by: Wrap<StartBy>,
+        check_sorted: bool,
     ) -> PyLazyGroupBy {
         let closed_window = closed.0;
         let by = by
             .into_iter()
             .map(|pyexpr| pyexpr.inner)
             .collect::<Vec<_>>();
         let ldf = self.ldf.clone();
         let lazy_gb = ldf.groupby_dynamic(
+            index_column.inner,
             by,
             DynamicGroupOptions {
-                index_column: index_column.into(),
                 every: Duration::parse(every),
                 period: Duration::parse(period),
                 offset: Duration::parse(offset),
                 truncate,
                 include_boundaries,
                 closed_window,
                 start_by: start_by.0,
+                check_sorted,
+                ..Default::default()
             },
         );
 
         PyLazyGroupBy { lgb: Some(lazy_gb) }
     }
 
-    pub fn with_context(&self, contexts: Vec<PyLazyFrame>) -> PyLazyFrame {
+    fn with_context(&self, contexts: Vec<Self>) -> Self {
         let contexts = contexts.into_iter().map(|ldf| ldf.ldf).collect::<Vec<_>>();
         self.ldf.clone().with_context(contexts).into()
     }
 
     #[allow(clippy::too_many_arguments)]
     #[cfg(feature = "asof_join")]
     #[pyo3(signature = (other, left_on, right_on, left_by, right_by, allow_parallel, force_parallel, suffix, strategy, tolerance, tolerance_str))]
-    pub fn join_asof(
+    fn join_asof(
         &self,
-        other: PyLazyFrame,
+        other: Self,
         left_on: PyExpr,
         right_on: PyExpr,
         left_by: Option<Vec<&str>>,
         right_by: Option<Vec<&str>>,
         allow_parallel: bool,
         force_parallel: bool,
         suffix: String,
@@ -690,17 +614,17 @@
             }))
             .suffix(suffix)
             .finish()
             .into())
     }
 
     #[allow(clippy::too_many_arguments)]
-    pub fn join(
+    fn join(
         &self,
-        other: PyLazyFrame,
+        other: Self,
         left_on: Vec<PyExpr>,
         right_on: Vec<PyExpr>,
         allow_parallel: bool,
         force_parallel: bool,
         how: Wrap<JoinType>,
         suffix: String,
     ) -> PyResult<Self> {
@@ -724,128 +648,133 @@
             .force_parallel(force_parallel)
             .how(how.0)
             .suffix(suffix)
             .finish()
             .into())
     }
 
-    pub fn with_column(&mut self, expr: PyExpr) -> PyLazyFrame {
+    fn with_column(&mut self, expr: PyExpr) -> Self {
         let ldf = self.ldf.clone();
         ldf.with_column(expr.inner).into()
     }
 
-    pub fn with_columns(&mut self, exprs: Vec<PyExpr>) -> PyLazyFrame {
+    fn with_columns(&mut self, exprs: Vec<PyExpr>) -> Self {
         let ldf = self.ldf.clone();
-        ldf.with_columns(py_exprs_to_exprs(exprs)).into()
+        ldf.with_columns(exprs.to_exprs()).into()
     }
 
-    pub fn rename(&mut self, existing: Vec<String>, new: Vec<String>) -> PyLazyFrame {
+    fn rename(&mut self, existing: Vec<String>, new: Vec<String>) -> Self {
         let ldf = self.ldf.clone();
         ldf.rename(existing, new).into()
     }
 
-    pub fn reverse(&self) -> Self {
+    fn reverse(&self) -> Self {
         let ldf = self.ldf.clone();
         ldf.reverse().into()
     }
 
-    pub fn shift(&self, periods: i64) -> Self {
+    fn shift(&self, periods: i64) -> Self {
         let ldf = self.ldf.clone();
         ldf.shift(periods).into()
     }
 
-    pub fn shift_and_fill(&self, periods: i64, fill_value: PyExpr) -> Self {
+    fn shift_and_fill(&self, periods: i64, fill_value: PyExpr) -> Self {
         let ldf = self.ldf.clone();
         ldf.shift_and_fill(periods, fill_value.inner).into()
     }
 
-    pub fn fill_nan(&self, fill_value: PyExpr) -> Self {
+    fn fill_nan(&self, fill_value: PyExpr) -> Self {
         let ldf = self.ldf.clone();
         ldf.fill_nan(fill_value.inner).into()
     }
 
-    pub fn min(&self) -> Self {
+    fn min(&self) -> Self {
         let ldf = self.ldf.clone();
         ldf.min().into()
     }
 
-    pub fn max(&self) -> Self {
+    fn max(&self) -> Self {
         let ldf = self.ldf.clone();
         ldf.max().into()
     }
 
-    pub fn sum(&self) -> Self {
+    fn sum(&self) -> Self {
         let ldf = self.ldf.clone();
         ldf.sum().into()
     }
 
-    pub fn mean(&self) -> Self {
+    fn mean(&self) -> Self {
         let ldf = self.ldf.clone();
         ldf.mean().into()
     }
 
-    pub fn std(&self, ddof: u8) -> Self {
+    fn std(&self, ddof: u8) -> Self {
         let ldf = self.ldf.clone();
         ldf.std(ddof).into()
     }
 
-    pub fn var(&self, ddof: u8) -> Self {
+    fn var(&self, ddof: u8) -> Self {
         let ldf = self.ldf.clone();
         ldf.var(ddof).into()
     }
 
-    pub fn median(&self) -> Self {
+    fn median(&self) -> Self {
         let ldf = self.ldf.clone();
         ldf.median().into()
     }
 
-    pub fn quantile(&self, quantile: PyExpr, interpolation: Wrap<QuantileInterpolOptions>) -> Self {
+    fn quantile(&self, quantile: PyExpr, interpolation: Wrap<QuantileInterpolOptions>) -> Self {
         let ldf = self.ldf.clone();
         ldf.quantile(quantile.inner, interpolation.0).into()
     }
 
-    pub fn explode(&self, column: Vec<PyExpr>) -> Self {
+    fn explode(&self, column: Vec<PyExpr>) -> Self {
         let ldf = self.ldf.clone();
-        let column = py_exprs_to_exprs(column);
+        let column = column.to_exprs();
         ldf.explode(column).into()
     }
 
+    fn null_count(&self) -> Self {
+        let ldf = self.ldf.clone();
+        ldf.null_count().into()
+    }
+
     #[pyo3(signature = (maintain_order, subset, keep))]
-    pub fn unique(
+    fn unique(
         &self,
         maintain_order: bool,
         subset: Option<Vec<String>>,
         keep: Wrap<UniqueKeepStrategy>,
     ) -> Self {
         let ldf = self.ldf.clone();
         match maintain_order {
             true => ldf.unique_stable(subset, keep.0),
             false => ldf.unique(subset, keep.0),
         }
         .into()
     }
 
-    pub fn drop_nulls(&self, subset: Option<Vec<String>>) -> Self {
+    fn drop_nulls(&self, subset: Option<Vec<String>>) -> Self {
         let ldf = self.ldf.clone();
         ldf.drop_nulls(subset.map(|v| v.into_iter().map(|s| col(&s)).collect()))
             .into()
     }
 
-    pub fn slice(&self, offset: i64, len: Option<IdxSize>) -> Self {
+    fn slice(&self, offset: i64, len: Option<IdxSize>) -> Self {
         let ldf = self.ldf.clone();
         ldf.slice(offset, len.unwrap_or(IdxSize::MAX)).into()
     }
 
-    pub fn tail(&self, n: IdxSize) -> Self {
+    fn tail(&self, n: IdxSize) -> Self {
         let ldf = self.ldf.clone();
         ldf.tail(n).into()
     }
 
     #[pyo3(signature = (id_vars, value_vars, value_name, variable_name, streamable))]
-    pub fn melt(
+    fn melt(
         &self,
         id_vars: Vec<String>,
         value_vars: Vec<String>,
         value_name: Option<String>,
         variable_name: Option<String>,
         streamable: bool,
     ) -> Self {
@@ -857,22 +786,22 @@
             streamable,
         };
 
         let ldf = self.ldf.clone();
         ldf.melt(args).into()
     }
 
-    pub fn with_row_count(&self, name: &str, offset: Option<IdxSize>) -> Self {
+    fn with_row_count(&self, name: &str, offset: Option<IdxSize>) -> Self {
         let ldf = self.ldf.clone();
         ldf.with_row_count(name, offset).into()
     }
 
     #[pyo3(signature = (lambda, predicate_pushdown, projection_pushdown, slice_pushdown, streamable, schema, validate_output))]
     #[allow(clippy::too_many_arguments)]
-    pub fn map(
+    fn map(
         &self,
         lambda: PyObject,
         predicate_pushdown: bool,
         projection_pushdown: bool,
         slice_pushdown: bool,
         streamable: bool,
         schema: Option<Wrap<Schema>>,
@@ -951,59 +880,59 @@
         let ldf = self.ldf.clone();
 
         let udf_schema =
             schema.map(move |s| Arc::new(move |_: &Schema| Ok(s.clone())) as Arc<dyn UdfSchema>);
         ldf.map(function, opt, udf_schema, None).into()
     }
 
-    pub fn drop(&self, columns: Vec<String>) -> Self {
+    fn drop(&self, columns: Vec<String>) -> Self {
         let ldf = self.ldf.clone();
         ldf.drop_columns(columns).into()
     }
 
-    pub fn clone(&self) -> PyLazyFrame {
+    fn clone(&self) -> Self {
         self.ldf.clone().into()
     }
 
-    pub fn columns(&self, py: Python) -> PyResult<PyObject> {
+    fn columns(&self, py: Python) -> PyResult<PyObject> {
         let schema = self.get_schema()?;
         let iter = schema.iter_names().map(|s| s.as_str());
         Ok(PyList::new(py, iter).to_object(py))
     }
 
-    pub fn dtypes(&self, py: Python) -> PyResult<PyObject> {
+    fn dtypes(&self, py: Python) -> PyResult<PyObject> {
         let schema = self.get_schema()?;
         let iter = schema
             .iter_dtypes()
             .map(|dt| Wrap(dt.clone()).to_object(py));
         Ok(PyList::new(py, iter).to_object(py))
     }
 
-    pub fn schema(&self, py: Python) -> PyResult<PyObject> {
+    fn schema(&self, py: Python) -> PyResult<PyObject> {
         let schema = self.get_schema()?;
         let schema_dict = PyDict::new(py);
 
         schema.iter_fields().for_each(|fld| {
             schema_dict
                 .set_item(fld.name().as_str(), Wrap(fld.data_type().clone()))
                 .unwrap()
         });
         Ok(schema_dict.to_object(py))
     }
 
-    pub fn unnest(&self, columns: Vec<String>) -> PyLazyFrame {
+    fn unnest(&self, columns: Vec<String>) -> Self {
         self.ldf.clone().unnest(columns).into()
     }
 
-    pub fn width(&self) -> PyResult<usize> {
+    fn width(&self) -> PyResult<usize> {
         Ok(self.get_schema()?.len())
     }
 
     #[cfg(feature = "merge_sorted")]
-    pub fn merge_sorted(&self, other: PyLazyFrame, key: &str) -> PyResult<Self> {
+    fn merge_sorted(&self, other: Self, key: &str) -> PyResult<Self> {
         let out = self
             .ldf
             .clone()
             .merge_sorted(other.ldf, key)
             .map_err(PyPolarsErr::from)?;
         Ok(out.into())
     }
```

### Comparing `polars_lts_cpu-0.17.9/src/lazy/mod.rs` & `polars_lts_cpu-0.18.0/src/expr/mod.rs`

 * *Files 22% similar despite different names*

```diff
@@ -1,36 +1,50 @@
-mod apply;
-pub mod dataframe;
-pub mod dsl;
+mod array;
+mod binary;
+mod categorical;
+mod datetime;
+mod general;
+mod list;
 #[cfg(feature = "meta")]
 mod meta;
-pub mod utils;
+mod string;
+mod r#struct;
 
-pub use apply::*;
-use dsl::*;
-use polars_lazy::prelude::*;
+use polars::lazy::dsl::Expr;
+use pyo3::prelude::*;
+
+#[pyclass]
+#[repr(transparent)]
+#[derive(Clone)]
+pub struct PyExpr {
+    pub inner: Expr,
+}
+
+impl From<Expr> for PyExpr {
+    fn from(expr: Expr) -> Self {
+        PyExpr { inner: expr }
+    }
+}
 
 pub(crate) trait ToExprs {
     fn to_exprs(self) -> Vec<Expr>;
 }
 
 impl ToExprs for Vec<PyExpr> {
     fn to_exprs(self) -> Vec<Expr> {
         // Safety
         // repr is transparent
-        // and has only got one inner field`
         unsafe { std::mem::transmute(self) }
     }
 }
 
 pub(crate) trait ToPyExprs {
     fn to_pyexprs(self) -> Vec<PyExpr>;
 }
 
 impl ToPyExprs for Vec<Expr> {
     fn to_pyexprs(self) -> Vec<PyExpr> {
         // Safety
         // repr is transparent
-        // and has only got one inner field`
         unsafe { std::mem::transmute(self) }
     }
 }
```

### Comparing `polars_lts_cpu-0.17.9/src/object.rs` & `polars_lts_cpu-0.18.0/src/object.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/src/set.rs` & `polars_lts_cpu-0.18.0/src/series/set_at_idx.rs`

 * *Files 19% similar despite different names*

```diff
@@ -1,11 +1,31 @@
 use polars::export::arrow::array::Array;
 use polars::prelude::*;
+use pyo3::prelude::*;
 
-pub(crate) fn set_at_idx(mut s: Series, idx: &Series, values: &Series) -> PolarsResult<Series> {
+use crate::error::PyPolarsErr;
+use crate::PySeries;
+
+#[pymethods]
+impl PySeries {
+    fn set_at_idx(&mut self, idx: PySeries, values: PySeries) -> PyResult<()> {
+        // we take the value because we want a ref count
+        // of 1 so that we can have mutable access
+        let s = std::mem::take(&mut self.series);
+        match set_at_idx(s, &idx.series, &values.series) {
+            Ok(out) => {
+                self.series = out;
+                Ok(())
+            }
+            Err(e) => Err(PyErr::from(PyPolarsErr::from(e))),
+        }
+    }
+}
+
+fn set_at_idx(mut s: Series, idx: &Series, values: &Series) -> PolarsResult<Series> {
     let logical_dtype = s.dtype().clone();
     let idx = idx.cast(&IDX_DTYPE)?;
     let idx = idx.rechunk();
     let idx = idx.idx().unwrap();
     let idx = idx.downcast_iter().next().unwrap();
 
     if idx.null_count() > 0 {
```

### Comparing `polars_lts_cpu-0.17.9/src/sql.rs` & `polars_lts_cpu-0.18.0/src/sql.rs`

 * *Files 20% similar despite different names*

```diff
@@ -21,19 +21,27 @@
     #[allow(clippy::new_without_default)]
     pub fn new() -> PySQLContext {
         PySQLContext {
             context: SQLContext::new(),
         }
     }
 
-    pub fn register(&mut self, name: &str, lf: PyLazyFrame) {
-        self.context.register(name, lf.ldf)
-    }
-
     pub fn execute(&mut self, query: &str) -> PyResult<PyLazyFrame> {
         Ok(self
             .context
             .execute(query)
             .map_err(PyPolarsErr::from)?
             .into())
     }
+
+    pub fn get_tables(&self) -> PyResult<Vec<String>> {
+        Ok(self.context.get_tables())
+    }
+
+    pub fn register(&mut self, name: &str, lf: PyLazyFrame) {
+        self.context.register(name, lf.ldf)
+    }
+
+    pub fn unregister(&mut self, name: &str) {
+        self.context.unregister(name)
+    }
 }
```

### Comparing `polars_lts_cpu-0.17.9/src/utils.rs` & `polars_lts_cpu-0.18.0/src/utils.rs`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/README.md` & `polars_lts_cpu-0.18.0/tests/README.md`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/benchmark/groupby-datagen.R` & `polars_lts_cpu-0.18.0/tests/benchmark/groupby-datagen.R`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/benchmark/run_h2oai_benchmark.py` & `polars_lts_cpu-0.18.0/tests/benchmark/run_h2oai_benchmark.py`

 * *Files 1% similar despite different names*

```diff
@@ -289,24 +289,24 @@
 assert out.shape == (9999995, 8)
 
 # Additional tests
 # the code below, does not belong to the db-benchmark
 # but it triggers other code paths so the checksums assertion
 # are a sort of integration tests
 out = (
-    x.filter(pl.col("id1") == pl.lit("id046"))
+    x.filter(pl.col("id1").eq_missing(pl.lit("id046")))
     .select([pl.sum("id6"), pl.sum("v3")])
     .collect()
 )
 assert out["id6"].to_list() == [430957682]
 assert np.isclose(out["v3"].to_list(), 4.724150165888001e6).all()
 print(out)
 
 out = (
-    x.filter(~(pl.col("id1") == pl.lit("id046")))
+    x.filter(~(pl.col("id1").eq_missing(pl.lit("id046"))))
     .select([pl.sum("id6"), pl.sum("v3")])
     .collect()
 )
 print(out)
 
 assert out["id6"].to_list() == [2137755425]
 assert np.isclose(out["v3"].to_list(), 4.7040828499563754e8).all()
```

### Comparing `polars_lts_cpu-0.17.9/tests/benchmark/test_release.py` & `polars_lts_cpu-0.18.0/tests/benchmark/test_release.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/docs/run_doctest.py` & `polars_lts_cpu-0.18.0/tests/docs/run_doctest.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/parametric/test_dataframe.py` & `polars_lts_cpu-0.18.0/tests/parametric/test_dataframe.py`

 * *Files 12% similar despite different names*

```diff
@@ -8,18 +8,21 @@
 
 import polars as pl
 from polars.testing import assert_frame_equal
 from polars.testing.parametric import column, dataframes
 
 
 @given(df=dataframes())
-@settings(max_examples=50)
 def test_repr(df: pl.DataFrame) -> None:
     assert isinstance(repr(df), str)
-    assert_frame_equal(df, df, check_exact=True, nans_compare_equal=True)
+
+
+@given(df=dataframes())
+def test_equal(df: pl.DataFrame) -> None:
+    assert_frame_equal(df, df.clone(), check_exact=True)
 
 
 @given(
     df=dataframes(
         cols=10,
         max_size=1,
         allowed_dtypes=[pl.Int8, pl.UInt16, pl.List(pl.Int32)],
@@ -34,15 +37,15 @@
 
 
 @given(
     df=dataframes(
         min_size=1,
         min_cols=1,
         null_probability=0.25,
-        excluded_dtypes=[pl.Utf8],
+        excluded_dtypes=[pl.Utf8, pl.List],
     )
 )
 @example(df=pl.DataFrame(schema=["x", "y", "z"]))
 @example(df=pl.DataFrame())
 def test_null_count(df: pl.DataFrame) -> None:
     # note: the zero-row and zero-col cases are always passed as explicit examples
     null_count, ncols = df.null_count(), len(df.columns)
```

### Comparing `polars_lts_cpu-0.17.9/tests/parametric/test_lazyframe.py` & `polars_lts_cpu-0.18.0/tests/parametric/test_lazyframe.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/parametric/test_series.py` & `polars_lts_cpu-0.18.0/tests/parametric/test_series.py`

 * *Files 2% similar despite different names*

```diff
@@ -180,15 +180,17 @@
     sliced_pl_data = srs[s].to_list()
 
     assert sliced_py_data == sliced_pl_data, f"slice [{start}:{stop}:{step}] failed"
     assert_series_equal(srs, srs, check_exact=True)
 
 
 @given(
-    s=series(min_size=1, max_size=10, excluded_dtypes=[pl.Categorical]).filter(
+    s=series(
+        min_size=1, max_size=10, excluded_dtypes=[pl.Categorical, pl.List, pl.Struct]
+    ).filter(
         lambda x: (
             getattr(x.dtype, "time_unit", None) in (None, "us", "ns")
             and (x.dtype != pl.Utf8 or not x.str.contains("\x00").any())
         )
     ),
 )
 @settings(max_examples=250)
```

### Comparing `polars_lts_cpu-0.17.9/tests/parametric/test_testing.py` & `polars_lts_cpu-0.18.0/tests/parametric/test_testing.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/conftest.py` & `polars_lts_cpu-0.18.0/tests/unit/conftest.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/datatypes/test_bool.py` & `polars_lts_cpu-0.18.0/tests/unit/datatypes/test_bool.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/datatypes/test_categorical.py` & `polars_lts_cpu-0.18.0/tests/unit/datatypes/test_categorical.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 from __future__ import annotations
 
 import io
+import typing
 from typing import Any
 
 import pytest
 
 import polars as pl
 from polars.testing import assert_frame_equal
 
@@ -208,23 +209,14 @@
         res = lf1.join(lf2, on="a", how="inner").collect().rows()
         assert sorted(res) == [("bar", 2, 2), ("foo", 1, 1), ("ham", 3, 3)]
 
     # no other scope active; NOW we expect the cache to have been invalidated
     assert pl.using_string_cache() is False
 
 
-def test_categorical_list_concat_4762() -> None:
-    df = pl.DataFrame({"x": "a"})
-    expected = {"x": [["a", "a"]]}
-
-    q = df.lazy().select([pl.concat_list([pl.col("x").cast(pl.Categorical)] * 2)])
-    with pl.StringCache():
-        assert q.collect().to_dict(False) == expected
-
-
 def test_categorical_max_null_5437() -> None:
     assert (
         pl.DataFrame({"strings": ["c", "b", "a", "c"], "values": [0, 1, 2, 3]})
         .with_columns(pl.col("strings").cast(pl.Categorical).alias("cats"))
         .select(pl.all().max())
     ).to_dict(False) == {"strings": ["c"], "values": [3], "cats": [None]}
 
@@ -246,15 +238,15 @@
     out = pl.Series("foo", [["a"], ["a", "b"]]).cast(dtype)
     assert out.dtype == dtype
     assert out.to_list() == [["a"], ["a", "b"]]
 
     with pytest.raises(
         pl.ComputeError, match=r"casting to categorical not allowed in `arr.eval`"
     ):
-        pl.Series("foo", [["a", "b"], ["a", "b"]]).arr.eval(
+        pl.Series("foo", [["a", "b"], ["a", "b"]]).list.eval(
             pl.element().cast(pl.Categorical)
         )
 
 
 @pytest.mark.slow()
 def test_stringcache() -> None:
     N = 1_500
@@ -328,15 +320,15 @@
         {
             "group": [1, 1, 2, 2, 2, 3, 3],
             "letter": ["a", "b", "c", "d", "e", "f", "g"],
         }
     ).with_columns([pl.col("letter").cast(pl.Categorical)]).groupby(
         maintain_order=True, by=["group"]
     ).all().with_columns(
-        [pl.col("letter").arr.lengths().alias("c_group")]
+        [pl.col("letter").list.lengths().alias("c_group")]
     ).groupby(
         by=["c_group"], maintain_order=True
     ).agg(
         pl.col("letter")
     ).to_dict(
         False
     ) == {
@@ -373,7 +365,31 @@
         {"col": ["a", None, "a"]}, schema={"col": pl.Categorical}
     ).fill_null("a").with_columns(pl.col("col").to_physical().alias("code")).to_dict(
         False
     ) == {
         "col": ["a", "a", "a"],
         "code": [0, 0, 0],
     }
+
+
+def test_categorical_fill_null_stringcache() -> None:
+    with pl.StringCache():
+        df = pl.LazyFrame(
+            {"index": [1, 2, 3], "cat": ["a", "b", None]},
+            schema={"index": pl.Int64(), "cat": pl.Categorical()},
+        )
+        a = df.select(pl.col("cat").fill_null("hi")).collect()
+
+    assert a.to_dict(False) == {"cat": ["a", "b", "hi"]}
+    assert a.dtypes == [pl.Categorical]
+
+
+@typing.no_type_check
+def test_fast_unique_flag_from_arrow() -> None:
+    df = pl.DataFrame(
+        {
+            "colB": ["1", "2", "3", "4", "5", "5", "5", "5"],
+        }
+    ).with_columns([pl.col("colB").cast(pl.Categorical)])
+
+    filtered = df.to_arrow().filter([True, False, True, True, False, True, True, True])
+    assert pl.from_arrow(filtered).select(pl.col("colB").n_unique()).item() == 4
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/datatypes/test_decimal.py` & `polars_lts_cpu-0.18.0/tests/unit/datatypes/test_decimal.py`

 * *Files 7% similar despite different names*

```diff
@@ -27,14 +27,15 @@
     for data in permutations_int_dec_none():
         s = pl.Series("name", data)
         assert s.dtype == pl.Decimal(None, 7)  # inferred scale = 7, precision = None
         assert s.name == "name"
         assert s.null_count() == 1
         for i, d in enumerate(data):
             assert s[i] == d
+        assert s.to_list() == [D(x) if x is not None else None for x in data]
 
 
 def test_frame_from_pydecimal_and_ints(monkeypatch: Any) -> None:
     monkeypatch.setenv("POLARS_ACTIVATE_DECIMAL", "1")
 
     class X(NamedTuple):
         a: int | D | None
@@ -96,7 +97,24 @@
         False
     ) == {"decimals": [D("2"), D("2")], "b2": [2.0, 2.0]}
 
 
 def test_decimal_scale_precision_roundtrip(monkeypatch: Any) -> None:
     monkeypatch.setenv("POLARS_ACTIVATE_DECIMAL", "1")
     assert pl.from_arrow(pl.Series("dec", [D("10.0")]).to_arrow()).item() == D("10.0")
+
+
+def test_utf8_to_decimal() -> None:
+    s = pl.Series(
+        ["40.12", "3420.13", "120134.19", "3212.98", "12.90", "143.09", "143.9"]
+    ).str.to_decimal()
+    assert s.dtype == pl.Decimal(8, 2)
+
+    assert s.to_list() == [
+        D("40.12"),
+        D("3420.13"),
+        D("120134.19"),
+        D("3212.98"),
+        D("12.90"),
+        D("143.09"),
+        D("143.90"),
+    ]
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/datatypes/test_list.py` & `polars_lts_cpu-0.18.0/tests/unit/datatypes/test_list.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,23 +1,23 @@
 from __future__ import annotations
 
 from datetime import date, datetime, time
 
 import pandas as pd
-import pytest
 
 import polars as pl
 
 
 def test_dtype() -> None:
     # inferred
     a = pl.Series("a", [[1, 2, 3], [2, 5], [6, 7, 8, 9]])
     assert a.dtype == pl.List
     assert a.inner_dtype == pl.Int64
     assert a.dtype.inner == pl.Int64  # type: ignore[union-attr]
+    assert a.dtype.is_(pl.List(pl.Int64))
 
     # explicit
     df = pl.DataFrame(
         data={
             "i": [[1, 2, 3]],
             "tm": [[time(10, 30, 45)]],
             "dt": [[date(2022, 12, 31)]],
@@ -32,14 +32,15 @@
     )
     assert df.schema == {
         "i": pl.List(pl.Int8),
         "tm": pl.List(pl.Time),
         "dt": pl.List(pl.Date),
         "dtm": pl.List(pl.Datetime),
     }
+    assert all(tp.is_nested for tp in df.dtypes)
     assert df.schema["i"].inner == pl.Int8  # type: ignore[union-attr]
     assert df.rows() == [
         (
             [1, 2, 3],
             [time(10, 30, 45)],
             [date(2022, 12, 31)],
             [datetime(2022, 12, 31, 1, 2, 3)],
@@ -67,72 +68,15 @@
             ]
         )
         .filter(pl.col("num_different_c") >= 2)
         .to_series(3)
     )
 
     assert out.inner_dtype == pl.Categorical
-
-
-def test_list_concat_rolling_window() -> None:
-    # inspired by:
-    # https://stackoverflow.com/questions/70377100/use-the-rolling-function-of-polars-to-get-a-list-of-all-values-in-the-rolling-wi
-    # this tests if it works without specifically creating list dtype upfront. note that
-    # the given answer is preferred over this snippet as that reuses the list array when
-    # shifting
-    df = pl.DataFrame(
-        {
-            "A": [1.0, 2.0, 9.0, 2.0, 13.0],
-        }
-    )
-    out = df.with_columns(
-        [pl.col("A").shift(i).alias(f"A_lag_{i}") for i in range(3)]
-    ).select(
-        [pl.concat_list([f"A_lag_{i}" for i in range(3)][::-1]).alias("A_rolling")]
-    )
-    assert out.shape == (5, 1)
-
-    s = out.to_series()
-    assert s.dtype == pl.List
-    assert s.to_list() == [
-        [None, None, 1.0],
-        [None, 1.0, 2.0],
-        [1.0, 2.0, 9.0],
-        [2.0, 9.0, 2.0],
-        [9.0, 2.0, 13.0],
-    ]
-
-    # this test proper null behavior of concat list
-    out = (
-        df.with_columns(pl.col("A").reshape((-1, 1)))  # first turn into a list
-        .with_columns(
-            [
-                pl.col("A").shift(i).alias(f"A_lag_{i}")
-                for i in range(3)  # slice the lists to a lag
-            ]
-        )
-        .select(
-            [
-                pl.all(),
-                pl.concat_list([f"A_lag_{i}" for i in range(3)][::-1]).alias(
-                    "A_rolling"
-                ),
-            ]
-        )
-    )
-    assert out.shape == (5, 5)
-
-    l64 = pl.List(pl.Float64)
-    assert out.schema == {
-        "A": l64,
-        "A_lag_0": l64,
-        "A_lag_1": l64,
-        "A_lag_2": l64,
-        "A_rolling": l64,
-    }
+    assert not out.inner_dtype.is_nested
 
 
 def test_cast_inner() -> None:
     a = pl.Series([[1, 2]])
     for t in [bool, pl.Boolean]:
         b = a.cast(pl.List(t))
         assert b.dtype == pl.List(pl.Boolean)
@@ -171,26 +115,26 @@
     ).to_dict(False) == {"groupby_column": [1], "n_unique_column": [[]]}
 
 
 def test_list_fill_null() -> None:
     df = pl.DataFrame({"C": [["a", "b", "c"], [], [], ["d", "e"]]})
     assert df.with_columns(
         [
-            pl.when(pl.col("C").arr.lengths() == 0)
+            pl.when(pl.col("C").list.lengths() == 0)
             .then(None)
             .otherwise(pl.col("C"))
             .alias("C")
         ]
     ).to_series().to_list() == [["a", "b", "c"], None, None, ["d", "e"]]
 
 
 def test_list_fill_list() -> None:
     assert pl.DataFrame({"a": [[1, 2, 3], []]}).select(
         [
-            pl.when(pl.col("a").arr.lengths() == 0)
+            pl.when(pl.col("a").list.lengths() == 0)
             .then([5])
             .otherwise(pl.col("a"))
             .alias("filled")
         ]
     ).to_dict(False) == {"filled": [[1, 2, 3], [5]]}
 
 
@@ -201,37 +145,14 @@
     ) == {"array": [[]], "not_array": [1234]}
 
     df = pl.DataFrame(schema=[("col", pl.List)])
     assert df.schema == {"col": pl.List}
     assert df.rows() == []
 
 
-def test_list_concat_nulls() -> None:
-    assert pl.DataFrame(
-        {
-            "a": [["a", "b"], None, ["c", "d", "e"], None],
-            "t": [["x"], ["y"], None, None],
-        }
-    ).with_columns(pl.concat_list(["a", "t"]).alias("concat"))["concat"].to_list() == [
-        ["a", "b", "x"],
-        None,
-        None,
-        None,
-    ]
-
-
-def test_list_concat_supertype() -> None:
-    df = pl.DataFrame(
-        [pl.Series("a", [1, 2], pl.UInt8), pl.Series("b", [10000, 20000], pl.UInt16)]
-    )
-    assert df.with_columns(pl.concat_list(pl.col(["a", "b"])).alias("concat_list"))[
-        "concat_list"
-    ].to_list() == [[1, 10000], [2, 20000]]
-
-
 def test_list_hash() -> None:
     out = pl.DataFrame({"a": [[1, 2, 3], [3, 4], [1, 2, 3]]}).with_columns(
         pl.col("a").hash().alias("b")
     )
     assert out.dtypes == [pl.List(pl.Int64), pl.UInt64]
     assert out[0, "b"] == out[2, "b"]
 
@@ -315,70 +236,28 @@
         "parents": [
             {"ref": 1, "tag": "t", "ratio": 62.3},
             {"ref": None, "tag": None, "ratio": None},
         ],
     }
 
 
-def test_concat_list_in_agg_6397() -> None:
-    df = pl.DataFrame({"group": [1, 2, 2, 3], "value": ["a", "b", "c", "d"]})
-
-    # single list
-    assert df.groupby("group").agg(
-        [
-            # this casts every element to a list
-            pl.concat_list(pl.col("value")),
-        ]
-    ).sort("group").to_dict(False) == {
-        "group": [1, 2, 3],
-        "value": [[["a"]], [["b"], ["c"]], [["d"]]],
-    }
-
-    # nested list
-    assert df.groupby("group").agg(
-        [
-            pl.concat_list(pl.col("value").implode()).alias("result"),
-        ]
-    ).sort("group").to_dict(False) == {
-        "group": [1, 2, 3],
-        "result": [[["a"]], [["b", "c"]], [["d"]]],
-    }
-
-
-def test_concat_list_empty_raises() -> None:
-    with pytest.raises(pl.ComputeError):
-        pl.DataFrame({"a": [1, 2, 3]}).with_columns(pl.concat_list([]))
-
-
 def test_flat_aggregation_to_list_conversion_6918() -> None:
     df = pl.DataFrame({"a": [1, 2, 2], "b": [[0, 1], [2, 3], [4, 5]]})
 
     assert df.groupby("a", maintain_order=True).agg(
-        pl.concat_list([pl.col("b").arr.get(i).mean().implode() for i in range(2)])
+        pl.concat_list([pl.col("b").list.get(i).mean().implode() for i in range(2)])
     ).to_dict(False) == {"a": [1, 2], "b": [[[0.0, 1.0]], [[3.0, 4.0]]]}
 
 
-def test_concat_list_with_lit() -> None:
-    df = pl.DataFrame({"a": [1, 2, 3]})
-
-    assert df.select(pl.concat_list([pl.col("a"), pl.lit(1)]).alias("a")).to_dict(
-        False
-    ) == {"a": [[1, 1], [2, 1], [3, 1]]}
-
-    assert df.select(pl.concat_list([pl.lit(1), pl.col("a")]).alias("a")).to_dict(
-        False
-    ) == {"a": [[1, 1], [1, 2], [1, 3]]}
-
-
 def test_list_count_match() -> None:
     assert pl.DataFrame({"listcol": [[], [1], [1, 2, 3, 2], [1, 2, 1], [4, 4]]}).select(
-        pl.col("listcol").arr.count_match(2).alias("number_of_twos")
+        pl.col("listcol").list.count_match(2).alias("number_of_twos")
     ).to_dict(False) == {"number_of_twos": [0, 0, 2, 1, 0]}
     assert pl.DataFrame({"listcol": [[], [1], [1, 2, 3, 2], [1, 2, 1], [4, 4]]}).select(
-        pl.col("listcol").arr.count_match(2).alias("number_of_twos")
+        pl.col("listcol").list.count_match(2).alias("number_of_twos")
     ).to_dict(False) == {"number_of_twos": [0, 0, 2, 1, 0]}
 
 
 def test_list_sum_and_dtypes() -> None:
     # ensure the dtypes of sum align with normal sum
     for dt_in, dt_out in [
         (pl.Int8, pl.Int64),
@@ -394,54 +273,56 @@
             {"a": [[1], [1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4, 5]]},
             schema={"a": pl.List(dt_in)},
         )
 
         summed = df.explode("a").sum()
         assert summed.dtypes == [dt_out]
         assert summed.item() == 32
-        assert df.select(pl.col("a").arr.sum()).dtypes == [dt_out]
+        assert df.select(pl.col("a").list.sum()).dtypes == [dt_out]
 
-    assert df.select(pl.col("a").arr.sum()).to_dict(False) == {"a": [1, 6, 10, 15]}
+    assert df.select(pl.col("a").list.sum()).to_dict(False) == {"a": [1, 6, 10, 15]}
 
     # include nulls
     assert pl.DataFrame(
         {"a": [[1], [1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4, 5], None]}
-    ).select(pl.col("a").arr.sum()).to_dict(False) == {"a": [1, 6, 10, 15, None]}
+    ).select(pl.col("a").list.sum()).to_dict(False) == {"a": [1, 6, 10, 15, None]}
 
 
 def test_list_mean() -> None:
     assert pl.DataFrame({"a": [[1], [1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4, 5]]}).select(
-        pl.col("a").arr.mean()
+        pl.col("a").list.mean()
     ).to_dict(False) == {"a": [1.0, 2.0, 2.5, 3.0]}
 
     assert pl.DataFrame({"a": [[1], [1, 2, 3], [1, 2, 3, 4], None]}).select(
-        pl.col("a").arr.mean()
+        pl.col("a").list.mean()
     ).to_dict(False) == {"a": [1.0, 2.0, 2.5, None]}
 
 
 def test_list_min_max() -> None:
     for dt in pl.NUMERIC_DTYPES:
         if dt == pl.Decimal:
             continue
         df = pl.DataFrame(
             {"a": [[1], [1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4, 5]]},
             schema={"a": pl.List(dt)},
         )
-        assert df.select(pl.col("a").arr.min())["a"].series_equal(
-            df.select(pl.col("a").arr.first())["a"]
+        assert df.select(pl.col("a").list.min())["a"].series_equal(
+            df.select(pl.col("a").list.first())["a"]
         )
-        assert df.select(pl.col("a").arr.max())["a"].series_equal(
-            df.select(pl.col("a").arr.last())["a"]
+        assert df.select(pl.col("a").list.max())["a"].series_equal(
+            df.select(pl.col("a").list.last())["a"]
         )
 
     df = pl.DataFrame(
         {"a": [[1], [1, 5, -1, 3], [1, 2, 3, 4], [1, 2, 3, 4, 5], None]},
     )
-    assert df.select(pl.col("a").arr.min()).to_dict(False) == {"a": [1, -1, 1, 1, None]}
-    assert df.select(pl.col("a").arr.max()).to_dict(False) == {"a": [1, 5, 4, 5, None]}
+    assert df.select(pl.col("a").list.min()).to_dict(False) == {
+        "a": [1, -1, 1, 1, None]
+    }
+    assert df.select(pl.col("a").list.max()).to_dict(False) == {"a": [1, 5, 4, 5, None]}
 
 
 def test_fill_null_empty_list() -> None:
     assert pl.Series([["a"], None]).fill_null([]).to_list() == [["a"], []]
 
 
 def test_nested_logical() -> None:
@@ -523,7 +404,14 @@
 def test_list_recursive_time_unit_cast() -> None:
     values = [[datetime(2000, 1, 1, 0, 0, 0)]]
     dtype = pl.List(pl.Datetime("ns"))
     s = pl.Series(values)
     out = s.cast(dtype)
     assert out.dtype == dtype
     assert out.to_list() == values
+
+
+def test_list_null_list_categorical_cast() -> None:
+    expected = pl.List(pl.Categorical)
+    s = pl.Series([[]], dtype=pl.List(pl.Null)).cast(expected)
+    assert s.dtype == expected
+    assert s.to_list() == [[]]
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/datatypes/test_object.py` & `polars_lts_cpu-0.18.0/tests/unit/datatypes/test_object.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/datatypes/test_struct.py` & `polars_lts_cpu-0.18.0/tests/unit/datatypes/test_struct.py`

 * *Files 4% similar despite different names*

```diff
@@ -37,14 +37,60 @@
             "e": [None, "foo", "foo", "foo"],
         }
     )
 
     assert_frame_equal(df, expected)
 
 
+def test_struct_equality() -> None:
+    # equal struct dimensions, equal values
+    s1 = pl.Series("misc", [{"x": "a", "y": 0}, {"x": "b", "y": 0}])
+    s2 = pl.Series("misc", [{"x": "a", "y": 0}, {"x": "b", "y": 0}])
+    assert (s1 == s2).all()
+    assert (~(s1 != s2)).all()
+
+    # equal struct dimensions, unequal values
+    s3 = pl.Series("misc", [{"x": "a", "y": 0}, {"x": "c", "y": 2}])
+    s4 = pl.Series("misc", [{"x": "b", "y": 1}, {"x": "d", "y": 3}])
+    assert (s3 != s4).all()
+    assert (~(s3 == s4)).all()
+
+    # unequal struct dimensions, equal values (where fields overlap)
+    s5 = pl.Series("misc", [{"x": "a", "y": 0}, {"x": "b", "y": 0}])
+    s6 = pl.Series("misc", [{"x": "a", "y": 0, "z": 0}, {"x": "b", "y": 0, "z": 0}])
+    assert (s5 != s6).all()
+    assert (~(s5 == s6)).all()
+
+    s7 = pl.Series("misc", [{"x": "a", "y": 0}, {"x": "b", "y": 0}])
+    s8 = pl.Series("misc", [{"x": "a", "y": 0}, {"x": "b", "y": 0}, {"x": "c", "y": 0}])
+    assert (s7 != s8).all()
+    assert (~(s7 == s8)).all()
+
+
+def test_struct_equality_strict() -> None:
+    s1 = pl.Struct(
+        [
+            pl.Field("a", pl.Int64),
+            pl.Field("b", pl.Boolean),
+            pl.Field("c", pl.List(pl.Int32)),
+        ]
+    )
+    s2 = pl.Struct(
+        [pl.Field("a", pl.Int64), pl.Field("b", pl.Boolean), pl.Field("c", pl.List)]
+    )
+
+    # strict
+    assert not (s1.is_(s2))
+    assert s1.is_not(s2)
+
+    # permissive (default)
+    assert s1 == s2
+    assert s1 == s2
+
+
 def test_struct_hashes() -> None:
     dtypes = (
         pl.Struct,
         pl.Struct([pl.Field("a", pl.Int64)]),
         pl.Struct([pl.Field("a", pl.Int64), pl.Field("b", pl.List(pl.Int64))]),
     )
     assert len({hash(tp) for tp in (dtypes)}) == 3
@@ -93,14 +139,15 @@
 def test_struct_unnest_multiple() -> None:
     df = pl.DataFrame({"a": [1, 2], "b": [3, 4], "c": [1.0, 2.0], "d": ["a", "b"]})
     df_structs = df.select(s1=pl.struct(["a", "b"]), s2=pl.struct(["c", "d"]))
 
     # List input
     result = df_structs.unnest(["s1", "s2"])
     assert_frame_equal(result, df)
+    assert all(tp.is_nested for tp in df_structs.dtypes)
 
     # Positional input
     result = df_structs.unnest("s1", "s2")
     assert_frame_equal(result, df)
 
 
 def test_struct_function_expansion() -> None:
@@ -167,30 +214,19 @@
     nest_l2 = nest_l1.to_struct("a").to_frame()
 
     assert isinstance(nest_l2.dtypes[0], pl.datatypes.Struct)
     assert [f.dtype for f in nest_l2.dtypes[0].fields] == nest_l1.dtypes
     assert isinstance(nest_l1.dtypes[0], pl.datatypes.Struct)
 
 
-def test_eager_struct() -> None:
-    with pytest.raises(pl.DuplicateError, match="multiple fields with name '' found"):
-        s = pl.struct([pl.Series([1, 2, 3]), pl.Series(["a", "b", "c"])], eager=True)
-
-    s = pl.struct(
-        [pl.Series("a", [1, 2, 3]), pl.Series("b", ["a", "b", "c"])], eager=True
-    )
-    assert s.dtype == pl.Struct
-
-
 def test_struct_to_pandas() -> None:
     df = pd.DataFrame([{"a": {"b": {"c": 2}}}])
     pl_df = pl.from_pandas(df)
 
     assert isinstance(pl_df.dtypes[0], pl.datatypes.Struct)
-
     assert pl_df.to_pandas().equals(df)
 
 
 def test_struct_logical_types_to_pandas() -> None:
     timestamp = datetime(2022, 1, 1)
     df = pd.DataFrame([{"struct": {"timestamp": timestamp}}])
     assert pl.from_pandas(df).dtypes == [pl.Struct]
@@ -262,38 +298,38 @@
         [{"b": 0, "c": 1}],
         [{"b": 1, "c": 2}],
     ]
 
 
 def test_list_to_struct() -> None:
     df = pl.DataFrame({"a": [[1, 2, 3], [1, 2]]})
-    assert df.select([pl.col("a").arr.to_struct()]).to_series().to_list() == [
+    assert df.select([pl.col("a").list.to_struct()]).to_series().to_list() == [
         {"field_0": 1, "field_1": 2, "field_2": 3},
         {"field_0": 1, "field_1": 2, "field_2": None},
     ]
 
     df = pl.DataFrame({"a": [[1, 2], [1, 2, 3]]})
     assert df.select(
-        [pl.col("a").arr.to_struct(name_generator=lambda idx: f"col_name_{idx}")]
+        [pl.col("a").list.to_struct(fields=lambda idx: f"col_name_{idx}")]
     ).to_series().to_list() == [
         {"col_name_0": 1, "col_name_1": 2},
         {"col_name_0": 1, "col_name_1": 2},
     ]
 
     df = pl.DataFrame({"a": [[1, 2], [1, 2, 3]]})
     assert df.select(
-        [pl.col("a").arr.to_struct(n_field_strategy="max_width")]
+        [pl.col("a").list.to_struct(n_field_strategy="max_width")]
     ).to_series().to_list() == [
         {"field_0": 1, "field_1": 2, "field_2": None},
         {"field_0": 1, "field_1": 2, "field_2": 3},
     ]
 
     # set upper bound
     df = pl.DataFrame({"lists": [[1, 1, 1], [0, 1, 0], [1, 0, 0]]})
-    assert df.lazy().select(pl.col("lists").arr.to_struct(upper_bound=3)).unnest(
+    assert df.lazy().select(pl.col("lists").list.to_struct(upper_bound=3)).unnest(
         "lists"
     ).sum().collect().columns == ["field_0", "field_1", "field_2"]
 
 
 def test_sort_df_with_list_struct() -> None:
     assert pl.DataFrame([{"a": 1, "b": [{"c": 1}]}]).sort("a").to_dict(False) == {
         "a": [1],
@@ -307,16 +343,16 @@
             "list_of_struct": [
                 [{"a": 1, "b": 4}, {"a": 3, "b": 6}],
                 [{"a": 10, "b": 40}, {"a": 20, "b": 50}, {"a": 30, "b": 60}],
             ]
         }
     ).with_columns(
         [
-            pl.col("list_of_struct").arr.head(1).alias("head"),
-            pl.col("list_of_struct").arr.tail(1).alias("tail"),
+            pl.col("list_of_struct").list.head(1).alias("head"),
+            pl.col("list_of_struct").list.tail(1).alias("tail"),
         ]
     ).to_dict(
         False
     ) == {
         "list_of_struct": [
             [{"a": 1, "b": 4}, {"a": 3, "b": 6}],
             [{"a": 10, "b": 40}, {"a": 20, "b": 50}, {"a": 30, "b": 60}],
@@ -369,21 +405,21 @@
             "list_struct": [
                 [{"a": 1, "b": 2}, {"a": 3, "b": 4}, {"a": 5, "b": 6}],
                 [{"a": 1, "b": 2}, {"a": 3, "b": 4}],
                 [{"a": 1, "b": 2}],
             ],
         }
     )
-    assert df.select([pl.col("list_struct").arr.first()]).to_dict(False) == {
+    assert df.select([pl.col("list_struct").list.first()]).to_dict(False) == {
         "list_struct": [{"a": 1, "b": 2}, {"a": 1, "b": 2}, {"a": 1, "b": 2}]
     }
-    assert df.select([pl.col("list_struct").arr.last()]).to_dict(False) == {
+    assert df.select([pl.col("list_struct").list.last()]).to_dict(False) == {
         "list_struct": [{"a": 5, "b": 6}, {"a": 3, "b": 4}, {"a": 1, "b": 2}]
     }
-    assert df.select([pl.col("list_struct").arr.get(0)]).to_dict(False) == {
+    assert df.select([pl.col("list_struct").list.get(0)]).to_dict(False) == {
         "list_struct": [{"a": 1, "b": 2}, {"a": 1, "b": 2}, {"a": 1, "b": 2}]
     }
 
 
 def test_struct_concat_list() -> None:
     assert pl.DataFrame(
         {
@@ -392,15 +428,17 @@
                 [{"a": 1, "b": 2}],
             ],
             "list_struct2": [
                 [{"a": 6, "b": 7}, {"a": 8, "b": 9}],
                 [{"a": 6, "b": 7}],
             ],
         }
-    ).with_columns([pl.col("list_struct1").arr.concat("list_struct2").alias("result")])[
+    ).with_columns(
+        [pl.col("list_struct1").list.concat("list_struct2").alias("result")]
+    )[
         "result"
     ].to_list() == [
         [{"a": 1, "b": 2}, {"a": 3, "b": 4}, {"a": 6, "b": 7}, {"a": 8, "b": 9}],
         [{"a": 1, "b": 2}, {"a": 6, "b": 7}],
     ]
 
 
@@ -408,15 +446,15 @@
     assert pl.DataFrame(
         {
             "list_struct": [
                 [{"a": 1, "b": 2}, {"a": 3, "b": 4}, {"a": 5, "b": 6}],
                 [{"a": 30, "b": 40}, {"a": 10, "b": 20}, {"a": 50, "b": 60}],
             ],
         }
-    ).with_columns([pl.col("list_struct").arr.reverse()]).to_dict(False) == {
+    ).with_columns([pl.col("list_struct").list.reverse()]).to_dict(False) == {
         "list_struct": [
             [{"a": 5, "b": 6}, {"a": 3, "b": 4}, {"a": 1, "b": 2}],
             [{"a": 50, "b": 60}, {"a": 10, "b": 20}, {"a": 30, "b": 40}],
         ]
     }
 
 
@@ -530,28 +568,28 @@
 
 
 def test_struct_arr_eval() -> None:
     df = pl.DataFrame(
         {"col_struct": [[{"a": 1, "b": 11}, {"a": 2, "b": 12}, {"a": 1, "b": 11}]]}
     )
     assert df.with_columns(
-        pl.col("col_struct").arr.eval(pl.element().first()).alias("first")
+        pl.col("col_struct").list.eval(pl.element().first()).alias("first")
     ).to_dict(False) == {
         "col_struct": [[{"a": 1, "b": 11}, {"a": 2, "b": 12}, {"a": 1, "b": 11}]],
         "first": [[{"a": 1, "b": 11}]],
     }
 
 
 @typing.no_type_check
 def test_arr_unique() -> None:
     df = pl.DataFrame(
         {"col_struct": [[{"a": 1, "b": 11}, {"a": 2, "b": 12}, {"a": 1, "b": 11}]]}
     )
     # the order is unpredictable
-    unique = df.with_columns(pl.col("col_struct").arr.unique().alias("unique"))[
+    unique = df.with_columns(pl.col("col_struct").list.unique().alias("unique"))[
         "unique"
     ].to_list()
     assert len(unique) == 1
     unique_el = unique[0]
     assert len(unique_el) == 2
     assert {"a": 2, "b": 12} in unique_el
     assert {"a": 1, "b": 11} in unique_el
@@ -633,65 +671,20 @@
     assert pl.Series([{"a": 1, "b": 2}]).struct[1].name == "b"
     assert pl.Series([{"a": 1, "b": 2}]).struct[-1].name == "b"
     assert pl.Series([{"a": 1, "b": 2}]).to_frame().select(
         [pl.col("").struct[0]]
     ).to_dict(False) == {"a": [1]}
 
 
-def test_struct_broadcasting() -> None:
-    df = pl.DataFrame(
-        {
-            "col1": [1, 2],
-            "col2": [10, 20],
-        }
-    )
-
-    assert (
-        df.select(
-            pl.struct(
-                [
-                    pl.lit("a").alias("a"),
-                    pl.col("col1").alias("col1"),
-                ]
-            ).alias("my_struct")
-        )
-    ).to_dict(False) == {"my_struct": [{"a": "a", "col1": 1}, {"a": "a", "col1": 2}]}
-
-
 def test_struct_supertype() -> None:
     assert pl.from_dicts(
         [{"vehicle": {"auto": "car"}}, {"vehicle": {"auto": None}}]
     ).to_dict(False) == {"vehicle": [{"auto": "car"}, {"auto": None}]}
 
 
-def test_suffix_in_struct_creation() -> None:
-    assert (
-        pl.DataFrame(
-            {
-                "a": [1, 2],
-                "b": [3, 4],
-                "c": [5, 6],
-            }
-        ).select(pl.struct(pl.col(["a", "c"]).suffix("_foo")).alias("bar"))
-    ).unnest("bar").to_dict(False) == {"a_foo": [1, 2], "c_foo": [5, 6]}
-
-
-def test_concat_list_reverse_struct_fields() -> None:
-    df = pl.DataFrame({"nums": [1, 2, 3, 4], "letters": ["a", "b", "c", "d"]}).select(
-        [
-            pl.col("nums"),
-            pl.struct(["letters", "nums"]).alias("combo"),
-            pl.struct(["nums", "letters"]).alias("reverse_combo"),
-        ]
-    )
-    result1 = df.select(pl.concat_list(["combo", "reverse_combo"]))
-    result2 = df.select(pl.concat_list(["combo", "combo"]))
-    assert_frame_equal(result1, result2)
-
-
 def test_struct_any_value_get_after_append() -> None:
     schema = {"a": pl.Int8, "b": pl.Int32}
     struct_def = pl.Struct(schema)
 
     a = pl.Series("s", [{"a": 1, "b": 2}], dtype=struct_def)
     b = pl.Series("s", [{"a": 2, "b": 3}], dtype=struct_def)
     a = a.append(b)
@@ -815,66 +808,26 @@
         {"sex": ["male", "female", "female"], "age": [22, 38, 26]}
     ).select(pl.struct(["sex", "age"]).sort()).unnest("sex").to_dict(False) == {
         "sex": ["female", "female", "male"],
         "age": [26, 38, 22],
     }
 
 
-def test_struct_args_kwargs() -> None:
-    df = pl.DataFrame({"a": [1, 2], "b": [3, 4], "c": ["a", "b"]})
-
-    # Single input
-    result = df.select(r=pl.struct((pl.col("a") + pl.col("b")).alias("p")))
-    expected = pl.DataFrame({"r": [{"p": 4}, {"p": 6}]})
-    assert_frame_equal(result, expected)
-
-    # List input
-    result = df.select(r=pl.struct([pl.col("a").alias("p"), pl.col("b").alias("q")]))
-    expected = pl.DataFrame({"r": [{"p": 1, "q": 3}, {"p": 2, "q": 4}]})
-    assert_frame_equal(result, expected)
-
-    # Positional input
-    result = df.select(r=pl.struct(pl.col("a").alias("p"), pl.col("b").alias("q")))
-    assert_frame_equal(result, expected)
-
-    # Keyword input
-    result = df.select(r=pl.struct(p="a", q="b"))
-    assert_frame_equal(result, expected)
-
-
 def test_struct_applies_as_map() -> None:
     df = pl.DataFrame({"id": [1, 1, 2], "x": ["a", "b", "c"], "y": ["d", "e", "f"]})
 
     # the window function doesn't really make sense
     # but it runs the test: #7286
     assert df.select(
         pl.struct([pl.col("x"), pl.col("y") + pl.col("y")]).over("id")
     ).to_dict(False) == {
         "x": [{"x": "a", "y": "dd"}, {"x": "b", "y": "ee"}, {"x": "c", "y": "ff"}]
     }
 
 
-def test_struct_with_lit() -> None:
-    expr = pl.struct([pl.col("a"), pl.lit(1).alias("b")])
-
-    assert (
-        pl.DataFrame({"a": pl.Series([], dtype=pl.Int64)}).select(expr).to_dict(False)
-    ) == {"a": []}
-
-    assert (
-        pl.DataFrame({"a": pl.Series([1], dtype=pl.Int64)}).select(expr).to_dict(False)
-    ) == {"a": [{"a": 1, "b": 1}]}
-
-    assert (
-        pl.DataFrame({"a": pl.Series([1, 2], dtype=pl.Int64)})
-        .select(expr)
-        .to_dict(False)
-    ) == {"a": [{"a": 1, "b": 1}, {"a": 2, "b": 1}]}
-
-
 def test_struct_unique_df() -> None:
     df = pl.DataFrame(
         {
             "numerical": [1, 2, 1],
             "struct": [{"x": 1, "y": 2}, {"x": 3, "y": 4}, {"x": 1, "y": 2}],
         }
     )
@@ -902,23 +855,14 @@
     payload = [[{"a": time(10)}], [{"a": time(10)}]]
     assert pl.Series(payload).to_list() == payload
     # double nested
     payload = [[[{"a": time(10)}]], [[{"a": time(10)}]]]
     assert pl.Series(payload).to_list() == payload
 
 
-def test_struct_list_cat_8235() -> None:
-    df = pl.DataFrame(
-        {"values": [["a", "b", "c"]]}, schema={"values": pl.List(pl.Categorical)}
-    )
-    assert df.select(pl.struct("values")).to_dict(False) == {
-        "values": [{"values": ["a", "b", "c"]}]
-    }
-
-
 def test_struct_name_passed_in_agg_apply() -> None:
     struct_expr = pl.struct(
         [
             pl.col("A").min(),
             pl.col("B").search_sorted(pl.Series([3, 4])),
         ]
     ).alias("index")
@@ -955,23 +899,7 @@
                 {"val": 0, "counts": 1},
                 {"val": 1, "counts": 1},
                 {"val": 2, "counts": 1},
                 {"val": 3, "counts": 1},
             ]
         ],
     }
-
-
-def test_struct_lit_cast() -> None:
-    df = pl.DataFrame({"a": [1, 2, 3]})
-    schema = {"a": pl.Int64, "b": pl.List(pl.Int64)}
-
-    for lit in [pl.lit(None), pl.lit([[]])]:
-        s = df.select(pl.struct([pl.col("a"), lit.alias("b")], schema=schema))["a"]  # type: ignore[arg-type]
-        assert s.dtype == pl.Struct(
-            [pl.Field("a", pl.Int64), pl.Field("b", pl.List(pl.Int64))]
-        )
-        assert s.to_list() == [
-            {"a": 1, "b": None},
-            {"a": 2, "b": None},
-            {"a": 3, "b": None},
-        ]
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/datatypes/test_temporal.py` & `polars_lts_cpu-0.18.0/tests/unit/datatypes/test_temporal.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,31 +1,32 @@
 from __future__ import annotations
 
+import contextlib
 import io
 from datetime import date, datetime, time, timedelta, timezone
 from typing import TYPE_CHECKING, Any, cast, no_type_check
 
 import numpy as np
 import pandas as pd
 import pyarrow as pa
 import pytest
 
 import polars as pl
 from polars.datatypes import DATETIME_DTYPES, DTYPE_TEMPORAL_UNITS, TEMPORAL_DTYPES
-from polars.exceptions import ArrowError, ComputeError, PolarsPanicError
+from polars.exceptions import ArrowError, ComputeError, TimeZoneAwareConstructorWarning
 from polars.testing import (
     assert_frame_equal,
     assert_series_equal,
     assert_series_not_equal,
 )
 
 if TYPE_CHECKING:
     from zoneinfo import ZoneInfo
 
-    from polars.type_aliases import PolarsTemporalType, TimeUnit
+    from polars.type_aliases import PolarsTemporalType, StartBy, TimeUnit
 else:
     from polars.utils.convert import get_zoneinfo as ZoneInfo
 
 
 def test_fill_null() -> None:
     dtm = datetime.strptime("2021-01-01", "%Y-%m-%d")
     s = pl.Series("A", [dtm, None])
@@ -171,17 +172,17 @@
             "guild": [1, 2, 3],
             "char": ["a", "a", "b"],
         }
     )
     out = (
         df.with_columns(
             pl.col("timestamp").str.strptime(pl.Date, format="%Y-%m-%d"),
-        ).with_columns(pl.col("timestamp").diff().implode().over("char"))
+        ).with_columns(pl.col("timestamp").diff().over("char", mapping_strategy="join"))
     )["timestamp"]
-    assert (out[0] == out[1]).all()
+    assert_series_equal(out[0], out[1])
 
 
 def test_from_pydatetime() -> None:
     datetimes = [
         datetime(2021, 1, 1),
         datetime(2021, 1, 2),
         datetime(2021, 1, 3),
@@ -337,31 +338,39 @@
     # Same as above, but for tz-aware
     test_data = [
         datetime(2000, 1, 1, 1, 1, 1, 555555, tzinfo=ZoneInfo("Asia/Kathmandu")),
         datetime(2514, 5, 30, 1, 53, 4, 986754, tzinfo=ZoneInfo("Asia/Kathmandu")),
         datetime(3099, 12, 31, 23, 59, 59, 123456, tzinfo=ZoneInfo("Asia/Kathmandu")),
         datetime(9999, 12, 31, 23, 59, 59, 999999, tzinfo=ZoneInfo("Asia/Kathmandu")),
     ]
-    ddf = pl.DataFrame({"dtm": test_data}).with_columns(
-        pl.col("dtm").dt.nanosecond().alias("ns")
-    )
+    with pytest.warns(
+        TimeZoneAwareConstructorWarning, match="Series with UTC time zone"
+    ):
+        ddf = pl.DataFrame({"dtm": test_data}).with_columns(
+            pl.col("dtm").dt.nanosecond().alias("ns")
+        )
     assert ddf.rows() == [
         (test_data[0], 555555000),
         (test_data[1], 986754000),
         (test_data[2], 123456000),
         (test_data[3], 999999000),
     ]
     # Similar to above, but check for no error when crossing DST
     test_data = [
         datetime(2021, 11, 7, 0, 0, tzinfo=ZoneInfo("US/Central")),
         datetime(2021, 11, 7, 1, 0, tzinfo=ZoneInfo("US/Central")),
         datetime(2021, 11, 7, 1, 0, fold=1, tzinfo=ZoneInfo("US/Central")),
         datetime(2021, 11, 7, 2, 0, tzinfo=ZoneInfo("US/Central")),
     ]
-    ddf = pl.DataFrame({"dtm": test_data})
+    with pytest.warns(
+        TimeZoneAwareConstructorWarning, match="Series with UTC time zone"
+    ):
+        ddf = pl.DataFrame({"dtm": test_data}).select(
+            pl.col("dtm").dt.convert_time_zone("US/Central")
+        )
     assert ddf.rows() == [
         (test_data[0],),
         (test_data[1],),
         (test_data[2],),
         (test_data[3],),
     ]
 
@@ -432,15 +441,19 @@
 
 def test_series_to_numpy() -> None:
     s0 = pl.Series("date", [123543, 283478, 1243]).cast(pl.Date)
     s1 = pl.Series(
         "datetime", [datetime(2021, 1, 2, 3, 4, 5), datetime(2021, 2, 3, 4, 5, 6)]
     )
     s2 = pl.date_range(
-        datetime(2021, 1, 1, 0), datetime(2021, 1, 1, 1), interval="1h", time_unit="ms"
+        datetime(2021, 1, 1, 0),
+        datetime(2021, 1, 1, 1),
+        interval="1h",
+        time_unit="ms",
+        eager=True,
     )
     assert str(s0.to_numpy()) == "['2308-04-02' '2746-02-20' '1973-05-28']"
     assert (
         str(s1.to_numpy()[:2])
         == "['2021-01-02T03:04:05.000000' '2021-02-03T04:05:06.000000']"
     )
     assert (
@@ -452,209 +465,14 @@
     assert (s3.to_numpy() == out).all()
 
     s4 = pl.Series([time(10, 30, 45), time(23, 59, 59)])
     out = np.array([time(10, 30, 45), time(23, 59, 59)], dtype="object")
     assert (s4.to_numpy() == out).all()
 
 
-def test_date_range() -> None:
-    result = pl.date_range(
-        date(1985, 1, 1), date(2015, 7, 1), timedelta(days=1, hours=12)
-    )
-    assert len(result) == 7426
-    assert result.dt[0] == datetime(1985, 1, 1)
-    assert result.dt[1] == datetime(1985, 1, 2, 12, 0)
-    assert result.dt[2] == datetime(1985, 1, 4, 0, 0)
-    assert result.dt[-1] == datetime(2015, 6, 30, 12, 0)
-
-    for time_unit in DTYPE_TEMPORAL_UNITS:
-        rng = pl.date_range(
-            datetime(2020, 1, 1), date(2020, 1, 2), "2h", time_unit=time_unit
-        )
-        assert rng.time_unit == time_unit
-        assert rng.shape == (13,)
-        assert rng.dt[0] == datetime(2020, 1, 1)
-        assert rng.dt[-1] == datetime(2020, 1, 2)
-
-    # if low/high are both date, range is also be date _iif_ the granularity is >= 1d
-    result = pl.date_range(date(2022, 1, 1), date(2022, 3, 1), "1mo", name="drange")
-    assert result.to_list() == [date(2022, 1, 1), date(2022, 2, 1), date(2022, 3, 1)]
-    assert result.name == "drange"
-
-    result = pl.date_range(date(2022, 1, 1), date(2022, 1, 2), "1h30m")
-    assert list(result) == [
-        datetime(2022, 1, 1, 0, 0),
-        datetime(2022, 1, 1, 1, 30),
-        datetime(2022, 1, 1, 3, 0),
-        datetime(2022, 1, 1, 4, 30),
-        datetime(2022, 1, 1, 6, 0),
-        datetime(2022, 1, 1, 7, 30),
-        datetime(2022, 1, 1, 9, 0),
-        datetime(2022, 1, 1, 10, 30),
-        datetime(2022, 1, 1, 12, 0),
-        datetime(2022, 1, 1, 13, 30),
-        datetime(2022, 1, 1, 15, 0),
-        datetime(2022, 1, 1, 16, 30),
-        datetime(2022, 1, 1, 18, 0),
-        datetime(2022, 1, 1, 19, 30),
-        datetime(2022, 1, 1, 21, 0),
-        datetime(2022, 1, 1, 22, 30),
-        datetime(2022, 1, 2, 0, 0),
-    ]
-
-    result = pl.date_range(
-        datetime(2022, 1, 1), datetime(2022, 1, 1, 0, 1), "987456321ns"
-    )
-    assert len(result) == 61
-    assert result.dtype.time_unit == "ns"  # type: ignore[union-attr]
-    assert result.dt.second()[-1] == 59
-    assert result.cast(pl.Utf8)[-1] == "2022-01-01 00:00:59.247379260"
-
-
-@pytest.mark.parametrize(
-    ("time_unit", "expected_micros"),
-    [
-        ("ms", 986000),
-        ("us", 986759),
-        ("ns", 986759),
-        (None, 986759),
-    ],
-)
-def test_date_range_precision(time_unit: TimeUnit | None, expected_micros: int) -> None:
-    micros = 986759
-    start = datetime(2000, 5, 30, 1, 53, 4, micros)
-    stop = datetime(2000, 5, 31, 1, 53, 4, micros)
-    result = pl.date_range(start, stop, time_unit=time_unit)
-    expected_start = start.replace(microsecond=expected_micros)
-    expected_stop = stop.replace(microsecond=expected_micros)
-    assert result[0] == expected_start
-    assert result[1] == expected_stop
-
-
-def test_range_invalid_unit() -> None:
-    with pytest.raises(PolarsPanicError, match="'D' not supported"):
-        pl.date_range(
-            start=datetime(2021, 12, 16), end=datetime(2021, 12, 16, 3), interval="1D"
-        )
-
-
-def test_date_range_lazy_with_literals() -> None:
-    df = pl.DataFrame({"misc": ["x"]}).with_columns(
-        pl.date_range(
-            date(2000, 1, 1),
-            date(2023, 8, 31),
-            interval="987d",
-            lazy=True,
-        )
-        .implode()
-        .alias("dts")
-    )
-    assert df.rows() == [
-        (
-            "x",
-            [
-                date(2000, 1, 1),
-                date(2002, 9, 14),
-                date(2005, 5, 28),
-                date(2008, 2, 9),
-                date(2010, 10, 23),
-                date(2013, 7, 6),
-                date(2016, 3, 19),
-                date(2018, 12, 1),
-                date(2021, 8, 14),
-            ],
-        )
-    ]
-    assert (
-        df.rows()[0][1]
-        == pd.date_range(
-            date(2000, 1, 1), date(2023, 12, 31), freq="987d"
-        ).date.tolist()
-    )
-
-
-@pytest.mark.parametrize("low", ["start", pl.col("start")])
-@pytest.mark.parametrize("high", ["stop", pl.col("stop")])
-def test_date_range_lazy_with_expressions(
-    low: str | pl.Expr, high: str | pl.Expr
-) -> None:
-    ldf = (
-        pl.DataFrame({"start": [date(2015, 6, 30)], "stop": [date(2022, 12, 31)]})
-        .with_columns(
-            pl.date_range(low, high, interval="678d", lazy=True).implode().alias("dts")
-        )
-        .lazy()
-    )
-
-    assert ldf.collect().rows() == [
-        (
-            date(2015, 6, 30),
-            date(2022, 12, 31),
-            [
-                date(2015, 6, 30),
-                date(2017, 5, 8),
-                date(2019, 3, 17),
-                date(2021, 1, 23),
-                date(2022, 12, 2),
-            ],
-        )
-    ]
-
-    assert pl.DataFrame(
-        {
-            "start": [date(2000, 1, 1), date(2022, 6, 1)],
-            "stop": [date(2000, 1, 2), date(2022, 6, 2)],
-        }
-    ).with_columns(
-        pl.date_range(
-            low,
-            high,
-            interval="1d",
-        ).alias("dts")
-    ).to_dict(
-        False
-    ) == {
-        "start": [date(2000, 1, 1), date(2022, 6, 1)],
-        "stop": [date(2000, 1, 2), date(2022, 6, 2)],
-        "dts": [
-            [date(2000, 1, 1), date(2000, 1, 2)],
-            [date(2022, 6, 1), date(2022, 6, 2)],
-        ],
-    }
-
-    assert pl.DataFrame(
-        {
-            "start": [datetime(2000, 1, 1), datetime(2022, 6, 1)],
-            "stop": [datetime(2000, 1, 2), datetime(2022, 6, 2)],
-        }
-    ).with_columns(
-        pl.date_range(
-            low,
-            high,
-            interval="1d",
-        ).alias("dts")
-    ).to_dict(
-        False
-    ) == {
-        "start": [datetime(2000, 1, 1, 0, 0), datetime(2022, 6, 1, 0, 0)],
-        "stop": [datetime(2000, 1, 2, 0, 0), datetime(2022, 6, 2, 0, 0)],
-        "dts": [
-            [datetime(2000, 1, 1, 0, 0), datetime(2000, 1, 2, 0, 0)],
-            [datetime(2022, 6, 1, 0, 0), datetime(2022, 6, 2, 0, 0)],
-        ],
-    }
-
-
-def test_date_range_invalid_time_zone() -> None:
-    with pytest.raises(ComputeError, match="unable to parse time zone: 'foo'"):
-        pl.date_range(
-            datetime(2001, 1, 1), datetime(2001, 1, 3), interval="1d", time_zone="foo"
-        )
-
-
 @pytest.mark.parametrize(
     ("one", "two"),
     [
         (date(2001, 1, 1), date(2001, 1, 2)),
         (datetime(2001, 1, 1), datetime(2001, 1, 2)),
         (time(20, 10, 0), time(20, 10, 1)),
         # also test if the conversion stays correct with wide date ranges
@@ -671,24 +489,26 @@
     assert (a >= one).to_list() == [True, True]
     assert (a < one).to_list() == [False, False]
     assert (a <= one).to_list() == [True, False]
 
 
 @pytest.mark.parametrize("tzinfo", [None, ZoneInfo("Asia/Kathmandu")])
 def test_truncate_negative_offset(tzinfo: ZoneInfo | None) -> None:
+    time_zone = tzinfo.key if tzinfo is not None else None
     df = pl.DataFrame(
         {
             "event_date": [
-                datetime(2021, 4, 11, tzinfo=tzinfo),
-                datetime(2021, 4, 29, tzinfo=tzinfo),
-                datetime(2021, 5, 29, tzinfo=tzinfo),
+                datetime(2021, 4, 11),
+                datetime(2021, 4, 29),
+                datetime(2021, 5, 29),
             ],
             "adm1_code": [1, 2, 1],
         }
     ).set_sorted("event_date")
+    df = df.with_columns(pl.col("event_date").dt.replace_time_zone(time_zone))
     out = df.groupby_dynamic(
         index_column="event_date",
         every="1mo",
         period="2mo",
         offset="-1mo",
         include_boundaries=True,
     ).agg(
@@ -701,25 +521,26 @@
         datetime(2021, 3, 1, tzinfo=tzinfo),
         datetime(2021, 4, 1, tzinfo=tzinfo),
         datetime(2021, 5, 1, tzinfo=tzinfo),
     ]
     df = pl.DataFrame(
         {
             "event_date": [
-                datetime(2021, 4, 11, tzinfo=tzinfo),
-                datetime(2021, 4, 29, tzinfo=tzinfo),
-                datetime(2021, 5, 29, tzinfo=tzinfo),
+                datetime(2021, 4, 11),
+                datetime(2021, 4, 29),
+                datetime(2021, 5, 29),
             ],
             "adm1_code": [1, 2, 1],
             "five_type": ["a", "b", "a"],
             "actor": ["a", "a", "a"],
             "admin": ["a", "a", "a"],
             "fatalities": [10, 20, 30],
         }
     ).set_sorted("event_date")
+    df = df.with_columns(pl.col("event_date").dt.replace_time_zone(time_zone))
 
     out = df.groupby_dynamic(
         index_column="event_date",
         every="1mo",
         by=["admin", "five_type", "actor"],
     ).agg([pl.col("adm1_code").unique(), (pl.col("fatalities") > 0).sum()])
 
@@ -799,29 +620,30 @@
                 "timestamp": ["1970-01-01 00:00:00+01:00", "1970-01-01 01:00:00+01:00"],
                 "value": [1, 1],
             }
         )
         .with_columns(
             pl.col("timestamp")
             .str.strptime(pl.Datetime, format="%Y-%m-%d %H:%M:%S%:z")
+            .dt.convert_time_zone("Africa/Lagos")
             .set_sorted()
         )
         .with_columns(
             pl.col("timestamp")
             .dt.convert_time_zone("UTC")
             .alias("timestamp_utc")
             .set_sorted()
         )
     )
     result = df.groupby_dynamic(
         index_column="timestamp", every="1d", closed="left"
     ).agg(pl.col("value").count())
     expected = pl.DataFrame({"timestamp": [datetime(1970, 1, 1)], "value": [2]})
     expected = expected.with_columns(
-        pl.col("timestamp").dt.replace_time_zone("+01:00"),
+        pl.col("timestamp").dt.replace_time_zone("Africa/Lagos"),
         pl.col("value").cast(pl.UInt32),
     )
     assert_frame_equal(result, expected)
     result = df.groupby_dynamic(
         index_column="timestamp_utc", every="1d", closed="left"
     ).agg(pl.col("value").count())
     expected = pl.DataFrame(
@@ -868,15 +690,14 @@
 
 
 @pytest.mark.parametrize(
     ("time_zone", "tzinfo"),
     [
         (None, None),
         ("Europe/Warsaw", ZoneInfo("Europe/Warsaw")),
-        ("+01:00", timezone(timedelta(hours=1))),
     ],
 )
 def test_upsample(time_zone: str | None, tzinfo: ZoneInfo | timezone | None) -> None:
     df = pl.DataFrame(
         {
             "time": [
                 datetime(2021, 2, 1),
@@ -940,15 +761,18 @@
     offset: str | None,
     expected_time: list[datetime],
     expected_values: list[int],
 ) -> None:
     df = pl.DataFrame(
         {
             "time": pl.date_range(
-                datetime(2021, 11, 6), datetime(2021, 11, 8), time_zone=time_zone
+                datetime(2021, 11, 6),
+                datetime(2021, 11, 8),
+                time_zone=time_zone,
+                eager=True,
             ),
             "values": [1, 2, 3],
         }
     )
     result = df.upsample(time_column="time", every="1d", offset=offset)
     expected = pl.DataFrame(
         {
@@ -959,27 +783,27 @@
     assert_frame_equal(result, expected)
 
 
 @pytest.mark.parametrize(
     ("time_zone", "tzinfo"),
     [
         (None, None),
-        ("+01:00", timezone(timedelta(hours=1))),
         ("Pacific/Rarotonga", ZoneInfo("Pacific/Rarotonga")),
     ],
 )
 def test_upsample_time_zones(
     time_zone: str | None, tzinfo: timezone | ZoneInfo | None
 ) -> None:
     df = pl.DataFrame(
         {
             "time": pl.date_range(
                 start=datetime(2021, 12, 16),
                 end=datetime(2021, 12, 16, 3),
                 interval="30m",
+                eager=True,
             ),
             "groups": ["a", "a", "a", "b", "b", "a", "a"],
             "values": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0],
         }
     )
     expected = pl.DataFrame(
         {
@@ -1029,86 +853,174 @@
     df.to_parquet(f)
     f.seek(0)
     df_in = pl.read_parquet(f)
     tz = ZoneInfo("UTC")
     assert df_in["Timestamp"][0] == datetime(2022, 1, 1, 0, 0, tzinfo=tz)
 
 
-@pytest.mark.parametrize("tzinfo", [None, ZoneInfo("Asia/Kathmandu")])
-def test_default_negative_every_offset_dynamic_groupby(tzinfo: ZoneInfo | None) -> None:
+@pytest.mark.parametrize("time_zone", [None, "Asia/Kathmandu"])
+def test_default_negative_every_offset_dynamic_groupby(time_zone: str | None) -> None:
     # 2791
     dts = [
-        datetime(2020, 1, 1, tzinfo=tzinfo),
-        datetime(2020, 1, 2, tzinfo=tzinfo),
-        datetime(2020, 2, 1, tzinfo=tzinfo),
-        datetime(2020, 3, 1, tzinfo=tzinfo),
+        datetime(2020, 1, 1),
+        datetime(2020, 1, 2),
+        datetime(2020, 2, 1),
+        datetime(2020, 3, 1),
     ]
     df = pl.DataFrame({"dt": dts, "idx": range(len(dts))}).set_sorted("dt")
+    df = df.with_columns(pl.col("dt").dt.replace_time_zone(time_zone))
     out = df.groupby_dynamic(index_column="dt", every="1mo", closed="right").agg(
         pl.col("idx")
     )
 
     expected = pl.DataFrame(
         {
             "dt": [
-                datetime(2019, 12, 1, 0, 0, tzinfo=tzinfo),
-                datetime(2020, 1, 1, 0, 0, tzinfo=tzinfo),
-                datetime(2020, 2, 1, 0, 0, tzinfo=tzinfo),
+                datetime(2019, 12, 1, 0, 0),
+                datetime(2020, 1, 1, 0, 0),
+                datetime(2020, 2, 1, 0, 0),
             ],
             "idx": [[0], [1, 2], [3]],
         }
     )
+    expected = expected.with_columns(pl.col("dt").dt.replace_time_zone(time_zone))
     assert_frame_equal(out, expected)
 
 
 @pytest.mark.parametrize(
     ("rule", "offset"),
     [
         ("1h", timedelta(hours=2)),
         ("1d", timedelta(days=2)),
         ("1w", timedelta(weeks=2)),
     ],
 )
 def test_groupby_dynamic_crossing_dst(rule: str, offset: timedelta) -> None:
     start_dt = datetime(2021, 11, 7)
     end_dt = start_dt + offset
-    date_range = pl.date_range(start_dt, end_dt, rule, time_zone="US/Central")
+    date_range = pl.date_range(
+        start_dt, end_dt, rule, time_zone="US/Central", eager=True
+    )
     df = pl.DataFrame({"time": date_range, "value": range(len(date_range))})
     result = df.groupby_dynamic("time", every=rule).agg(pl.col("value").mean())
     expected = pl.DataFrame(
         {"time": date_range, "value": range(len(date_range))},
         schema_overrides={"value": pl.Float64},
     )
     assert_frame_equal(result, expected)
 
 
-def test_groupby_dynamic_startby_monday_crossing_dst() -> None:
+@pytest.mark.parametrize(
+    ("start_by", "expected_time", "expected_value"),
+    [
+        (
+            "monday",
+            [
+                datetime(2021, 11, 1),
+                datetime(2021, 11, 8),
+            ],
+            [0.0, 4.0],
+        ),
+        (
+            "tuesday",
+            [
+                datetime(2021, 11, 2),
+                datetime(2021, 11, 9),
+            ],
+            [0.5, 4.5],
+        ),
+        (
+            "wednesday",
+            [
+                datetime(2021, 11, 3),
+                datetime(2021, 11, 10),
+            ],
+            [1.0, 5.0],
+        ),
+        (
+            "thursday",
+            [
+                datetime(2021, 11, 4),
+                datetime(2021, 11, 11),
+            ],
+            [1.5, 5.5],
+        ),
+        (
+            "friday",
+            [
+                datetime(2021, 11, 5),
+                datetime(2021, 11, 12),
+            ],
+            [2.0, 6.0],
+        ),
+        (
+            "saturday",
+            [
+                datetime(2021, 11, 6),
+                datetime(2021, 11, 13),
+            ],
+            [2.5, 6.5],
+        ),
+        (
+            "sunday",
+            [
+                datetime(2021, 11, 7),
+                datetime(2021, 11, 14),
+            ],
+            [3.0, 7.0],
+        ),
+    ],
+)
+def test_groupby_dynamic_startby_monday_crossing_dst(
+    start_by: StartBy, expected_time: list[datetime], expected_value: list[float]
+) -> None:
     start_dt = datetime(2021, 11, 7)
     end_dt = datetime(2021, 11, 14)
-    date_range = pl.date_range(start_dt, end_dt, "1d", time_zone="US/Central")
+    date_range = pl.date_range(
+        start_dt, end_dt, "1d", time_zone="US/Central", eager=True
+    )
+    df = pl.DataFrame({"time": date_range, "value": range(len(date_range))})
+    result = df.groupby_dynamic("time", every="1w", start_by=start_by).agg(
+        pl.col("value").mean()
+    )
+    expected = pl.DataFrame(
+        {"time": expected_time, "value": expected_value},
+    )
+    expected = expected.with_columns(pl.col("time").dt.replace_time_zone("US/Central"))
+    assert_frame_equal(result, expected)
+
+
+def test_groupby_dynamic_startby_monday_dst_8737() -> None:
+    start_dt = datetime(2021, 11, 6, 20)
+    stop_dt = datetime(2021, 11, 7, 20)
+    date_range = pl.date_range(
+        start_dt, stop_dt, "1d", time_zone="US/Central", eager=True
+    )
     df = pl.DataFrame({"time": date_range, "value": range(len(date_range))})
     result = df.groupby_dynamic("time", every="1w", start_by="monday").agg(
         pl.col("value").mean()
     )
     expected = pl.DataFrame(
         {
             "time": [
-                datetime(2021, 11, 1, tzinfo=ZoneInfo("US/Central")),
-                datetime(2021, 11, 8, tzinfo=ZoneInfo("US/Central")),
+                datetime(2021, 11, 1),
             ],
-            "value": [0.0, 4.0],
+            "value": [0.5],
         },
     )
+    expected = expected.with_columns(pl.col("time").dt.replace_time_zone("US/Central"))
     assert_frame_equal(result, expected)
 
 
 def test_groupby_dynamic_monthly_crossing_dst() -> None:
     start_dt = datetime(2021, 11, 1)
     end_dt = datetime(2021, 12, 1)
-    date_range = pl.date_range(start_dt, end_dt, "1mo", time_zone="US/Central")
+    date_range = pl.date_range(
+        start_dt, end_dt, "1mo", time_zone="US/Central", eager=True
+    )
     df = pl.DataFrame({"time": date_range, "value": range(len(date_range))})
     result = df.groupby_dynamic("time", every="1mo").agg(pl.col("value").mean())
     expected = pl.DataFrame(
         {"time": date_range, "value": range(len(date_range))},
         schema_overrides={"value": pl.Float64},
     )
     assert_frame_equal(result, expected)
@@ -1404,14 +1316,37 @@
     assert quotes.join_asof(trades, on="dates", strategy="backward", tolerance="5ms")[
         "bid_right"
     ].to_list() == [51.95, 51.95, None, 51.95, 98.0, 98.0, None, None]
     assert quotes.join_asof(trades, on="dates", strategy="forward", tolerance="5ms")[
         "bid_right"
     ].to_list() == [51.95, 51.95, None, None, 720.77, None, None, None]
 
+    assert trades.join_asof(quotes, on="dates", strategy="nearest")[
+        "bid_right"
+    ].to_list() == [51.95, 51.99, 720.5, 720.5, 720.5]
+    assert quotes.join_asof(trades, on="dates", strategy="nearest")[
+        "bid_right"
+    ].to_list() == [51.95, 51.95, 51.95, 51.95, 98.0, 98.0, 98.0, 98.0]
+
+    assert trades.sort(by=["ticker", "dates"]).join_asof(
+        quotes.sort(by=["ticker", "dates"]), on="dates", by="ticker", strategy="nearest"
+    )["bid_right"].to_list() == [97.99, 720.5, 720.5, 51.95, 51.99]
+    assert quotes.sort(by=["ticker", "dates"]).join_asof(
+        trades.sort(by=["ticker", "dates"]), on="dates", by="ticker", strategy="nearest"
+    )["bid_right"].to_list() == [
+        98.0,
+        720.92,
+        720.92,
+        720.92,
+        51.95,
+        51.95,
+        51.95,
+        51.95,
+    ]
+
 
 @pytest.mark.parametrize(
     ("skip_nulls", "expected_value"),
     [
         (True, None),
         (False, datetime(2010, 9, 12)),
     ],
@@ -1594,21 +1529,33 @@
     }
 
 
 def test_datetime_units() -> None:
     df = pl.DataFrame(
         {
             "ns": pl.date_range(
-                datetime(2020, 1, 1), datetime(2020, 5, 1), "1mo", time_unit="ns"
+                datetime(2020, 1, 1),
+                datetime(2020, 5, 1),
+                "1mo",
+                time_unit="ns",
+                eager=True,
             ),
             "us": pl.date_range(
-                datetime(2020, 1, 1), datetime(2020, 5, 1), "1mo", time_unit="us"
+                datetime(2020, 1, 1),
+                datetime(2020, 5, 1),
+                "1mo",
+                time_unit="us",
+                eager=True,
             ),
             "ms": pl.date_range(
-                datetime(2020, 1, 1), datetime(2020, 5, 1), "1mo", time_unit="ms"
+                datetime(2020, 1, 1),
+                datetime(2020, 5, 1),
+                "1mo",
+                time_unit="ms",
+                eager=True,
             ),
         }
     )
     names = set(df.columns)
 
     for unit in DTYPE_TEMPORAL_UNITS:
         subset = names - {unit}
@@ -1640,15 +1587,17 @@
 
     assert [] == list(df.select(pl.exclude(DATETIME_DTYPES)))
 
 
 def test_unique_counts_on_dates() -> None:
     assert pl.DataFrame(
         {
-            "dt_ns": pl.date_range(datetime(2020, 1, 1), datetime(2020, 3, 1), "1mo"),
+            "dt_ns": pl.date_range(
+                datetime(2020, 1, 1), datetime(2020, 3, 1), "1mo", eager=True
+            ),
         }
     ).with_columns(
         [
             pl.col("dt_ns").dt.cast_time_unit("us").alias("dt_us"),
             pl.col("dt_ns").dt.cast_time_unit("ms").alias("dt_ms"),
             pl.col("dt_ns").cast(pl.Date).alias("date"),
         ]
@@ -1711,15 +1660,15 @@
 
 
 def test_groupby_rolling_by_() -> None:
     df = pl.DataFrame({"group": pl.arange(0, 3, eager=True)}).join(
         pl.DataFrame(
             {
                 "datetime": pl.date_range(
-                    datetime(2020, 1, 1), datetime(2020, 1, 5), "1d"
+                    datetime(2020, 1, 1), datetime(2020, 1, 5), "1d", eager=True
                 ),
             }
         ),
         how="cross",
     )
     out = (
         df.sort("datetime")
@@ -1798,15 +1747,17 @@
         "sec": [150],
     }
 
 
 def test_supertype_timezones_4174() -> None:
     df = pl.DataFrame(
         {
-            "dt": pl.date_range(datetime(2020, 3, 1), datetime(2020, 5, 1), "1mo"),
+            "dt": pl.date_range(
+                datetime(2020, 3, 1), datetime(2020, 5, 1), "1mo", eager=True
+            ),
         }
     ).with_columns(pl.col("dt").dt.replace_time_zone("Europe/London").suffix("_London"))
 
     # test if this runs without error
     date_to_fill = df["dt_London"][0]
     df.with_columns(df["dt_London"].shift_and_fill(date_to_fill, periods=1))
 
@@ -1838,22 +1789,24 @@
 
 
 def test_date_arr_concat() -> None:
     expected = {"d": [[date(2000, 1, 1), date(2000, 1, 1)]]}
 
     # type date
     df = pl.DataFrame({"d": [date(2000, 1, 1)]})
-    assert df.select(pl.col("d").arr.concat(pl.col("d"))).to_dict(False) == expected
+    assert df.select(pl.col("d").list.concat(pl.col("d"))).to_dict(False) == expected
     # type list[date]
     df = pl.DataFrame({"d": [[date(2000, 1, 1)]]})
-    assert df.select(pl.col("d").arr.concat(pl.col("d"))).to_dict(False) == expected
+    assert df.select(pl.col("d").list.concat(pl.col("d"))).to_dict(False) == expected
 
 
 def test_date_timedelta() -> None:
-    df = pl.DataFrame({"date": pl.date_range(date(2001, 1, 1), date(2001, 1, 3), "1d")})
+    df = pl.DataFrame(
+        {"date": pl.date_range(date(2001, 1, 1), date(2001, 1, 3), "1d", eager=True)}
+    )
     assert df.with_columns(
         [
             (pl.col("date") + timedelta(days=1)).alias("date_plus_one"),
             (pl.col("date") - timedelta(days=1)).alias("date_min_one"),
         ]
     ).to_dict(False) == {
         "date": [date(2001, 1, 1), date(2001, 1, 2), date(2001, 1, 3)],
@@ -1882,15 +1835,15 @@
         schema=[
             ("x", pl.Datetime("ms")),
             ("y", pl.Datetime("us")),
             ("z", pl.Datetime("ns")),
         ],
     )
     assert df.select(
-        [pl.col("x").dt.strftime("%F %T").alias("w")]
+        [pl.col("x").dt.to_string("%F %T").alias("w")]
         + [pl.col(d).cast(str) for d in df.columns]
     ).rows() == [
         (
             "2022-08-30 10:30:45",
             "2022-08-30 10:30:45.123",
             "2022-08-30 10:30:45.123456",
             "2022-08-30 10:30:45.123456789",
@@ -1912,20 +1865,19 @@
         "b": [datetime(2022, 9, 25, 14, 0, tzinfo=ny)],
     }
 
 
 @pytest.mark.parametrize(
     ("to_tz", "tzinfo"),
     [
-        ("+01:00", timezone(timedelta(seconds=3600))),
         ("America/Barbados", ZoneInfo(key="America/Barbados")),
         (None, None),
     ],
 )
-@pytest.mark.parametrize("from_tz", ["Asia/Seoul", "-01:00", None])
+@pytest.mark.parametrize("from_tz", ["Asia/Seoul", None])
 @pytest.mark.parametrize("time_unit", ["ms", "us", "ns"])
 def test_replace_timezone_from_to(
     from_tz: str,
     to_tz: str,
     tzinfo: timezone | ZoneInfo,
     time_unit: TimeUnit,
 ) -> None:
@@ -1945,52 +1897,72 @@
 
 
 @pytest.mark.parametrize(
     ("time_unit", "time_zone"),
     [
         ("us", "Europe/London"),
         ("ms", None),
-        ("ns", "+01:00"),
+        ("ns", "Africa/Lagos"),
     ],
 )
 def test_strptime_empty(time_unit: TimeUnit, time_zone: str | None) -> None:
     ts = pl.Series([None]).cast(pl.Utf8).str.strptime(pl.Datetime(time_unit, time_zone))
     assert ts.dtype == pl.Datetime(time_unit, time_zone)
 
 
 def test_strptime_with_invalid_tz() -> None:
-    with pytest.raises(ComputeError, match="unable to parse time zone: 'foo'"):
+    with pytest.raises(
+        ComputeError, match="unable to parse time zone: 'foo'"
+    ), pytest.warns(
+        FutureWarning,
+        match="time zones other than those in `zoneinfo.available_timezones",
+    ):
         pl.Series(["2020-01-01 03:00:00"]).str.strptime(pl.Datetime("us", "foo"))
     with pytest.raises(
         ComputeError,
         match="cannot use strptime with both a tz-aware format and a tz-aware dtype",
     ):
         pl.Series(["2020-01-01 03:00:00+01:00"]).str.strptime(
             pl.Datetime("us", "foo"), "%Y-%m-%d %H:%M:%S%z"
         )
-    with pytest.raises(
-        ComputeError,
-        match="cannot use strptime with both 'utc=True' and tz-aware dtype",
+
+
+def test_utc_deprecation() -> None:
+    with pytest.warns(
+        DeprecationWarning,
+        match="The `utc` argument is now a no-op and has no effect. You can safely remove it",
     ):
         pl.Series(["2020-01-01 03:00:00"]).str.strptime(
-            pl.Datetime("us", "foo"), "%Y-%m-%d %H:%M:%S", utc=True
+            pl.Datetime("us"), "%Y-%m-%d %H:%M:%S", utc=True
+        )
+    with pytest.warns(
+        DeprecationWarning,
+        match="The `utc` argument is now a no-op and has no effect. You can safely remove it",
+    ):
+        pl.Series(["2020-01-01 03:00:00"]).str.to_datetime(
+            "%Y-%m-%d %H:%M:%S", utc=True
         )
 
 
 def test_strptime_unguessable_format() -> None:
     with pytest.raises(
         ComputeError,
         match="could not find an appropriate format to parse dates, please define a fmt",
     ):
         pl.Series(["foobar"]).str.strptime(pl.Datetime)
 
 
 def test_convert_time_zone_invalid() -> None:
     ts = pl.Series(["2020-01-01"]).str.strptime(pl.Datetime)
-    with pytest.raises(ComputeError, match="unable to parse time zone: 'foo'"):
+    with pytest.raises(
+        ComputeError, match="unable to parse time zone: 'foo'"
+    ), pytest.warns(
+        FutureWarning,
+        match="time zones other than those in `zoneinfo.available_timezones",
+    ):
         ts.dt.replace_time_zone("UTC").dt.convert_time_zone("foo")
 
 
 def test_convert_time_zone_lazy_schema() -> None:
     ts_us = pl.Series(["2020-01-01"]).str.strptime(pl.Datetime("us", "UTC"))
     ts_ms = pl.Series(["2020-01-01"]).str.strptime(pl.Datetime("ms", "UTC"))
     ldf = pl.DataFrame({"ts_us": ts_us, "ts_ms": ts_ms}).lazy()
@@ -2012,21 +1984,14 @@
     with pytest.raises(
         ComputeError,
         match="cannot call `convert_time_zone` on tz-naive; set a time zone first with `replace_time_zone`",
     ):
         ts.dt.convert_time_zone("Africa/Bamako")
 
 
-def test_convert_time_zone_fixed_offset() -> None:
-    ts = pl.Series(["2020-01-01"]).str.strptime(pl.Datetime)
-    result = ts.dt.replace_time_zone("+00:00")
-    assert result.dtype == pl.Datetime("us", "+00:00")
-    assert result.item() == datetime(2020, 1, 1, 0, 0, tzinfo=timezone.utc)
-
-
 def test_tz_aware_get_idx_5010() -> None:
     when = int(datetime(2022, 1, 1, 12, tzinfo=ZoneInfo("Asia/Shanghai")).timestamp())
     a = pa.array([when]).cast(pa.timestamp("s", tz="Asia/Shanghai"))
     assert int(pl.from_arrow(a)[0].timestamp()) == when  # type: ignore[union-attr]
 
 
 def test_tz_datetime_duration_arithm_5221() -> None:
@@ -2046,141 +2011,20 @@
             datetime(2022, 1, 3, 0, 0, tzinfo=utc),
         ]
     }
 
 
 def test_auto_infer_time_zone() -> None:
     dt = datetime(2022, 10, 17, 10, tzinfo=ZoneInfo("Asia/Shanghai"))
-    s = pl.Series([dt])
-    assert s.dtype == pl.Datetime("us", "Asia/Shanghai")
-    assert s[0] == dt
-
-
-def test_timezone_aware_date_range() -> None:
-    low = datetime(2022, 10, 17, 10, tzinfo=ZoneInfo("Asia/Shanghai"))
-    high = datetime(2022, 11, 17, 10, tzinfo=ZoneInfo("Asia/Shanghai"))
-
-    assert pl.date_range(low, high, interval=timedelta(days=5)).to_list() == [
-        datetime(2022, 10, 17, 10, 0, tzinfo=ZoneInfo(key="Asia/Shanghai")),
-        datetime(2022, 10, 22, 10, 0, tzinfo=ZoneInfo(key="Asia/Shanghai")),
-        datetime(2022, 10, 27, 10, 0, tzinfo=ZoneInfo(key="Asia/Shanghai")),
-        datetime(2022, 11, 1, 10, 0, tzinfo=ZoneInfo(key="Asia/Shanghai")),
-        datetime(2022, 11, 6, 10, 0, tzinfo=ZoneInfo(key="Asia/Shanghai")),
-        datetime(2022, 11, 11, 10, 0, tzinfo=ZoneInfo(key="Asia/Shanghai")),
-        datetime(2022, 11, 16, 10, 0, tzinfo=ZoneInfo(key="Asia/Shanghai")),
-    ]
-
-    with pytest.raises(
-        ValueError,
-        match="Cannot mix different timezone aware datetimes. "
-        "Got: 'Asia/Shanghai' and 'None'",
-    ):
-        pl.date_range(
-            low, high.replace(tzinfo=None), interval=timedelta(days=5), time_zone="UTC"
-        )
-
-    with pytest.raises(
-        ValueError,
-        match="Given time_zone is different from that of timezone aware datetimes. "
-        "Given: 'UTC', got: 'Asia/Shanghai'.",
+    with pytest.warns(
+        TimeZoneAwareConstructorWarning, match="Series with UTC time zone"
     ):
-        pl.date_range(low, high, interval=timedelta(days=5), time_zone="UTC")
-
-
-def test_tzaware_date_range_crossing_dst_hourly() -> None:
-    result = pl.date_range(
-        datetime(2021, 11, 7), datetime(2021, 11, 7, 2), "1h", time_zone="US/Central"
-    )
-    assert result.to_list() == [
-        datetime(2021, 11, 7, 0, 0, tzinfo=ZoneInfo("US/Central")),
-        datetime(2021, 11, 7, 1, 0, tzinfo=ZoneInfo("US/Central")),
-        datetime(2021, 11, 7, 1, 0, fold=1, tzinfo=ZoneInfo("US/Central")),
-        datetime(2021, 11, 7, 2, 0, tzinfo=ZoneInfo("US/Central")),
-    ]
-
-
-def test_tzaware_date_range_crossing_dst_daily() -> None:
-    result = pl.date_range(
-        datetime(2021, 11, 7), datetime(2021, 11, 11), "2d", time_zone="US/Central"
-    )
-    assert result.to_list() == [
-        datetime(2021, 11, 7, 0, 0, tzinfo=ZoneInfo("US/Central")),
-        datetime(2021, 11, 9, 0, 0, tzinfo=ZoneInfo("US/Central")),
-        datetime(2021, 11, 11, 0, 0, tzinfo=ZoneInfo("US/Central")),
-    ]
-
-
-def test_tzaware_date_range_crossing_dst_weekly() -> None:
-    result = pl.date_range(
-        datetime(2021, 11, 7), datetime(2021, 11, 20), "1w", time_zone="US/Central"
-    )
-    assert result.to_list() == [
-        datetime(2021, 11, 7, 0, 0, tzinfo=ZoneInfo("US/Central")),
-        datetime(2021, 11, 14, 0, 0, tzinfo=ZoneInfo("US/Central")),
-    ]
-
-
-def test_tzaware_date_range_crossing_dst_monthly() -> None:
-    result = pl.date_range(
-        datetime(2021, 11, 7), datetime(2021, 12, 20), "1mo", time_zone="US/Central"
-    )
-    assert result.to_list() == [
-        datetime(2021, 11, 7, 0, 0, tzinfo=ZoneInfo("US/Central")),
-        datetime(2021, 12, 7, 0, 0, tzinfo=ZoneInfo("US/Central")),
-    ]
-
-
-def test_tzaware_date_range_with_fixed_offset() -> None:
-    result = pl.date_range(
-        datetime(2021, 11, 7), datetime(2021, 11, 7, 2), "1h", time_zone="+01:00"
-    )
-    assert result.to_list() == [
-        datetime(2021, 11, 7, 0, 0, tzinfo=timezone(timedelta(hours=1))),
-        datetime(2021, 11, 7, 1, 0, tzinfo=timezone(timedelta(hours=1))),
-        datetime(2021, 11, 7, 2, 0, tzinfo=timezone(timedelta(hours=1))),
-    ]
-
-
-def test_date_range_with_unsupported_datetimes() -> None:
-    with pytest.raises(
-        ComputeError,
-        match=r"datetime '2021-11-07 01:00:00' is ambiguous in time zone 'US/Central'",
-    ):
-        pl.date_range(
-            datetime(2021, 11, 7, 1),
-            datetime(2021, 11, 7, 2),
-            "1h",
-            time_zone="US/Central",
-        )
-    with pytest.raises(
-        ComputeError,
-        match=r"datetime '2021-03-28 02:30:00' is non-existent in time zone 'Europe/Vienna'",
-    ):
-        pl.date_range(
-            datetime(2021, 3, 28, 2, 30),
-            datetime(2021, 3, 28, 4),
-            "1h",
-            time_zone="Europe/Vienna",
-        )
-
-
-def test_date_range_descending() -> None:
-    with pytest.raises(ComputeError, match="'start' cannot be greater than 'stop'"):
-        pl.date_range(datetime(2000, 3, 20), datetime(2000, 3, 5), interval="1h")
-    with pytest.raises(ComputeError, match="'interval' cannot be negative"):
-        pl.date_range(datetime(2000, 3, 20), datetime(2000, 3, 21), interval="-1h")
-
-
-def test_date_range_end_of_month_5441() -> None:
-    start = date(2020, 1, 31)
-    stop = date(2021, 1, 31)
-    with pytest.raises(
-        ComputeError, match=r"cannot advance '2020-01-31 00:00:00' by 1 month\(s\)"
-    ):
-        pl.date_range(start, stop, interval="1mo")
+        s = pl.Series([dt])
+    assert s.dtype == pl.Datetime("us", "UTC")
+    assert s[0] == dt
 
 
 def test_logical_nested_take() -> None:
     frame = pl.DataFrame(
         {
             "ix": [2, 1],
             "dt": [[datetime(2001, 1, 1)], [datetime(2001, 1, 2)]],
@@ -2268,15 +2112,18 @@
     assert result == datetime(2020, 1, 1, 4)
 
 
 def test_tz_aware_truncate() -> None:
     test = pl.DataFrame(
         {
             "dt": pl.date_range(
-                start=datetime(2022, 11, 1), end=datetime(2022, 11, 4), interval="12h"
+                start=datetime(2022, 11, 1),
+                end=datetime(2022, 11, 4),
+                interval="12h",
+                eager=True,
             ).dt.replace_time_zone("America/New_York")
         }
     )
     assert test.with_columns(pl.col("dt").dt.truncate("1d").alias("trunced")).to_dict(
         False
     ) == {
         "dt": [
@@ -2302,14 +2149,15 @@
     # 5507
     lf = pl.DataFrame(
         {
             "naive": pl.date_range(
                 start=datetime(2021, 12, 31, 23),
                 end=datetime(2022, 1, 1, 6),
                 interval="1h",
+                eager=True,
             )
         }
     ).lazy()
     lf = lf.with_columns(pl.col("naive").dt.replace_time_zone("UTC").alias("UTC"))
     lf = lf.with_columns(pl.col("UTC").dt.convert_time_zone("US/Central").alias("CST"))
     lf = lf.with_columns(pl.col("CST").dt.truncate("1d").alias("CST truncated"))
     assert lf.collect().to_dict(False) == {
@@ -2352,33 +2200,36 @@
             datetime(2021, 12, 31, 0, 0, tzinfo=ZoneInfo(key="US/Central")),
             datetime(2021, 12, 31, 0, 0, tzinfo=ZoneInfo(key="US/Central")),
             datetime(2022, 1, 1, 0, 0, tzinfo=ZoneInfo(key="US/Central")),
         ],
     }
 
 
-def test_strftime_invalid_format() -> None:
+def test_to_string_invalid_format() -> None:
     tz_naive = pl.Series(["2020-01-01"]).str.strptime(pl.Datetime)
     with pytest.raises(
         ComputeError, match="cannot format NaiveDateTime with format '%z'"
     ):
-        tz_naive.dt.strftime("%z")
+        tz_naive.dt.to_string("%z")
     with pytest.raises(ComputeError, match="cannot format DateTime with format '%q'"):
-        tz_naive.dt.replace_time_zone("UTC").dt.strftime("%q")
+        tz_naive.dt.replace_time_zone("UTC").dt.to_string("%q")
 
 
-def test_tz_aware_strftime() -> None:
+def test_tz_aware_to_string() -> None:
     df = pl.DataFrame(
         {
             "dt": pl.date_range(
-                start=datetime(2022, 11, 1), end=datetime(2022, 11, 4), interval="24h"
+                start=datetime(2022, 11, 1),
+                end=datetime(2022, 11, 4),
+                interval="24h",
+                eager=True,
             ).dt.replace_time_zone("America/New_York")
         }
     )
-    assert df.with_columns(pl.col("dt").dt.strftime("%c").alias("fmt")).to_dict(
+    assert df.with_columns(pl.col("dt").dt.to_string("%c").alias("fmt")).to_dict(
         False
     ) == {
         "dt": [
             datetime(2022, 11, 1, 0, 0, tzinfo=ZoneInfo(key="America/New_York")),
             datetime(2022, 11, 2, 0, 0, tzinfo=ZoneInfo(key="America/New_York")),
             datetime(2022, 11, 3, 0, 0, tzinfo=ZoneInfo(key="America/New_York")),
             datetime(2022, 11, 4, 0, 0, tzinfo=ZoneInfo(key="America/New_York")),
@@ -2393,24 +2244,22 @@
 
 
 @pytest.mark.parametrize(
     ("time_zone", "directive", "expected"),
     [
         ("Pacific/Pohnpei", "%z", "+1100"),
         ("Pacific/Pohnpei", "%Z", "+11"),
-        ("+01:00", "%z", "+0100"),
-        ("+01:00", "%Z", "+01:00"),
     ],
 )
 def test_tz_aware_with_timezone_directive(
     time_zone: str, directive: str, expected: str
 ) -> None:
     tz_naive = pl.Series(["2020-01-01 03:00:00"]).str.strptime(pl.Datetime)
     tz_aware = tz_naive.dt.replace_time_zone(time_zone)
-    result = tz_aware.dt.strftime(directive).item()
+    result = tz_aware.dt.to_string(directive).item()
     assert result == expected
 
 
 def test_local_time_zone_name() -> None:
     ser = pl.Series(["2020-01-01 03:00ACST"]).str.strptime(
         pl.Datetime, "%Y-%m-%d %H:%M%Z"
     )
@@ -2421,15 +2270,15 @@
 
 def test_tz_aware_filter_lit() -> None:
     start = datetime(1970, 1, 1)
     stop = datetime(1970, 1, 1, 7)
     dt = datetime(1970, 1, 1, 6, tzinfo=ZoneInfo("America/New_York"))
 
     assert (
-        pl.DataFrame({"date": pl.date_range(start, stop, "1h")})
+        pl.DataFrame({"date": pl.date_range(start, stop, "1h", eager=True)})
         .with_columns(
             pl.col("date").dt.replace_time_zone("America/New_York").alias("nyc")
         )
         .filter(pl.col("nyc") < dt)
     ).to_dict(False) == {
         "date": [
             datetime(1970, 1, 1, 0, 0),
@@ -2472,15 +2321,16 @@
 
 def test_truncate_by_calendar_weeks() -> None:
     # 5557
     start = datetime(2022, 11, 14, 0, 0, 0)
     end = datetime(2022, 11, 20, 0, 0, 0)
 
     assert (
-        pl.date_range(start, end, timedelta(days=1), name="date")
+        pl.date_range(start, end, timedelta(days=1), eager=True)
+        .alias("date")
         .to_frame()
         .select([pl.col("date").dt.truncate("1w")])
     ).to_dict(False) == {
         "date": [
             datetime(2022, 11, 14),
             datetime(2022, 11, 14),
             datetime(2022, 11, 14),
@@ -2561,15 +2411,15 @@
         )
     ).to_dict(False) == {
         "7d": [date(1998, 4, 9), date(2022, 12, 1)],
         "1w": [date(1998, 4, 13), date(2022, 11, 28)],
     }
 
 
-@pytest.mark.parametrize("time_zone", [None, "Asia/Kathmandu", "+01:00"])
+@pytest.mark.parametrize("time_zone", [None, "Asia/Kathmandu"])
 def test_round_by_day_datetime(time_zone: str | None) -> None:
     ser = pl.Series([datetime(2021, 11, 7, 3)]).dt.replace_time_zone(time_zone)
     result = ser.dt.round("1d")
     expected = pl.Series([datetime(2021, 11, 7)]).dt.replace_time_zone(time_zone)
     assert_series_equal(result, expected)
 
 
@@ -2579,15 +2429,19 @@
     ).item() == timedelta(seconds=2)
 
 
 def test_tz_aware_day_weekday() -> None:
     start = datetime(2001, 1, 1)
     stop = datetime(2001, 1, 9)
     df = pl.DataFrame(
-        {"date": pl.date_range(start, stop, timedelta(days=3), time_zone="UTC")}
+        {
+            "date": pl.date_range(
+                start, stop, timedelta(days=3), time_zone="UTC", eager=True
+            )
+        }
     )
 
     df = df.with_columns(
         [
             pl.col("date").dt.convert_time_zone("Asia/Tokyo").alias("tyo_date"),
             pl.col("date").dt.convert_time_zone("America/New_York").alias("ny_date"),
         ]
@@ -2728,15 +2582,15 @@
 
 def test_infer_iso8601_tz_aware_datetime(iso8601_tz_aware_format_datetime: str) -> None:
     # construct an example time string
     time_string = (
         iso8601_tz_aware_format_datetime.replace("%Y", "2134")
         .replace("%m", "12")
         .replace("%d", "13")
-        .replace("%H", "01")
+        .replace("%H", "02")
         .replace("%M", "12")
         .replace("%S", "34")
         .replace("%3f", "123")
         .replace("%6f", "123456")
         .replace("%9f", "123456789")
         .replace("%#z", "+01:00")
     )
@@ -2752,15 +2606,15 @@
         assert parsed.dt.second().item() == 34
     if "%9f" in iso8601_tz_aware_format_datetime:
         assert parsed.dt.nanosecond().item() == 123456789
     if "%6f" in iso8601_tz_aware_format_datetime:
         assert parsed.dt.nanosecond().item() == 123456000
     if "%3f" in iso8601_tz_aware_format_datetime:
         assert parsed.dt.nanosecond().item() == 123000000
-    assert parsed.dtype == pl.Datetime("ns", "+01:00")
+    assert parsed.dtype == pl.Datetime("ns", "UTC")
 
 
 def test_infer_iso8601_date(iso8601_format_date: str) -> None:
     # construct an example date string
     time_string = (
         iso8601_format_date.replace("%Y", "2134")
         .replace("%m", "12")
@@ -2782,46 +2636,55 @@
 
     s = pl.Series([datetime(2023, 2, 14, 11, 12, 13)], dtype=pl.Datetime)
     for tp in (pl.Datetime, [pl.Datetime], [pl.Time, pl.Datetime]):  # type: ignore[assignment]
         assert s.is_temporal(excluding=tp) is False
 
 
 @pytest.mark.parametrize(
-    "time_zone",
+    ("time_zone", "warn"),
     [
-        None,
-        timezone.utc,
-        "America/Caracas",
-        "Asia/Kathmandu",
-        "Asia/Taipei",
-        "Europe/Amsterdam",
-        "Europe/Lisbon",
-        "Indian/Maldives",
-        "Pacific/Norfolk",
-        "Pacific/Samoa",
-        "Turkey",
-        "US/Eastern",
-        "UTC",
-        "Zulu",
+        (None, False),
+        (timezone.utc, False),
+        ("America/Caracas", True),
+        ("Asia/Kathmandu", True),
+        ("Asia/Taipei", True),
+        ("Europe/Amsterdam", True),
+        ("Europe/Lisbon", True),
+        ("Indian/Maldives", True),
+        ("Pacific/Norfolk", True),
+        ("Pacific/Samoa", True),
+        ("Turkey", True),
+        ("US/Eastern", True),
+        ("UTC", False),
+        ("Zulu", True),
     ],
 )
-def test_misc_precision_any_value_conversion(time_zone: Any) -> None:
-    tz = ZoneInfo(time_zone) if isinstance(time_zone, str) else time_zone
+def test_misc_precision_any_value_conversion(time_zone: Any, warn: bool) -> None:
+    context_manager: contextlib.AbstractContextManager[pytest.WarningsRecorder | None]
+    msg = r"UTC time zone"
+    if warn:
+        context_manager = pytest.warns(TimeZoneAwareConstructorWarning, match=msg)
+    else:
+        context_manager = contextlib.nullcontext()
 
+    tz = ZoneInfo(time_zone) if isinstance(time_zone, str) else time_zone
     # default precision (s)
     dt = datetime(2514, 5, 30, 1, 53, 4, 986754, tzinfo=tz)
-    assert pl.Series([dt]).to_list() == [dt]
+    with context_manager:
+        assert pl.Series([dt]).to_list() == [dt]
 
     # ms precision
     dt = datetime(2243, 1, 1, 0, 0, 0, 1000, tzinfo=tz)
-    assert pl.Series([dt]).cast(pl.Datetime("ms", time_zone)).to_list() == [dt]
+    with context_manager:
+        assert pl.Series([dt]).cast(pl.Datetime("ms", time_zone)).to_list() == [dt]
 
     # ns precision
     dt = datetime(2256, 1, 1, 0, 0, 0, 1, tzinfo=tz)
-    assert pl.Series([dt]).cast(pl.Datetime("ns", time_zone)).to_list() == [dt]
+    with context_manager:
+        assert pl.Series([dt]).cast(pl.Datetime("ns", time_zone)).to_list() == [dt]
 
 
 @pytest.mark.parametrize(
     "tm",
     [
         time(0, 20, 30, 1),
         time(8, 40, 15, 8888),
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/files/delta-table/_delta_log/00000000000000000000.json` & `polars_lts_cpu-0.18.0/tests/unit/io/files/delta-table/_delta_log/00000000000000000000.json`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/files/delta-table/_delta_log/00000000000000000001.json` & `polars_lts_cpu-0.18.0/tests/unit/io/files/delta-table/_delta_log/00000000000000000001.json`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/files/delta-table/part-00000-e42312d7-60e5-454d-acbc-db192d220e73-c000.snappy.parquet` & `polars_lts_cpu-0.18.0/tests/unit/io/files/delta-table/part-00000-e42312d7-60e5-454d-acbc-db192d220e73-c000.snappy.parquet`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/files/delta-table/part-00000-e4a999da-df45-4fb0-bdc4-d999fc0f58aa-c000.snappy.parquet` & `polars_lts_cpu-0.18.0/tests/unit/io/files/delta-table/part-00000-e4a999da-df45-4fb0-bdc4-d999fc0f58aa-c000.snappy.parquet`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/files/example.xlsx` & `polars_lts_cpu-0.18.0/tests/unit/io/files/example.xlsx`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/files/foods1.ipc` & `polars_lts_cpu-0.18.0/tests/unit/io/files/foods1.ipc`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/files/foods1.ndjson` & `polars_lts_cpu-0.18.0/tests/unit/io/files/foods1.ndjson`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/files/foods1.parquet` & `polars_lts_cpu-0.18.0/tests/unit/io/files/foods1.parquet`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/files/foods2.ipc` & `polars_lts_cpu-0.18.0/tests/unit/io/files/foods2.ipc`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/files/foods2.ndjson` & `polars_lts_cpu-0.18.0/tests/unit/io/files/foods2.ndjson`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/files/foods2.parquet` & `polars_lts_cpu-0.18.0/tests/unit/io/files/foods2.parquet`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/files/small.parquet` & `polars_lts_cpu-0.18.0/tests/unit/io/files/small.parquet`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/test_avro.py` & `polars_lts_cpu-0.18.0/tests/unit/io/test_avro.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/test_csv.py` & `polars_lts_cpu-0.18.0/tests/unit/io/test_csv.py`

 * *Files 1% similar despite different names*

```diff
@@ -743,34 +743,35 @@
     2021-10-10,2021-10-10
     """
     )
     df = pl.read_csv(data.encode(), try_parse_dates=True)
     assert df.null_count().row(0) == (0, 0)
 
 
-@pytest.mark.xfail(reason="Not yet supported, GH8213", strict=True)
 def test_tz_aware_try_parse_dates() -> None:
     data = (
         "a,b,c,d\n"
-        "2020-01-01T01:00:00+01:00,2021-04-28T00:00:00+02:00,2021-03-28T00:00:00+01:00,2\n"
-        "2020-01-01T02:00:00+01:00,2021-04-29T00:00:00+02:00,2021-03-29T00:00:00+02:00,3\n"
+        "2020-01-01T02:00:00+01:00,2021-04-28T00:00:00+02:00,2021-03-28T00:00:00+01:00,2\n"
+        "2020-01-01T03:00:00+01:00,2021-04-29T00:00:00+02:00,2021-03-29T00:00:00+02:00,3\n"
     )
     result = pl.read_csv(io.StringIO(data), try_parse_dates=True)
     expected = pl.DataFrame(
         {
             "a": [
-                datetime(2020, 1, 1, 1, tzinfo=timezone(timedelta(hours=1))),
-                datetime(2020, 1, 1, 2, tzinfo=timezone(timedelta(hours=1))),
+                datetime(2020, 1, 1, 1, tzinfo=timezone.utc),
+                datetime(2020, 1, 1, 2, tzinfo=timezone.utc),
             ],
             "b": [
-                datetime(2021, 4, 28, tzinfo=timezone(timedelta(hours=2))),
-                datetime(2021, 4, 29, tzinfo=timezone(timedelta(hours=2))),
+                datetime(2021, 4, 27, 22, tzinfo=timezone.utc),
+                datetime(2021, 4, 28, 22, tzinfo=timezone.utc),
+            ],
+            "c": [
+                datetime(2021, 3, 27, 23, tzinfo=timezone.utc),
+                datetime(2021, 3, 28, 22, tzinfo=timezone.utc),
             ],
-            # column 'c' has mixed offsets, so `try_parse_dates`  can't parse it
-            "c": ["2021-03-28T00:00:00+01:00", "2021-03-29T00:00:00+02:00"],
             "d": [2, 3],
         }
     )
     assert_frame_equal(result, expected)
 
 
 def test_csv_string_escaping() -> None:
@@ -991,15 +992,15 @@
             ("y", pl.Datetime(tu2)),
         ],
     )
     assert expected == df.write_csv()
 
 
 def test_inferred_datetime_format_mixed() -> None:
-    ts = pl.date_range(datetime(2000, 1, 1), datetime(2000, 1, 2))
+    ts = pl.date_range(datetime(2000, 1, 1), datetime(2000, 1, 2), eager=True)
     df = pl.DataFrame({"naive": ts, "aware": ts.dt.replace_time_zone("UTC")})
     result = df.write_csv()
     expected = (
         "naive,aware\n"
         "2000-01-01T00:00:00.000000,2000-01-01T00:00:00.000000+0000\n"
         "2000-01-02T00:00:00.000000,2000-01-02T00:00:00.000000+0000\n"
     )
@@ -1101,15 +1102,15 @@
     df = pl.read_csv(csv.encode(), dtypes={"a": pl.Categorical, "b": pl.Categorical})
     assert df.dtypes == [pl.Categorical, pl.Categorical]
     assert df.to_dict(False) == {
         "a": ["needs_escape", ' "needs escape foo', ' "needs escape foo'],
         "b": ["b", "b", None],
     }
 
-    assert (df["a"] == df["b"]).to_list() == [False, False, False]
+    assert (df["a"] == df["b"]).to_list() == [False, False, None]
 
 
 def test_csv_categorical_categorical_merge() -> None:
     N = 50
     f = io.BytesIO()
     pl.DataFrame({"x": ["A"] * N + ["B"] * N}).write_csv(f)
     f.seek(0)
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/test_database.py` & `polars_lts_cpu-0.18.0/tests/unit/io/test_database.py`

 * *Files 2% similar despite different names*

```diff
@@ -21,31 +21,39 @@
 
 @pytest.fixture()
 def sample_df() -> pl.DataFrame:
     return pl.DataFrame(
         {
             "id": [1, 2],
             "name": ["misc", "other"],
-            "value": [100.0, -99.5],
+            "value": [100.0, -99.0],
             "date": ["2020-01-01", "2021-12-31"],
         }
     )
 
 
 def create_temp_sqlite_db(test_db: str) -> None:
     import sqlite3
 
+    if os.path.exists(test_db):
+        os.unlink(test_db)
+
+    # NOTE: at the time of writing adcb/connectorx have weak SQLite support (poor or
+    # no bool/date/datetime dtypes, for example) and there is a bug in connectorx that
+    # causes float rounding < py 3.11, hence we are only testing/storing simple values
+    # in this test db for now. as support improves, we can add/test additional dtypes).
+
     conn = sqlite3.connect(test_db)
     # 
     #  id   name   value  date       
     #  ---  ---    ---    ---        
     #  i64  str    f64    date       
     # 
     #  1    misc   100.0  2020-01-01 
-    #  2    other  -99.5  2021-12-31 
+    #  2    other  -99.0  2021-12-31 
     # 
     conn.executescript(
         """
         CREATE TABLE test_data (
             id    INTEGER PRIMARY KEY,
             name  TEXT NOT NULL,
             value FLOAT,
@@ -196,12 +204,9 @@
                 if_exists="append",
                 engine=engine,
             )
             sample_df = pl.concat([sample_df, sample_df])
 
         result = pl.read_database("SELECT * FROM test_data", f"sqlite:///{test_db}")
 
-    # TODO: Fix this bug! Floats shouldn't be rounded
-    sample_df = sample_df.with_columns(pl.col("value").ceil())
-
     sample_df = sample_df.with_columns(pl.col("date").cast(pl.Utf8))
     assert_frame_equal(sample_df, result)
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/test_excel.py` & `polars_lts_cpu-0.18.0/tests/unit/io/test_excel.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/test_ipc.py` & `polars_lts_cpu-0.18.0/tests/unit/io/test_ipc.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/test_json.py` & `polars_lts_cpu-0.18.0/tests/unit/io/test_json.py`

 * *Files 9% similar despite different names*

```diff
@@ -122,7 +122,16 @@
 {"c":[{"b": [], "a": "1"}]},
 {"c":[{"b":[]}]},
 {"c":[{"b":["1"], "a": "1"}]}]
 """
     python_infer = pl.from_records(json.loads(json_string))
     polars_infer = pl.read_json(io.StringIO(json_string))
     assert_frame_equal(python_infer, polars_infer)
+
+
+def test_json_sliced_list_serialization() -> None:
+    data = {"col1": [0, 2], "col2": [[3, 4, 5], [6, 7, 8]]}
+    df = pl.DataFrame(data)
+    f = io.BytesIO()
+    sliced_df = df[1, :]
+    sliced_df.write_ndjson(f)
+    assert f.getvalue() == b'{"col1":2,"col2":[6,7,8]}\n'
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/test_lazy_csv.py` & `polars_lts_cpu-0.18.0/tests/unit/io/test_lazy_csv.py`

 * *Files 4% similar despite different names*

```diff
@@ -17,14 +17,19 @@
 
 
 def test_scan_csv(io_files_path: Path) -> None:
     df = pl.scan_csv(io_files_path / "small.csv")
     assert df.collect().shape == (4, 3)
 
 
+def test_scan_csv_no_cse_deadlock(io_files_path: Path) -> None:
+    dfs = [pl.scan_csv(io_files_path / "small.csv")] * (pl.threadpool_size() + 1)
+    pl.concat(dfs, parallel=True).collect(common_subplan_elimination=False)
+
+
 def test_scan_empty_csv(io_files_path: Path) -> None:
     with pytest.raises(Exception) as excinfo:
         pl.scan_csv(io_files_path / "empty.csv").collect()
     assert "empty csv" in str(excinfo.value)
 
 
 @pytest.mark.write_disk()
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/test_lazy_ipc.py` & `polars_lts_cpu-0.18.0/tests/unit/io/test_lazy_ipc.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/test_lazy_json.py` & `polars_lts_cpu-0.18.0/tests/unit/io/test_lazy_json.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/test_lazy_parquet.py` & `polars_lts_cpu-0.18.0/tests/unit/io/test_lazy_parquet.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/test_other.py` & `polars_lts_cpu-0.18.0/tests/unit/io/test_other.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/test_parquet.py` & `polars_lts_cpu-0.18.0/tests/unit/io/test_parquet.py`

 * *Files 2% similar despite different names*

```diff
@@ -459,7 +459,19 @@
         }
     )
 
     df.write_parquet(f)
     f.seek(0)
     df_read = pl.read_parquet(f)
     assert_frame_equal(df_read, df)
+
+
+@typing.no_type_check
+def test_parquet_nested_list_pandas() -> None:
+    # pandas/pyarrow writes as nested null dict
+    df = pd.DataFrame({"listcol": [[] * 10]})
+    f = io.BytesIO()
+    df.to_parquet(f)
+    f.seek(0)
+    df = pl.read_parquet(f)
+    assert df.dtypes == [pl.List(pl.Null)]
+    assert df.to_dict(False) == {"listcol": [[]]}
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/test_pickle.py` & `polars_lts_cpu-0.18.0/tests/unit/io/test_pickle.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/io/test_pyarrow_dataset.py` & `polars_lts_cpu-0.18.0/tests/unit/io/test_pyarrow_dataset.py`

 * *Files 16% similar despite different names*

```diff
@@ -94,7 +94,27 @@
         # not yet supported in pyarrow
         helper_dataset_test(
             file_path,
             lambda lf: lf.filter(pl.col("time") >= time(microsecond=100))
             .select(["bools", "time", "date"])
             .collect(),
         )
+
+        # pushdown is_in
+        helper_dataset_test(
+            file_path,
+            lambda lf: lf.filter(pl.col("int").is_in([1, 3, 20]))
+            .select(["bools", "floats", "date"])
+            .collect(),
+        )
+        helper_dataset_test(
+            file_path,
+            lambda lf: lf.filter(pl.col("int").is_in(list(range(120))))
+            .select(["bools", "floats", "date"])
+            .collect(),
+        )
+        helper_dataset_test(
+            file_path,
+            lambda lf: lf.filter(pl.col("cat").is_in([]))
+            .select(["bools", "floats", "date"])
+            .collect(),
+        )
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/namespaces/test_binary.py` & `polars_lts_cpu-0.18.0/tests/unit/namespaces/test_binary.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/namespaces/test_categorical.py` & `polars_lts_cpu-0.18.0/tests/unit/namespaces/test_categorical.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/namespaces/test_datetime.py` & `polars_lts_cpu-0.18.0/tests/unit/namespaces/test_datetime.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,47 +1,49 @@
 from __future__ import annotations
 
-from datetime import date, datetime, time, timedelta, timezone
+import sys
+from datetime import date, datetime, time, timedelta
 from typing import TYPE_CHECKING
 
 import pytest
 
 import polars as pl
 from polars.datatypes import DTYPE_TEMPORAL_UNITS
+from polars.dependencies import _ZONEINFO_AVAILABLE
 from polars.exceptions import ComputeError, InvalidOperationError
 from polars.testing import assert_series_equal
 
-if TYPE_CHECKING:
-    from polars.type_aliases import TimeUnit
-import sys
-
-from polars.dependencies import _ZONEINFO_AVAILABLE
-
 if sys.version_info >= (3, 9):
     from zoneinfo import ZoneInfo
 elif _ZONEINFO_AVAILABLE:
     # Import from submodule due to typing issue with backports.zoneinfo package:
     # https://github.com/pganssle/zoneinfo/issues/125
     from backports.zoneinfo._zoneinfo import ZoneInfo
 
+if TYPE_CHECKING:
+    from polars.type_aliases import TimeUnit
+
 
 @pytest.fixture()
 def series_of_int_dates() -> pl.Series:
     return pl.Series([10000, 20000, 30000], dtype=pl.Date)
 
 
 @pytest.fixture()
 def series_of_str_dates() -> pl.Series:
     return pl.Series(["2020-01-01 00:00:00.000000000", "2020-02-02 03:20:10.987654321"])
 
 
-def test_dt_strftime(series_of_int_dates: pl.Series) -> None:
+def test_dt_to_string(series_of_int_dates: pl.Series) -> None:
     expected_str_dates = pl.Series(["1997-05-19", "2024-10-04", "2052-02-20"])
 
     assert series_of_int_dates.dtype == pl.Date
+    assert_series_equal(series_of_int_dates.dt.to_string("%F"), expected_str_dates)
+
+    # Check strftime alias as well
     assert_series_equal(series_of_int_dates.dt.strftime("%F"), expected_str_dates)
 
 
 @pytest.mark.parametrize(
     ("unit_attr", "expected"),
     [
         ("year", pl.Series(values=[1997, 2024, 2052], dtype=pl.Int32)),
@@ -77,15 +79,15 @@
     series_of_str_dates: pl.Series,
 ) -> None:
     s = series_of_str_dates.str.strptime(pl.Datetime, format="%Y-%m-%d %H:%M:%S.%9f")
 
     assert_series_equal(getattr(s.dt, unit_attr)(), expected)
 
 
-@pytest.mark.parametrize("time_zone", [None, "Asia/Kathmandu", "+03:00"])
+@pytest.mark.parametrize("time_zone", [None, "Asia/Kathmandu"])
 @pytest.mark.parametrize(
     ("attribute", "expected"),
     [
         ("date", date(2022, 1, 1)),
         ("time", time(23)),
     ],
 )
@@ -93,15 +95,15 @@
     attribute: str, time_zone: None | str, expected: date | time
 ) -> None:
     ser = pl.Series([datetime(2022, 1, 1, 23)]).dt.replace_time_zone(time_zone)
     result = getattr(ser.dt, attribute)().item()
     assert result == expected
 
 
-@pytest.mark.parametrize("time_zone", [None, "Asia/Kathmandu", "+03:00"])
+@pytest.mark.parametrize("time_zone", [None, "Asia/Kathmandu"])
 @pytest.mark.parametrize("time_unit", ["us", "ns", "ms"])
 def test_dt_datetime(time_zone: str | None, time_unit: TimeUnit) -> None:
     ser = (
         pl.Series([datetime(2022, 1, 1, 23)])
         .dt.cast_time_unit(time_unit)
         .dt.replace_time_zone(time_zone)
     )
@@ -132,24 +134,29 @@
         (datetime(2022, 3, 15, 3), datetime(2022, 3, 1, 3)),
         (datetime(2022, 3, 15, 3, 2, 1, 123000), datetime(2022, 3, 1, 3, 2, 1, 123000)),
         (datetime(2022, 3, 15), datetime(2022, 3, 1)),
         (datetime(2022, 3, 1), datetime(2022, 3, 1)),
     ],
 )
 @pytest.mark.parametrize(
-    "tzinfo", [None, ZoneInfo("Asia/Kathmandu"), timezone(timedelta(hours=1))]
+    ("tzinfo", "time_zone"),
+    [
+        (None, None),
+        (ZoneInfo("Asia/Kathmandu"), "Asia/Kathmandu"),
+    ],
 )
 @pytest.mark.parametrize("time_unit", ["ms", "us", "ns"])
 def test_month_start_datetime(
     dt: datetime,
     expected: datetime,
     time_unit: TimeUnit,
-    tzinfo: ZoneInfo | timezone | None,
+    tzinfo: ZoneInfo | None,
+    time_zone: str | None,
 ) -> None:
-    ser = pl.Series([dt.replace(tzinfo=tzinfo)]).dt.cast_time_unit(time_unit)
+    ser = pl.Series([dt]).dt.replace_time_zone(time_zone).dt.cast_time_unit(time_unit)
     result = ser.dt.month_start().item()
     assert result == expected.replace(tzinfo=tzinfo)
 
 
 @pytest.mark.parametrize(
     ("dt", "expected"),
     [
@@ -172,24 +179,29 @@
             datetime(2022, 3, 31, 3, 2, 1, 123000),
         ),
         (datetime(2022, 3, 15), datetime(2022, 3, 31)),
         (datetime(2022, 3, 31), datetime(2022, 3, 31)),
     ],
 )
 @pytest.mark.parametrize(
-    "tzinfo", [None, ZoneInfo("Asia/Kathmandu"), timezone(timedelta(hours=1))]
+    ("tzinfo", "time_zone"),
+    [
+        (None, None),
+        (ZoneInfo("Asia/Kathmandu"), "Asia/Kathmandu"),
+    ],
 )
 @pytest.mark.parametrize("time_unit", ["ms", "us", "ns"])
 def test_month_end_datetime(
     dt: datetime,
     expected: datetime,
     time_unit: TimeUnit,
-    tzinfo: ZoneInfo | timezone | None,
+    tzinfo: ZoneInfo | None,
+    time_zone: str | None,
 ) -> None:
-    ser = pl.Series([dt.replace(tzinfo=tzinfo)]).dt.cast_time_unit(time_unit)
+    ser = pl.Series([dt]).dt.replace_time_zone(time_zone).dt.cast_time_unit(time_unit)
     result = ser.dt.month_end().item()
     assert result == expected.replace(tzinfo=tzinfo)
 
 
 @pytest.mark.parametrize(
     ("dt", "expected"),
     [
@@ -282,17 +294,17 @@
     every: str | timedelta,
 ) -> None:
     start, stop = datetime(2022, 1, 1), datetime(2022, 1, 2)
     s = pl.date_range(
         start,
         stop,
         timedelta(minutes=30),
-        name=f"dates[{time_unit}]",
         time_unit=time_unit,
-    )
+        eager=True,
+    ).alias(f"dates[{time_unit}]")
 
     # can pass strings and time-deltas
     out = s.dt.truncate(every)
     assert out.dt[0] == start
     assert out.dt[1] == start
     assert out.dt[2] == start + timedelta(hours=1)
     assert out.dt[3] == start + timedelta(hours=1)
@@ -316,17 +328,17 @@
     every: str | timedelta,
 ) -> None:
     start, stop = datetime(2022, 1, 1), datetime(2022, 1, 2)
     s = pl.date_range(
         start,
         stop,
         timedelta(minutes=30),
-        name=f"dates[{time_unit}]",
         time_unit=time_unit,
-    )
+        eager=True,
+    ).alias(f"dates[{time_unit}]")
 
     # can pass strings and time-deltas
     out = s.dt.round(every)
     assert out.dt[0] == start
     assert out.dt[1] == start + timedelta(hours=1)
     assert out.dt[2] == start + timedelta(hours=1)
     assert out.dt[3] == start + timedelta(hours=2)
@@ -364,37 +376,36 @@
     assert_series_equal(
         dates.dt.epoch("d"),
         (dates.dt.timestamp("ms") // (1000 * 3600 * 24)).cast(pl.Int32),
     )
 
 
 @pytest.mark.parametrize(
-    ("tzinfo", "expected_time_zone"),
+    ("tzinfo", "time_zone"),
     [(None, None), (ZoneInfo("Asia/Kathmandu"), "Asia/Kathmandu")],
 )
-def test_date_time_combine(
-    tzinfo: ZoneInfo | None, expected_time_zone: str | None
-) -> None:
+def test_date_time_combine(tzinfo: ZoneInfo | None, time_zone: str | None) -> None:
     # Define a DataFrame with columns for datetime, date, and time
     df = pl.DataFrame(
         {
             "dtm": [
-                datetime(2022, 12, 31, 10, 30, 45, tzinfo=tzinfo),
-                datetime(2023, 7, 5, 23, 59, 59, tzinfo=tzinfo),
+                datetime(2022, 12, 31, 10, 30, 45),
+                datetime(2023, 7, 5, 23, 59, 59),
             ],
             "dt": [
                 date(2022, 10, 10),
                 date(2022, 7, 5),
             ],
             "tm": [
                 time(1, 2, 3, 456000),
                 time(7, 8, 9, 101000),
             ],
         }
     )
+    df = df.with_columns(pl.col("dtm").dt.replace_time_zone(time_zone))
 
     # Combine datetime/date with time
     df = df.select(
         [
             pl.col("dtm").dt.combine(pl.col("tm")).alias("d1"),  # datetime & time
             pl.col("dt").dt.combine(pl.col("tm")).alias("d2"),  # date & time
             pl.col("dt").dt.combine(time(4, 5, 6)).alias("d3"),  # date & specified time
@@ -415,46 +426,40 @@
             datetime(2022, 10, 10, 4, 5, 6),
             datetime(2022, 7, 5, 4, 5, 6),
         ],
     }
     assert df.to_dict(False) == expected_dict
 
     expected_schema = {
-        "d1": pl.Datetime("us", expected_time_zone),
+        "d1": pl.Datetime("us", time_zone),
         "d2": pl.Datetime("us"),
         "d3": pl.Datetime("us"),
     }
     assert df.schema == expected_schema
 
 
 def test_combine_unsupported_types() -> None:
     with pytest.raises(ComputeError, match="expected Date or Datetime, got time"):
         pl.Series([time(1, 2)]).dt.combine(time(3, 4))
 
 
 @pytest.mark.parametrize("time_unit", ["ms", "us", "ns"])
-@pytest.mark.parametrize(
-    ("tzinfo", "expected_time_zone"),
-    [
-        (ZoneInfo("Asia/Kathmandu"), "Asia/Kathmandu"),
-        (None, None),
-    ],
-)
+@pytest.mark.parametrize("time_zone", ["Asia/Kathmandu", None])
 def test_combine_lazy_schema_datetime(
-    tzinfo: ZoneInfo | None,
-    expected_time_zone: str | None,
+    time_zone: str | None,
     time_unit: TimeUnit,
 ) -> None:
-    df = pl.DataFrame({"ts": pl.Series([datetime(2020, 1, 1, tzinfo=tzinfo)])})
+    df = pl.DataFrame({"ts": pl.Series([datetime(2020, 1, 1)])})
+    df = df.with_columns(pl.col("ts").dt.replace_time_zone(time_zone))
     result = (
         df.lazy()
         .select(pl.col("ts").dt.combine(time(1, 2, 3), time_unit=time_unit))
         .dtypes
     )
-    expected = [pl.Datetime(time_unit, expected_time_zone)]
+    expected = [pl.Datetime(time_unit, time_zone)]
     assert result == expected
 
 
 @pytest.mark.parametrize("time_unit", ["ms", "us", "ns"])
 def test_combine_lazy_schema_date(time_unit: TimeUnit) -> None:
     df = pl.DataFrame({"ts": pl.Series([date(2020, 1, 1)])})
     result = (
@@ -464,15 +469,15 @@
     )
     expected = [pl.Datetime(time_unit, None)]
     assert result == expected
 
 
 def test_is_leap_year() -> None:
     assert pl.date_range(
-        datetime(1990, 1, 1), datetime(2004, 1, 1), "1y"
+        datetime(1990, 1, 1), datetime(2004, 1, 1), "1y", eager=True
     ).dt.is_leap_year().to_list() == [
         False,
         False,
         True,  # 1992
         False,
         False,
         False,
@@ -486,21 +491,25 @@
         False,
         True,  # 2004
     ]
 
 
 def test_quarter() -> None:
     assert pl.date_range(
-        datetime(2022, 1, 1), datetime(2022, 12, 1), "1mo"
+        datetime(2022, 1, 1), datetime(2022, 12, 1), "1mo", eager=True
     ).dt.quarter().to_list() == [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4]
 
 
 def test_date_offset() -> None:
     df = pl.DataFrame(
-        {"dates": pl.date_range(datetime(2000, 1, 1), datetime(2020, 1, 1), "1y")}
+        {
+            "dates": pl.date_range(
+                datetime(2000, 1, 1), datetime(2020, 1, 1), "1y", eager=True
+            )
+        }
     )
 
     # Add two new columns to the DataFrame using the offset_by() method
     df = df.with_columns(
         [
             df["dates"].dt.offset_by("1y").alias("date_plus_1y"),
             df["dates"].dt.offset_by("-1y2mo").alias("date_min"),
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/namespaces/test_list.py` & `polars_lts_cpu-0.18.0/tests/unit/test_queries.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,415 +1,421 @@
 from __future__ import annotations
 
-import typing
-from datetime import date, datetime
+from datetime import datetime, timedelta
+from typing import Any
 
 import numpy as np
-import pytest
+import pandas as pd
 
 import polars as pl
-from polars.testing import assert_frame_equal, assert_series_equal
+from polars.testing import assert_frame_equal
 
 
-def test_list_arr_get() -> None:
-    a = pl.Series("a", [[1, 2, 3], [4, 5], [6, 7, 8, 9]])
-    out = a.arr.get(0)
-    expected = pl.Series("a", [1, 4, 6])
-    assert_series_equal(out, expected)
-    out = a.arr[0]
-    expected = pl.Series("a", [1, 4, 6])
-    assert_series_equal(out, expected)
-    out = a.arr.first()
-    assert_series_equal(out, expected)
-    out = pl.select(pl.lit(a).arr.first()).to_series()
-    assert_series_equal(out, expected)
-
-    out = a.arr.get(-1)
-    expected = pl.Series("a", [3, 5, 9])
-    assert_series_equal(out, expected)
-    out = a.arr.last()
-    assert_series_equal(out, expected)
-    out = pl.select(pl.lit(a).arr.last()).to_series()
-    assert_series_equal(out, expected)
-
-    a = pl.Series("a", [[1, 2, 3], [4, 5], [6, 7, 8, 9]])
-    out = a.arr.get(-3)
-    expected = pl.Series("a", [1, None, 7])
-    assert_series_equal(out, expected)
-
-    assert pl.DataFrame(
-        {"a": [[1], [2], [3], [4, 5, 6], [7, 8, 9], [None, 11]]}
-    ).with_columns(
-        [pl.col("a").arr.get(i).alias(f"get_{i}") for i in range(4)]
-    ).to_dict(
-        False
-    ) == {
-        "a": [[1], [2], [3], [4, 5, 6], [7, 8, 9], [None, 11]],
-        "get_0": [1, 2, 3, 4, 7, None],
-        "get_1": [None, None, None, 5, 8, 11],
-        "get_2": [None, None, None, 6, 9, None],
-        "get_3": [None, None, None, None, None, None],
-    }
-
-    # get by indexes where some are out of bounds
-    df = pl.DataFrame({"cars": [[1, 2, 3], [2, 3], [4], []], "indexes": [-2, 1, -3, 0]})
-
-    assert df.select([pl.col("cars").arr.get("indexes")]).to_dict(False) == {
-        "cars": [2, 3, None, None]
-    }
-    # exact on oob boundary
+def test_sort_by_bools() -> None:
+    # tests dispatch
     df = pl.DataFrame(
         {
-            "index": [3, 3, 3],
-            "lists": [[3, 4, 5], [4, 5, 6], [7, 8, 9, 4]],
+            "foo": [1, 2, 3],
+            "bar": [6.0, 7.0, 8.0],
+            "ham": ["a", "b", "c"],
         }
     )
+    out = df.with_columns((pl.col("foo") % 2 == 1).alias("foo_odd")).sort(
+        by=["foo_odd", "foo"]
+    )
+    assert out.rows() == [
+        (2, 7.0, "b", False),
+        (1, 6.0, "a", True),
+        (3, 8.0, "c", True),
+    ]
+    assert out.shape == (3, 4)
 
-    assert df.select(pl.col("lists").arr.get(3)).to_dict(False) == {
-        "lists": [None, None, 4]
-    }
-    assert df.select(pl.col("lists").arr.get(pl.col("index"))).to_dict(False) == {
-        "lists": [None, None, 4]
-    }
-
-
-def test_contains() -> None:
-    a = pl.Series("a", [[1, 2, 3], [2, 5], [6, 7, 8, 9]])
-    out = a.arr.contains(2)
-    expected = pl.Series("a", [True, True, False])
-    assert_series_equal(out, expected)
-
-    out = pl.select(pl.lit(a).arr.contains(2)).to_series()
-    assert_series_equal(out, expected)
 
+def test_type_coercion_when_then_otherwise_2806() -> None:
+    out = (
+        pl.DataFrame({"names": ["foo", "spam", "spam"], "nrs": [1, 2, 3]})
+        .select(
+            [
+                pl.when(pl.col("names") == "spam")
+                .then(pl.col("nrs") * 2)
+                .otherwise(pl.lit("other"))
+                .alias("new_col"),
+            ]
+        )
+        .to_series()
+    )
+    expected = pl.Series("new_col", ["other", "4", "6"])
+    assert out.to_list() == expected.to_list()
 
-def test_list_concat() -> None:
-    df = pl.DataFrame({"a": [[1, 2], [1], [1, 2, 3]]})
+    # test it remains float32
+    assert (
+        pl.Series("a", [1.0, 2.0, 3.0], dtype=pl.Float32)
+        .to_frame()
+        .select(pl.when(pl.col("a") > 2.0).then(pl.col("a")).otherwise(0.0))
+    ).to_series().dtype == pl.Float32
 
-    out = df.select([pl.col("a").arr.concat(pl.Series([[1, 2]]))])
-    assert out["a"][0].to_list() == [1, 2, 1, 2]
 
-    out = df.select([pl.col("a").arr.concat([1, 4])])
-    assert out["a"][0].to_list() == [1, 2, 1, 4]
+def test_repeat_expansion_in_groupby() -> None:
+    out = (
+        pl.DataFrame({"g": [1, 2, 2, 3, 3, 3]})
+        .groupby("g", maintain_order=True)
+        .agg(pl.repeat(1, pl.count()).cumsum())
+        .to_dict(False)
+    )
+    assert out == {"g": [1, 2, 3], "repeat": [[1], [1, 2], [1, 2, 3]]}
 
-    out_s = df["a"].arr.concat([4, 1])
-    assert out_s[0].to_list() == [1, 2, 4, 1]
 
+def test_agg_after_head() -> None:
+    a = [1, 1, 1, 2, 2, 3, 3, 3, 3]
 
-def test_list_arr_empty() -> None:
-    df = pl.DataFrame({"cars": [[1, 2, 3], [2, 3], [4], []]})
+    df = pl.DataFrame({"a": a, "b": pl.arange(1, len(a) + 1, eager=True)})
 
-    out = df.select(
-        [
-            pl.col("cars").arr.first().alias("cars_first"),
-            pl.when(pl.col("cars").arr.first() == 2)
-            .then(1)
-            .when(pl.col("cars").arr.contains(2))
-            .then(2)
-            .otherwise(3)
-            .alias("cars_literal"),
-        ]
-    )
-    expected = pl.DataFrame(
-        {"cars_first": [1, 2, 4, None], "cars_literal": [2, 1, 3, 3]},
-        schema_overrides={"cars_literal": pl.Int32},  # Literals default to Int32
-    )
-    assert_frame_equal(out, expected)
+    expected = pl.DataFrame({"a": [1, 2, 3], "b": [6, 9, 21]})
 
+    for maintain_order in [True, False]:
+        out = df.groupby("a", maintain_order=maintain_order).agg(
+            [pl.col("b").head(3).sum()]
+        )
 
-def test_list_argminmax() -> None:
-    s = pl.Series("a", [[1, 2], [3, 2, 1]])
-    expected = pl.Series("a", [0, 2], dtype=pl.UInt32)
-    assert_series_equal(s.arr.arg_min(), expected)
-    expected = pl.Series("a", [1, 0], dtype=pl.UInt32)
-    assert_series_equal(s.arr.arg_max(), expected)
+        if not maintain_order:
+            out = out.sort("a")
 
+        assert_frame_equal(out, expected)
 
-def test_list_shift() -> None:
-    s = pl.Series("a", [[1, 2], [3, 2, 1]])
-    expected = pl.Series("a", [[None, 1], [None, 3, 2]])
-    assert s.arr.shift().to_list() == expected.to_list()
 
+def test_overflow_uint16_agg_mean() -> None:
+    assert (
+        pl.DataFrame(
+            {
+                "col1": ["A" for _ in range(1025)],
+                "col3": [64 for i in range(1025)],
+            }
+        )
+        .with_columns(
+            [
+                pl.col("col3").cast(pl.UInt16),
+            ]
+        )
+        .groupby(["col1"])
+        .agg(pl.col("col3").mean())
+        .to_dict(False)
+    ) == {"col1": ["A"], "col3": [64.0]}
 
-def test_list_diff() -> None:
-    s = pl.Series("a", [[1, 2], [10, 2, 1]])
-    expected = pl.Series("a", [[None, 1], [None, -8, -1]])
-    assert s.arr.diff().to_list() == expected.to_list()
 
+def test_binary_on_list_agg_3345() -> None:
+    df = pl.DataFrame(
+        {
+            "group": ["A", "A", "A", "B", "B", "B", "B"],
+            "id": [1, 2, 1, 4, 5, 4, 6],
+        }
+    )
 
-def test_slice() -> None:
-    vals = [[1, 2, 3, 4], [10, 2, 1]]
-    s = pl.Series("a", vals)
-    assert s.arr.head(2).to_list() == [[1, 2], [10, 2]]
-    assert s.arr.tail(2).to_list() == [[3, 4], [2, 1]]
-    assert s.arr.tail(200).to_list() == vals
-    assert s.arr.head(200).to_list() == vals
-    assert s.arr.slice(1, 2).to_list() == [[2, 3], [2, 1]]
+    assert (
+        df.groupby(["group"], maintain_order=True)
+        .agg(
+            [
+                (
+                    (pl.col("id").unique_counts() / pl.col("id").len()).log()
+                    * -1
+                    * (pl.col("id").unique_counts() / pl.col("id").len())
+                ).sum()
+            ]
+        )
+        .to_dict(False)
+    ) == {"group": ["A", "B"], "id": [0.6365141682948128, 1.0397207708399179]}
 
 
-def test_list_eval_dtype_inference() -> None:
-    grades = pl.DataFrame(
+def test_maintain_order_after_sampling() -> None:
+    # internally samples cardinality
+    # check if the maintain_order kwarg is dispatched
+    df = pl.DataFrame(
         {
-            "student": ["bas", "laura", "tim", "jenny"],
-            "arithmetic": [10, 5, 6, 8],
-            "biology": [4, 6, 2, 7],
-            "geography": [8, 4, 9, 7],
+            "type": ["A", "B", "C", "D", "A", "B", "C", "D"],
+            "value": [1, 3, 2, 3, 4, 5, 3, 4],
         }
     )
+    assert df.groupby("type", maintain_order=True).agg(pl.col("value").sum()).to_dict(
+        False
+    ) == {"type": ["A", "B", "C", "D"], "value": [5, 8, 5, 7]}
 
-    rank_pct = pl.col("").rank(descending=True) / pl.col("").count().cast(pl.UInt16)
 
-    # the .arr.first() would fail if .arr.eval did not correctly infer the output type
-    assert grades.with_columns(
-        pl.concat_list(pl.all().exclude("student")).alias("all_grades")
-    ).select(
-        [
-            pl.col("all_grades")
-            .arr.eval(rank_pct, parallel=True)
-            .alias("grades_rank")
-            .arr.first()
-        ]
-    ).to_series().to_list() == [
-        0.3333333432674408,
-        0.6666666865348816,
-        0.6666666865348816,
-        0.3333333432674408,
-    ]
+def test_sorted_groupby_optimization(monkeypatch: Any) -> None:
+    monkeypatch.setenv("POLARS_NO_STREAMING_GROUPBY", "1")
+
+    df = pl.DataFrame({"a": np.random.randint(0, 5, 20)})
+
+    # the sorted optimization should not randomize the
+    # groups, so this is tests that we hit the sorted optimization
+    for descending in [True, False]:
+        sorted_implicit = (
+            df.with_columns(pl.col("a").sort(descending=descending))
+            .groupby("a")
+            .agg(pl.count())
+        )
+        sorted_explicit = (
+            df.groupby("a").agg(pl.count()).sort("a", descending=descending)
+        )
+        assert_frame_equal(sorted_explicit, sorted_implicit)
 
 
-def test_list_ternary_concat() -> None:
+def test_median_on_shifted_col_3522() -> None:
     df = pl.DataFrame(
         {
-            "list1": [["123", "456"], None],
-            "list2": [["789"], ["zzz"]],
+            "foo": [
+                datetime(2022, 5, 5, 12, 31, 34),
+                datetime(2022, 5, 5, 12, 47, 1),
+                datetime(2022, 5, 6, 8, 59, 11),
+            ]
         }
     )
+    diffs = df.select(pl.col("foo").diff().dt.seconds())
+    assert diffs.select(pl.col("foo").median()).to_series()[0] == 36828.5
 
-    assert df.with_columns(
-        pl.when(pl.col("list1").is_null())
-        .then(pl.col("list1").arr.concat(pl.col("list2")))
-        .otherwise(pl.col("list2"))
-        .alias("result")
-    ).to_dict(False) == {
-        "list1": [["123", "456"], None],
-        "list2": [["789"], ["zzz"]],
-        "result": [["789"], None],
-    }
 
-    assert df.with_columns(
-        pl.when(pl.col("list1").is_null())
-        .then(pl.col("list2"))
-        .otherwise(pl.col("list1").arr.concat(pl.col("list2")))
-        .alias("result")
+def test_groupby_agg_equals_zero_3535() -> None:
+    # setup test frame
+    df = pl.DataFrame(
+        data=[
+            # note: the 'bb'-keyed values should clearly sum to 0
+            ("aa", 10, None),
+            ("bb", -10, 0.5),
+            ("bb", 10, -0.5),
+            ("cc", -99, 10.5),
+            ("cc", None, 0.0),
+        ],
+        schema=[
+            ("key", pl.Utf8),
+            ("val1", pl.Int16),
+            ("val2", pl.Float32),
+        ],
+    )
+    # group by the key, aggregating the two numeric cols
+    assert df.groupby(pl.col("key"), maintain_order=True).agg(
+        [pl.col("val1").sum(), pl.col("val2").sum()]
     ).to_dict(False) == {
-        "list1": [["123", "456"], None],
-        "list2": [["789"], ["zzz"]],
-        "result": [["123", "456", "789"], ["zzz"]],
+        "key": ["aa", "bb", "cc"],
+        "val1": [10, 0, -99],
+        "val2": [None, 0.0, 10.5],
     }
 
 
-def test_arr_contains_categorical() -> None:
+def test_arithmetic_in_aggregation_3739() -> None:
+    def demean_dot() -> pl.Expr:
+        x = pl.col("x")
+        y = pl.col("y")
+        x1 = x - x.mean()
+        y1 = y - y.mean()
+        return (x1 * y1).sum().alias("demean_dot")
+
+    assert (
+        pl.DataFrame(
+            {
+                "key": ["a", "a", "a", "a"],
+                "x": [4, 2, 2, 4],
+                "y": [2, 0, 2, 0],
+            }
+        )
+        .groupby("key")
+        .agg(
+            [
+                demean_dot(),
+            ]
+        )
+    ).to_dict(False) == {"key": ["a"], "demean_dot": [0.0]}
+
+
+def test_dtype_concat_3735() -> None:
+    for dt in [
+        pl.Int8,
+        pl.Int16,
+        pl.Int32,
+        pl.Int64,
+        pl.UInt8,
+        pl.UInt16,
+        pl.UInt32,
+        pl.UInt64,
+        pl.Float32,
+        pl.Float64,
+    ]:
+        d1 = pl.DataFrame([pl.Series("val", [1, 2], dtype=dt)])
+
+    d2 = pl.DataFrame([pl.Series("val", [3, 4], dtype=dt)])
+    df = pl.concat([d1, d2])
+
+    assert df.shape == (4, 1)
+    assert df.columns == ["val"]
+    assert df.to_series().to_list() == [1, 2, 3, 4]
+
+
+def test_opaque_filter_on_lists_3784() -> None:
     df = pl.DataFrame(
         {"str": ["A", "B", "A", "B", "C"], "group": [1, 1, 2, 1, 2]}
     ).lazy()
     df = df.with_columns(pl.col("str").cast(pl.Categorical))
+
     df_groups = df.groupby("group").agg([pl.col("str").alias("str_list")])
-    assert df_groups.filter(pl.col("str_list").arr.contains("C")).collect().to_dict(
-        False
-    ) == {"group": [2], "str_list": [["A", "C"]]}
 
+    pre = "A"
+    succ = "B"
 
-def test_list_eval_type_coercion() -> None:
-    last_non_null_value = pl.element().fill_null(3).last()
-    df = pl.DataFrame(
-        {
-            "array_cols": [[1, None]],
-        }
-    )
+    assert (
+        df_groups.filter(
+            pl.col("str_list").apply(
+                lambda variant: pre in variant
+                and succ in variant
+                and variant.to_list().index(pre) < variant.to_list().index(succ)
+            )
+        )
+    ).collect().to_dict(False) == {"group": [1], "str_list": [["A", "B", "B"]]}
 
-    assert df.select(
-        [
-            pl.col("array_cols")
-            .arr.eval(last_non_null_value, parallel=False)
-            .alias("col_last")
-        ]
-    ).to_dict(False) == {"col_last": [[3]]}
 
+def test_ternary_none_struct() -> None:
+    ignore_nulls = False
 
-def test_list_slice() -> None:
-    df = pl.DataFrame(
-        {
-            "lst": [[1, 2, 3, 4], [10, 2, 1]],
-            "offset": [1, 2],
-            "len": [3, 2],
-        }
-    )
+    def map_expr(name: str) -> pl.Expr:
+        return (
+            pl.when(ignore_nulls or pl.col(name).null_count() == 0)
+            .then(
+                pl.struct(
+                    [
+                        pl.sum(name).alias("sum"),
+                        (pl.count() - pl.col(name).null_count()).alias("count"),
+                    ]
+                ),
+            )
+            .otherwise(None)
+        ).alias("out")
 
-    assert df.select([pl.col("lst").arr.slice("offset", "len")]).to_dict(False) == {
-        "lst": [[2, 3, 4], [1]]
-    }
-    assert df.select([pl.col("lst").arr.slice("offset", 1)]).to_dict(False) == {
-        "lst": [[2], [1]]
-    }
-    assert df.select([pl.col("lst").arr.slice(-2, "len")]).to_dict(False) == {
-        "lst": [[3, 4], [2, 1]]
+    assert (
+        pl.DataFrame({"groups": [1, 2, 3, 4], "values": [None, None, 1, 2]})
+        .groupby("groups", maintain_order=True)
+        .agg([map_expr("values")])
+    ).to_dict(False) == {
+        "groups": [1, 2, 3, 4],
+        "out": [
+            {"sum": None, "count": None},
+            {"sum": None, "count": None},
+            {"sum": 1, "count": 1},
+            {"sum": 2, "count": 1},
+        ],
     }
 
 
-@typing.no_type_check
-def test_list_sliced_get_5186() -> None:
-    # https://github.com/pola-rs/polars/issues/5186
-    n = 30
-    df = pl.from_dict(
-        {
-            "ind": pl.arange(0, n, eager=True),
-            "inds": np.stack([np.arange(n), -np.arange(n)], axis=-1),
-        }
-    )
+def test_when_then_edge_cases_3994() -> None:
+    df = pl.DataFrame(data={"id": [1, 1], "type": [2, 2]})
 
-    exprs = [
-        "ind",
-        pl.col("inds").arr.first().alias("first_element"),
-        pl.col("inds").arr.last().alias("last_element"),
-    ]
-    out1 = df.select(exprs)[10:20]
-    out2 = df[10:20].select(exprs)
-    assert_frame_equal(out1, out2)
+    # this tests if lazy correctly assigns the list schema to the column aggregation
+    assert (
+        df.lazy()
+        .groupby(["id"])
+        .agg(pl.col("type"))
+        .with_columns(
+            pl.when(pl.col("type").list.lengths() == 0)
+            .then(pl.lit(None))
+            .otherwise(pl.col("type"))
+            .keep_name()
+        )
+        .collect()
+    ).to_dict(False) == {"id": [1], "type": [[2, 2]]}
 
+    # this tests ternary with an empty argument
+    assert (
+        df.filter(pl.col("id") == 42)
+        .groupby(["id"])
+        .agg(pl.col("type"))
+        .with_columns(
+            pl.when(pl.col("type").list.lengths() == 0)
+            .then(pl.lit(None))
+            .otherwise(pl.col("type"))
+            .keep_name()
+        )
+    ).to_dict(False) == {"id": [], "type": []}
 
-def test_empty_eval_dtype_5546() -> None:
-    # https://github.com/pola-rs/polars/issues/5546
-    df = pl.DataFrame([{"a": [{"name": 1}, {"name": 2}]}])
 
-    dtype = df.dtypes[0]
+def test_edge_cast_string_duplicates_4259() -> None:
+    # carefully constructed data.
+    # note that row 2, 3 concatenated are the same string ('5461214484')
+    df = pl.DataFrame(
+        {
+            "a": [99, 54612, 546121],
+            "b": [1, 14484, 4484],
+        }
+    ).with_columns(pl.all().cast(pl.Utf8))
 
-    assert (
-        df.limit(0).with_columns(
-            pl.col("a")
-            .arr.eval(pl.element().filter(pl.first().struct.field("name") == 1))
-            .alias("a_filtered")
-        )
-    ).dtypes == [dtype, dtype]
-
-
-def test_list_amortized_apply_explode_5812() -> None:
-    s = pl.Series([None, [1, 3], [0, -3], [1, 2, 2]])
-    assert s.arr.sum().to_list() == [None, 4, -3, 5]
-    assert s.arr.min().to_list() == [None, 1, -3, 1]
-    assert s.arr.max().to_list() == [None, 3, 0, 2]
-    assert s.arr.arg_min().to_list() == [None, 0, 1, 0]
-    assert s.arr.arg_max().to_list() == [None, 1, 0, 1]
-
-
-def test_list_slice_5866() -> None:
-    vals = [[1, 2, 3, 4], [10, 2, 1]]
-    s = pl.Series("a", vals)
-    assert s.arr.slice(1).to_list() == [[2, 3, 4], [2, 1]]
-
-
-def test_list_take() -> None:
-    s = pl.Series("a", [[1, 2, 3], [4, 5], [6, 7, 8]])
-    # mypy: we make it work, but idomatic is `arr.get`.
-    assert s.arr.take(0).to_list() == [[1], [4], [6]]  # type: ignore[arg-type]
-    assert s.arr.take([0, 1]).to_list() == [[1, 2], [4, 5], [6, 7]]
-
-    assert s.arr.take([-1, 1]).to_list() == [[3, 2], [5, 5], [8, 7]]
-
-    # use another list to make sure negative indices are respected
-    taker = pl.Series([[-1, 1], [-1, 1], [-1, -2]])
-    assert s.arr.take(taker).to_list() == [[3, 2], [5, 5], [8, 7]]
-    with pytest.raises(pl.ComputeError, match=r"take indices are out of bounds"):
-        s.arr.take([1, 2])
-    s = pl.Series(
-        [["A", "B", "C"], ["A"], ["B"], ["1", "2"], ["e"]],
-    )
-
-    assert s.arr.take([0, 2], null_on_oob=True).to_list() == [
-        ["A", "C"],
-        ["A", None],
-        ["B", None],
-        ["1", None],
-        ["e", None],
-    ]
-    assert s.arr.take([0, 1, 2], null_on_oob=True).to_list() == [
-        ["A", "B", "C"],
-        ["A", None, None],
-        ["B", None, None],
-        ["1", "2", None],
-        ["e", None, None],
-    ]
-    s = pl.Series([[42, 1, 2], [5, 6, 7]])
+    mask = df.select(["a", "b"]).is_duplicated()
+    df_filtered = df.filter(pl.lit(mask))
 
-    with pytest.raises(pl.ComputeError, match=r"take indices are out of bounds"):
-        s.arr.take([[0, 1, 2, 3], [0, 1, 2, 3]])
+    assert df_filtered.shape == (0, 2)
+    assert df_filtered.rows() == []
 
-    assert s.arr.take([0, 1, 2, 3], null_on_oob=True).to_list() == [
-        [42, 1, 2, None],
-        [5, 6, 7, None],
-    ]
 
+def test_query_4438() -> None:
+    df = pl.DataFrame({"x": [1, 2, 3, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 1, 1]})
 
-def test_list_eval_all_null() -> None:
-    df = pl.DataFrame({"foo": [1, 2, 3], "bar": [None, None, None]}).with_columns(
-        pl.col("bar").cast(pl.List(pl.Utf8))
+    q = (
+        df.lazy()
+        .with_columns(pl.col("x").rolling_max(window_size=3).alias("rolling_max"))
+        .fill_null(strategy="backward")
+        .with_columns(
+            pl.col("rolling_max").rolling_max(window_size=3).alias("rolling_max_2")
+        )
     )
-
-    assert df.select(pl.col("bar").arr.eval(pl.element())).to_dict(False) == {
-        "bar": [None, None, None]
-    }
+    assert q.collect()["rolling_max_2"].to_list() == [
+        None,
+        None,
+        3,
+        10,
+        10,
+        10,
+        10,
+        10,
+        9,
+        8,
+        7,
+        6,
+        5,
+        4,
+        3,
+    ]
 
 
-def test_list_function_group_awareness() -> None:
+def test_query_4538() -> None:
     df = pl.DataFrame(
-        {
-            "a": [100, 103, 105, 106, 105, 104, 103, 106, 100, 102],
-            "group": [0, 0, 1, 1, 1, 1, 1, 1, 2, 2],
-        }
-    )
-
-    assert df.groupby("group").agg(
         [
-            pl.col("a").implode().arr.get(0).alias("get"),
-            pl.col("a").implode().arr.take([0]).alias("take"),
-            pl.col("a").implode().arr.slice(0, 3).alias("slice"),
+            pl.Series("value", ["aaa", "bbb"]),
         ]
-    ).sort("group").to_dict(False) == {
-        "group": [0, 1, 2],
-        "get": [100, 105, 100],
-        "take": [[100], [105], [100]],
-        "slice": [[100, 103], [105, 106, 105], [100, 102]],
-    }
+    )
+    assert df.select([pl.col("value").str.to_uppercase().is_in(["AAA"])])[
+        "value"
+    ].to_list() == [True, False]
 
 
-def test_list_get_logical_types() -> None:
+def test_none_comparison_4773() -> None:
     df = pl.DataFrame(
         {
-            "date_col": [[datetime(2023, 2, 1).date(), datetime(2023, 2, 2).date()]],
-            "datetime_col": [[datetime(2023, 2, 1), datetime(2023, 2, 2)]],
+            "x": [0, 1, None, 2],
+            "y": [1, 2, None, 3],
         }
-    )
-
-    assert df.select(pl.all().arr.get(1).suffix("_element_1")).to_dict(False) == {
-        "date_col_element_1": [date(2023, 2, 2)],
-        "datetime_col_element_1": [datetime(2023, 2, 2, 0, 0)],
-    }
+    ).filter(pl.col("x") != pl.col("y"))
+    assert df.shape == (3, 2)
+    assert df.rows() == [(0, 1), (1, 2), (2, 3)]
 
 
-def test_list_take_logical_type() -> None:
-    df = pl.DataFrame(
-        {"foo": [["foo", "foo", "bar"]], "bar": [[5.0, 10.0, 12.0]]}
-    ).with_columns(pl.col("foo").cast(pl.List(pl.Categorical)))
-
-    df = pl.concat([df, df], rechunk=False)
-    assert df.n_chunks() == 2
-    assert df.select(pl.all().take([0, 1])).to_dict(False) == {
-        "foo": [["foo", "foo", "bar"], ["foo", "foo", "bar"]],
-        "bar": [[5.0, 10.0, 12.0], [5.0, 10.0, 12.0]],
-    }
-
-
-def test_list_unique() -> None:
-    assert (
-        pl.Series([[1, 1, 2, 2, 3], [3, 3, 3, 2, 1, 2]])
-        .arr.unique(maintain_order=True)
-        .series_equal(pl.Series([[1, 2, 3], [3, 2, 1]]))
+def test_datetime_supertype_5236() -> None:
+    df = pd.DataFrame(
+        {
+            "StartDateTime": [
+                pd.Timestamp(datetime.utcnow(), tz="UTC"),
+                pd.Timestamp(datetime.utcnow(), tz="UTC"),
+            ],
+            "EndDateTime": [
+                pd.Timestamp(datetime.utcnow(), tz="UTC"),
+                pd.Timestamp(datetime.utcnow(), tz="UTC"),
+            ],
+        }
+    )
+    out = pl.from_pandas(df).filter(
+        pl.col("StartDateTime")
+        < (pl.col("EndDateTime").dt.truncate("1d").max() - timedelta(days=1))
     )
+    assert out.shape == (0, 2)
+    assert out.dtypes == [pl.Datetime("ns", "UTC")] * 2
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/namespaces/test_meta.py` & `polars_lts_cpu-0.18.0/tests/unit/namespaces/test_meta.py`

 * *Files 4% similar despite different names*

```diff
@@ -8,14 +8,17 @@
 def test_meta_pop_and_cmp() -> None:
     e = pl.col("foo").alias("bar")
 
     first = e.meta.pop()[0]
     assert first.meta == pl.col("foo")
     assert first.meta != pl.col("bar")
 
+    assert first.meta.eq(pl.col("foo"))
+    assert first.meta.ne(pl.col("bar"))
+
 
 def test_root_and_output_names() -> None:
     e = pl.col("foo") * pl.col("bar")
     assert e.meta.output_name() == "foo"
     assert e.meta.root_names() == ["foo", "bar"]
 
     e = pl.col("foo").filter(pl.col("bar") == 13)
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/namespaces/test_string.py` & `polars_lts_cpu-0.18.0/tests/unit/namespaces/test_string.py`

 * *Files 1% similar despite different names*

```diff
@@ -625,25 +625,14 @@
     assert df.select(pl.col("strings").str.decode("base64", strict=False)).to_dict(
         False
     ) == {"strings": [b"\xd0\x86\xd0\xbd77", None, None]}
     with pytest.raises(pl.ComputeError):
         df.select(pl.col("strings").str.decode("base64", strict=True))
 
 
-def test_wildcard_expansion() -> None:
-    # one function requires wildcard expansion the other need
-    # this tests the nested behavior
-    # see: #2867
-
-    df = pl.DataFrame({"a": ["x", "Y", "z"], "b": ["S", "o", "S"]})
-    assert df.select(
-        pl.concat_str(pl.all()).str.to_lowercase()
-    ).to_series().to_list() == ["xs", "yo", "zs"]
-
-
 def test_split() -> None:
     df = pl.DataFrame({"x": ["a_a", None, "b", "c_c_c"]})
     out = df.select([pl.col("x").str.split("_")])
 
     expected = pl.DataFrame(
         [
             {"x": ["a", "a"]},
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/namespaces/test_strptime.py` & `polars_lts_cpu-0.18.0/tests/unit/namespaces/test_strptime.py`

 * *Files 20% similar despite different names*

```diff
@@ -40,68 +40,57 @@
     s = pl.Series(["00:00:00", "03:20:10"])
     expected = pl.Series([0, 12010000000000], dtype=pl.Time)
     assert_series_equal(s.str.strptime(pl.Time, "%H:%M:%S"), expected)
 
 
 def test_date_parse_omit_day() -> None:
     df = pl.DataFrame({"month": ["2022-01"]})
+    assert df.select(pl.col("month").str.to_date(format="%Y-%m")).item() == date(
+        2022, 1, 1
+    )
     assert df.select(
-        pl.col("month").str.strptime(pl.Date, format="%Y-%m")
-    ).item() == date(2022, 1, 1)
-    assert df.select(
-        pl.col("month").str.strptime(pl.Datetime, format="%Y-%m")
+        pl.col("month").str.to_datetime(format="%Y-%m")
     ).item() == datetime(2022, 1, 1)
 
 
-def test_strptime_precision() -> None:
+def test_to_datetime_precision() -> None:
     s = pl.Series(
         "date", ["2022-09-12 21:54:36.789321456", "2022-09-13 12:34:56.987456321"]
     )
-    ds = s.str.strptime(pl.Datetime)
+    ds = s.str.to_datetime()
     assert ds.cast(pl.Date) != None  # noqa: E711  (note: *deliberately* testing "!=")
     assert getattr(ds.dtype, "time_unit", None) == "us"
 
     time_units: list[TimeUnit] = ["ms", "us", "ns"]
     suffixes = ["%.3f", "%.6f", "%.9f"]
     test_data = zip(
         time_units,
         suffixes,
         (
             [789000000, 987000000],
             [789321000, 987456000],
             [789321456, 987456321],
         ),
     )
-    for precision, suffix, expected_values in test_data:
-        ds = s.str.strptime(pl.Datetime(precision), f"%Y-%m-%d %H:%M:%S{suffix}")
-        assert getattr(ds.dtype, "time_unit", None) == precision
+    for time_unit, suffix, expected_values in test_data:
+        ds = s.str.to_datetime(f"%Y-%m-%d %H:%M:%S{suffix}", time_unit=time_unit)
+        assert getattr(ds.dtype, "time_unit", None) == time_unit
         assert ds.dt.nanosecond().to_list() == expected_values
 
 
 @pytest.mark.parametrize(
-    ("unit", "expected"),
+    ("time_unit", "expected"),
     [("ms", "123000000"), ("us", "123456000"), ("ns", "123456789")],
 )
 @pytest.mark.parametrize("format", ["%Y-%m-%d %H:%M:%S%.f", None])
-def test_strptime_precision_with_time_unit(
-    unit: TimeUnit, expected: str, format: str
+def test_to_datetime_precision_with_time_unit(
+    time_unit: TimeUnit, expected: str, format: str
 ) -> None:
-    ser = pl.Series(["2020-01-01 00:00:00.123456789"])
-    result = ser.str.strptime(pl.Datetime(unit), format=format).dt.strftime("%f")[0]
-    assert result == expected
-
-
-@pytest.mark.parametrize("fmt", ["%Y-%m-%dT%H:%M:%S", None])
-def test_utc_with_tz_naive(fmt: str | None) -> None:
-    result = (
-        pl.Series(["2020-01-01T00:00:00"])
-        .str.strptime(pl.Datetime, fmt, utc=True)
-        .item()
-    )
-    expected = datetime(2020, 1, 1, tzinfo=timezone.utc)
+    s = pl.Series(["2020-01-01 00:00:00.123456789"])
+    result = s.str.to_datetime(format, time_unit=time_unit).dt.to_string("%f")[0]
     assert result == expected
 
 
 @pytest.mark.parametrize(
     ("tz_string", "timedelta"),
     [("+01:00", timedelta(minutes=60)), ("-01:30", timedelta(hours=-1, minutes=-30))],
 )
@@ -112,48 +101,46 @@
                 "2021-12-05 06:00:00" + tz_string,
                 "2021-12-05 07:00:00" + tz_string,
                 "2021-12-05 08:00:00" + tz_string,
             ]
         }
     )
     assert times.with_columns(
-        pl.col("delivery_datetime").str.strptime(
-            pl.Datetime, format="%Y-%m-%d %H:%M:%S%z"
-        )
+        pl.col("delivery_datetime").str.to_datetime(format="%Y-%m-%d %H:%M:%S%z")
     ).to_dict(False) == {
         "delivery_datetime": [
             datetime(2021, 12, 5, 6, 0, tzinfo=timezone(timedelta)),
             datetime(2021, 12, 5, 7, 0, tzinfo=timezone(timedelta)),
             datetime(2021, 12, 5, 8, 0, tzinfo=timezone(timedelta)),
         ]
     }
 
 
-def test_non_exact_strptime() -> None:
+def test_to_date_non_exact_strptime() -> None:
     s = pl.Series("a", ["2022-01-16", "2022-01-17", "foo2022-01-18", "b2022-01-19ar"])
-    fmt = "%Y-%m-%d"
+    format = "%Y-%m-%d"
 
-    result = s.str.strptime(pl.Date, fmt, strict=False, exact=True)
+    result = s.str.to_date(format, strict=False, exact=True)
     expected = pl.Series("a", [date(2022, 1, 16), date(2022, 1, 17), None, None])
     assert_series_equal(result, expected)
 
-    result = s.str.strptime(pl.Date, fmt, strict=False, exact=False)
+    result = s.str.to_date(format, strict=False, exact=False)
     expected = pl.Series(
         "a",
         [date(2022, 1, 16), date(2022, 1, 17), date(2022, 1, 18), date(2022, 1, 19)],
     )
     assert_series_equal(result, expected)
 
     with pytest.raises(Exception):
-        s.str.strptime(pl.Date, fmt, strict=True, exact=True)
+        s.str.to_date(format, strict=True, exact=True)
 
 
-def test_strptime_dates_datetimes() -> None:
+def test_to_datetime_dates_datetimes() -> None:
     s = pl.Series("date", ["2021-04-22", "2022-01-04 00:00:00"])
-    assert s.str.strptime(pl.Datetime).to_list() == [
+    assert s.str.to_datetime().to_list() == [
         datetime(2021, 4, 22, 0, 0),
         datetime(2022, 1, 4, 0, 0),
     ]
 
 
 @pytest.mark.parametrize(
     ("time_string", "expected"),
@@ -194,49 +181,42 @@
             "2019-04-18T22:45:55.555123+01:00",
             datetime(
                 2019, 4, 18, 22, 45, 55, 555123, tzinfo=timezone(timedelta(hours=1))
             ),
         ),
     ],
 )
-def test_datetime_strptime_patterns_single(time_string: str, expected: str) -> None:
-    result = pl.Series([time_string]).str.strptime(pl.Datetime).item()
+def test_to_datetime_patterns_single(time_string: str, expected: str) -> None:
+    result = pl.Series([time_string]).str.to_datetime().item()
     assert result == expected
 
 
 @pytest.mark.parametrize("time_unit", ["ms", "us", "ns"])
 def test_infer_tz_aware_time_unit(time_unit: TimeUnit) -> None:
-    result = pl.Series(["2020-01-02T04:00:00+02:00"]).str.strptime(
-        pl.Datetime(time_unit)
-    )
-    assert result.dtype == pl.Datetime(time_unit, "+02:00")
-    assert result.item() == datetime(
-        2020, 1, 2, 4, 0, tzinfo=timezone(timedelta(hours=2))
+    result = pl.Series(["2020-01-02T04:00:00+02:00"]).str.to_datetime(
+        time_unit=time_unit
     )
+    assert result.dtype == pl.Datetime(time_unit, "UTC")
+    assert result.item() == datetime(2020, 1, 2, 2, 0, tzinfo=timezone.utc)
 
 
 @pytest.mark.parametrize("time_unit", ["ms", "us", "ns"])
 def test_infer_tz_aware_with_utc(time_unit: TimeUnit) -> None:
-    result = pl.Series(["2020-01-02T04:00:00+02:00"]).str.strptime(
-        pl.Datetime(time_unit), utc=True
+    result = pl.Series(["2020-01-02T04:00:00+02:00"]).str.to_datetime(
+        time_unit=time_unit
     )
     assert result.dtype == pl.Datetime(time_unit, "UTC")
     assert result.item() == datetime(2020, 1, 2, 2, 0, tzinfo=timezone.utc)
 
 
 def test_infer_tz_aware_raises() -> None:
     msg = "cannot parse tz-aware values with tz-aware dtype - please drop the time zone from the dtype"
     with pytest.raises(ComputeError, match=msg):
-        pl.Series(["2020-01-02T04:00:00+02:00"]).str.strptime(
-            pl.Datetime("us", "Europe/Vienna")
-        )
-    msg = "cannot use strptime with both 'utc=True' and tz-aware dtype, please drop time zone from the dtype"
-    with pytest.raises(ComputeError, match=msg):
-        pl.Series(["2020-01-02T04:00:00+02:00"]).str.strptime(
-            pl.Datetime("us", "Europe/Vienna"), utc=True
+        pl.Series(["2020-01-02T04:00:00+02:00"]).str.to_datetime(
+            time_unit="us", time_zone="Europe/Vienna"
         )
 
 
 def test_datetime_strptime_patterns_consistent() -> None:
     # note that all should be year first
     df = pl.Series(
         "date",
@@ -249,17 +229,15 @@
             "2018-09-05T14:24:02.123Z",
             "2019-04-18T02:45:55.555000000",
             "2019-04-18T22:45:55.555123",
         ],
     ).to_frame()
     s = df.with_columns(
         [
-            pl.col("date")
-            .str.strptime(pl.Datetime, format=None, strict=False)
-            .alias("parsed"),
+            pl.col("date").str.to_datetime(strict=False).alias("parsed"),
         ]
     )["parsed"]
     assert s.null_count() == 1
     assert s[5] is None
 
 
 def test_datetime_strptime_patterns_inconsistent() -> None:
@@ -275,21 +253,17 @@
             "2018-09-05T04:24:02.11",
             "2018-09-05T14:24:02.123",
             "2018-09-05T14:24:02.123Z",
             "2019-04-18T02:45:55.555000000",
             "2019-04-18T22:45:55.555123",
         ],
     ).to_frame()
-    s = df.with_columns(
-        [
-            pl.col("date")
-            .str.strptime(pl.Datetime, format=None, strict=False)
-            .alias("parsed"),
-        ]
-    )["parsed"]
+    s = df.with_columns(pl.col("date").str.to_datetime(strict=False).alias("parsed"))[
+        "parsed"
+    ]
     assert s.null_count() == 8
     assert s[0] is not None
 
 
 @pytest.mark.parametrize(
     (
         "ts",
@@ -312,56 +286,56 @@
     exp_year: int,
     exp_month: int,
     exp_day: int,
     exp_hour: int,
     exp_minute: int,
     exp_second: int,
 ) -> None:
-    ser = pl.Series([ts])
-    result = ser.str.strptime(pl.Datetime("ms"), format=format)
+    s = pl.Series([ts])
+    result = s.str.to_datetime(format, time_unit="ms")
     # Python datetime.datetime doesn't support negative dates, so comparing
     # with `result.item()` directly won't work.
     assert result.dt.year().item() == exp_year
     assert result.dt.month().item() == exp_month
     assert result.dt.day().item() == exp_day
     assert result.dt.hour().item() == exp_hour
     assert result.dt.minute().item() == exp_minute
     assert result.dt.second().item() == exp_second
 
 
 def test_short_formats() -> None:
     s = pl.Series(["20202020", "2020"])
-    assert s.str.strptime(pl.Date, "%Y", strict=False).to_list() == [
+    assert s.str.to_date("%Y", strict=False).to_list() == [
         None,
         date(2020, 1, 1),
     ]
-    assert s.str.strptime(pl.Date, "%bar", strict=False).to_list() == [None, None]
+    assert s.str.to_date("%bar", strict=False).to_list() == [None, None]
 
 
 @pytest.mark.parametrize(
     ("time_string", "fmt", "datatype", "expected"),
     [
         ("Jul/2020", "%b/%Y", pl.Date, date(2020, 7, 1)),
         ("Jan/2020", "%b/%Y", pl.Date, date(2020, 1, 1)),
         ("02/Apr/2020", "%d/%b/%Y", pl.Date, date(2020, 4, 2)),
         ("Dec/2020", "%b/%Y", pl.Datetime, datetime(2020, 12, 1, 0, 0)),
         ("Nov/2020", "%b/%Y", pl.Datetime, datetime(2020, 11, 1, 0, 0)),
         ("02/Feb/2020", "%d/%b/%Y", pl.Datetime, datetime(2020, 2, 2, 0, 0)),
     ],
 )
-def test_abbrev_month(
+def test_strptime_abbrev_month(
     time_string: str, fmt: str, datatype: PolarsTemporalType, expected: date
 ) -> None:
     s = pl.Series([time_string])
     result = s.str.strptime(datatype, fmt).item()
     assert result == expected
 
 
 def test_full_month_name() -> None:
-    s = pl.Series(["2022-December-01"]).str.strptime(pl.Datetime, "%Y-%B-%d")
+    s = pl.Series(["2022-December-01"]).str.to_datetime("%Y-%B-%d")
     assert s[0] == datetime(2022, 12, 1)
 
 
 @pytest.mark.parametrize(
     ("datatype", "expected"),
     [
         (pl.Datetime, datetime(2022, 1, 1)),
@@ -372,38 +346,45 @@
     datatype: PolarsTemporalType, expected: datetime | date
 ) -> None:
     s = pl.Series(["2022-1-1"]).str.strptime(datatype, "%Y-%m-%d")
     assert s[0] == expected
 
 
 def test_invalid_date_parsing_4898() -> None:
-    assert pl.Series(["2022-09-18", "2022-09-50"]).str.strptime(
-        pl.Date, "%Y-%m-%d", strict=False
+    assert pl.Series(["2022-09-18", "2022-09-50"]).str.to_date(
+        "%Y-%m-%d", strict=False
     ).to_list() == [date(2022, 9, 18), None]
 
 
 def test_strptime_invalid_timezone() -> None:
-    ts = pl.Series(["2020-01-01 00:00:00+01:00"]).str.strptime(
-        pl.Datetime, "%Y-%m-%d %H:%M:%S%z"
-    )
-    with pytest.raises(ComputeError, match=r"unable to parse time zone: 'foo'"):
+    ts = pl.Series(["2020-01-01 00:00:00+01:00"]).str.to_datetime("%Y-%m-%d %H:%M:%S%z")
+    with pytest.raises(
+        ComputeError, match=r"unable to parse time zone: 'foo'"
+    ), pytest.warns(
+        DeprecationWarning,
+        match="time zones other than those in `zoneinfo.available_timezones",
+    ):
         ts.dt.replace_time_zone("foo")
 
 
-def test_strptime_ambiguous_or_non_existent() -> None:
+def test_to_datetime_ambiguous_or_non_existent() -> None:
     with pytest.raises(
         ArrowError,
         match="datetime '2021-11-07 01:00:00' is ambiguous in time zone 'US/Central'",
     ):
-        pl.Series(["2021-11-07 01:00"]).str.strptime(pl.Datetime("us", "US/Central"))
+        pl.Series(["2021-11-07 01:00"]).str.to_datetime(
+            time_unit="us", time_zone="US/Central"
+        )
     with pytest.raises(
         ArrowError,
         match="datetime '2021-03-28 02:30:00' is non-existent in time zone 'Europe/Warsaw'",
     ):
-        pl.Series(["2021-03-28 02:30"]).str.strptime(pl.Datetime("us", "Europe/Warsaw"))
+        pl.Series(["2021-03-28 02:30"]).str.to_datetime(
+            time_unit="us", time_zone="Europe/Warsaw"
+        )
 
 
 @pytest.mark.parametrize(
     ("ts", "fmt", "expected"),
     [
         ("2020-01-01T00:00:00Z", None, datetime(2020, 1, 1, tzinfo=timezone.utc)),
         ("2020-01-01T00:00:00Z", "%+", datetime(2020, 1, 1, tzinfo=timezone.utc)),
@@ -420,79 +401,76 @@
         (
             "2020-01-01T00:00:00+01:00",
             "%Y-%m-%dT%H:%M:%S%#z",
             datetime(2020, 1, 1, tzinfo=timezone(timedelta(seconds=3600))),
         ),
     ],
 )
-def test_tz_aware_strptime(ts: str, fmt: str, expected: datetime) -> None:
-    result = pl.Series([ts]).str.strptime(pl.Datetime, fmt).item()
+def test_to_datetime_tz_aware_strptime(ts: str, fmt: str, expected: datetime) -> None:
+    result = pl.Series([ts]).str.to_datetime(fmt).item()
     assert result == expected
 
 
-@pytest.mark.parametrize("fmt", ["%+", "%Y-%m-%dT%H:%M:%S%z"])
-def test_crossing_dst(fmt: str) -> None:
+@pytest.mark.parametrize("format", ["%+", "%Y-%m-%dT%H:%M:%S%z"])
+def test_crossing_dst(format: str) -> None:
     ts = ["2021-03-27T23:59:59+01:00", "2021-03-28T23:59:59+02:00"]
-    result = pl.Series(ts).str.strptime(pl.Datetime, fmt, utc=True)
+    result = pl.Series(ts).str.to_datetime(format)
     assert result[0] == datetime(2021, 3, 27, 22, 59, 59, tzinfo=ZoneInfo(key="UTC"))
     assert result[1] == datetime(2021, 3, 28, 21, 59, 59, tzinfo=ZoneInfo(key="UTC"))
 
 
-@pytest.mark.parametrize("fmt", ["%+", "%Y-%m-%dT%H:%M:%S%z"])
-def test_crossing_dst_tz_aware(fmt: str) -> None:
+@pytest.mark.parametrize("format", ["%+", "%Y-%m-%dT%H:%M:%S%z"])
+def test_crossing_dst_tz_aware(format: str) -> None:
     ts = ["2021-03-27T23:59:59+01:00", "2021-03-28T23:59:59+02:00"]
-    with pytest.raises(
-        ComputeError, match="^different timezones found during 'strptime' operation"
-    ):
-        pl.Series(ts).str.strptime(pl.Datetime, fmt, utc=False)
-
-
-def test_tz_aware_without_fmt() -> None:
-    with pytest.raises(
-        ComputeError,
-        match=(
-            r"^passing 'tz_aware=True' without 'format' is not yet supported, "
-            r"please specify 'format'$"
-        ),
-    ), pytest.warns(
-        DeprecationWarning,
-        match="`tz_aware` is now auto-inferred from `format` and will be removed "
-        "in a future version. You can safely drop this argument.",
-    ):
-        pl.Series(["2020-01-01"]).str.strptime(pl.Datetime, tz_aware=True)
+    result = pl.Series(ts).str.to_datetime(format)
+    expected = pl.Series(
+        [
+            datetime(2021, 3, 27, 22, 59, 59, tzinfo=timezone.utc),
+            datetime(2021, 3, 28, 21, 59, 59, tzinfo=timezone.utc),
+        ]
+    )
+    assert_series_equal(result, expected)
 
 
 @pytest.mark.parametrize(
     ("data", "format", "expected"),
     [
         (
             "2023-02-05T05:10:10.074000",
             "%Y-%m-%dT%H:%M:%S%.f",
             datetime(2023, 2, 5, 5, 10, 10, 74000),
         ),
     ],
 )
 def test_strptime_subseconds_datetime(data: str, format: str, expected: time) -> None:
     s = pl.Series([data])
-    result = s.str.strptime(pl.Datetime, format).item()
+    result = s.str.to_datetime(format).item()
     assert result == expected
 
 
+def test_strptime_hour_without_minute_8849() -> None:
+    with pytest.raises(
+        ComputeError,
+        match="Invalid format string: found hour, but no minute directive",
+    ):
+        pl.Series(["2023-05-04|7", "2023-05-04|10"]).str.to_datetime("%Y-%m-%d|%H")
+
+
 @pytest.mark.parametrize(
     ("data", "format", "expected"),
     [
         ("05:10:10.074000", "%H:%M:%S%.f", time(5, 10, 10, 74000)),
         ("05:10:10.074000", "%T%.6f", time(5, 10, 10, 74000)),
         ("05:10:10.074000", "%H:%M:%S%.3f", time(5, 10, 10, 74000)),
     ],
 )
-def test_strptime_subseconds_time(data: str, format: str, expected: time) -> None:
+def test_to_time_subseconds(data: str, format: str, expected: time) -> None:
     s = pl.Series([data])
-    result = s.str.strptime(pl.Time, format).item()
+    result = s.str.to_time(format).item()
     assert result == expected
 
 
-def test_strptime_format_warning() -> None:
+def test_to_time_format_warning() -> None:
     s = pl.Series(["05:10:10.074000"])
     with pytest.warns(pl.ChronoFormatWarning, match=".%f"):
-        result = s.str.strptime(pl.Time, "%H:%M:%S.%f").item()
+        result = s.str.to_time("%H:%M:%S.%f").item()
     assert result == time(5, 10, 10, 74)
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/namespaces/test_struct.py` & `polars_lts_cpu-0.18.0/tests/unit/namespaces/test_struct.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/operations/test_aggregations.py` & `polars_lts_cpu-0.18.0/tests/unit/operations/test_aggregations.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+import math
 import typing
 from datetime import date, datetime, timedelta
 
 import numpy as np
 import pytest
 
 import polars as pl
@@ -39,18 +40,24 @@
     }
 
 
 def test_duration_aggs() -> None:
     df = pl.DataFrame(
         {
             "time1": pl.date_range(
-                start=datetime(2022, 12, 12), end=datetime(2022, 12, 18), interval="1d"
+                start=datetime(2022, 12, 12),
+                end=datetime(2022, 12, 18),
+                interval="1d",
+                eager=True,
             ),
             "time2": pl.date_range(
-                start=datetime(2023, 1, 12), end=datetime(2023, 1, 18), interval="1d"
+                start=datetime(2023, 1, 12),
+                end=datetime(2023, 1, 18),
+                interval="1d",
+                eager=True,
             ),
         }
     )
 
     df = df.with_columns((pl.col("time2") - pl.col("time1")).alias("time_difference"))
 
     assert df.select("time_difference").mean().to_dict(False) == {
@@ -84,14 +91,20 @@
 
 
 def test_median() -> None:
     s = pl.Series([1, 2, 3])
     assert s.median() == 2
 
 
+def test_single_element_std() -> None:
+    s = pl.Series([1])
+    assert math.isnan(typing.cast(float, s.std(ddof=1)))
+    assert s.std(ddof=0) == 0.0
+
+
 def test_quantile() -> None:
     s = pl.Series([1, 2, 3])
     assert s.quantile(0.5, "nearest") == 2
     assert s.quantile(0.5, "lower") == 2
     assert s.quantile(0.5, "higher") == 2
 
 
@@ -215,7 +228,33 @@
 
     assert_frame_equal(
         df.groupby("id")
         .agg(pl.all().exclude("id").std())
         .select(["no_nulls", "nulls"]),
         df.select(pl.all().exclude("id").std()),
     )
+
+
+def test_err_on_implode_and_agg() -> None:
+    df = pl.DataFrame({"type": ["water", "fire", "water", "earth"]})
+
+    # this would OOB
+    with pytest.raises(
+        pl.InvalidOperationError,
+        match=r"'implode' followed by an aggregation is not allowed",
+    ):
+        df.groupby("type").agg(pl.col("type").implode().first().alias("foo"))
+
+    # implode + function should be allowed in groupby
+    assert df.groupby("type", maintain_order=True).agg(
+        pl.col("type").implode().list.head().alias("foo")
+    ).to_dict(False) == {
+        "type": ["water", "fire", "earth"],
+        "foo": [["water", "water"], ["fire"], ["earth"]],
+    }
+
+    # but not during a window function as the groups cannot be mapped back
+    with pytest.raises(
+        pl.InvalidOperationError,
+        match=r"'implode' followed by an aggregation is not allowed",
+    ):
+        df.lazy().select(pl.col("type").implode().list.head(1).over("type")).collect()
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/operations/test_apply.py` & `polars_lts_cpu-0.18.0/tests/unit/operations/test_apply.py`

 * *Files 2% similar despite different names*

```diff
@@ -313,7 +313,28 @@
     ).to_dict(False) == {
         "bin": [
             "111111111111111111111111",
             "222222222222222222222222",
             "aaaaaaaaaaaaaaaaaaaaaaaa",
         ]
     }
+
+
+def test_apply_no_dtype_set_8531() -> None:
+    assert (
+        pl.DataFrame({"a": [1]})
+        .with_columns(
+            pl.col("a").map(lambda x: x * 2).shift_and_fill(fill_value=0, periods=0)
+        )
+        .item()
+        == 2
+    )
+
+
+def test_apply_set_datetime_output_8984() -> None:
+    df = pl.DataFrame({"a": [""]})
+    payload = datetime(2001, 1, 1)
+    assert df.select(
+        pl.col("a").apply(lambda _: payload, return_dtype=pl.Datetime),
+    )[
+        "a"
+    ].to_list() == [payload]
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/operations/test_drop.py` & `polars_lts_cpu-0.18.0/tests/unit/operations/test_drop.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,7 +1,11 @@
+from typing import Any
+
+import pytest
+
 import polars as pl
 from polars.testing import assert_frame_equal
 
 
 def test_drop_explode_6641() -> None:
     df = pl.DataFrame(
         {
@@ -28,15 +32,23 @@
                 {"identifier": "chr1:10426:10429:ACC>*", "alternate": "T"},
                 {"identifier": "chr1:10426:10429:ACC>*", "alternate": "T"},
             ],
         ]
     }
 
 
-def test_drop_nulls() -> None:
+@pytest.mark.parametrize(
+    "subset",
+    [
+        "foo",
+        ["foo"],
+        {"foo"},
+    ],
+)
+def test_drop_nulls(subset: Any) -> None:
     df = pl.DataFrame(
         {
             "foo": [1, 2, 3],
             "bar": [6, None, 8],
             "ham": ["a", "b", "c"],
         }
     )
@@ -47,15 +59,15 @@
             "bar": [6, 8],
             "ham": ["a", "c"],
         }
     )
     assert_frame_equal(result, expected)
 
     # below we only drop entries if they are null in the column 'foo'
-    result = df.drop_nulls("foo")
+    result = df.drop_nulls(subset)
     assert_frame_equal(result, df)
 
 
 def test_drop() -> None:
     df = pl.DataFrame({"a": [2, 1, 3], "b": ["a", "b", "c"], "c": [1, 2, 3]})
     df = df.drop(columns="a")
     assert df.shape == (3, 2)
@@ -79,14 +91,20 @@
 def test_drop_columns() -> None:
     out = pl.DataFrame({"a": [1], "b": [2], "c": [3]}).lazy().drop(["a", "b"])
     assert out.columns == ["c"]
 
     out = pl.DataFrame({"a": [1], "b": [2], "c": [3]}).lazy().drop("a")
     assert out.columns == ["b", "c"]
 
+    out2 = pl.DataFrame({"a": [1], "b": [2], "c": [3]}).drop("a", "b")
+    assert out2.columns == ["c"]
+
+    out2 = pl.DataFrame({"a": [1], "b": [2], "c": [3]}).drop({"a"}, "b", "c")
+    assert out2.columns == []
+
 
 def test_drop_nan_ignore_null_3525() -> None:
     df = pl.DataFrame({"a": [1.0, float("NaN"), 2.0, None, 3.0, 4.0]})
     assert df.select(pl.col("a").drop_nans()).to_series().to_list() == [
         1.0,
         2.0,
         None,
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/operations/test_explode.py` & `polars_lts_cpu-0.18.0/tests/unit/operations/test_explode.py`

 * *Files 3% similar despite different names*

```diff
@@ -29,15 +29,17 @@
 
     expected = pl.DataFrame({"group": ["a", "b"], "values": [[1, 2], [2, 3, 4]]})
     assert_frame_equal(result, expected)
 
 
 def test_groupby_flatten_string() -> None:
     df = pl.DataFrame({"group": ["a", "b", "b"], "values": ["foo", "bar", "baz"]})
-    result = df.groupby("group", maintain_order=True).agg(pl.col("values").flatten())
+    result = df.groupby("group", maintain_order=True).agg(
+        pl.col("values").str.explode()
+    )
 
     expected = pl.DataFrame(
         {
             "group": ["a", "b"],
             "values": [["f", "o", "o"], ["b", "a", "r", "b", "a", "z"]],
         }
     )
@@ -119,30 +121,30 @@
         schema_overrides={"row_nr": pl.UInt32},
     )
     assert_frame_equal(df.slice(0, 10).explode(["b"]), expected)
 
 
 def test_sliced_null_explode() -> None:
     s = pl.Series("", [[1], [2], [3], [4], [], [6]])
-    assert s.slice(2, 4).arr.explode().to_list() == [3, 4, None, 6]
-    assert s.slice(2, 2).arr.explode().to_list() == [3, 4]
+    assert s.slice(2, 4).list.explode().to_list() == [3, 4, None, 6]
+    assert s.slice(2, 2).list.explode().to_list() == [3, 4]
     assert pl.Series("", [[1], [2], None, [4], [], [6]]).slice(
         2, 4
-    ).arr.explode().to_list() == [None, 4, None, 6]
+    ).list.explode().to_list() == [None, 4, None, 6]
 
     s = pl.Series("", [["a"], ["b"], ["c"], ["d"], [], ["e"]])
-    assert s.slice(2, 4).arr.explode().to_list() == ["c", "d", None, "e"]
-    assert s.slice(2, 2).arr.explode().to_list() == ["c", "d"]
+    assert s.slice(2, 4).list.explode().to_list() == ["c", "d", None, "e"]
+    assert s.slice(2, 2).list.explode().to_list() == ["c", "d"]
     assert pl.Series("", [["a"], ["b"], None, ["d"], [], ["e"]]).slice(
         2, 4
-    ).arr.explode().to_list() == [None, "d", None, "e"]
+    ).list.explode().to_list() == [None, "d", None, "e"]
 
     s = pl.Series("", [[False], [False], [True], [False], [], [True]])
-    assert s.slice(2, 2).arr.explode().to_list() == [True, False]
-    assert s.slice(2, 4).arr.explode().to_list() == [True, False, None, True]
+    assert s.slice(2, 2).list.explode().to_list() == [True, False]
+    assert s.slice(2, 4).list.explode().to_list() == [True, False, None, True]
 
 
 def test_utf8_explode() -> None:
     assert pl.Series(["foobar", None]).str.explode().to_list() == [
         "f",
         "o",
         "o",
@@ -225,15 +227,15 @@
     df = pl.DataFrame(
         data={"id": [1, 1, 1], "categories": [["a"], ["b"], ["a", "c"]]}
     ).lazy()
 
     assert (
         df.groupby("id")
         .agg(pl.col("categories"))
-        .with_columns(pl.col("categories").arr.eval(pl.element().arr.explode()))
+        .with_columns(pl.col("categories").list.eval(pl.element().list.explode()))
     ).collect().to_dict(False) == {"id": [1], "categories": [["a", "b", "a", "c"]]}
 
 
 def test_list_struct_explode_6905() -> None:
     assert pl.DataFrame(
         {
             "group": [
@@ -241,35 +243,35 @@
                 [
                     {"params": [1]},
                     {"params": []},
                 ],
             ]
         },
         schema={"group": pl.List(pl.Struct([pl.Field("params", pl.List(pl.Int32))]))},
-    )["group"].arr.explode().to_list() == [
+    )["group"].list.explode().to_list() == [
         {"params": None},
         {"params": [1]},
         {"params": []},
     ]
 
 
 def test_explode_binary() -> None:
     assert pl.Series([[1, 2], [3]]).cast(
         pl.List(pl.Binary)
-    ).arr.explode().to_list() == [
+    ).list.explode().to_list() == [
         b"1",
         b"2",
         b"3",
     ]
 
 
 def test_explode_null_list() -> None:
     assert pl.Series([["a"], None], dtype=pl.List(pl.Utf8))[
         1:2
-    ].arr.min().to_list() == [None]
+    ].list.min().to_list() == [None]
 
 
 def test_explode_invalid_element_count() -> None:
     df = pl.DataFrame(
         {
             "col1": [["X", "Y", "Z"], ["F", "G"], ["P"]],
             "col2": [["A", "B", "C"], ["C"], ["D", "E"]],
@@ -290,7 +292,13 @@
         .groupby(1)
         .agg(pl.struct("cats"))
         .explode("cats")
         .unnest("cats")
     )
     assert out["cats"].dtype == pl.Categorical
     assert out["cats"].to_list() == ["Value1", "Value2", "Value1"]
+
+
+def test_explode_inner_null() -> None:
+    expected = pl.DataFrame({"A": [None, None]}, schema={"A": pl.Null})
+    out = pl.DataFrame({"A": [[], []]}, schema={"A": pl.List(pl.Null)}).explode("A")
+    assert_frame_equal(out, expected)
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/operations/test_filter.py` & `polars_lts_cpu-0.18.0/tests/unit/operations/test_filter.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/operations/test_folds.py` & `polars_lts_cpu-0.18.0/tests/unit/operations/test_folds.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/operations/test_groupby.py` & `polars_lts_cpu-0.18.0/tests/unit/operations/test_groupby.py`

 * *Files 10% similar despite different names*

```diff
@@ -195,40 +195,14 @@
         result = df_or_lazy.groupby("a", maintain_order=True).agg(good_param)
         if lazy:
             result = result.collect()  # type: ignore[union-attr]
         assert_frame_equal(result, expected)
 
 
 @pytest.mark.parametrize("lazy", [True, False])
-def test_groupby_rolling_agg_input_types(lazy: bool) -> None:
-    df = pl.DataFrame({"index_column": [0, 1, 2, 3], "b": [1, 3, 1, 2]}).set_sorted(
-        "index_column"
-    )
-    df_or_lazy: pl.DataFrame | pl.LazyFrame = df.lazy() if lazy else df
-
-    for bad_param in bad_agg_parameters():
-        with pytest.raises(TypeError):  # noqa: PT012
-            result = df_or_lazy.groupby_rolling(
-                index_column="index_column", period="2i"
-            ).agg(bad_param)
-            if lazy:
-                result.collect()  # type: ignore[union-attr]
-
-    expected = pl.DataFrame({"index_column": [0, 1, 2, 3], "b": [1, 4, 4, 3]})
-
-    for good_param in good_agg_parameters():
-        result = df_or_lazy.groupby_rolling(
-            index_column="index_column", period="2i"
-        ).agg(good_param)
-        if lazy:
-            result = result.collect()  # type: ignore[union-attr]
-        assert_frame_equal(result, expected)
-
-
-@pytest.mark.parametrize("lazy", [True, False])
 def test_groupby_dynamic_agg_input_types(lazy: bool) -> None:
     df = pl.DataFrame({"index_column": [0, 1, 2, 3], "b": [1, 3, 1, 2]}).set_sorted(
         "index_column"
     )
     df_or_lazy: pl.DataFrame | pl.LazyFrame = df.lazy() if lazy else df
 
     for bad_param in bad_agg_parameters():
@@ -301,80 +275,14 @@
         .groupby("k", maintain_order=True)
         .agg(
             pl.col("v").take(pl.col("t").arg_max()).sqrt()
         )  # <- fails for sqrt, exp, log, pow, etc.
     ).to_dict(False) == {"k": ["a", "b"], "v": [1.4142135623730951, 2.0]}
 
 
-def test_groupby_rolling_negative_offset_3914() -> None:
-    df = pl.DataFrame(
-        {
-            "datetime": pl.date_range(datetime(2020, 1, 1), datetime(2020, 1, 5), "1d"),
-        }
-    )
-    assert df.groupby_rolling(index_column="datetime", period="2d", offset="-4d").agg(
-        pl.count().alias("count")
-    )["count"].to_list() == [0, 0, 1, 2, 2]
-
-    df = pl.DataFrame(
-        {
-            "ints": range(0, 20),
-        }
-    )
-
-    assert df.groupby_rolling(index_column="ints", period="2i", offset="-5i").agg(
-        [pl.col("ints").alias("matches")]
-    )["matches"].to_list() == [
-        [],
-        [],
-        [],
-        [0],
-        [0, 1],
-        [1, 2],
-        [2, 3],
-        [3, 4],
-        [4, 5],
-        [5, 6],
-        [6, 7],
-        [7, 8],
-        [8, 9],
-        [9, 10],
-        [10, 11],
-        [11, 12],
-        [12, 13],
-        [13, 14],
-        [14, 15],
-        [15, 16],
-    ]
-
-
-@pytest.mark.parametrize("time_zone", [None, "US/Central", "+01:00"])
-def test_groupby_rolling_negative_offset_crossing_dst(time_zone: str | None) -> None:
-    df = pl.DataFrame(
-        {
-            "datetime": pl.date_range(
-                datetime(2021, 11, 6), datetime(2021, 11, 9), "1d", time_zone=time_zone
-            ),
-            "value": [1, 4, 9, 155],
-        }
-    )
-    result = df.groupby_rolling(index_column="datetime", period="2d", offset="-1d").agg(
-        pl.col("value")
-    )
-    expected = pl.DataFrame(
-        {
-            "datetime": pl.date_range(
-                datetime(2021, 11, 6), datetime(2021, 11, 9), "1d", time_zone=time_zone
-            ),
-            "value": [[1, 4], [4, 9], [9, 155], [155]],
-        }
-    )
-    assert_frame_equal(result, expected)
-
-
 def test_groupby_signed_transmutes() -> None:
     df = pl.DataFrame({"foo": [-1, -2, -3, -4, -5], "bar": [500, 600, 700, 800, 900]})
 
     for dt in [pl.Int8, pl.Int16, pl.Int32, pl.Int64]:
         df = (
             df.with_columns([pl.col("foo").cast(dt), pl.col("bar")])
             .groupby("foo", maintain_order=True)
@@ -447,28 +355,28 @@
 @pytest.mark.parametrize(
     ("every", "period"),
     [
         ("10s", timedelta(seconds=100)),
         (timedelta(seconds=10), "100s"),
     ],
 )
-@pytest.mark.parametrize("tzinfo", [None, ZoneInfo("Asia/Kathmandu")])
+@pytest.mark.parametrize("time_zone", [None, "Asia/Kathmandu"])
 def test_groupby_dynamic_overlapping_groups_flat_apply_multiple_5038(
-    every: str | timedelta, period: str | timedelta, tzinfo: ZoneInfo | None
+    every: str | timedelta, period: str | timedelta, time_zone: str | None
 ) -> None:
     assert (
         pl.DataFrame(
             {
                 "a": [
-                    datetime(2021, 1, 1, tzinfo=tzinfo) + timedelta(seconds=2**i)
-                    for i in range(10)
+                    datetime(2021, 1, 1) + timedelta(seconds=2**i) for i in range(10)
                 ],
                 "b": [float(i) for i in range(10)],
             }
         )
+        .with_columns(pl.col("a").dt.replace_time_zone(time_zone))
         .lazy()
         .set_sorted("a")
         .groupby_dynamic("a", every=every, period=period)
         .agg([pl.col("b").var().sqrt().alias("corr")])
     ).collect().sum().to_dict(False) == pytest.approx(
         {"a": [None], "corr": [6.988674024215477]}
     )
@@ -549,25 +457,27 @@
         "literal": [[False, True], [True, True, False]],
     }
 
 
 @pytest.mark.parametrize("every", ["1h", timedelta(hours=1)])
 @pytest.mark.parametrize("tzinfo", [None, ZoneInfo("Asia/Kathmandu")])
 def test_groupby_dynamic_iter(every: str | timedelta, tzinfo: ZoneInfo | None) -> None:
+    time_zone = tzinfo.key if tzinfo is not None else None
     df = pl.DataFrame(
         {
             "datetime": [
-                datetime(2020, 1, 1, 10, 0, tzinfo=tzinfo),
-                datetime(2020, 1, 1, 10, 50, tzinfo=tzinfo),
-                datetime(2020, 1, 1, 11, 10, tzinfo=tzinfo),
+                datetime(2020, 1, 1, 10, 0),
+                datetime(2020, 1, 1, 10, 50),
+                datetime(2020, 1, 1, 11, 10),
             ],
             "a": [1, 2, 2],
             "b": [4, 5, 6],
         }
     ).set_sorted("datetime")
+    df = df.with_columns(pl.col("datetime").dt.replace_time_zone(time_zone))
 
     # Without 'by' argument
     result1 = [
         (name, data.shape)
         for name, data in df.groupby_dynamic("datetime", every=every, closed="left")
     ]
     expected1 = [
@@ -596,14 +506,15 @@
 def test_groupby_dynamic_lazy(every: str | timedelta, tzinfo: ZoneInfo | None) -> None:
     ldf = pl.LazyFrame(
         {
             "time": pl.date_range(
                 start=datetime(2021, 12, 16, tzinfo=tzinfo),
                 end=datetime(2021, 12, 16, 2, tzinfo=tzinfo),
                 interval="30m",
+                eager=True,
             ),
             "n": range(5),
         }
     )
     df = (
         ldf.groupby_dynamic("time", every=every, closed="right")
         .agg(
@@ -643,45 +554,45 @@
         ]
     )
     assert df.groupby("group").agg(pl.col("data").mean()).sort(by="group").to_dict(
         False
     ) == {"group": [1, 2], "data": [10000000.0, 10000000.0]}
 
 
-@pytest.mark.parametrize("tzinfo", [None, ZoneInfo("Asia/Kathmandu")])
+@pytest.mark.parametrize("time_zone", [None, "Asia/Kathmandu"])
 def test_groupby_dynamic_elementwise_following_mean_agg_6904(
-    tzinfo: ZoneInfo | None,
+    time_zone: str | None,
 ) -> None:
     df = (
         pl.DataFrame(
             {
                 "a": [
-                    datetime(2021, 1, 1, tzinfo=tzinfo) + timedelta(seconds=2**i)
-                    for i in range(5)
+                    datetime(2021, 1, 1) + timedelta(seconds=2**i) for i in range(5)
                 ],
                 "b": [float(i) for i in range(5)],
             }
         )
+        .with_columns(pl.col("a").dt.replace_time_zone(time_zone))
         .lazy()
         .set_sorted("a")
         .groupby_dynamic("a", every="10s", period="100s")
         .agg([pl.col("b").mean().sin().alias("c")])
         .collect()
     )
     assert_frame_equal(
         df,
         pl.DataFrame(
             {
                 "a": [
-                    datetime(2021, 1, 1, 0, 0, tzinfo=tzinfo),
-                    datetime(2021, 1, 1, 0, 0, 10, tzinfo=tzinfo),
+                    datetime(2021, 1, 1, 0, 0),
+                    datetime(2021, 1, 1, 0, 0, 10),
                 ],
                 "c": [0.9092974268256817, -0.7568024953079282],
             }
-        ),
+        ).with_columns(pl.col("a").dt.replace_time_zone(time_zone)),
     )
 
 
 def test_groupby_multiple_column_reference() -> None:
     # Issue #7181
     df = pl.DataFrame(
         {
@@ -722,7 +633,206 @@
     result = df.groupby("b", maintain_order=True).agg(
         getattr(pl.col("a").filter(pl.col("b") != 2), aggregation)(*args)
     )
     expected = pl.DataFrame({"b": [1, 2], "a": expected_values}).with_columns(
         pl.col("a").cast(expected_dtype)
     )
     assert_frame_equal(result, expected)
+
+
+def test_perfect_hash_table_null_values_8663() -> None:
+    s = pl.Series(
+        "a",
+        [
+            "3",
+            "41",
+            "17",
+            "5",
+            "26",
+            "27",
+            "43",
+            "45",
+            "41",
+            "13",
+            "45",
+            "48",
+            "17",
+            "22",
+            "31",
+            "25",
+            "28",
+            "13",
+            "7",
+            "26",
+            "17",
+            "4",
+            "43",
+            "47",
+            "30",
+            "28",
+            "8",
+            "27",
+            "6",
+            "7",
+            "26",
+            "11",
+            "37",
+            "29",
+            "49",
+            "20",
+            "29",
+            "28",
+            "23",
+            "9",
+            None,
+            "38",
+            "19",
+            "7",
+            "38",
+            "3",
+            "30",
+            "37",
+            "41",
+            "5",
+            "16",
+            "26",
+            "31",
+            "6",
+            "25",
+            "11",
+            "17",
+            "31",
+            "31",
+            "20",
+            "26",
+            None,
+            "39",
+            "10",
+            "38",
+            "4",
+            "39",
+            "15",
+            "13",
+            "35",
+            "38",
+            "11",
+            "39",
+            "11",
+            "48",
+            "36",
+            "18",
+            "11",
+            "34",
+            "16",
+            "28",
+            "9",
+            "37",
+            "8",
+            "17",
+            "48",
+            "44",
+            "28",
+            "25",
+            "30",
+            "37",
+            "30",
+            "18",
+            "12",
+            None,
+            "27",
+            "10",
+            "3",
+            "16",
+            "27",
+            "6",
+        ],
+        dtype=pl.Categorical,
+    )
+
+    assert s.to_frame("a").groupby("a").agg(pl.col("a").alias("agg")).to_dict(
+        False
+    ) == {
+        "a": [
+            "3",
+            "41",
+            "17",
+            "5",
+            "26",
+            "27",
+            "43",
+            "45",
+            "13",
+            "48",
+            "22",
+            "31",
+            "25",
+            "28",
+            "7",
+            "4",
+            "47",
+            "30",
+            "8",
+            "6",
+            "11",
+            "37",
+            "29",
+            "49",
+            "20",
+            "23",
+            "9",
+            "38",
+            "19",
+            "16",
+            "39",
+            "10",
+            "15",
+            "35",
+            "36",
+            "18",
+            "34",
+            "44",
+            "12",
+            None,
+        ],
+        "agg": [
+            ["3", "3", "3"],
+            ["41", "41", "41"],
+            ["17", "17", "17", "17", "17"],
+            ["5", "5"],
+            ["26", "26", "26", "26", "26"],
+            ["27", "27", "27", "27"],
+            ["43", "43"],
+            ["45", "45"],
+            ["13", "13", "13"],
+            ["48", "48", "48"],
+            ["22"],
+            ["31", "31", "31", "31"],
+            ["25", "25", "25"],
+            ["28", "28", "28", "28", "28"],
+            ["7", "7", "7"],
+            ["4", "4"],
+            ["47"],
+            ["30", "30", "30", "30"],
+            ["8", "8"],
+            ["6", "6", "6"],
+            ["11", "11", "11", "11", "11"],
+            ["37", "37", "37", "37"],
+            ["29", "29"],
+            ["49"],
+            ["20", "20"],
+            ["23"],
+            ["9", "9"],
+            ["38", "38", "38", "38"],
+            ["19"],
+            ["16", "16", "16"],
+            ["39", "39", "39"],
+            ["10", "10"],
+            ["15"],
+            ["35"],
+            ["36"],
+            ["18", "18"],
+            ["34"],
+            ["44"],
+            ["12"],
+            [None, None, None],
+        ],
+    }
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/operations/test_is_in.py` & `polars_lts_cpu-0.18.0/tests/unit/operations/test_is_in.py`

 * *Files 4% similar despite different names*

```diff
@@ -6,21 +6,21 @@
 
 import polars as pl
 
 
 def test_struct_logical_is_in() -> None:
     df1 = pl.DataFrame(
         {
-            "x": pl.date_range(date(2022, 1, 1), date(2022, 1, 7)),
+            "x": pl.date_range(date(2022, 1, 1), date(2022, 1, 7), eager=True),
             "y": [0, 4, 6, 2, 3, 4, 5],
         }
     )
     df2 = pl.DataFrame(
         {
-            "x": pl.date_range(date(2022, 1, 3), date(2022, 1, 9)),
+            "x": pl.date_range(date(2022, 1, 3), date(2022, 1, 9), eager=True),
             "y": [6, 2, 3, 4, 5, 0, 1],
         }
     )
 
     s1 = df1.select(pl.struct(["x", "y"])).to_series()
     s2 = df2.select(pl.struct(["x", "y"])).to_series()
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/operations/test_join.py` & `polars_lts_cpu-0.18.0/tests/unit/operations/test_join.py`

 * *Files 2% similar despite different names*

```diff
@@ -501,14 +501,20 @@
         "B": [4, 500, 6, 700],
     }
     df1 = pl.DataFrame({"a": [1, 2, 3], "b": [4, 5, 6]})
     df2 = pl.DataFrame({"a": [2, 3], "b": [8, 9]})
 
     assert df1.update(df2, on="a").to_dict(False) == {"a": [1, 2, 3], "b": [4, 8, 9]}
 
+    a = pl.DataFrame({"a": [1, 2, 3]})
+    b = pl.DataFrame({"b": [4, 5]})
+    c = a.update(b)
+
+    assert c.rows() == a.rows()
+
 
 @typing.no_type_check
 def test_join_frame_consistency() -> None:
     df = pl.DataFrame({"A": [1, 2, 3]})
     ldf = pl.DataFrame({"A": [1, 2, 5]}).lazy()
 
     with pytest.raises(TypeError, match="Expected 'other'.* LazyFrame"):
@@ -546,7 +552,20 @@
     }
     assert df1.join(df2, on="x", how="anti").to_dict(False) == {"x": [1]}
     assert df1.join(df2, on="x", how="semi").to_dict(False) == {"x": [0, 0]}
     assert df1.join(df2, on="x", how="outer").to_dict(False) == {
         "x": [0, 0, 1, None],
         "y": [0, 0, None, 1],
     }
+
+
+@typing.no_type_check
+def test_outer_join_list_() -> None:
+    schema = {"id": pl.Int64, "vals": pl.List(pl.Float64)}
+
+    df1 = pl.DataFrame({"id": [1], "vals": [[]]}, schema=schema)
+    df2 = pl.DataFrame({"id": [2, 3], "vals": [[], [4]]}, schema=schema)
+    assert df1.join(df2, on="id", how="outer").to_dict(False) == {
+        "id": [2, 3, 1],
+        "vals": [None, None, []],
+        "vals_right": [[], [4.0], None],
+    }
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/operations/test_join_asof.py` & `polars_lts_cpu-0.18.0/tests/unit/operations/test_join_asof.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,8 +1,9 @@
-from datetime import datetime
+from datetime import date, datetime
+from typing import Any
 
 import numpy as np
 
 import polars as pl
 from polars.testing import assert_frame_equal
 
 
@@ -315,15 +316,17 @@
         "df2_date": [None, None, None, 20221012, 20221015],
         "df1_date": [20221011, 20221012, 20221013, 20221014, 20221016],
     }
 
 
 def test_asof_join_by_logical_types() -> None:
     dates = (
-        pl.date_range(datetime(2022, 1, 1), datetime(2022, 1, 2), interval="2h")
+        pl.date_range(
+            datetime(2022, 1, 1), datetime(2022, 1, 2), interval="2h", eager=True
+        )
         .cast(pl.Datetime("ns"))
         .head(9)
     )
     x = pl.DataFrame({"a": dates, "b": map(float, range(9)), "c": ["1", "2", "3"] * 3})
     assert x.join_asof(x, on=pl.col("b").set_sorted(), by=["c", "a"]).to_dict(
         False
     ) == {
@@ -346,7 +349,140 @@
 def test_join_asof_projection_7481() -> None:
     ldf1 = pl.DataFrame({"a": [1, 2, 2], "b": "bleft"}).lazy().set_sorted("a")
     ldf2 = pl.DataFrame({"a": 2, "b": [1, 2, 2]}).lazy().set_sorted("b")
 
     assert (
         ldf1.join_asof(ldf2, left_on="a", right_on="b").select("a", "b")
     ).collect().to_dict(False) == {"a": [1, 2, 2], "b": ["bleft", "bleft", "bleft"]}
+
+
+def test_asof_join_sorted_by_group(capsys: Any) -> None:
+    df1 = pl.DataFrame(
+        {
+            "key": ["a", "a", "a", "b", "b", "b"],
+            "asof_key": [2.0, 1.0, 3.0, 1.0, 2.0, 3.0],
+            "a": [102, 101, 103, 104, 105, 106],
+        }
+    ).sort(by=["key", "asof_key"])
+
+    df2 = pl.DataFrame(
+        {
+            "key": ["a", "a", "a", "b", "b", "b"],
+            "asof_key": [0.9, 1.9, 2.9, 0.9, 1.9, 2.9],
+            "b": [201, 202, 203, 204, 205, 206],
+        }
+    ).sort(by=["key", "asof_key"])
+
+    expected = pl.DataFrame(
+        [
+            pl.Series("key", ["a", "a", "a", "b", "b", "b"], dtype=pl.Utf8),
+            pl.Series("asof_key", [1.0, 2.0, 3.0, 1.0, 2.0, 3.0], dtype=pl.Float64),
+            pl.Series("a", [101, 102, 103, 104, 105, 106], dtype=pl.Int64),
+            pl.Series("b", [201, 202, 203, 204, 205, 206], dtype=pl.Int64),
+        ]
+    )
+
+    out = df1.join_asof(df2, on="asof_key", by="key")
+    assert_frame_equal(out, expected)
+
+    _, err = capsys.readouterr()
+    assert "is not explicitly sorted" not in err
+
+
+def test_asof_join_nearest() -> None:
+    df1 = pl.DataFrame(
+        {
+            "asof_key": [-1, 1, 2, 4, 6],
+            "a": [1, 2, 3, 4, 5],
+        }
+    ).sort(by="asof_key")
+
+    df2 = pl.DataFrame(
+        {
+            "asof_key": [1, 2, 4, 5],
+            "b": [1, 2, 3, 4],
+        }
+    ).sort(by="asof_key")
+
+    expected = pl.DataFrame(
+        {"asof_key": [-1, 1, 2, 4, 6], "a": [1, 2, 3, 4, 5], "b": [1, 1, 2, 3, 4]}
+    )
+
+    out = df1.join_asof(df2, on="asof_key", strategy="nearest")
+    assert_frame_equal(out, expected)
+
+
+def test_asof_join_nearest_by() -> None:
+    df1 = pl.DataFrame(
+        {
+            "asof_key": [-1, 1, 2, 6, 1],
+            "group": [1, 1, 1, 2, 2],
+            "a": [1, 2, 3, 2, 5],
+        }
+    ).sort(by=["group", "asof_key"])
+
+    df2 = pl.DataFrame(
+        {
+            "asof_key": [1, 2, 5, 1],
+            "group": [1, 1, 2, 2],
+            "b": [1, 2, 3, 4],
+        }
+    ).sort(by=["group", "asof_key"])
+
+    expected = pl.DataFrame(
+        {
+            "asof_key": [-1, 1, 2, 6, 1],
+            "group": [1, 1, 1, 2, 2],
+            "a": [1, 2, 3, 2, 5],
+            "b": [1, 1, 2, 3, 4],
+        }
+    ).sort(by=["group", "asof_key"])
+
+    out = df1.join_asof(df2, on="asof_key", by="group", strategy="nearest")
+    assert_frame_equal(out, expected)
+
+
+def test_asof_join_nearest_by_date() -> None:
+    df1 = pl.DataFrame(
+        {
+            "asof_key": [
+                date(2019, 12, 30),
+                date(2020, 1, 1),
+                date(2020, 1, 2),
+                date(2020, 1, 6),
+                date(2020, 1, 1),
+            ],
+            "group": [1, 1, 1, 2, 2],
+            "a": [1, 2, 3, 2, 5],
+        }
+    ).sort(by=["group", "asof_key"])
+
+    df2 = pl.DataFrame(
+        {
+            "asof_key": [
+                date(2020, 1, 1),
+                date(2020, 1, 2),
+                date(2020, 1, 5),
+                date(2020, 1, 1),
+            ],
+            "group": [1, 1, 2, 2],
+            "b": [1, 2, 3, 4],
+        }
+    ).sort(by=["group", "asof_key"])
+
+    expected = pl.DataFrame(
+        {
+            "asof_key": [
+                date(2019, 12, 30),
+                date(2020, 1, 1),
+                date(2020, 1, 2),
+                date(2020, 1, 6),
+                date(2020, 1, 1),
+            ],
+            "group": [1, 1, 1, 2, 2],
+            "a": [1, 2, 3, 2, 5],
+            "b": [1, 1, 2, 3, 4],
+        }
+    ).sort(by=["group", "asof_key"])
+
+    out = df1.join_asof(df2, on="asof_key", by="group", strategy="nearest")
+    assert_frame_equal(out, expected)
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/operations/test_melt.py` & `polars_lts_cpu-0.18.0/tests/unit/operations/test_melt.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/operations/test_pivot.py` & `polars_lts_cpu-0.18.0/tests/unit/operations/test_pivot.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/operations/test_rolling.py` & `polars_lts_cpu-0.18.0/tests/unit/operations/test_rolling.py`

 * *Files 2% similar despite different names*

```diff
@@ -92,15 +92,15 @@
             0.0,
             1.9001038154942962,
             0.16923763134384154,
         ]
     )
 
 
-@pytest.mark.parametrize("time_zone", [None, "US/Central", "+01:00"])
+@pytest.mark.parametrize("time_zone", [None, "US/Central"])
 @pytest.mark.parametrize(
     ("rolling_fn", "expected_values"),
     [
         ("rolling_mean", [None, 1.0, 2.0, 3.0, 4.0, 5.0]),
         ("rolling_sum", [None, 1, 2, 3, 4, 5]),
         ("rolling_min", [None, 1, 2, 3, 4, 5]),
         ("rolling_max", [None, 1, 2, 3, 4, 5]),
@@ -108,15 +108,15 @@
         ("rolling_var", [None, 0.0, 0.0, 0.0, 0.0, 0.0]),
     ],
 )
 def test_rolling_crossing_dst(
     time_zone: str | None, rolling_fn: str, expected_values: list[int | None | float]
 ) -> None:
     ts = pl.date_range(
-        datetime(2021, 11, 5), datetime(2021, 11, 10), "1d", time_zone="UTC"
+        datetime(2021, 11, 5), datetime(2021, 11, 10), "1d", time_zone="UTC", eager=True
     ).dt.replace_time_zone(time_zone)
     df = pl.DataFrame({"ts": ts, "value": [1, 2, 3, 4, 5, 6]})
     result = df.with_columns(getattr(pl.col("value"), rolling_fn)("1d", by="ts"))
     expected = pl.DataFrame({"ts": ts, "value": expected_values})
     assert_frame_equal(result, expected)
 
 
@@ -333,17 +333,17 @@
 
 def test_overlapping_groups_4628() -> None:
     df = pl.DataFrame(
         {
             "index": [1, 2, 3, 4, 5, 6],
             "val": [10, 20, 40, 70, 110, 160],
         }
-    ).set_sorted("index")
+    )
     assert (
-        df.groupby_rolling(index_column="index", period="3i").agg(
+        df.groupby_rolling(index_column=pl.col("index").set_sorted(), period="3i").agg(
             [
                 pl.col("val").diff(n=1).alias("val.diff"),
                 (pl.col("val") - pl.col("val").shift(1)).alias("val - val.shift"),
             ]
         )
     ).to_dict(False) == {
         "index": [1, 2, 3, 4, 5, 6],
@@ -410,17 +410,19 @@
     df = pl.DataFrame(
         (
             pl.date_range(
                 datetime(2020, 1, 1),
                 datetime(2020, 1, 10),
                 timedelta(days=1),
                 time_unit="ns",
-                name="datetime",
-            ).dt.replace_time_zone("UTC"),
-            pl.Series("value", pl.arange(1, 11, eager=True)),
+                eager=True,
+            )
+            .alias("datetime")
+            .dt.replace_time_zone("UTC"),
+            pl.arange(1, 11, eager=True).alias("value"),
         )
     )
 
     for every, offset in (("3d", "-1d"), (timedelta(days=3), timedelta(days=-1))):
         assert (
             df.groupby_dynamic(
                 "datetime",
@@ -434,15 +436,15 @@
 
 
 @pytest.mark.parametrize("tzinfo", [None, ZoneInfo("Asia/Kathmandu")])
 def test_groupby_dynamic_startby_5599(tzinfo: ZoneInfo | None) -> None:
     # start by datapoint
     start = datetime(2022, 12, 16, tzinfo=tzinfo)
     stop = datetime(2022, 12, 16, hour=3, tzinfo=tzinfo)
-    df = pl.DataFrame({"date": pl.date_range(start, stop, "30m")})
+    df = pl.DataFrame({"date": pl.date_range(start, stop, "30m", eager=True)})
 
     assert df.groupby_dynamic(
         "date",
         every="31m",
         include_boundaries=True,
         truncate=False,
         start_by="datapoint",
@@ -470,30 +472,31 @@
             datetime(2022, 12, 16, 2, 0, tzinfo=tzinfo),
             datetime(2022, 12, 16, 2, 30, tzinfo=tzinfo),
             datetime(2022, 12, 16, 3, 0, tzinfo=tzinfo),
         ],
         "count": [2, 1, 1, 1, 1, 1],
     }
 
-    # start by week
+    # start by monday
     start = datetime(2022, 1, 1, tzinfo=tzinfo)
     stop = datetime(2022, 1, 12, 7, tzinfo=tzinfo)
 
-    df = pl.DataFrame({"date": pl.date_range(start, stop, "12h")}).with_columns(
-        pl.col("date").dt.weekday().alias("day")
-    )
+    df = pl.DataFrame(
+        {"date": pl.date_range(start, stop, "12h", eager=True)}
+    ).with_columns(pl.col("date").dt.weekday().alias("day"))
 
-    assert df.groupby_dynamic(
+    result = df.groupby_dynamic(
         "date",
         every="1w",
         period="3d",
         include_boundaries=True,
         start_by="monday",
         truncate=False,
-    ).agg([pl.count(), pl.col("day").first().alias("data_day")]).to_dict(False) == {
+    ).agg([pl.count(), pl.col("day").first().alias("data_day")])
+    assert result.to_dict(False) == {
         "_lower_boundary": [
             datetime(2022, 1, 3, 0, 0, tzinfo=tzinfo),
             datetime(2022, 1, 10, 0, 0, tzinfo=tzinfo),
         ],
         "_upper_boundary": [
             datetime(2022, 1, 6, 0, 0, tzinfo=tzinfo),
             datetime(2022, 1, 13, 0, 0, tzinfo=tzinfo),
@@ -501,14 +504,39 @@
         "date": [
             datetime(2022, 1, 3, 0, 0, tzinfo=tzinfo),
             datetime(2022, 1, 10, 0, 0, tzinfo=tzinfo),
         ],
         "count": [6, 5],
         "data_day": [1, 1],
     }
+    # start by saturday
+    result = df.groupby_dynamic(
+        "date",
+        every="1w",
+        period="3d",
+        include_boundaries=True,
+        start_by="saturday",
+        truncate=False,
+    ).agg([pl.count(), pl.col("day").first().alias("data_day")])
+    assert result.to_dict(False) == {
+        "_lower_boundary": [
+            datetime(2022, 1, 1, 0, 0, tzinfo=tzinfo),
+            datetime(2022, 1, 8, 0, 0, tzinfo=tzinfo),
+        ],
+        "_upper_boundary": [
+            datetime(2022, 1, 4, 0, 0, tzinfo=tzinfo),
+            datetime(2022, 1, 11, 0, 0, tzinfo=tzinfo),
+        ],
+        "date": [
+            datetime(2022, 1, 1, 0, 0, tzinfo=tzinfo),
+            datetime(2022, 1, 8, 0, 0, tzinfo=tzinfo),
+        ],
+        "count": [6, 6],
+        "data_day": [6, 6],
+    }
 
 
 def test_groupby_dynamic_by_monday_and_offset_5444() -> None:
     df = pl.DataFrame(
         {
             "date": [
                 "2022-11-01",
@@ -601,7 +629,23 @@
     ) == {
         "time": [0, 1, 2, 3],
         "value": [[0, 1, 2], [1, 2, 3], [2, 3], [3]],
         "min_value": [0, 1, 2, 3],
         "max_value": [2, 3, 3, 3],
         "sum_value": [3, 6, 5, 3],
     }
+
+
+def test_rolling_cov_corr() -> None:
+    df = pl.DataFrame({"x": [3, 3, 3, 5, 8], "y": [3, 4, 4, 4, 8]})
+
+    assert (
+        str(
+            df.select(
+                [
+                    pl.rolling_cov("x", "y", window_size=3).alias("cov"),
+                    pl.rolling_corr("x", "y", window_size=3).alias("corr"),
+                ]
+            ).to_dict(False)
+        )
+        == "{'cov': [None, None, 0.0, 0.0, 5.333333333333336], 'corr': [None, None, nan, nan, 0.9176629354822473]}"
+    )
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/operations/test_sort.py` & `polars_lts_cpu-0.18.0/tests/unit/operations/test_sort.py`

 * *Files 1% similar despite different names*

```diff
@@ -205,14 +205,37 @@
 
 
 def test_sorted_flag() -> None:
     s = pl.arange(0, 7, eager=True)
     assert s.flags["SORTED_ASC"]
     assert s.reverse().flags["SORTED_DESC"]
     assert pl.Series([b"a"]).set_sorted().flags["SORTED_ASC"]
+    assert (
+        pl.Series([date(2020, 1, 1), date(2020, 1, 2)])
+        .set_sorted()
+        .cast(pl.Datetime)
+        .flags["SORTED_ASC"]
+    )
+
+    # empty
+    q = pl.LazyFrame(
+        schema={
+            "store_id": pl.UInt16,
+            "item_id": pl.UInt32,
+            "timestamp": pl.Datetime,
+        }
+    ).sort("timestamp")
+
+    assert q.collect()["timestamp"].flags["SORTED_ASC"]
+    assert q.collect(streaming=True)["timestamp"].flags["SORTED_ASC"]
+
+    # top-k/bottom-k
+    df = pl.DataFrame({"foo": [56, 2, 3]})
+    assert df.top_k(2, by="foo")["foo"].flags["SORTED_DESC"]
+    assert df.bottom_k(2, by="foo")["foo"].flags["SORTED_ASC"]
 
     # ensure we don't panic for these types
     # struct
     pl.Series([{"a": 1}]).set_sorted(descending=True)
     # list
     pl.Series([[{"a": 1}]]).set_sorted(descending=True)
     # object
@@ -398,21 +421,21 @@
         "sorted_1": [0, 2, 1, 4, 5, 3],
         "sorted_2": [None, 1, 0, 3, 4, None],
     }
 
 
 def test_merge_sorted() -> None:
     df_a = (
-        pl.date_range(datetime(2022, 1, 1), datetime(2022, 12, 1), "1mo")
+        pl.date_range(datetime(2022, 1, 1), datetime(2022, 12, 1), "1mo", eager=True)
         .to_frame("range")
         .with_row_count()
     )
 
     df_b = (
-        pl.date_range(datetime(2022, 1, 1), datetime(2022, 12, 1), "2mo")
+        pl.date_range(datetime(2022, 1, 1), datetime(2022, 12, 1), "2mo", eager=True)
         .to_frame("range")
         .with_row_count()
         .with_columns(pl.col("row_nr") * 10)
     )
     out = df_a.merge_sorted(df_b, key="range")
     assert out["range"].is_sorted()
     assert out.to_dict(False) == {
@@ -516,15 +539,15 @@
                 df_pd.sort_values(["strs", "vals"], ascending=not descending)
             ),
         )
 
 
 @pytest.mark.slow()
 def test_streaming_sort_multiple_columns(monkeypatch: Any, capfd: Any) -> None:
-    monkeypatch.setenv("POLARS_FORCE_OOC_SORT", "1")
+    monkeypatch.setenv("POLARS_FORCE_OOC", "1")
     monkeypatch.setenv("POLARS_VERBOSE", "1")
     df = get_str_ints_df(1000)
 
     out = df.lazy().sort(["strs", "vals"]).collect(streaming=True)
     assert_frame_equal(out, out.sort(["strs", "vals"]))
     err = capfd.readouterr().err
     assert "OOC sort forced" in err
@@ -658,7 +681,20 @@
     )
     # this triggers fast path as head is equal to n-rows
     assert df.lazy().sort("b").head(3).collect().to_dict(False) == {
         "a": [None, 2, 1],
         "b": [4.0, 5.0, 6.0],
         "c": ["b", "c", "a"],
     }
+
+
+def test_sorted_flag_groupby_dynamic() -> None:
+    df = pl.DataFrame({"ts": [date(2020, 1, 1), date(2020, 1, 2)], "val": [1, 2]})
+    assert (
+        (
+            df.groupby_dynamic(pl.col("ts").set_sorted(), every="1d").agg(
+                pl.col("val").sum()
+            )
+        )
+        .to_series()
+        .flags["SORTED_ASC"]
+    )
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/operations/test_statistics.py` & `polars_lts_cpu-0.18.0/tests/unit/operations/test_statistics.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/operations/test_transpose.py` & `polars_lts_cpu-0.18.0/tests/unit/operations/test_transpose.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,11 +1,14 @@
 from datetime import date, datetime
 from typing import Iterator
 
+import pytest
+
 import polars as pl
+from polars.exceptions import ComputeError
 from polars.testing import assert_frame_equal
 
 
 def test_transpose_supertype() -> None:
     df = pl.DataFrame({"a": [1, 2, 3], "b": ["foo", "bar", "ham"]})
     result = df.transpose()
     expected = pl.DataFrame(
@@ -14,14 +17,29 @@
             "column_1": ["2", "bar"],
             "column_2": ["3", "ham"],
         }
     )
     assert_frame_equal(result, expected)
 
 
+def test_transpose_tz_naive_and_tz_aware() -> None:
+    df = pl.DataFrame(
+        {
+            "a": [datetime(2020, 1, 1)],
+            "b": [datetime(2020, 1, 1)],
+        }
+    )
+    df = df.with_columns(pl.col("b").dt.replace_time_zone("Asia/Kathmandu"))
+    with pytest.raises(
+        ComputeError,
+        match=r"failed to determine supertype of datetime\[s\] and datetime\[s, Asia/Kathmandu\]",
+    ):
+        df.transpose()
+
+
 def test_transpose_struct() -> None:
     df = pl.DataFrame(
         {
             "a": ["foo", "bar", "ham"],
             "b": [
                 {"a": date(2022, 1, 1), "b": True},
                 {"a": date(2022, 1, 2), "b": False},
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/operations/test_unique.py` & `polars_lts_cpu-0.18.0/tests/unit/operations/test_unique.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/operations/test_window.py` & `polars_lts_cpu-0.18.0/tests/unit/operations/test_window.py`

 * *Files 14% similar despite different names*

```diff
@@ -89,27 +89,22 @@
             "groups": ["A", "A", "B", "B", "B"],
             "groups_not_sorted": ["A", "B", "A", "B", "A"],
             "values": range(5),
         }
     ).with_columns(
         [
             pl.col("values")
-            .implode()
-            .over("groups")
+            .over("groups", mapping_strategy="join")
             .alias("values_list"),  # aggregation to list + join
             pl.col("values")
-            .implode()
-            .over("groups")
-            .flatten()
+            .over("groups", mapping_strategy="explode")
             .alias("values_flat"),  # aggregation to list + explode and concat back
             pl.col("values")
             .reverse()
-            .implode()
-            .over("groups")
-            .flatten()
+            .over("groups", mapping_strategy="explode")
             .alias("values_rev"),  # use flatten to reverse within a group
         ]
     )
 
     assert out["values_list"].to_list() == [
         [0, 1],
         [0, 1],
@@ -119,22 +114,23 @@
     ]
     assert out["values_flat"].to_list() == [0, 1, 2, 3, 4]
     assert out["values_rev"].to_list() == [1, 0, 4, 3, 2]
 
 
 def test_arange_no_rows() -> None:
     df = pl.DataFrame({"x": [5, 5, 4, 4, 2, 2]})
-    expr = pl.arange(0, pl.count()).over("x")  # type: ignore[union-attr]
+    expr = pl.arange(0, pl.count()).over("x")
     out = df.with_columns(expr)
     assert_frame_equal(
         out, pl.DataFrame({"x": [5, 5, 4, 4, 2, 2], "arange": [0, 1, 0, 1, 0, 1]})
     )
 
     df = pl.DataFrame({"x": []})
     out = df.with_columns(expr)
+    print(out)
     expected = pl.DataFrame(
         {"x": [], "arange": []}, schema={"x": pl.Float32, "arange": pl.Int64}
     )
     assert_frame_equal(out, expected)
 
 
 def test_no_panic_on_nan_3067() -> None:
@@ -246,17 +242,15 @@
     ].to_list() == [None, [1], [1], [2]]
 
     # filling with None is allowed, but does not make any sense
     # as it is the same as shift.
     # that's why we don't add it to the allowed types.
     assert (
         df.select(
-            pl.col("col_list")
-            .shift_and_fill(None, periods=1)  # type: ignore[arg-type]
-            .alias("list_shifted")
+            pl.col("col_list").shift_and_fill(None, periods=1).alias("list_shifted")
         )
     )["list_shifted"].to_list() == [None, [1], [1], [2]]
 
     assert (
         df.select(
             pl.col("col_list").shift_and_fill([], periods=1).alias("list_shifted")
         )
@@ -333,7 +327,81 @@
     result = (
         df.drop_nulls()
         .with_columns(pl.col("a").set_sorted())
         .select(pl.col("a").sum().over("a"))
         .get_column("a")
     )
     assert_series_equal(result, expected)
+
+
+def test_window_function_implode_contention_8536() -> None:
+    df = pl.DataFrame(
+        data={
+            "policy": ["a", "b", "c", "c", "d", "d", "d", "d", "e", "e"],
+            "memo": ["LE", "RM", "", "", "", "LE", "", "", "", "RM"],
+        },
+        schema={"policy": pl.Utf8, "memo": pl.Utf8},
+    )
+
+    assert df.select(
+        [
+            (pl.lit("LE").is_in(pl.col("memo").over("policy", mapping_strategy="join")))
+            | (
+                pl.lit("RM").is_in(
+                    pl.col("memo").over("policy", mapping_strategy="join")
+                )
+            )
+        ]
+    ).to_series().to_list() == [
+        True,
+        True,
+        False,
+        False,
+        True,
+        True,
+        True,
+        True,
+        True,
+        True,
+    ]
+
+
+def test_cached_windows_sync_8803() -> None:
+    assert (
+        pl.DataFrame(
+            [
+                pl.Series("id", [4, 5, 4, 6, 4, 5], dtype=pl.Int64),
+                pl.Series(
+                    "is_valid",
+                    [True, False, False, False, False, False],
+                    dtype=pl.Boolean,
+                ),
+            ]
+        )
+        .with_columns(
+            a=pl.lit(True).is_in(pl.col("is_valid")).over("id"),
+            b=pl.col("is_valid").sum().gt(0).over("id"),
+        )
+        .sum()
+    ).to_dict(False) == {"id": [28], "is_valid": [1], "a": [3], "b": [3]}
+
+
+def test_window_filtered_aggregation() -> None:
+    df = pl.DataFrame(
+        {
+            "group": ["A", "A", "B", "B"],
+            "field1": [2, 4, 6, 8],
+            "flag": [1, 0, 1, 1],
+        }
+    )
+    out = df.with_columns(
+        pl.col("field1").filter(pl.col("flag") == 1).mean().over("group").alias("mean")
+    )
+    expected = pl.DataFrame(
+        {
+            "group": ["A", "A", "B", "B"],
+            "field1": [2, 4, 6, 8],
+            "flag": [1, 0, 1, 1],
+            "mean": [2.0, 2.0, 7.0, 7.0],
+        }
+    )
+    assert_frame_equal(out, expected)
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_api.py` & `polars_lts_cpu-0.18.0/tests/unit/test_api.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_arity.py` & `polars_lts_cpu-0.18.0/tests/unit/test_arity.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_cfg.py` & `polars_lts_cpu-0.18.0/tests/unit/test_cfg.py`

 * *Files 0% similar despite different names*

```diff
@@ -8,16 +8,16 @@
 import polars as pl
 from polars.config import _get_float_fmt
 from polars.testing import assert_frame_equal
 
 
 @pytest.fixture(autouse=True)
 def _environ() -> Iterator[None]:
-    """Fixture to restore the environment variables/state after the test."""
-    with pl.StringCache(), pl.Config():
+    """Fixture to restore the environment after/during tests."""
+    with pl.StringCache(), pl.Config(restore_defaults=True):
         yield
 
 
 def test_ascii_tables() -> None:
     df = pl.DataFrame({"a": [1, 2, 3], "b": [4, 5, 6], "c": [7, 8, 9]})
 
     # note: expect to render ascii only within the given scope
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_constructors.py` & `polars_lts_cpu-0.18.0/tests/unit/test_constructors.py`

 * *Files 3% similar despite different names*

```diff
@@ -10,14 +10,15 @@
 import numpy as np
 import pandas as pd
 import pyarrow as pa
 import pytest
 
 import polars as pl
 from polars.dependencies import _ZONEINFO_AVAILABLE, dataclasses, pydantic
+from polars.exceptions import TimeZoneAwareConstructorWarning
 from polars.testing import assert_frame_equal, assert_series_equal
 from polars.utils._construction import type_hints
 
 if TYPE_CHECKING:
     from polars.datatypes import PolarsDataType
 
 if sys.version_info >= (3, 9):
@@ -250,14 +251,42 @@
         }
         assert df.rows() == raw_data
 
         # cover a miscellaneous edge-case when detecting the annotations
         assert type_hints(obj=type(None)) == {}
 
 
+def test_init_structured_objects_unhashable() -> None:
+    # cover an edge-case with namedtuple fields that aren't hashable
+
+    class Test(NamedTuple):
+        dt: datetime
+        info: dict[str, int]
+
+    test_data = [
+        Test(datetime(2017, 1, 1), {"a": 1, "b": 2}),
+        Test(datetime(2017, 1, 2), {"a": 2, "b": 2}),
+    ]
+    df = pl.DataFrame(test_data)
+    # shape: (2, 2)
+    # 
+    #  dt                   info      
+    #  ---                  ---       
+    #  datetime[s]         struct[2] 
+    # 
+    #  2017-01-01 00:00:00  {1,2}     
+    #  2017-01-02 00:00:00  {2,2}     
+    # 
+    assert df.schema == {
+        "dt": pl.Datetime(time_unit="us", time_zone=None),
+        "info": pl.Struct([pl.Field("a", pl.Int64), pl.Field("b", pl.Int64)]),
+    }
+    assert df.rows() == test_data
+
+
 def test_init_structured_objects_nested() -> None:
     for Foo, Bar, Baz in (
         (_TestFooDC, _TestBarDC, _TestBazDC),
         (_TestFooPD, _TestBarPD, _TestBazPD),
         (_TestFooNT, _TestBarNT, _TestBazNT),
     ):
         data = [
@@ -363,14 +392,29 @@
                 800,
                 datetime(2023, 4, 12, 10, 30),
                 -10.5,
                 "world",
             )
 
 
+def test_dataclasses_initvar_typing() -> None:
+    @dataclasses.dataclass
+    class ABC:
+        x: date
+        y: float
+        z: dataclasses.InitVar[list[str]] = None
+
+    # should be able to parse the initvar typing...
+    abc = ABC(x=date(1999, 12, 31), y=100.0)
+    df = pl.DataFrame([abc])
+
+    # ...but should not load the initvar field into the DataFrame
+    assert dataclasses.asdict(abc) == df.rows(named=True)[0]
+
+
 def test_init_ndarray(monkeypatch: Any) -> None:
     # Empty array
     df = pl.DataFrame(np.array([]))
     assert_frame_equal(df, pl.DataFrame())
 
     # 1D array
     df = pl.DataFrame(np.array([1, 2, 3], dtype=np.int64), schema=["a"])
@@ -627,24 +671,30 @@
     # datetimes sequence
     df = pl.DataFrame([datetime(2020, 1, 1)], schema={"ts": pl.Datetime("ms")})
     assert df.schema == {"ts": pl.Datetime("ms")}
     df = pl.DataFrame(
         [datetime(2020, 1, 1, tzinfo=timezone.utc)], schema={"ts": pl.Datetime("ms")}
     )
     assert df.schema == {"ts": pl.Datetime("ms", "UTC")}
-    df = pl.DataFrame(
-        [datetime(2020, 1, 1, tzinfo=timezone(timedelta(hours=1)))],
-        schema={"ts": pl.Datetime("ms")},
-    )
-    assert df.schema == {"ts": pl.Datetime("ms", "+01:00")}
-    df = pl.DataFrame(
-        [datetime(2020, 1, 1, tzinfo=ZoneInfo("Asia/Kathmandu"))],
-        schema={"ts": pl.Datetime("ms")},
-    )
-    assert df.schema == {"ts": pl.Datetime("ms", "Asia/Kathmandu")}
+    with pytest.warns(
+        TimeZoneAwareConstructorWarning, match="Series with UTC time zone"
+    ):
+        df = pl.DataFrame(
+            [datetime(2020, 1, 1, tzinfo=timezone(timedelta(hours=1)))],
+            schema={"ts": pl.Datetime("ms")},
+        )
+    assert df.schema == {"ts": pl.Datetime("ms", "UTC")}
+    with pytest.warns(
+        TimeZoneAwareConstructorWarning, match="Series with UTC time zone"
+    ):
+        df = pl.DataFrame(
+            [datetime(2020, 1, 1, tzinfo=ZoneInfo("Asia/Kathmandu"))],
+            schema={"ts": pl.Datetime("ms")},
+        )
+    assert df.schema == {"ts": pl.Datetime("ms", "UTC")}
 
 
 def test_init_pandas(monkeypatch: Any) -> None:
     pandas_df = pd.DataFrame([[1, 2], [3, 4]], columns=[1, 2])
 
     # integer column names
     df = pl.DataFrame(pandas_df)
@@ -778,15 +828,15 @@
             ]
         )
         expected.insert_at_idx(3, pl.Series("d", [], pl.List(pl.UInt8)))
 
         assert df.shape == (0, 4)
         assert_frame_equal(df, expected)
         assert df.dtypes == [pl.Date, pl.UInt64, pl.Int8, pl.List]
-        assert df.schema["d"].inner == pl.UInt8  # type: ignore[union-attr]
+        assert pl.List(pl.UInt8).is_(df.schema["d"])
 
         dfe = df.clear()
         assert len(dfe) == 0
         assert df.schema == dfe.schema
         assert dfe.shape == df.shape
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_cse.py` & `polars_lts_cpu-0.18.0/tests/unit/test_cse.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_datatypes.py` & `polars_lts_cpu-0.18.0/tests/unit/test_datatypes.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_df.py` & `polars_lts_cpu-0.18.0/tests/unit/test_df.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,18 +8,18 @@
 from io import BytesIO
 from operator import floordiv, truediv
 from typing import TYPE_CHECKING, Any, Callable, Iterator, Sequence, cast
 
 import numpy as np
 import pyarrow as pa
 import pytest
-from numpy.testing import assert_array_equal
+from numpy.testing import assert_array_equal, assert_equal
 
 import polars as pl
-from polars.datatypes import DTYPE_TEMPORAL_UNITS, INTEGER_DTYPES
+from polars.datatypes import DTYPE_TEMPORAL_UNITS, FLOAT_DTYPES, INTEGER_DTYPES
 from polars.testing import (
     assert_frame_equal,
     assert_frame_not_equal,
     assert_series_equal,
 )
 from polars.testing.parametric import columns
 from polars.utils._construction import iterable_to_pydf
@@ -544,25 +544,24 @@
 
     for index in [[0, 1], pl.Series([0, 1]), np.array([0, 1])]:
         out = df.sort("fruits").select(
             [
                 pl.col("B")
                 .reverse()
                 .take(index)  # type: ignore[arg-type]
-                .implode()
-                .over("fruits"),
+                .over("fruits", mapping_strategy="join"),
                 "fruits",
             ]
         )
 
         assert out[0, "B"].to_list() == [2, 3]
         assert out[4, "B"].to_list() == [1, 4]
 
     out = df.sort("fruits").select(
-        [pl.col("B").reverse().take(pl.lit(1)).implode().over("fruits"), "fruits"]
+        [pl.col("B").reverse().take(pl.lit(1)).over("fruits"), "fruits"]
     )
     assert out[0, "B"] == 3
     assert out[4, "B"] == 4
 
 
 def test_slice() -> None:
     df = pl.DataFrame({"a": [1, 2, 3], "b": ["a", "b", "c"]})
@@ -736,14 +735,20 @@
                 ],
             }
         ).with_columns(
             pl.col("cat").cast(pl.Categorical),
         )
         assert_frame_equal(df1, expected)
 
+        # 8745
+        df = pl.DataFrame([{"age": 1}, {"age": 2}, {"age": 3}])
+        df = df[:-1]
+        tail = pl.DataFrame([{"age": 8}])
+        assert df.extend(tail).to_dict(False) == {"age": [1, 2, 8]}
+
 
 def test_file_buffer() -> None:
     f = BytesIO()
     f.write(b"1,2,3,4,5,6\n7,8,9,10,11,12")
     f.seek(0)
     df = pl.read_csv(f, has_header=False)
     assert df.shape == (2, 6)
@@ -860,25 +865,15 @@
     assert df4.rows() == [(1, 1), (2, 2)]
 
     # misc error conditions
     with pytest.raises(ValueError):
         _ = pl.concat([])
 
     with pytest.raises(ValueError):
-        pl.concat([df1, df1], how="rubbish")  # type: ignore[call-overload]
-
-
-def test_concat_str() -> None:
-    df = pl.DataFrame({"a": ["a", "b", "c"], "b": [1, 2, 3]})
-
-    out = df.select([pl.concat_str(["a", "b"], separator="-")])
-    assert out["a"].to_list() == ["a-1", "b-2", "c-3"]
-
-    out = df.select([pl.format("foo_{}_bar_{}", pl.col("a"), "b").alias("fmt")])
-    assert out["fmt"].to_list() == ["foo_a_bar_1", "foo_b_bar_2", "foo_c_bar_3"]
+        pl.concat([df1, df1], how="rubbish")  # type: ignore[arg-type]
 
 
 def test_arg_where() -> None:
     s = pl.Series([True, False, True, False])
     assert_series_equal(pl.arg_where(s, eager=True).cast(int), pl.Series([0, 2]))
 
 
@@ -907,20 +902,14 @@
 
     # test sorted fast path
     assert pl.DataFrame({"x": pl.arange(0, 3, eager=True)}).to_dummies("x").to_dict(
         False
     ) == {"x_0": [1, 0, 0], "x_1": [0, 1, 0], "x_2": [0, 0, 1]}
 
 
-def test_get_dummies_function_deprecated() -> None:
-    df = pl.DataFrame({"a": [1, 2, 3]})
-    with pytest.deprecated_call():
-        pl.get_dummies(df)
-
-
 def test_to_pandas(df: pl.DataFrame) -> None:
     # pyarrow cannot deal with unsigned dictionary integer yet.
     # pyarrow cannot convert a time64 w/ non-zero nanoseconds
     df = df.drop(["cat", "time"])
     df.to_arrow()
     df.to_pandas()
     # test shifted df
@@ -1014,14 +1003,37 @@
         }
     )
     for a in (tbl, tbl[:0]):
         df = cast(pl.DataFrame, pl.from_arrow(a))
         assert df.columns == ["a", "b"]
 
 
+def test_init_series_edge_cases() -> None:
+    # confirm that we don't modify the name of the input series in-place
+    s1 = pl.Series("X", [1, 2, 3])
+    df1 = pl.DataFrame({"A": s1}, schema_overrides={"A": pl.UInt8})
+    assert s1.name == "X"
+    assert df1["A"].name == "A"
+
+    # init same series object under different names
+    df2 = pl.DataFrame({"A": s1, "B": s1})
+    assert df2.rows(named=True) == [
+        {"A": 1, "B": 1},
+        {"A": 2, "B": 2},
+        {"A": 3, "B": 3},
+    ]
+
+    # empty series names should not be overwritten
+    s2 = pl.Series([1, 2, 3])
+    s3 = pl.Series([2, 3, 4])
+    df3 = pl.DataFrame([s2, s3])
+    assert s2.name == s3.name == ""
+    assert df3.columns == ["column_0", "column_1"]
+
+
 def test_head_groupby() -> None:
     commodity_prices = {
         "commodity": [
             "Wheat",
             "Wheat",
             "Wheat",
             "Wheat",
@@ -1397,22 +1409,75 @@
     # test if we can assign in case of single column
     df = df.with_columns(pl.col("a") * 2)
     assert list(df["a"]) == [2, 4, 6]
 
 
 def test_to_numpy() -> None:
     df = pl.DataFrame({"a": [1, 2, 3], "b": [1.0, 2.0, 3.0]})
+
     out_array = df.to_numpy()
     expected_array = np.array([[1.0, 1.0], [2.0, 2.0], [3.0, 3.0]], dtype=np.float64)
     assert_array_equal(out_array, expected_array)
     assert out_array.flags["F_CONTIGUOUS"] is True
 
+    structured_array = df.to_numpy(structured=True)
+    expected_array = np.array(
+        [(1, 1.0), (2, 2.0), (3, 3.0)], dtype=[("a", "<i8"), ("b", "<f8")]
+    )
+    assert_array_equal(structured_array, expected_array)
+    assert structured_array.flags["F_CONTIGUOUS"] is True
+
+
+def test_to_numpy_structured() -> None:
+    # round-trip structured array: validate init/export
+    structured_array = np.array(
+        [
+            ("Google Pixel 7", 521.90, True),
+            ("Apple iPhone 14 Pro", 999.00, True),
+            ("OnePlus 11", 699.00, True),
+            ("Samsung Galaxy S23 Ultra", 1199.99, False),
+        ],
+        dtype=np.dtype(
+            [
+                ("product", "U24"),
+                ("price_usd", "float64"),
+                ("in_stock", "bool"),
+            ]
+        ),
+    )
+
+    df = pl.from_numpy(structured_array)
+    assert df.schema == {
+        "product": pl.Utf8,
+        "price_usd": pl.Float64,
+        "in_stock": pl.Boolean,
+    }
+    exported_array = df.to_numpy(structured=True)
+    assert exported_array["product"].dtype == np.dtype("U24")
+    assert_array_equal(exported_array, structured_array)
+
+    # none/nan values
+    df = pl.DataFrame({"x": ["a", None, "b"], "y": [5.5, None, -5.5]})
+    exported_array = df.to_numpy(structured=True)
+
+    assert exported_array.dtype == np.dtype([("x", object), ("y", float)])
+    for name in df.columns:
+        assert_equal(
+            list(exported_array[name]),
+            (
+                df[name].fill_null(float("nan"))
+                if df.schema[name] in FLOAT_DTYPES
+                else df[name]
+            ).to_list(),
+        )
+
 
 def test__array__() -> None:
     df = pl.DataFrame({"a": [1, 2, 3], "b": [1.0, 2.0, 3.0]})
+
     out_array = np.asarray(df.to_numpy())
     expected_array = np.array([[1.0, 1.0], [2.0, 2.0], [3.0, 3.0]], dtype=np.float64)
     assert_array_equal(out_array, expected_array)
     assert out_array.flags["F_CONTIGUOUS"] is True
 
     out_array = np.asarray(df.to_numpy(), np.uint8)
     expected_array = np.array([[1, 1], [2, 2], [3, 3]], dtype=np.uint8)
@@ -1678,14 +1743,50 @@
         assert df2.schema == {"id": pl.Int16, "value": pl.Int32, "_meta": pl.Utf8}
 
         df3 = df_init(records, schema=overrides)
         assert df3.rows() == [(1, 100), (2, 101)]
         assert df3.schema == {"id": pl.Int16, "value": pl.Int32}
 
 
+def test_repeat_by_unequal_lengths_panic() -> None:
+    df = pl.DataFrame(
+        {
+            "a": ["x", "y", "z"],
+        }
+    )
+    with pytest.raises(
+        pl.ComputeError,
+        match="""Length of repeat_by argument needs to be 1 or equal to the length of the Series.""",
+    ):
+        df.select(pl.col("a").repeat_by(pl.Series([2, 2])))
+
+
+@pytest.mark.parametrize(
+    ("a", "a_expected"),
+    [
+        ([1.2, 2.2, 3.3], [[1.2, 1.2, 1.2], [2.2, 2.2, 2.2], [3.3, 3.3, 3.3]]),
+        ([True, False], [[True, True, True], [False, False, False]]),
+        (["x", "y", "z"], [["x", "x", "x"], ["y", "y", "y"], ["z", "z", "z"]]),
+    ],
+)
+def test_repeat_by_parameterized(
+    a: list[float | bool | str], a_expected: list[list[float | bool | str]]
+) -> None:
+    df = pl.DataFrame(
+        {
+            "a": a,
+        }
+    )
+    expected = pl.DataFrame({"a": a_expected})
+    result = df.select(pl.col("a").repeat_by(3))
+    assert_frame_equal(result, expected)
+    result = df.select(pl.col("a").repeat_by(pl.lit(3)))
+    assert_frame_equal(result, expected)
+
+
 def test_repeat_by() -> None:
     df = pl.DataFrame({"name": ["foo", "bar"], "n": [2, 3]})
     out = df.select(pl.col("n").repeat_by("n"))
     s = out["n"]
 
     assert s[0].to_list() == [2, 2]
     assert s[1].to_list() == [3, 3, 3]
@@ -1693,14 +1794,15 @@
 
 def test_join_dates() -> None:
     dts_in = pl.date_range(
         datetime(2021, 6, 24),
         datetime(2021, 6, 24, 10, 0, 0),
         interval=timedelta(hours=1),
         closed="left",
+        eager=True,
     )
     dts = (
         dts_in.cast(int)
         .apply(lambda x: x + np.random.randint(1_000 * 60, 60_000 * 60))
         .cast(pl.Datetime)
     )
 
@@ -1915,15 +2017,15 @@
             ]
         )
         .with_columns(pl.col("str_column").cast(pl.Categorical).alias("cat_column"))
         .groupby("int_column", maintain_order=True)
         .agg([pl.col("cat_column")])["cat_column"]
     )
 
-    out = grouped.str.explode()
+    out = grouped.explode()
     assert out.dtype == pl.Categorical
     assert out[0] == "a"
 
 
 def test_groupby_agg_n_unique_floats() -> None:
     # tests proper dispatch
     df = pl.DataFrame({"a": [1, 1, 3], "b": [1.0, 2.0, 2.0]})
@@ -1994,20 +2096,20 @@
     assert sys.getrefcount(foos[0]) == base_count
 
     df = pl.DataFrame({"groups": [1, 1, 2], "a": foos})
     assert sys.getrefcount(foos[0]) == base_count + 1
 
     out = df.groupby("groups", maintain_order=True).agg(pl.col("a").alias("a"))
     assert sys.getrefcount(foos[0]) == base_count + 2
-    s = out["a"].arr.explode()
+    s = out["a"].list.explode()
     assert sys.getrefcount(foos[0]) == base_count + 3
     del s
     assert sys.getrefcount(foos[0]) == base_count + 2
 
-    assert out["a"].arr.explode().to_list() == foos
+    assert out["a"].list.explode().to_list() == foos
     assert sys.getrefcount(foos[0]) == base_count + 2
     del out
     assert sys.getrefcount(foos[0]) == base_count + 1
     del df
     assert sys.getrefcount(foos[0]) == base_count
 
 
@@ -2699,26 +2801,14 @@
     for how in join_strategies:
         # no need for an assert, we error if wrong
         df_a.join(df_b, on="A", suffix="_y", how=how)["B_y"]
 
     df_a.join_asof(df_b, on=pl.col("A").set_sorted(), suffix="_y")["B_y"]
 
 
-def test_preservation_of_subclasses_after_groupby_statements() -> None:
-    """Group by operations should preserve inherited dataframe classes."""
-
-    class SubClassedDataFrame(pl.DataFrame):
-        pass
-
-    # A group by operation should preserve the subclass
-    subclassed_df = SubClassedDataFrame({"a": [1, 2], "b": [3, 4]})
-    groupby = subclassed_df.groupby("a")
-    assert isinstance(groupby.agg(pl.count()), SubClassedDataFrame)
-
-
 def test_explode_empty() -> None:
     df = (
         pl.DataFrame({"x": ["a", "a", "b", "b"], "y": [1, 1, 2, 2]})
         .groupby("x", maintain_order=True)
         .agg(pl.col("y").take([]))
     )
     assert df.explode("y").to_dict(False) == {"x": ["a", "b"], "y": [None, None]}
@@ -3366,21 +3456,33 @@
         {
             "a": [1.0, 2.8, 3.0],
             "b": [4, 5, None],
             "c": [True, False, True],
             "d": [None, "b", "c"],
             "e": ["usd", "eur", None],
             "f": pl.date_range(
-                datetime(2023, 1, 1), datetime(2023, 1, 3), "1d", time_unit="us"
+                datetime(2023, 1, 1),
+                datetime(2023, 1, 3),
+                "1d",
+                time_unit="us",
+                eager=True,
             ),
             "g": pl.date_range(
-                datetime(2023, 1, 1), datetime(2023, 1, 3), "1d", time_unit="ms"
+                datetime(2023, 1, 1),
+                datetime(2023, 1, 3),
+                "1d",
+                time_unit="ms",
+                eager=True,
             ),
             "h": pl.date_range(
-                datetime(2023, 1, 1), datetime(2023, 1, 3), "1d", time_unit="ns"
+                datetime(2023, 1, 1),
+                datetime(2023, 1, 3),
+                "1d",
+                time_unit="ns",
+                eager=True,
             ),
             "i": [[5, 6], [3, 4], [9, 8]],
             "j": [[5.0, 6.0], [3.0, 4.0], [9.0, 8.0]],
             "k": [["A", "a"], ["B", "b"], ["C", "c"]],
         }
     )
     result = df.glimpse(return_as_string=True)
@@ -3542,15 +3644,15 @@
 
 
 def test_deadlocks_3409() -> None:
     assert (
         pl.DataFrame({"col1": [[1, 2, 3]]})
         .with_columns(
             [
-                pl.col("col1").arr.eval(
+                pl.col("col1").list.eval(
                     pl.element().apply(lambda x: x, return_dtype=pl.Int64)
                 )
             ]
         )
         .to_dict(False)
     ) == {"col1": [[1, 2, 3]]}
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_empty.py` & `polars_lts_cpu-0.18.0/tests/unit/test_empty.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,7 +1,9 @@
+import pytest
+
 import polars as pl
 from polars.testing import assert_frame_equal
 
 
 def test_empty_str_concat_lit() -> None:
     df = pl.DataFrame({"a": [], "b": []}, schema=[("a", pl.Utf8), ("b", pl.Utf8)])
     assert df.with_columns(pl.lit("asd") + pl.col("a")).schema == {
@@ -28,22 +30,33 @@
     s = pl.Series("", [], dtype=pl.Utf8)
     assert s.str.replace("a", "b", literal=True).series_equal(s)
     assert s.str.replace("a", "b").series_equal(s)
     assert s.str.replace("ab", "b", literal=True).series_equal(s)
     assert s.str.replace("ab", "b").series_equal(s)
 
 
-def test_empty_duration() -> None:
-    s = pl.DataFrame([], {"days": pl.Int32}).select(pl.duration(days="days"))
-    assert s.dtypes == [pl.Duration("ns")]
-    assert s.shape == (0, 1)
-
-
 def test_empty_window_function() -> None:
     expr = (pl.col("VAL") / pl.col("VAL").sum()).over("KEY")
 
     df = pl.DataFrame(schema={"KEY": pl.Utf8, "VAL": pl.Float64})
     df.select(expr)  # ComputeError
 
     lf = pl.DataFrame(schema={"KEY": pl.Utf8, "VAL": pl.Float64}).lazy()
     expected = pl.DataFrame(schema={"VAL": pl.Float64})
     assert_frame_equal(lf.select(expr).collect(), expected)
+
+
+def test_empty_count_window() -> None:
+    df = pl.DataFrame(
+        {"ID": [], "DESC": [], "dataset": []},
+        schema={"ID": pl.Utf8, "DESC": pl.Utf8, "dataset": pl.Utf8},
+    )
+
+    out = df.select(pl.col("ID").count().over(["ID", "DESC"]))
+    assert out.schema == {"ID": pl.UInt32}
+    assert out.height == 0
+
+
+def test_empty_sort_by_args() -> None:
+    df = pl.DataFrame([1, 2, 3])
+    with pytest.raises(pl.InvalidOperationError):
+        df.select(pl.all().sort_by([]))
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_errors.py` & `polars_lts_cpu-0.18.0/tests/unit/test_errors.py`

 * *Files 4% similar despite different names*

```diff
@@ -4,42 +4,43 @@
 import typing
 from datetime import date, datetime, time, timedelta
 
 import numpy as np
 import pytest
 
 import polars as pl
+from polars.datatypes.convert import dtype_to_py_type
 
 
 def test_error_on_empty_groupby() -> None:
     with pytest.raises(
         pl.ComputeError, match="at least one key is required in a groupby operation"
     ):
         pl.DataFrame({"x": [0, 0, 1, 1]}).groupby([]).agg(pl.count())
 
 
 def test_error_on_reducing_map() -> None:
     df = pl.DataFrame(
         {"id": [0, 0, 0, 1, 1, 1], "t": [2, 4, 5, 10, 11, 14], "y": [0, 1, 1, 2, 3, 4]}
     )
-
     with pytest.raises(
         pl.InvalidOperationError,
         match=(
-            "output length of `map` must be equal to that of the input length; consider using `apply` instead"
+            r"output length of `map` \(6\) must be equal to "
+            r"the input length \(1\); consider using `apply` instead"
         ),
     ):
         df.groupby("id").agg(pl.map(["t", "y"], np.trapz))
 
     df = pl.DataFrame({"x": [1, 2, 3, 4], "group": [1, 2, 1, 2]})
-
     with pytest.raises(
         pl.InvalidOperationError,
         match=(
-            "output length of `map` must be equal to that of the input length; consider using `apply` instead"
+            r"output length of `map` \(4\) must be equal to "
+            r"the input length \(1\); consider using `apply` instead"
         ),
     ):
         df.select(
             pl.col("x")
             .map(lambda x: x.cut(bins=[1, 2, 3], maintain_order=True))
             .over("group")
         )
@@ -55,14 +56,29 @@
     ).set_sorted("b")
 
     df2 = df1.with_columns(pl.col("a").cast(pl.Categorical))
     with pytest.raises(pl.ComputeError):
         df1.join_asof(df2, on="b", by=["a", "c"])
 
 
+def test_error_on_invalid_series_init() -> None:
+    for dtype in pl.TEMPORAL_DTYPES:
+        py_type = dtype_to_py_type(dtype)
+        with pytest.raises(
+            TypeError,
+            match=f"'float' object cannot be interpreted as a {py_type.__name__}",
+        ):
+            pl.Series([1.5, 2.0, 3.75], dtype=dtype)
+
+    with pytest.raises(
+        TypeError, match="'float' object cannot be interpreted as an integer"
+    ):
+        pl.Series([1.5, 2.0, 3.75], dtype=pl.Int32)
+
+
 def test_error_on_invalid_struct_field() -> None:
     with pytest.raises(pl.StructFieldNotFoundError):
         pl.struct(
             [pl.Series("a", [1, 2]), pl.Series("b", ["a", "b"])], eager=True
         ).struct.field("z")
 
 
@@ -278,30 +294,28 @@
     df2 = pl.DataFrame(
         {
             "foo": [3, 4],
             "ham": ["c", "d"],
             "bar": [8, 9],
         }
     )
-
-    for how in ["horizontal"]:
-        with pytest.raises(
-            ValueError,
-            match="'LazyFrame' only allows {{'vertical', 'diagonal'}} concat strategy.",
-        ):
-            pl.concat([df1.lazy(), df2.lazy()], how=how).collect()
+    with pytest.raises(
+        ValueError,
+        match="'LazyFrame' only allows {'vertical','diagonal','align'} concat strategies.",
+    ):
+        pl.concat([df1.lazy(), df2.lazy()], how="horizontal").collect()
 
 
 @typing.no_type_check
 def test_series_concat_err() -> None:
     s = pl.Series([1, 2, 3])
-    for how in ["horizontal", "diagonal"]:
+    for how in ("horizontal", "diagonal"):
         with pytest.raises(
             ValueError,
-            match="'Series' only allows {{'vertical'}} concat strategy.",
+            match="'Series' only allows {'vertical'} concat strategy.",
         ):
             pl.concat([s, s], how=how)
 
 
 def test_invalid_sort_by() -> None:
     df = pl.DataFrame(
         {
@@ -353,15 +367,15 @@
 
 def test_arr_eval_named_cols() -> None:
     df = pl.DataFrame({"A": ["a", "b"], "B": [["a", "b"], ["c", "d"]]})
 
     with pytest.raises(
         pl.ComputeError,
     ):
-        df.select(pl.col("B").arr.eval(pl.element().append(pl.col("A"))))
+        df.select(pl.col("B").list.eval(pl.element().append(pl.col("A"))))
 
 
 def test_alias_in_join_keys() -> None:
     df = pl.DataFrame({"A": ["a", "b"], "B": [["a", "b"], ["c", "d"]]})
     with pytest.raises(
         pl.ComputeError,
         match=r"'alias' is not allowed in a join key, use 'with_columns' first",
@@ -514,15 +528,15 @@
     with pytest.raises(pl.ComputeError, match=r"cannot cast `Time` to `Datetime`"):
         s.cast(pl.Datetime)
 
 
 def test_invalid_inner_type_cast_list() -> None:
     s = pl.Series([[-1, 1]])
     with pytest.raises(
-        pl.ComputeError, match=r"cannot cast list inner type: 'Int64' to Categorical"
+        pl.ComputeError, match=r"cannot cast List inner type: 'Int64' to Categorical"
     ):
         s.cast(pl.List(pl.Categorical))
 
 
 @pytest.mark.parametrize(
     ("every", "match"),
     [
@@ -573,16 +587,67 @@
     with pytest.raises(
         ValueError,
         match=r"'aggs' argument should be one or multiple expressions, got: '{'a': 'sum'}'",
     ):
         df.groupby(1).agg({"a": "sum"})
 
 
-def test_no_sorted_warning(capfd: typing.Any) -> None:
+def test_no_sorted_err() -> None:
     df = pl.DataFrame(
         {
             "dt": [datetime(2001, 1, 1), datetime(2001, 1, 2)],
         }
     )
-    df.groupby_dynamic("dt", every="1h").agg(pl.all().count().suffix("_foo"))
-    (_, err) = capfd.readouterr()
-    assert "argument in operation 'groupby_dynamic' is not explicitly sorted" in err
+    with pytest.raises(
+        pl.InvalidOperationError,
+        match=r"argument in operation 'groupby_dynamic' is not explicitly sorted",
+    ):
+        df.groupby_dynamic("dt", every="1h").agg(pl.all().count().suffix("_foo"))
+
+
+def test_serde_validation() -> None:
+    f = io.StringIO(
+        """
+    {
+      "columns": [
+        {
+          "name": "a",
+          "datatype": "Int64",
+          "values": [
+            1,
+            2
+          ]
+        },
+        {
+          "name": "b",
+          "datatype": "Int64",
+          "values": [
+            1
+          ]
+        }
+      ]
+    }
+    """
+    )
+    with pytest.raises(
+        pl.ComputeError,
+        match=r"lengths don't match",
+    ):
+        pl.read_json(f)
+
+
+def test_transpose_categorical_cached() -> None:
+    with pytest.raises(
+        pl.ComputeError,
+        match=r"'transpose' of categorical can only be done if all are from the same global string cache",
+    ):
+        pl.DataFrame(
+            {"b": pl.Series(["a", "b", "c"], dtype=pl.Categorical)}
+        ).transpose()
+
+
+def test_overflow_msg() -> None:
+    with pytest.raises(
+        pl.ComputeError,
+        match=r"could not append value: 2147483648 of type: i64 to the builder",
+    ):
+        pl.DataFrame([[2**31]], [("a", pl.Int32)], orient="row")
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_expr_multi_cols.py` & `polars_lts_cpu-0.18.0/tests/unit/test_expr_multi_cols.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_exprs.py` & `polars_lts_cpu-0.18.0/tests/unit/test_exprs.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from __future__ import annotations
 
 import random
 import sys
 import typing
-from datetime import datetime, timedelta, timezone
+from datetime import date, datetime, time, timedelta, timezone
 from itertools import permutations
 from typing import Any, cast
 
 if sys.version_info >= (3, 9):
     from zoneinfo import ZoneInfo
 else:
     # Import from submodule due to typing issue with backports.zoneinfo package:
@@ -135,15 +135,15 @@
     expected = pl.Series("max", [7, 5, 6])
     assert_series_equal(out, expected)
 
 
 def test_list_join_strings() -> None:
     s = pl.Series("a", [["ab", "c", "d"], ["e", "f"], ["g"], []])
     expected = pl.Series("a", ["ab-c-d", "e-f", "g", ""])
-    assert_series_equal(s.arr.join("-"), expected)
+    assert_series_equal(s.list.join("-"), expected)
 
 
 def test_count_expr() -> None:
     df = pl.DataFrame({"a": [1, 2, 3, 3, 3], "b": ["a", "a", "b", "a", "a"]})
 
     out = df.select(pl.count())
     assert out.shape == (1, 1)
@@ -330,23 +330,23 @@
 
 def test_list_eval_expression() -> None:
     df = pl.DataFrame({"a": [1, 8, 3], "b": [4, 5, 2]})
 
     for parallel in [True, False]:
         assert df.with_columns(
             pl.concat_list(["a", "b"])
-            .arr.eval(pl.first().rank(), parallel=parallel)
+            .list.eval(pl.first().rank(), parallel=parallel)
             .alias("rank")
         ).to_dict(False) == {
             "a": [1, 8, 3],
             "b": [4, 5, 2],
             "rank": [[1.0, 2.0], [2.0, 1.0], [2.0, 1.0]],
         }
 
-        assert df["a"].reshape((1, -1)).arr.eval(
+        assert df["a"].reshape((1, -1)).list.eval(
             pl.first(), parallel=parallel
         ).to_list() == [[1, 8, 3]]
 
 
 def test_null_count_expr() -> None:
     df = pl.DataFrame({"key": ["a", "b", "b", "a"], "val": [1, 2, None, 1]})
 
@@ -422,15 +422,15 @@
                 ["cat", "mouse", "dog"],
                 ["dog", "mouse", "cat"],
                 ["dog", "mouse", "aardvark"],
             ],
         }
     )
     assert df_groups.lazy().filter(
-        pl.col("str_list").arr.contains("cat")
+        pl.col("str_list").list.contains("cat")
     ).collect().to_dict(False) == {
         "str_list": [["cat", "mouse", "dog"], ["dog", "mouse", "cat"]]
     }
 
 
 def test_rank_so_4109() -> None:
     # also tests ranks null behavior
@@ -800,15 +800,15 @@
         ),
     )
 
     float_dict = {1.0: "b", 3.0: "d"}
 
     with pytest.raises(
         pl.ComputeError,
-        match="Remapping keys for map_dict could not be converted to Int16: ",
+        match=".*'float' object cannot be interpreted as an integer",
     ):
         df.with_columns(pl.col("int").map_dict(float_dict))
 
     df_int_as_str = df.with_columns(pl.col("int").cast(pl.Utf8))
 
     with pytest.raises(
         pl.ComputeError,
@@ -1046,7 +1046,76 @@
 
 def test_tail() -> None:
     df = pl.DataFrame({"a": [1, 2, 3, 4, 5]})
     assert df.select(pl.col("a").tail(0)).to_dict(False) == {"a": []}
     assert df.select(pl.col("a").tail(3)).to_dict(False) == {"a": [3, 4, 5]}
     assert df.select(pl.col("a").tail(10)).to_dict(False) == {"a": [1, 2, 3, 4, 5]}
     assert df.select(pl.col("a").tail(pl.count() / 2)).to_dict(False) == {"a": [4, 5]}
+
+
+def test_cache_expr(monkeypatch: Any, capfd: Any) -> None:
+    monkeypatch.setenv("POLARS_VERBOSE", "1")
+    df = pl.DataFrame(
+        {
+            "x": [3, 3, 3, 5, 8],
+        }
+    )
+    x = (pl.col("x") * 10).cache()
+
+    assert (df.groupby(1).agg([x * x * x])).to_dict(False) == {
+        "literal": [1],
+        "x": [[27000, 27000, 27000, 125000, 512000]],
+    }
+    _, err = capfd.readouterr()
+    assert """cache hit: CACHE [(col("x")) * (10)]""" in err
+
+
+@pytest.mark.parametrize(
+    ("const", "dtype"),
+    [
+        (1, pl.Int8),
+        (4, pl.UInt32),
+        (4.5, pl.Float32),
+        (None, pl.Float64),
+        ("", pl.Utf8),
+        (date.today(), pl.Date),
+        (datetime.now(), pl.Datetime("ns")),
+        (time(23, 59, 59), pl.Time),
+        (timedelta(hours=7, seconds=123), pl.Duration("ms")),
+    ],
+)
+def test_extend_constant(const: Any, dtype: pl.PolarsDataType) -> None:
+    df = pl.DataFrame({"a": pl.Series("s", [None], dtype=dtype)})
+
+    expected = pl.DataFrame(
+        {"a": pl.Series("s", [None, const, const, const], dtype=dtype)}
+    )
+
+    assert_frame_equal(df.select(pl.col("a").extend_constant(const, 3)), expected)
+
+
+@pytest.mark.parametrize(
+    ("const", "dtype"),
+    [
+        (1, pl.Int8),
+        (4, pl.UInt32),
+        (4.5, pl.Float32),
+        (None, pl.Float64),
+        ("", pl.Utf8),
+        (date.today(), pl.Date),
+        (datetime.now(), pl.Datetime("ns")),
+        (time(23, 59, 59), pl.Time),
+        (timedelta(hours=7, seconds=123), pl.Duration("ms")),
+    ],
+)
+def test_extend_constant_arr(const: Any, dtype: pl.PolarsDataType) -> None:
+    """
+    Test extend_constant in pl.List array.
+
+    NOTE: This function currently fails when the Series is a list with a single [None]
+          value. Hence, this function does not begin with [[None]], but [[const]].
+    """
+    s = pl.Series("s", [[const]], dtype=pl.List(dtype))
+
+    expected = pl.Series("s", [[const, const, const, const]], dtype=pl.List(dtype))
+
+    assert_series_equal(s.list.eval(pl.element().extend_constant(const, 3)), expected)
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_fmt.py` & `polars_lts_cpu-0.18.0/tests/unit/test_fmt.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,14 +1,21 @@
-from typing import Any, List
+from typing import Any, Iterator, List
 
 import pytest
 
 import polars as pl
 
 
+@pytest.fixture(autouse=True)
+def _environ() -> Iterator[None]:
+    """Fixture to ensure we run with default Config settings during tests."""
+    with pl.Config(restore_defaults=True):
+        yield
+
+
 @pytest.mark.parametrize(
     ("expected", "values"),
     [
         pytest.param(
             """shape: (1,)
 Series: 'foo' [str]
 [
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_functions.py` & `polars_lts_cpu-0.18.0/tests/unit/namespaces/test_list.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,388 +1,450 @@
 from __future__ import annotations
 
 import typing
-from datetime import timedelta
+from datetime import date, datetime
 
 import numpy as np
 import pytest
 
 import polars as pl
 from polars.testing import assert_frame_equal, assert_series_equal
 
 
-def test_date_datetime() -> None:
+def test_list_arr_get() -> None:
+    a = pl.Series("a", [[1, 2, 3], [4, 5], [6, 7, 8, 9]])
+    out = a.list.get(0)
+    expected = pl.Series("a", [1, 4, 6])
+    assert_series_equal(out, expected)
+    out = a.list[0]
+    expected = pl.Series("a", [1, 4, 6])
+    assert_series_equal(out, expected)
+    out = a.list.first()
+    assert_series_equal(out, expected)
+    out = pl.select(pl.lit(a).list.first()).to_series()
+    assert_series_equal(out, expected)
+
+    out = a.list.get(-1)
+    expected = pl.Series("a", [3, 5, 9])
+    assert_series_equal(out, expected)
+    out = a.list.last()
+    assert_series_equal(out, expected)
+    out = pl.select(pl.lit(a).list.last()).to_series()
+    assert_series_equal(out, expected)
+
+    a = pl.Series("a", [[1, 2, 3], [4, 5], [6, 7, 8, 9]])
+    out = a.list.get(-3)
+    expected = pl.Series("a", [1, None, 7])
+    assert_series_equal(out, expected)
+
+    assert pl.DataFrame(
+        {"a": [[1], [2], [3], [4, 5, 6], [7, 8, 9], [None, 11]]}
+    ).with_columns(
+        [pl.col("a").list.get(i).alias(f"get_{i}") for i in range(4)]
+    ).to_dict(
+        False
+    ) == {
+        "a": [[1], [2], [3], [4, 5, 6], [7, 8, 9], [None, 11]],
+        "get_0": [1, 2, 3, 4, 7, None],
+        "get_1": [None, None, None, 5, 8, 11],
+        "get_2": [None, None, None, 6, 9, None],
+        "get_3": [None, None, None, None, None, None],
+    }
+
+    # get by indexes where some are out of bounds
+    df = pl.DataFrame({"cars": [[1, 2, 3], [2, 3], [4], []], "indexes": [-2, 1, -3, 0]})
+
+    assert df.select([pl.col("cars").list.get("indexes")]).to_dict(False) == {
+        "cars": [2, 3, None, None]
+    }
+    # exact on oob boundary
     df = pl.DataFrame(
         {
-            "year": [2001, 2002, 2003],
-            "month": [1, 2, 3],
-            "day": [1, 2, 3],
-            "hour": [23, 12, 8],
+            "index": [3, 3, 3],
+            "lists": [[3, 4, 5], [4, 5, 6], [7, 8, 9, 4]],
         }
     )
-    out = df.select(
-        [
-            pl.all(),
-            pl.datetime("year", "month", "day", "hour").dt.hour().cast(int).alias("h2"),
-            pl.date("year", "month", "day").dt.day().cast(int).alias("date"),
-        ]
-    )
-    assert_series_equal(out["date"], df["day"].rename("date"))
-    assert_series_equal(out["h2"], df["hour"].rename("h2"))
 
+    assert df.select(pl.col("lists").list.get(3)).to_dict(False) == {
+        "lists": [None, None, 4]
+    }
+    assert df.select(pl.col("lists").list.get(pl.col("index"))).to_dict(False) == {
+        "lists": [None, None, 4]
+    }
 
-def test_diag_concat() -> None:
-    a = pl.DataFrame({"a": [1, 2]})
-    b = pl.DataFrame({"b": ["a", "b"], "c": [1, 2]})
-    c = pl.DataFrame({"a": [5, 7], "c": [1, 2], "d": [1, 2]})
-
-    for out in [
-        pl.concat([a, b, c], how="diagonal"),
-        pl.concat([a.lazy(), b.lazy(), c.lazy()], how="diagonal").collect(),
-    ]:
-        expected = pl.DataFrame(
-            {
-                "a": [1, 2, None, None, 5, 7],
-                "b": [None, None, "a", "b", None, None],
-                "c": [None, None, 1, 2, 1, 2],
-                "d": [None, None, None, None, 1, 2],
-            }
-        )
 
-        assert_frame_equal(out, expected)
+def test_contains() -> None:
+    a = pl.Series("a", [[1, 2, 3], [2, 5], [6, 7, 8, 9]])
+    out = a.list.contains(2)
+    expected = pl.Series("a", [True, True, False])
+    assert_series_equal(out, expected)
 
+    out = pl.select(pl.lit(a).list.contains(2)).to_series()
+    assert_series_equal(out, expected)
 
-def test_concat_horizontal() -> None:
-    a = pl.DataFrame({"a": ["a", "b"], "b": [1, 2]})
-    b = pl.DataFrame({"c": [5, 7, 8, 9], "d": [1, 2, 1, 2], "e": [1, 2, 1, 2]})
 
-    out = pl.concat([a, b], how="horizontal")
-    expected = pl.DataFrame(
-        {
-            "a": ["a", "b", None, None],
-            "b": [1, 2, None, None],
-            "c": [5, 7, 8, 9],
-            "d": [1, 2, 1, 2],
-            "e": [1, 2, 1, 2],
-        }
-    )
-    assert_frame_equal(out, expected)
+def test_list_concat() -> None:
+    df = pl.DataFrame({"a": [[1, 2], [1], [1, 2, 3]]})
 
+    out = df.select([pl.col("a").list.concat(pl.Series([[1, 2]]))])
+    assert out["a"][0].to_list() == [1, 2, 1, 2]
 
-def test_all_any_horizontally() -> None:
-    df = pl.DataFrame(
+    out = df.select([pl.col("a").list.concat([1, 4])])
+    assert out["a"][0].to_list() == [1, 2, 1, 4]
+
+    out_s = df["a"].list.concat([4, 1])
+    assert out_s[0].to_list() == [1, 2, 4, 1]
+
+
+def test_list_arr_empty() -> None:
+    df = pl.DataFrame({"cars": [[1, 2, 3], [2, 3], [4], []]})
+
+    out = df.select(
         [
-            [False, False, True],
-            [False, False, True],
-            [True, False, False],
-            [False, None, True],
-            [None, None, False],
-        ],
-        schema=["var1", "var2", "var3"],
+            pl.col("cars").list.first().alias("cars_first"),
+            pl.when(pl.col("cars").list.first() == 2)
+            .then(1)
+            .when(pl.col("cars").list.contains(2))
+            .then(2)
+            .otherwise(3)
+            .alias("cars_literal"),
+        ]
     )
     expected = pl.DataFrame(
+        {"cars_first": [1, 2, 4, None], "cars_literal": [2, 1, 3, 3]},
+        schema_overrides={"cars_literal": pl.Int32},  # Literals default to Int32
+    )
+    assert_frame_equal(out, expected)
+
+
+def test_list_argminmax() -> None:
+    s = pl.Series("a", [[1, 2], [3, 2, 1]])
+    expected = pl.Series("a", [0, 2], dtype=pl.UInt32)
+    assert_series_equal(s.list.arg_min(), expected)
+    expected = pl.Series("a", [1, 0], dtype=pl.UInt32)
+    assert_series_equal(s.list.arg_max(), expected)
+
+
+def test_list_shift() -> None:
+    s = pl.Series("a", [[1, 2], [3, 2, 1]])
+    expected = pl.Series("a", [[None, 1], [None, 3, 2]])
+    assert s.list.shift().to_list() == expected.to_list()
+
+
+def test_list_diff() -> None:
+    s = pl.Series("a", [[1, 2], [10, 2, 1]])
+    expected = pl.Series("a", [[None, 1], [None, -8, -1]])
+    assert s.list.diff().to_list() == expected.to_list()
+
+
+def test_slice() -> None:
+    vals = [[1, 2, 3, 4], [10, 2, 1]]
+    s = pl.Series("a", vals)
+    assert s.list.head(2).to_list() == [[1, 2], [10, 2]]
+    assert s.list.tail(2).to_list() == [[3, 4], [2, 1]]
+    assert s.list.tail(200).to_list() == vals
+    assert s.list.head(200).to_list() == vals
+    assert s.list.slice(1, 2).to_list() == [[2, 3], [2, 1]]
+
+
+def test_list_eval_dtype_inference() -> None:
+    grades = pl.DataFrame(
         {
-            "any": [True, True, False, True, None],
-            "all": [False, False, False, None, False],
+            "student": ["bas", "laura", "tim", "jenny"],
+            "arithmetic": [10, 5, 6, 8],
+            "biology": [4, 6, 2, 7],
+            "geography": [8, 4, 9, 7],
         }
     )
-    result = df.select(
+
+    rank_pct = pl.col("").rank(descending=True) / pl.col("").count().cast(pl.UInt16)
+
+    # the .list.first() would fail if .list.eval did not correctly infer the output type
+    assert grades.with_columns(
+        pl.concat_list(pl.all().exclude("student")).alias("all_grades")
+    ).select(
         [
-            pl.any([pl.col("var2"), pl.col("var3")]),
-            pl.all([pl.col("var2"), pl.col("var3")]),
+            pl.col("all_grades")
+            .list.eval(rank_pct, parallel=True)
+            .alias("grades_rank")
+            .list.first()
         ]
+    ).to_series().to_list() == [
+        0.3333333432674408,
+        0.6666666865348816,
+        0.6666666865348816,
+        0.3333333432674408,
+    ]
+
+
+def test_list_ternary_concat() -> None:
+    df = pl.DataFrame(
+        {
+            "list1": [["123", "456"], None],
+            "list2": [["789"], ["zzz"]],
+        }
     )
-    assert_frame_equal(result, expected)
 
+    assert df.with_columns(
+        pl.when(pl.col("list1").is_null())
+        .then(pl.col("list1").list.concat(pl.col("list2")))
+        .otherwise(pl.col("list2"))
+        .alias("result")
+    ).to_dict(False) == {
+        "list1": [["123", "456"], None],
+        "list2": [["789"], ["zzz"]],
+        "result": [["789"], None],
+    }
 
-def test_cut_deprecated() -> None:
-    with pytest.deprecated_call():
-        a = pl.Series("a", [v / 10 for v in range(-30, 30, 5)])
-        pl.cut(a, bins=[-1, 1])
+    assert df.with_columns(
+        pl.when(pl.col("list1").is_null())
+        .then(pl.col("list2"))
+        .otherwise(pl.col("list1").list.concat(pl.col("list2")))
+        .alias("result")
+    ).to_dict(False) == {
+        "list1": [["123", "456"], None],
+        "list2": [["789"], ["zzz"]],
+        "result": [["123", "456", "789"], ["zzz"]],
+    }
 
 
-def test_null_handling_correlation() -> None:
-    df = pl.DataFrame({"a": [1, 2, 3, None, 4], "b": [1, 2, 3, 10, 4]})
+def test_arr_contains_categorical() -> None:
+    df = pl.DataFrame(
+        {"str": ["A", "B", "A", "B", "C"], "group": [1, 1, 2, 1, 2]}
+    ).lazy()
+    df = df.with_columns(pl.col("str").cast(pl.Categorical))
+    df_groups = df.groupby("group").agg([pl.col("str").alias("str_list")])
+    assert df_groups.filter(pl.col("str_list").list.contains("C")).collect().to_dict(
+        False
+    ) == {"group": [2], "str_list": [["A", "C"]]}
 
-    out = df.select(
+
+def test_list_eval_type_coercion() -> None:
+    last_non_null_value = pl.element().fill_null(3).last()
+    df = pl.DataFrame(
+        {
+            "array_cols": [[1, None]],
+        }
+    )
+
+    assert df.select(
         [
-            pl.corr("a", "b").alias("pearson"),
-            pl.corr("a", "b", method="spearman").alias("spearman"),
+            pl.col("array_cols")
+            .list.eval(last_non_null_value, parallel=False)
+            .alias("col_last")
         ]
-    )
-    assert out["pearson"][0] == pytest.approx(1.0)
-    assert out["spearman"][0] == pytest.approx(1.0)
+    ).to_dict(False) == {"col_last": [[3]]}
 
-    # see #4930
-    df1 = pl.DataFrame({"a": [None, 1, 2], "b": [None, 2, 1]})
-    df2 = pl.DataFrame({"a": [np.nan, 1, 2], "b": [np.nan, 2, 1]})
 
-    assert np.isclose(df1.select(pl.corr("a", "b", method="spearman")).item(), -1.0)
-    assert (
-        str(
-            df2.select(pl.corr("a", "b", method="spearman", propagate_nans=True)).item()
-        )
-        == "nan"
+def test_list_slice() -> None:
+    df = pl.DataFrame(
+        {
+            "lst": [[1, 2, 3, 4], [10, 2, 1]],
+            "offset": [1, 2],
+            "len": [3, 2],
+        }
     )
 
+    assert df.select([pl.col("lst").list.slice("offset", "len")]).to_dict(False) == {
+        "lst": [[2, 3, 4], [1]]
+    }
+    assert df.select([pl.col("lst").list.slice("offset", 1)]).to_dict(False) == {
+        "lst": [[2], [1]]
+    }
+    assert df.select([pl.col("lst").list.slice(-2, "len")]).to_dict(False) == {
+        "lst": [[3, 4], [2, 1]]
+    }
 
-def test_align_frames() -> None:
-    import numpy as np
-    import pandas as pd
 
-    # setup some test frames
-    df1 = pd.DataFrame(
+@typing.no_type_check
+def test_list_sliced_get_5186() -> None:
+    # https://github.com/pola-rs/polars/issues/5186
+    n = 30
+    df = pl.from_dict(
         {
-            "date": pd.date_range(start="2019-01-02", periods=9),
-            "a": np.array([0, 1, 2, np.nan, 4, 5, 6, 7, 8], dtype=np.float64),
-            "b": np.arange(9, 18, dtype=np.float64),
+            "ind": pl.arange(0, n, eager=True),
+            "inds": np.stack([np.arange(n), -np.arange(n)], axis=-1),
         }
-    ).set_index("date")
+    )
 
-    df2 = pd.DataFrame(
-        {
-            "date": pd.date_range(start="2019-01-04", periods=7),
-            "a": np.arange(9, 16, dtype=np.float64),
-            "b": np.arange(10, 17, dtype=np.float64),
-        }
-    ).set_index("date")
+    exprs = [
+        "ind",
+        pl.col("inds").list.first().alias("first_element"),
+        pl.col("inds").list.last().alias("last_element"),
+    ]
+    out1 = df.select(exprs)[10:20]
+    out2 = df[10:20].select(exprs)
+    assert_frame_equal(out1, out2)
 
-    # calculate dot-product in pandas
-    pd_dot = (df1 * df2).sum(axis="columns").to_frame("dot").reset_index()
 
-    # use "align_frames" to calculate dot-product from disjoint rows. pandas uses an
-    # index to automatically infer the correct frame-alignment for the calculation;
-    # we need to do it explicitly (which also makes it clearer what is happening)
-    pf1, pf2 = pl.align_frames(
-        pl.from_pandas(df1.reset_index()),
-        pl.from_pandas(df2.reset_index()),
-        on="date",
-    )
-    pl_dot = (
-        (pf1[["a", "b"]] * pf2[["a", "b"]])
-        .fill_null(0)
-        .select(pl.sum(pl.col("*")).alias("dot"))
-        .insert_at_idx(0, pf1["date"])
-    )
-    # confirm we match the same operation in pandas
-    assert_frame_equal(pl_dot, pl.from_pandas(pd_dot))
-    pd.testing.assert_frame_equal(pd_dot, pl_dot.to_pandas())
-
-    # (also: confirm alignment function works with lazyframes)
-    lf1, lf2 = pl.align_frames(
-        pl.from_pandas(df1.reset_index()).lazy(),
-        pl.from_pandas(df2.reset_index()).lazy(),
-        on="date",
-    )
-    assert isinstance(lf1, pl.LazyFrame)
-    assert_frame_equal(lf1.collect(), pf1)
-    assert_frame_equal(lf2.collect(), pf2)
-
-    # misc
-    assert [] == pl.align_frames(on="date")
-
-    # expected error condition
-    with pytest.raises(TypeError):
-        pl.align_frames(  # type: ignore[call-overload]
-            pl.from_pandas(df1.reset_index()).lazy(),
-            pl.from_pandas(df2.reset_index()),
-            on="date",
-        )
+def test_empty_eval_dtype_5546() -> None:
+    # https://github.com/pola-rs/polars/issues/5546
+    df = pl.DataFrame([{"a": [{"name": 1}, {"name": 2}]}])
 
-    # descending
-    pf1, pf2 = pl.align_frames(
-        pl.DataFrame([[3, 5, 6], [5, 8, 9]], orient="row"),
-        pl.DataFrame([[2, 5, 6], [3, 8, 9], [4, 2, 0]], orient="row"),
-        on="column_0",
-        descending=True,
-    )
-    assert pf1.rows() == [(5, 8, 9), (4, None, None), (3, 5, 6), (2, None, None)]
-    assert pf2.rows() == [(5, None, None), (4, 2, 0), (3, 8, 9), (2, 5, 6)]
-
-
-def test_nan_aggregations() -> None:
-    df = pl.DataFrame({"a": [1.0, float("nan"), 2.0, 3.0], "b": [1, 1, 1, 1]})
-
-    aggs = [
-        pl.col("a").max().alias("max"),
-        pl.col("a").min().alias("min"),
-        pl.col("a").nan_max().alias("nan_max"),
-        pl.col("a").nan_min().alias("nan_min"),
-    ]
+    dtype = df.dtypes[0]
 
     assert (
-        str(df.select(aggs).to_dict(False))
-        == "{'max': [3.0], 'min': [1.0], 'nan_max': [nan], 'nan_min': [nan]}"
-    )
-    assert (
-        str(df.groupby("b").agg(aggs).to_dict(False))
-        == "{'b': [1], 'max': [3.0], 'min': [2.0], 'nan_max': [nan], 'nan_min': [nan]}"
-    )
-
-
-def test_coalesce() -> None:
-    df = pl.DataFrame(
-        {
-            "a": [1, None, None, None],
-            "b": [1, 2, None, None],
-            "c": [5, None, 3, None],
-        }
-    )
+        df.limit(0).with_columns(
+            pl.col("a")
+            .list.eval(pl.element().filter(pl.first().struct.field("name") == 1))
+            .alias("a_filtered")
+        )
+    ).dtypes == [dtype, dtype]
 
-    # List inputs
-    expected = pl.Series("d", [1, 2, 3, 10]).to_frame()
-    result = df.select(pl.coalesce(["a", "b", "c", 10]).alias("d"))
-    assert_frame_equal(expected, result)
 
-    # Positional inputs
-    expected = pl.Series("d", [1.0, 2.0, 3.0, 10.0]).to_frame()
-    result = df.select(pl.coalesce(pl.col(["a", "b", "c"]), 10.0).alias("d"))
-    assert_frame_equal(result, expected)
+def test_list_amortized_apply_explode_5812() -> None:
+    s = pl.Series([None, [1, 3], [0, -3], [1, 2, 2]])
+    assert s.list.sum().to_list() == [None, 4, -3, 5]
+    assert s.list.min().to_list() == [None, 1, -3, 1]
+    assert s.list.max().to_list() == [None, 3, 0, 2]
+    assert s.list.arg_min().to_list() == [None, 0, 1, 0]
+    assert s.list.arg_max().to_list() == [None, 1, 0, 1]
+
+
+def test_list_slice_5866() -> None:
+    vals = [[1, 2, 3, 4], [10, 2, 1]]
+    s = pl.Series("a", vals)
+    assert s.list.slice(1).to_list() == [[2, 3, 4], [2, 1]]
+
+
+def test_list_take() -> None:
+    s = pl.Series("a", [[1, 2, 3], [4, 5], [6, 7, 8]])
+    # mypy: we make it work, but idomatic is `arr.get`.
+    assert s.list.take(0).to_list() == [[1], [4], [6]]  # type: ignore[arg-type]
+    assert s.list.take([0, 1]).to_list() == [[1, 2], [4, 5], [6, 7]]
+
+    assert s.list.take([-1, 1]).to_list() == [[3, 2], [5, 5], [8, 7]]
+
+    # use another list to make sure negative indices are respected
+    taker = pl.Series([[-1, 1], [-1, 1], [-1, -2]])
+    assert s.list.take(taker).to_list() == [[3, 2], [5, 5], [8, 7]]
+    with pytest.raises(pl.ComputeError, match=r"take indices are out of bounds"):
+        s.list.take([1, 2])
+    s = pl.Series(
+        [["A", "B", "C"], ["A"], ["B"], ["1", "2"], ["e"]],
+    )
+
+    assert s.list.take([0, 2], null_on_oob=True).to_list() == [
+        ["A", "C"],
+        ["A", None],
+        ["B", None],
+        ["1", None],
+        ["e", None],
+    ]
+    assert s.list.take([0, 1, 2], null_on_oob=True).to_list() == [
+        ["A", "B", "C"],
+        ["A", None, None],
+        ["B", None, None],
+        ["1", "2", None],
+        ["e", None, None],
+    ]
+    s = pl.Series([[42, 1, 2], [5, 6, 7]])
 
+    with pytest.raises(pl.ComputeError, match=r"take indices are out of bounds"):
+        s.list.take([[0, 1, 2, 3], [0, 1, 2, 3]])
 
-def test_ones_zeros() -> None:
-    ones = pl.ones(5)
-    assert ones.dtype == pl.Float64
-    assert ones.to_list() == [1.0, 1.0, 1.0, 1.0, 1.0]
+    assert s.list.take([0, 1, 2, 3], null_on_oob=True).to_list() == [
+        [42, 1, 2, None],
+        [5, 6, 7, None],
+    ]
 
-    ones = pl.ones(3, dtype=pl.UInt8)
-    assert ones.dtype == pl.UInt8
-    assert ones.to_list() == [1, 1, 1]
 
-    zeros = pl.zeros(5)
-    assert zeros.dtype == pl.Float64
-    assert zeros.to_list() == [0.0, 0.0, 0.0, 0.0, 0.0]
+def test_list_eval_all_null() -> None:
+    df = pl.DataFrame({"foo": [1, 2, 3], "bar": [None, None, None]}).with_columns(
+        pl.col("bar").cast(pl.List(pl.Utf8))
+    )
 
-    zeros = pl.zeros(3, dtype=pl.UInt8)
-    assert zeros.dtype == pl.UInt8
-    assert zeros.to_list() == [0, 0, 0]
+    assert df.select(pl.col("bar").list.eval(pl.element())).to_dict(False) == {
+        "bar": [None, None, None]
+    }
 
 
-def test_overflow_diff() -> None:
+def test_list_function_group_awareness() -> None:
     df = pl.DataFrame(
         {
-            "a": [20, 10, 30],
+            "a": [100, 103, 105, 106, 105, 104, 103, 106, 100, 102],
+            "group": [0, 0, 1, 1, 1, 1, 1, 1, 2, 2],
         }
     )
-    assert df.select(pl.col("a").cast(pl.UInt64).diff()).to_dict(False) == {
-        "a": [None, -10, 20]
+
+    assert df.groupby("group").agg(
+        [
+            pl.col("a").implode().list.get(0).alias("get"),
+            pl.col("a").implode().list.take([0]).alias("take"),
+            pl.col("a").implode().list.slice(0, 3).alias("slice"),
+        ]
+    ).sort("group").to_dict(False) == {
+        "group": [0, 1, 2],
+        "get": [100, 105, 100],
+        "take": [[100], [105], [100]],
+        "slice": [[100, 103], [105, 106, 105], [100, 102]],
     }
 
 
-@typing.no_type_check
-def test_fill_null_unknown_output_type() -> None:
+def test_list_get_logical_types() -> None:
     df = pl.DataFrame(
         {
-            "a": [
-                None,
-                2,
-                3,
-                4,
-                5,
-            ]
+            "date_col": [[datetime(2023, 2, 1).date(), datetime(2023, 2, 2).date()]],
+            "datetime_col": [[datetime(2023, 2, 1), datetime(2023, 2, 2)]],
         }
     )
-    assert df.with_columns(
-        np.exp(pl.col("a")).fill_null(pl.lit(1, pl.Float64))
-    ).to_dict(False) == {
-        "a": [
-            1.0,
-            7.38905609893065,
-            20.085536923187668,
-            54.598150033144236,
-            148.4131591025766,
-        ]
+
+    assert df.select(pl.all().list.get(1).suffix("_element_1")).to_dict(False) == {
+        "date_col_element_1": [date(2023, 2, 2)],
+        "datetime_col_element_1": [datetime(2023, 2, 2, 0, 0)],
+    }
+
+
+def test_list_take_logical_type() -> None:
+    df = pl.DataFrame(
+        {"foo": [["foo", "foo", "bar"]], "bar": [[5.0, 10.0, 12.0]]}
+    ).with_columns(pl.col("foo").cast(pl.List(pl.Categorical)))
+
+    df = pl.concat([df, df], rechunk=False)
+    assert df.n_chunks() == 2
+    assert df.select(pl.all().take([0, 1])).to_dict(False) == {
+        "foo": [["foo", "foo", "bar"], ["foo", "foo", "bar"]],
+        "bar": [[5.0, 10.0, 12.0], [5.0, 10.0, 12.0]],
     }
 
 
-def test_repeat() -> None:
-    s = pl.select(pl.repeat(2**31 - 1, 3)).to_series()
-    assert s.dtype == pl.Int32
-    assert s.len() == 3
-    assert s.to_list() == [2**31 - 1] * 3
-    s = pl.select(pl.repeat(-(2**31), 4)).to_series()
-    assert s.dtype == pl.Int32
-    assert s.len() == 4
-    assert s.to_list() == [-(2**31)] * 4
-    s = pl.select(pl.repeat(2**31, 5)).to_series()
-    assert s.dtype == pl.Int64
-    assert s.len() == 5
-    assert s.to_list() == [2**31] * 5
-    s = pl.select(pl.repeat(-(2**31) - 1, 3)).to_series()
-    assert s.dtype == pl.Int64
-    assert s.len() == 3
-    assert s.to_list() == [-(2**31) - 1] * 3
-    s = pl.select(pl.repeat("foo", 2)).to_series()
-    assert s.dtype == pl.Utf8
-    assert s.len() == 2
-    assert s.to_list() == ["foo"] * 2
-    s = pl.select(pl.repeat(1.0, 5)).to_series()
-    assert s.dtype == pl.Float64
-    assert s.len() == 5
-    assert s.to_list() == [1.0] * 5
-    s = pl.select(pl.repeat(True, 4)).to_series()
-    assert s.dtype == pl.Boolean
-    assert s.len() == 4
-    assert s.to_list() == [True] * 4
-    s = pl.select(pl.repeat(None, 7)).to_series()
-    assert s.dtype == pl.Null
-    assert s.len() == 7
-    assert s.to_list() == [None] * 7
-    s = pl.select(pl.repeat(0, 0)).to_series()
-    assert s.dtype == pl.Int32
-    assert s.len() == 0
-
-
-def test_min() -> None:
-    s = pl.Series([1, 2, 3])
-    assert pl.min(s) == 1
-
-    df = pl.DataFrame({"a": [1, 4], "b": [3, 2]})
-    assert df.select(pl.min("a")).item() == 1
-
-    result = df.select(pl.min(["a", "b"]))
-    assert_frame_equal(result, pl.DataFrame({"min": [1, 2]}))
-
-    result = df.select(pl.min("a", 3))
-    assert_frame_equal(result, pl.DataFrame({"min": [1, 3]}))
-
-
-def test_max() -> None:
-    s = pl.Series([1, 2, 3])
-    assert pl.max(s) == 3
-
-    df = pl.DataFrame({"a": [1, 4], "b": [3, 2]})
-    assert df.select(pl.max("a")).item() == 4
-
-    result = df.select(pl.max(["a", "b"]))
-    assert_frame_equal(result, pl.DataFrame({"max": [3, 4]}))
-
-    result = df.select(pl.max("a", 3))
-    assert_frame_equal(result, pl.DataFrame({"max": [3, 4]}))
-
-
-def test_abs_logical_type() -> None:
-    s = pl.Series([timedelta(hours=1), timedelta(hours=-1)])
-    assert s.abs().to_list() == [timedelta(hours=1), timedelta(hours=1)]
-
-
-def test_approx_unique() -> None:
-    df1 = pl.DataFrame({"a": [None, 1, 2], "b": [None, 2, 1]})
-
-    assert_frame_equal(
-        df1.select(pl.approx_unique("b")),
-        pl.DataFrame({"b": pl.Series(values=[3], dtype=pl.UInt32)}),
-    )
-
-    assert_frame_equal(
-        df1.select(pl.approx_unique(pl.col("b"))),
-        pl.DataFrame({"b": pl.Series(values=[3], dtype=pl.UInt32)}),
-    )
-
-    assert_frame_equal(
-        df1.select(pl.col("b").approx_unique()),
-        pl.DataFrame({"b": pl.Series(values=[3], dtype=pl.UInt32)}),
+def test_list_unique() -> None:
+    assert (
+        pl.Series([[1, 1, 2, 2, 3], [3, 3, 3, 2, 1, 2]])
+        .list.unique(maintain_order=True)
+        .series_equal(pl.Series([[1, 2, 3], [3, 2, 1]]))
     )
 
 
-def test_range_decreasing() -> None:
-    assert pl.arange(10, 1, -2, eager=True).to_list() == list(range(10, 1, -2))
+def test_list_to_struct() -> None:
+    df = pl.DataFrame({"n": [[0, 1, 2], [0, 1]]})
+
+    assert df.select(pl.col("n").list.to_struct()).rows(named=True) == [
+        {"n": {"field_0": 0, "field_1": 1, "field_2": 2}},
+        {"n": {"field_0": 0, "field_1": 1, "field_2": None}},
+    ]
+
+    assert df.select(pl.col("n").list.to_struct(fields=lambda idx: f"n{idx}")).rows(
+        named=True
+    ) == [
+        {"n": {"n0": 0, "n1": 1, "n2": 2}},
+        {"n": {"n0": 0, "n1": 1, "n2": None}},
+    ]
+
+    assert df.select(pl.col("n").list.to_struct(fields=["one", "two", "three"])).rows(
+        named=True
+    ) == [
+        {"n": {"one": 0, "two": 1, "three": 2}},
+        {"n": {"one": 0, "two": 1, "three": None}},
+    ]
+
+
+def test_list_arr_get_8810() -> None:
+    assert pl.DataFrame(pl.Series("a", [None], pl.List(pl.Int64))).select(
+        pl.col("a").list.get(0)
+    ).to_dict(False) == {"a": [None]}
+
+
+def test_list_tail_underflow_9087() -> None:
+    assert pl.Series([["a", "b", "c"]]).list.tail(pl.lit(1, pl.UInt32)).to_list() == [
+        ["c"]
+    ]
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_interchange.py` & `polars_lts_cpu-0.18.0/tests/unit/test_interchange.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_interop.py` & `polars_lts_cpu-0.18.0/tests/unit/test_interop.py`

 * *Files 11% similar despite different names*

```diff
@@ -382,14 +382,64 @@
         schema_overrides={"a": pl.UInt32, "b": pl.UInt32},
     )
     assert df.shape == (3, 2)
     assert df.rows() == [(1, 4), (2, 5), (3, 6)]
     assert df.schema == {"a": pl.UInt32, "b": pl.UInt32}
 
 
+def test_from_numpy_structured() -> None:
+    test_data = [
+        ("Google Pixel 7", 521.90, True),
+        ("Apple iPhone 14 Pro", 999.00, True),
+        ("Samsung Galaxy S23 Ultra", 1199.99, False),
+        ("OnePlus 11", 699.00, True),
+    ]
+    # create a numpy structured array...
+    arr_structured = np.array(
+        test_data,
+        dtype=np.dtype(
+            [
+                ("product", "U32"),
+                ("price_usd", "float64"),
+                ("in_stock", "bool"),
+            ]
+        ),
+    )
+    # ...and also establish as a record array view
+    arr_records = arr_structured.view(np.recarray)
+
+    # confirm that we can cleanly initialise a DataFrame from both,
+    # respecting the native dtypes and any schema overrides, etc.
+    for arr in (arr_structured, arr_records):
+        df = pl.DataFrame(data=arr).sort(by="price_usd", descending=True)
+
+        assert df.schema == {
+            "product": pl.Utf8,
+            "price_usd": pl.Float64,
+            "in_stock": pl.Boolean,
+        }
+        assert df.rows() == sorted(test_data, key=lambda row: -row[1])
+
+        for df in (
+            pl.DataFrame(
+                data=arr, schema=["phone", ("price_usd", pl.Float32), "available"]
+            ),
+            pl.DataFrame(
+                data=arr,
+                schema=["phone", "price_usd", "available"],
+                schema_overrides={"price_usd": pl.Float32},
+            ),
+        ):
+            assert df.schema == {
+                "phone": pl.Utf8,
+                "price_usd": pl.Float32,
+                "available": pl.Boolean,
+            }
+
+
 def test_from_arrow() -> None:
     data = pa.table({"a": [1, 2, 3], "b": [4, 5, 6]})
     df = pl.from_arrow(data)
     assert df.shape == (3, 2)
     assert df.rows() == [(1, 4), (2, 5), (3, 6)]  # type: ignore[union-attr]
 
     # if not a PyArrow type, raise a ValueError
@@ -665,17 +715,17 @@
 def test_from_pyarrow_chunked_array() -> None:
     column = pa.chunked_array([[1], [2]])
     series = pl.Series("column", column)
     assert series.to_list() == [1, 2]
 
 
 def test_numpy_preserve_uint64_4112() -> None:
-    assert pl.DataFrame({"a": [1, 2, 3]}).with_columns(
-        pl.col("a").hash()
-    ).to_numpy().dtype == np.dtype("uint64")
+    df = pl.DataFrame({"a": [1, 2, 3]}).with_columns(pl.col("a").hash())
+    assert df.to_numpy().dtype == np.dtype("uint64")
+    assert df.to_numpy(structured=True).dtype == np.dtype([("a", "uint64")])
 
 
 def test_view_ub() -> None:
     # this would be UB if the series was dropped and not passed to the view
     assert np.sum(pl.Series([3, 1, 5]).sort().view()) == 9
 
 
@@ -813,14 +863,29 @@
         "q1": pl.Int8,
         "q2": pl.Int16,
         "q3": pl.Int32,
         "q4": pl.Int64,
         "total": pl.Float64,
     }
 
+    # empty frame with no dtypes
+    df = cast(
+        pl.DataFrame,
+        pl.from_repr(
+            """
+        
+         misc  other 
+        
+        
+        """
+        ),
+    )
+    assert df.rows() == []
+    assert df.schema == {"misc": pl.Utf8, "other": pl.Utf8}
+
     df = cast(
         pl.DataFrame,
         pl.from_repr(
             """
         # >>> Missing cols with old-style ellipsis, nulls, commented out
         # 
         #  dt          c1   c2   c3   ...  c96  c97  c98  c99  
@@ -850,14 +915,36 @@
         (None, 9, 18, 27, 864, 873, 882, 891),
     ]
 
     df = cast(
         pl.DataFrame,
         pl.from_repr(
             """
+        # >>> no dtypes:
+        # 
+        #  dt          c99  
+        # 
+        #  2023-03-25  99   
+        #  1999-12-31  null 
+        #  null        891  
+        # 
+        """
+        ),
+    )
+    assert df.schema == {"dt": pl.Date, "c99": pl.Int64}
+    assert df.rows() == [
+        (date(2023, 3, 25), 99),
+        (date(1999, 12, 31), None),
+        (None, 891),
+    ]
+
+    df = cast(
+        pl.DataFrame,
+        pl.from_repr(
+            """
         In [2]: with pl.Config() as cfg:
            ...:     pl.Config.set_tbl_formatting("UTF8_FULL", rounded_corners=True)
            ...:     print(df)
            ...:
         shape: (1, 5)
         
          source_ac  source_cha    ident  timestamp                      
@@ -968,7 +1055,41 @@
                 pl.col("c").cast(pl.Categorical),
                 pl.col("h").cast(pl.Datetime("ns")),
             )
             .collect()
         )
 
         assert_frame_equal(eval(df.to_init_repr().replace("datetime.", "")), df)
+
+
+def test_untrusted_categorical_input() -> None:
+    df = pd.DataFrame({"x": pd.Categorical(["x"], ["x", "y"])})
+    assert pl.from_pandas(df).groupby("x").count().to_dict(False) == {
+        "x": ["x"],
+        "count": [1],
+    }
+
+
+@typing.no_type_check
+def test_sliced_struct_from_arrow() -> None:
+    # Create a dataset with 3 rows
+    tbl = pa.Table.from_arrays(
+        arrays=[
+            pa.StructArray.from_arrays(
+                arrays=[
+                    pa.array([1, 2, 3], pa.int32()),
+                    pa.array(["foo", "bar", "baz"], pa.utf8()),
+                ],
+                names=["a", "b"],
+            )
+        ],
+        names=["struct_col"],
+    )
+
+    # slice the table
+    # check if FFI correctly reads sliced
+    assert pl.from_arrow(tbl.slice(1, 2)).to_dict(False) == {
+        "struct_col": [{"a": 2, "b": "bar"}, {"a": 3, "b": "baz"}]
+    }
+    assert pl.from_arrow(tbl.slice(1, 1)).to_dict(False) == {
+        "struct_col": [{"a": 2, "b": "bar"}]
+    }
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_lazy.py` & `polars_lts_cpu-0.18.0/tests/unit/test_lazy.py`

 * *Files 4% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 from typing import TYPE_CHECKING, Any, cast
 
 import numpy as np
 import pytest
 
 import polars as pl
 from polars import lit, when
-from polars.datatypes import NUMERIC_DTYPES
+from polars.datatypes import FLOAT_DTYPES, NUMERIC_DTYPES
 from polars.testing import assert_frame_equal
 from polars.testing.asserts import assert_series_equal
 
 if TYPE_CHECKING:
     from _pytest.capture import CaptureFixture
 
 
@@ -264,21 +264,14 @@
     assert out["a"].null_count() == 0
 
     # use df method
     out = ldf.shift_and_fill(pl.col("b").std(), periods=2).collect()
     assert out["a"].null_count() == 0
 
 
-def test_arange() -> None:
-    ldf = pl.LazyFrame({"a": [1, 1, 1]})
-    result = ldf.filter(pl.col("a") >= pl.arange(0, 3)).collect()
-    expected = pl.DataFrame({"a": [1, 1]})
-    assert_frame_equal(result, expected)
-
-
 def test_arg_unique() -> None:
     ldf = pl.LazyFrame({"a": [4, 1, 4]})
     col_a_unique = ldf.select(pl.col("a").arg_unique()).collect()["a"]
     assert_series_equal(col_a_unique, pl.Series("a", [0, 1]).cast(pl.UInt32))
 
 
 def test_is_unique() -> None:
@@ -365,28 +358,14 @@
 
 
 def test_fetch(fruits_cars: pl.DataFrame) -> None:
     res = fruits_cars.lazy().select("*").fetch(2)
     assert_frame_equal(res, res[:2])
 
 
-def test_concat_str() -> None:
-    ldf = pl.LazyFrame({"a": ["a", "b", "c"], "b": [1, 2, 3]})
-
-    out = ldf.select([pl.concat_str(["a", "b"], separator="-")])
-    assert out.collect()["a"].to_list() == ["a-1", "b-2", "c-3"]
-
-    out = ldf.select([pl.format("foo_{}_bar_{}", pl.col("a"), "b").alias("fmt")])
-    assert out.collect()["fmt"].to_list() == [
-        "foo_a_bar_1",
-        "foo_b_bar_2",
-        "foo_c_bar_3",
-    ]
-
-
 def test_fold_filter() -> None:
     ldf = pl.LazyFrame({"a": [1, 2, 3], "b": [0, 1, 2]})
 
     out = ldf.filter(
         pl.fold(
             acc=pl.lit(True),
             function=lambda a, b: a & b,
@@ -528,17 +507,41 @@
 
 
 def test_floor() -> None:
     ldf = pl.LazyFrame({"a": [1.8, 1.2, 3.0]}).select(pl.col("a").floor())
     assert_series_equal(ldf.collect()["a"], pl.Series("a", [1, 1, 3]).cast(pl.Float64))
 
 
-def test_round() -> None:
-    ldf = pl.LazyFrame({"a": [1.8, 1.2, 3.0]}).select(pl.col("a").round(decimals=0))
-    assert_series_equal(ldf.collect()["a"], pl.Series("a", [2, 1, 3]).cast(pl.Float64))
+@pytest.mark.parametrize(
+    ("n", "ndigits", "expected"),
+    [
+        (1.005, 2, 1.0),
+        (1234.00000254495, 10, 1234.000002545),
+        (1835.665, 2, 1835.67),
+        (-1835.665, 2, -1835.67),
+        (1.27499, 2, 1.27),
+        (123.45678, 2, 123.46),
+        (1254, 2, 1254.0),
+        (1254, 0, 1254.0),
+        (123.55, 0, 124.0),
+        (123.55, 1, 123.6),
+        (-1.23456789, 6, -1.234568),
+        (-1835.665, 2, -1835.67),
+        (1.0e-5, 5, 0.00001),
+        (1.0e-20, 20, 1e-20),
+        (1.0e20, 2, 100000000000000000000.0),
+    ],
+)
+def test_round(n: float, ndigits: int, expected: float) -> None:
+    for float_dtype in FLOAT_DTYPES:
+        ldf = pl.LazyFrame({"value": [n]}, schema_overrides={"value": float_dtype})
+        assert_series_equal(
+            ldf.select(pl.col("value").round(decimals=ndigits)).collect().to_series(),
+            pl.Series("value", [expected], dtype=float_dtype),
+        )
 
 
 def test_dot() -> None:
     ldf = pl.LazyFrame({"a": [1.8, 1.2, 3.0], "b": [3.2, 1, 2]}).select(
         pl.col("a").dot(pl.col("b"))
     )
     assert cast(float, ldf.collect().item()) == 12.96
@@ -657,57 +660,14 @@
 
 def test_backward_fill() -> None:
     ldf = pl.LazyFrame({"a": [1.0, None, 3.0]})
     col_a_backward_fill = ldf.select([pl.col("a").backward_fill()]).collect()["a"]
     assert_series_equal(col_a_backward_fill, pl.Series("a", [1, 3, 3]).cast(pl.Float64))
 
 
-def test_take(fruits_cars: pl.DataFrame) -> None:
-    ldf = fruits_cars.lazy()
-
-    # out of bounds error
-    with pytest.raises(pl.ComputeError):
-        (
-            ldf.sort("fruits")
-            .select(
-                [pl.col("B").reverse().take([1, 2]).implode().over("fruits"), "fruits"]
-            )
-            .collect()
-        )
-
-    for index in [[0, 1], pl.Series([0, 1]), np.array([0, 1])]:
-        out = (
-            ldf.sort("fruits")
-            .select(
-                [
-                    pl.col("B")
-                    .reverse()
-                    .take(index)  # type: ignore[arg-type]
-                    .implode()
-                    .over("fruits"),
-                    "fruits",
-                ]
-            )
-            .collect()
-        )
-
-        assert out[0, "B"].to_list() == [2, 3]
-        assert out[4, "B"].to_list() == [1, 4]
-
-    out = (
-        ldf.sort("fruits")
-        .select(
-            [pl.col("B").reverse().take(pl.lit(1)).implode().over("fruits"), "fruits"]
-        )
-        .collect()
-    )
-    assert out[0, "B"] == 3
-    assert out[4, "B"] == 4
-
-
 def test_select_by_col_list(fruits_cars: pl.DataFrame) -> None:
     ldf = fruits_cars.lazy()
     result = ldf.select(pl.col(["A", "B"]).sum())
     expected = pl.LazyFrame({"A": 15, "B": 15})
     assert_frame_equal(result, expected)
 
 
@@ -787,22 +747,46 @@
 
 
 def test_arr_namespace(fruits_cars: pl.DataFrame) -> None:
     ldf = fruits_cars.lazy()
     out = ldf.select(
         [
             "fruits",
-            pl.col("B").implode().over("fruits").arr.min().alias("B_by_fruits_min1"),
-            pl.col("B").min().implode().over("fruits").alias("B_by_fruits_min2"),
-            pl.col("B").implode().over("fruits").arr.max().alias("B_by_fruits_max1"),
-            pl.col("B").max().implode().over("fruits").alias("B_by_fruits_max2"),
-            pl.col("B").implode().over("fruits").arr.sum().alias("B_by_fruits_sum1"),
-            pl.col("B").sum().implode().over("fruits").alias("B_by_fruits_sum2"),
-            pl.col("B").implode().over("fruits").arr.mean().alias("B_by_fruits_mean1"),
-            pl.col("B").mean().implode().over("fruits").alias("B_by_fruits_mean2"),
+            pl.col("B")
+            .over("fruits", mapping_strategy="join")
+            .list.min()
+            .alias("B_by_fruits_min1"),
+            pl.col("B")
+            .min()
+            .over("fruits", mapping_strategy="join")
+            .alias("B_by_fruits_min2"),
+            pl.col("B")
+            .over("fruits", mapping_strategy="join")
+            .list.max()
+            .alias("B_by_fruits_max1"),
+            pl.col("B")
+            .max()
+            .over("fruits", mapping_strategy="join")
+            .alias("B_by_fruits_max2"),
+            pl.col("B")
+            .over("fruits", mapping_strategy="join")
+            .list.sum()
+            .alias("B_by_fruits_sum1"),
+            pl.col("B")
+            .sum()
+            .over("fruits", mapping_strategy="join")
+            .alias("B_by_fruits_sum2"),
+            pl.col("B")
+            .over("fruits", mapping_strategy="join")
+            .list.mean()
+            .alias("B_by_fruits_mean1"),
+            pl.col("B")
+            .mean()
+            .over("fruits", mapping_strategy="join")
+            .alias("B_by_fruits_mean2"),
         ]
     )
     expected = pl.DataFrame(
         {
             "fruits": ["banana", "banana", "apple", "apple", "banana"],
             "B_by_fruits_min1": [1, 1, 2, 2, 1],
             "B_by_fruits_min2": [1, 1, 2, 2, 1],
@@ -1111,14 +1095,19 @@
     assert fruits_cars.lazy().quantile(0.24, "midpoint").collect()["A"][0] == 1.5
     assert fruits_cars.select(pl.col("A").quantile(0.24, "midpoint"))["A"][0] == 1.5
 
     assert fruits_cars.lazy().quantile(0.24, "linear").collect()["A"][0] == 1.96
     assert fruits_cars.select(pl.col("A").quantile(0.24, "linear"))["A"][0] == 1.96
 
 
+def test_null_count() -> None:
+    lf = pl.LazyFrame({"a": [1, 2, None, 2], "b": [None, 3, None, 3]})
+    assert lf.null_count().collect().rows() == [(1, 2)]
+
+
 def test_unique() -> None:
     ldf = pl.LazyFrame({"a": [1, 2, 2], "b": [3, 3, 3]})
 
     expected = pl.DataFrame({"a": [1, 2], "b": [3, 3]})
     assert_frame_equal(ldf.unique(maintain_order=True).collect(), expected)
 
     result = ldf.unique(subset="b", maintain_order=True).collect()
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_polars_import.py` & `polars_lts_cpu-0.18.0/tests/unit/test_polars_import.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_predicates.py` & `polars_lts_cpu-0.18.0/tests/unit/test_predicates.py`

 * *Files 2% similar despite different names*

```diff
@@ -29,18 +29,16 @@
         {
             "team": ["A", "A", "A", "B", "B", "C"],
             "points": [11, 8, 10, 6, 6, 5],
         }
     )
 
     assert df.select(
-        [
-            pl.when(pl.col("points") > 7).then("Foo"),
-            pl.when(pl.col("points") > 7).then("Foo").alias("bar"),
-        ]
+        pl.when(pl.col("points") > 7).then("Foo"),
+        pl.when(pl.col("points") > 7).then("Foo").alias("bar"),
     ).to_dict(False) == {
         "literal": ["Foo", "Foo", "Foo", None, None, None],
         "bar": ["Foo", "Foo", "Foo", None, None, None],
     }
 
 
 def test_predicate_null_block_asof_join() -> None:
@@ -129,20 +127,33 @@
             "b": [6, 5, 4, 3, 2, 1],
         }
     )
 
     assert (
         df.lazy()
         .with_columns(pl.col("a").implode())
-        .with_columns(pl.col("a").arr.first())
+        .with_columns(pl.col("a").list.first())
         .filter(pl.col("a") == pl.col("b"))
         .collect()
     ).to_dict(False) == {"a": [1], "b": [1]}
 
 
 def test_fast_path_comparisons() -> None:
     s = pl.Series(np.sort(np.random.randint(0, 50, 100)))
 
     assert (s > 25).series_equal(s.set_sorted() > 25)
     assert (s >= 25).series_equal(s.set_sorted() >= 25)
     assert (s < 25).series_equal(s.set_sorted() < 25)
     assert (s <= 25).series_equal(s.set_sorted() <= 25)
+
+
+def test_predicate_pushdown_block_8661() -> None:
+    df = pl.DataFrame(
+        {
+            "g": [1, 1, 1, 1, 2, 2, 2, 2],
+            "t": [1, 2, 3, 4, 4, 3, 2, 1],
+            "x": [10, 20, 30, 40, 10, 20, 30, 40],
+        }
+    )
+    assert df.lazy().sort(["g", "t"]).filter(
+        (pl.col("x").shift() > 20).over("g")
+    ).collect().to_dict(False) == {"g": [1, 2, 2], "t": [4, 2, 3], "x": [40, 30, 20]}
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_projections.py` & `polars_lts_cpu-0.18.0/tests/unit/test_projections.py`

 * *Files 2% similar despite different names*

```diff
@@ -98,17 +98,15 @@
             ],
         }
     ).lazy()
 
     q = df.with_columns(
         pl.col("genres")
         .str.split("|")
-        .arr.to_struct(
-            n_field_strategy="max_width", name_generator=lambda i: f"genre{i+1}"
-        )
+        .list.to_struct(n_field_strategy="max_width", fields=lambda i: f"genre{i+1}")
     ).unnest("genres")
 
     out = q.collect()
     assert out.to_dict(False) == {
         "title": ["Avatar", "spectre", "King Kong"],
         "content_rating": ["PG-13", "PG-13", "PG-13"],
         "genre1": ["Action", "Action", "Action"],
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_queries.py` & `polars_lts_cpu-0.18.0/tests/unit/test_schema.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,421 +1,455 @@
-from __future__ import annotations
+import typing
 
-from datetime import datetime, timedelta
-from typing import Any
-
-import numpy as np
-import pandas as pd
+import pytest
 
 import polars as pl
 from polars.testing import assert_frame_equal
 
 
-def test_sort_by_bools() -> None:
-    # tests dispatch
+def test_schema_on_agg() -> None:
+    df = pl.DataFrame({"a": ["x", "x", "y", "n"], "b": [1, 2, 3, 4]})
+
+    assert (
+        df.lazy()
+        .groupby("a")
+        .agg(
+            [
+                pl.col("b").min().alias("min"),
+                pl.col("b").max().alias("max"),
+                pl.col("b").sum().alias("sum"),
+                pl.col("b").first().alias("first"),
+                pl.col("b").last().alias("last"),
+            ]
+        )
+    ).schema == {
+        "a": pl.Utf8,
+        "min": pl.Int64,
+        "max": pl.Int64,
+        "sum": pl.Int64,
+        "first": pl.Int64,
+        "last": pl.Int64,
+    }
+
+
+def test_fill_null_minimal_upcast_4056() -> None:
+    df = pl.DataFrame({"a": [-1, 2, None]})
+    df = df.with_columns(pl.col("a").cast(pl.Int8))
+    assert df.with_columns(pl.col(pl.Int8).fill_null(-1)).dtypes[0] == pl.Int8
+    assert df.with_columns(pl.col(pl.Int8).fill_null(-1000)).dtypes[0] == pl.Int32
+
+
+def test_pow_dtype() -> None:
     df = pl.DataFrame(
         {
-            "foo": [1, 2, 3],
-            "bar": [6.0, 7.0, 8.0],
-            "ham": ["a", "b", "c"],
+            "foo": [1, 2, 3, 4, 5],
         }
+    ).lazy()
+
+    df = df.with_columns([pl.col("foo").cast(pl.UInt32)]).with_columns(
+        [
+            (pl.col("foo") * 2**2).alias("scaled_foo"),
+            (pl.col("foo") * 2**2.1).alias("scaled_foo2"),
+        ]
     )
-    out = df.with_columns((pl.col("foo") % 2 == 1).alias("foo_odd")).sort(
-        by=["foo_odd", "foo"]
-    )
-    assert out.rows() == [
-        (2, 7.0, "b", False),
-        (1, 6.0, "a", True),
-        (3, 8.0, "c", True),
-    ]
-    assert out.shape == (3, 4)
+    assert df.collect().dtypes == [pl.UInt32, pl.UInt32, pl.Float64]
 
 
-def test_type_coercion_when_then_otherwise_2806() -> None:
-    out = (
-        pl.DataFrame({"names": ["foo", "spam", "spam"], "nrs": [1, 2, 3]})
-        .select(
-            [
-                pl.when(pl.col("names") == "spam")
-                .then(pl.col("nrs") * 2)
-                .otherwise(pl.lit("other"))
-                .alias("new_col"),
-            ]
+def test_bool_numeric_supertype() -> None:
+    df = pl.DataFrame({"v": [1, 2, 3, 4, 5, 6]})
+    for dt in [
+        pl.UInt8,
+        pl.UInt16,
+        pl.UInt32,
+        pl.UInt64,
+        pl.Int8,
+        pl.Int16,
+        pl.Int32,
+        pl.Int64,
+    ]:
+        assert (
+            df.select([(pl.col("v") < 3).sum().cast(dt) / pl.count()]).item()
+            - 0.3333333
+            <= 0.00001
         )
-        .to_series()
-    )
-    expected = pl.Series("new_col", ["other", "4", "6"])
-    assert out.to_list() == expected.to_list()
 
-    # test it remains float32
+
+def test_with_context() -> None:
+    df_a = pl.DataFrame({"a": [1, 2, 3], "b": ["a", "c", None]}).lazy()
+    df_b = pl.DataFrame({"c": ["foo", "ham"]})
+
     assert (
-        pl.Series("a", [1.0, 2.0, 3.0], dtype=pl.Float32)
-        .to_frame()
-        .select(pl.when(pl.col("a") > 2.0).then(pl.col("a")).otherwise(0.0))
-    ).to_series().dtype == pl.Float32
+        df_a.with_context(df_b.lazy()).select([pl.col("b") + pl.col("c").first()])
+    ).collect().to_dict(False) == {"b": ["afoo", "cfoo", None]}
 
+    with pytest.raises(pl.ComputeError):
+        (df_a.with_context(df_b.lazy()).select(["a", "c"])).collect()
 
-def test_repeat_expansion_in_groupby() -> None:
-    out = (
-        pl.DataFrame({"g": [1, 2, 2, 3, 3, 3]})
-        .groupby("g", maintain_order=True)
-        .agg(pl.repeat(1, pl.count()).cumsum())
-        .to_dict(False)
-    )
-    assert out == {"g": [1, 2, 3], "literal": [[1], [1, 2], [1, 2, 3]]}
 
+def test_from_dicts_nested_nulls() -> None:
+    assert pl.from_dicts([{"a": [None, None]}, {"a": [1, 2]}]).to_dict(False) == {
+        "a": [[None, None], [1, 2]]
+    }
 
-def test_agg_after_head() -> None:
-    a = [1, 1, 1, 2, 2, 3, 3, 3, 3]
 
-    df = pl.DataFrame({"a": a, "b": pl.arange(1, len(a) + 1, eager=True)})
+def test_group_schema_err() -> None:
+    df = pl.DataFrame({"foo": [None, 1, 2], "bar": [1, 2, 3]}).lazy()
+    with pytest.raises(pl.ColumnNotFoundError):
+        df.groupby("not-existent").agg(pl.col("bar").max().alias("max_bar")).schema
 
-    expected = pl.DataFrame({"a": [1, 2, 3], "b": [6, 9, 21]})
 
-    for maintain_order in [True, False]:
-        out = df.groupby("a", maintain_order=maintain_order).agg(
-            [pl.col("b").head(3).sum()]
-        )
+def test_schema_inference_from_rows() -> None:
+    # these have to upcast to float
+    assert pl.from_records([[1, 2.1, 3], [4, 5, 6.4]]).to_dict(False) == {
+        "column_0": [1.0, 2.1, 3.0],
+        "column_1": [4.0, 5.0, 6.4],
+    }
+    assert pl.from_dicts([{"a": 1, "b": 2}, {"a": 3.1, "b": 4.5}]).to_dict(False) == {
+        "a": [1.0, 3.1],
+        "b": [2.0, 4.5],
+    }
 
-        if not maintain_order:
-            out = out.sort("a")
 
-        assert_frame_equal(out, expected)
+def test_lazy_map_schema() -> None:
+    df = pl.DataFrame({"a": [1, 2, 3], "b": ["a", "b", "c"]})
 
+    # identity
+    assert_frame_equal(df.lazy().map(lambda x: x).collect(), df)
 
-def test_overflow_uint16_agg_mean() -> None:
-    assert (
-        pl.DataFrame(
-            {
-                "col1": ["A" for _ in range(1025)],
-                "col3": [64 for i in range(1025)],
-            }
-        )
-        .with_columns(
-            [
-                pl.col("col3").cast(pl.UInt16),
-            ]
-        )
-        .groupby(["col1"])
-        .agg(pl.col("col3").mean())
-        .to_dict(False)
-    ) == {"col1": ["A"], "col3": [64.0]}
+    def custom(df: pl.DataFrame) -> pl.Series:
+        return df["a"]
 
+    with pytest.raises(
+        pl.ComputeError,
+        match="Expected 'LazyFrame.map' to return a 'DataFrame', got a",
+    ):
+        df.lazy().map(custom).collect()  # type: ignore[arg-type]
+
+    def custom2(
+        df: pl.DataFrame,
+    ) -> pl.DataFrame:
+        # changes schema
+        return df.select(pl.all().cast(pl.Utf8))
+
+    with pytest.raises(
+        pl.ComputeError,
+        match="The output schema of 'LazyFrame.map' is incorrect. Expected",
+    ):
+        df.lazy().map(custom2).collect()
 
-def test_binary_on_list_agg_3345() -> None:
-    df = pl.DataFrame(
-        {
-            "group": ["A", "A", "A", "B", "B", "B", "B"],
-            "id": [1, 2, 1, 4, 5, 4, 6],
-        }
-    )
+    assert df.lazy().map(custom2, validate_output_schema=False).collect().to_dict(
+        False
+    ) == {"a": ["1", "2", "3"], "b": ["a", "b", "c"]}
 
-    assert (
-        df.groupby(["group"], maintain_order=True)
-        .agg(
-            [
-                (
-                    (pl.col("id").unique_counts() / pl.col("id").len()).log()
-                    * -1
-                    * (pl.col("id").unique_counts() / pl.col("id").len())
-                ).sum()
-            ]
-        )
-        .to_dict(False)
-    ) == {"group": ["A", "B"], "id": [0.6365141682948128, 1.0397207708399179]}
 
+def test_join_as_of_by_schema() -> None:
+    a = pl.DataFrame({"a": [1], "b": [2], "c": [3]}).lazy()
+    b = pl.DataFrame({"a": [1], "b": [2], "d": [4]}).lazy()
+    q = a.join_asof(b, on=pl.col("a").set_sorted(), by="b")
+    assert q.collect().columns == q.columns
 
-def test_maintain_order_after_sampling() -> None:
-    # internally samples cardinality
-    # check if the maintain_order kwarg is dispatched
+
+def test_unknown_apply() -> None:
     df = pl.DataFrame(
-        {
-            "type": ["A", "B", "C", "D", "A", "B", "C", "D"],
-            "value": [1, 3, 2, 3, 4, 5, 3, 4],
-        }
+        {"Amount": [10, 1, 1, 5], "Flour": ["1000g", "100g", "50g", "75g"]}
     )
-    assert df.groupby("type", maintain_order=True).agg(pl.col("value").sum()).to_dict(
-        False
-    ) == {"type": ["A", "B", "C", "D"], "value": [5, 8, 5, 7]}
 
+    q = df.lazy().select(
+        [
+            pl.col("Amount"),
+            pl.col("Flour").apply(lambda x: 100.0) / pl.col("Amount"),
+        ]
+    )
 
-def test_sorted_groupby_optimization(monkeypatch: Any) -> None:
-    monkeypatch.setenv("POLARS_NO_STREAMING_GROUPBY", "1")
+    assert q.collect().to_dict(False) == {
+        "Amount": [10, 1, 1, 5],
+        "Flour": [10.0, 100.0, 100.0, 20.0],
+    }
+    assert q.dtypes == [pl.Int64, pl.Unknown]
 
-    df = pl.DataFrame({"a": np.random.randint(0, 5, 20)})
 
-    # the sorted optimization should not randomize the
-    # groups, so this is tests that we hit the sorted optimization
-    for descending in [True, False]:
-        sorted_implicit = (
-            df.with_columns(pl.col("a").sort(descending=descending))
-            .groupby("a")
-            .agg(pl.count())
-        )
-        sorted_explicit = (
-            df.groupby("a").agg(pl.count()).sort("a", descending=descending)
-        )
-        assert_frame_equal(sorted_explicit, sorted_implicit)
+def test_remove_redundant_mapping_4668() -> None:
+    df = pl.DataFrame([["a"]] * 2, ["A", "B "]).lazy()
+    clean_name_dict = {x: " ".join(x.split()) for x in df.columns}
+    df = df.rename(clean_name_dict)
+    assert df.columns == ["A", "B"]
 
 
-def test_median_on_shifted_col_3522() -> None:
+def test_fold_all_schema() -> None:
     df = pl.DataFrame(
         {
-            "foo": [
-                datetime(2022, 5, 5, 12, 31, 34),
-                datetime(2022, 5, 5, 12, 47, 1),
-                datetime(2022, 5, 6, 8, 59, 11),
-            ]
+            "A": [1, 2, 3, 4, 5],
+            "fruits": ["banana", "banana", "apple", "apple", "banana"],
+            "B": [5, 4, 3, 2, 1],
+            "cars": ["beetle", "audi", "beetle", "beetle", "beetle"],
+            "optional": [28, 300, None, 2, -30],
         }
     )
-    diffs = df.select(pl.col("foo").diff().dt.seconds())
-    assert diffs.select(pl.col("foo").median()).to_series()[0] == 36828.5
+    # divide because of overflow
+    assert df.select(pl.sum(pl.all().hash(seed=1) // int(1e8))).dtypes == [pl.UInt64]
 
 
-def test_groupby_agg_equals_zero_3535() -> None:
-    # setup test frame
-    df = pl.DataFrame(
-        data=[
-            # note: the 'bb'-keyed values should clearly sum to 0
-            ("aa", 10, None),
-            ("bb", -10, 0.5),
-            ("bb", 10, -0.5),
-            ("cc", -99, 10.5),
-            ("cc", None, 0.0),
-        ],
-        schema=[
-            ("key", pl.Utf8),
-            ("val1", pl.Int16),
-            ("val2", pl.Float32),
-        ],
-    )
-    # group by the key, aggregating the two numeric cols
-    assert df.groupby(pl.col("key"), maintain_order=True).agg(
-        [pl.col("val1").sum(), pl.col("val2").sum()]
-    ).to_dict(False) == {
-        "key": ["aa", "bb", "cc"],
-        "val1": [10, 0, -99],
-        "val2": [None, 0.0, 10.5],
-    }
-
-
-def test_arithmetic_in_aggregation_3739() -> None:
-    def demean_dot() -> pl.Expr:
-        x = pl.col("x")
-        y = pl.col("y")
-        x1 = x - x.mean()
-        y1 = y - y.mean()
-        return (x1 * y1).sum().alias("demean_dot")
+def test_fill_null_static_schema_4843() -> None:
+    df1 = pl.DataFrame(
+        {
+            "a": [1, 2, None],
+            "b": [1, None, 4],
+        }
+    ).lazy()
 
-    assert (
-        pl.DataFrame(
-            {
-                "key": ["a", "a", "a", "a"],
-                "x": [4, 2, 2, 4],
-                "y": [2, 0, 2, 0],
-            }
-        )
-        .groupby("key")
-        .agg(
-            [
-                demean_dot(),
-            ]
-        )
-    ).to_dict(False) == {"key": ["a"], "demean_dot": [0.0]}
+    df2 = df1.select([pl.col(pl.Int64).fill_null(0)])
+    df3 = df2.select(pl.col(pl.Int64))
+    assert df3.schema == {"a": pl.Int64, "b": pl.Int64}
 
 
-def test_dtype_concat_3735() -> None:
-    for dt in [
+def test_shrink_dtype() -> None:
+    out = pl.DataFrame(
+        {
+            "a": [1, 2, 3],
+            "b": [1, 2, 2 << 32],
+            "c": [-1, 2, 1 << 30],
+            "d": [-112, 2, 112],
+            "e": [-112, 2, 129],
+            "f": ["a", "b", "c"],
+            "g": [0.1, 1.32, 0.12],
+            "h": [True, None, False],
+        }
+    ).select(pl.all().shrink_dtype())
+    assert out.dtypes == [
         pl.Int8,
-        pl.Int16,
-        pl.Int32,
         pl.Int64,
-        pl.UInt8,
-        pl.UInt16,
-        pl.UInt32,
-        pl.UInt64,
+        pl.Int32,
+        pl.Int8,
+        pl.Int16,
+        pl.Utf8,
         pl.Float32,
-        pl.Float64,
-    ]:
-        d1 = pl.DataFrame([pl.Series("val", [1, 2], dtype=dt)])
+        pl.Boolean,
+    ]
+
+    assert out.to_dict(False) == {
+        "a": [1, 2, 3],
+        "b": [1, 2, 8589934592],
+        "c": [-1, 2, 1073741824],
+        "d": [-112, 2, 112],
+        "e": [-112, 2, 129],
+        "f": ["a", "b", "c"],
+        "g": [0.10000000149011612, 1.3200000524520874, 0.11999999731779099],
+        "h": [True, None, False],
+    }
 
-    d2 = pl.DataFrame([pl.Series("val", [3, 4], dtype=dt)])
-    df = pl.concat([d1, d2])
 
-    assert df.shape == (4, 1)
-    assert df.columns == ["val"]
-    assert df.to_series().to_list() == [1, 2, 3, 4]
+def test_diff_duration_dtype() -> None:
+    dates = ["2022-01-01", "2022-01-02", "2022-01-03", "2022-01-03"]
+    df = pl.DataFrame({"date": pl.Series(dates).str.strptime(pl.Date, "%Y-%m-%d")})
 
+    assert df.select(pl.col("date").diff() < pl.duration(days=1))["date"].to_list() == [
+        None,
+        False,
+        False,
+        True,
+    ]
 
-def test_opaque_filter_on_lists_3784() -> None:
+
+def test_boolean_agg_schema() -> None:
     df = pl.DataFrame(
-        {"str": ["A", "B", "A", "B", "C"], "group": [1, 1, 2, 1, 2]}
+        {
+            "x": [1, 1, 1],
+            "y": [False, True, False],
+        }
     ).lazy()
-    df = df.with_columns(pl.col("str").cast(pl.Categorical))
 
-    df_groups = df.groupby("group").agg([pl.col("str").alias("str_list")])
+    agg_df = df.groupby("x").agg(pl.col("y").max().alias("max_y"))
 
-    pre = "A"
-    succ = "B"
-
-    assert (
-        df_groups.filter(
-            pl.col("str_list").apply(
-                lambda variant: pre in variant
-                and succ in variant
-                and variant.to_list().index(pre) < variant.to_list().index(succ)
-            )
+    for streaming in [True, False]:
+        assert (
+            agg_df.collect(streaming=streaming).schema
+            == agg_df.schema
+            == {"x": pl.Int64, "max_y": pl.Boolean}
         )
-    ).collect().to_dict(False) == {"group": [1], "str_list": [["A", "B", "B"]]}
 
 
-def test_ternary_none_struct() -> None:
-    ignore_nulls = False
+def test_schema_owned_arithmetic_5669() -> None:
+    df = (
+        pl.DataFrame({"A": [1, 2, 3]})
+        .lazy()
+        .filter(pl.col("A") >= 3)
+        .with_columns(-pl.col("A").alias("B"))
+        .collect()
+    )
+    assert df.columns == ["A", "literal"], df.columns
 
-    def map_expr(name: str) -> pl.Expr:
-        return (
-            pl.when(ignore_nulls or pl.col(name).null_count() == 0)
-            .then(
-                pl.struct(
-                    [
-                        pl.sum(name).alias("sum"),
-                        (pl.count() - pl.col(name).null_count()).alias("count"),
-                    ]
-                ),
-            )
-            .otherwise(None)
-        ).alias("out")
 
-    assert (
-        pl.DataFrame({"groups": [1, 2, 3, 4], "values": [None, None, 1, 2]})
-        .groupby("groups", maintain_order=True)
-        .agg([map_expr("values")])
-    ).to_dict(False) == {
-        "groups": [1, 2, 3, 4],
-        "out": [
-            {"sum": None, "count": None},
-            {"sum": None, "count": None},
-            {"sum": 1, "count": 1},
-            {"sum": 2, "count": 1},
-        ],
-    }
+def test_fill_null_f32_with_lit() -> None:
+    # ensure the literal integer does not upcast the f32 to an f64
+    df = pl.DataFrame({"a": [1.1, 1.2]}, schema=[("a", pl.Float32)])
+    assert df.fill_null(value=0).dtypes == [pl.Float32]
 
 
-def test_when_then_edge_cases_3994() -> None:
-    df = pl.DataFrame(data={"id": [1, 1], "type": [2, 2]})
+def test_lazy_rename() -> None:
+    df = pl.DataFrame({"x": [1], "y": [2]})
 
-    # this tests if lazy correctly assigns the list schema to the column aggregation
     assert (
-        df.lazy()
-        .groupby(["id"])
-        .agg(pl.col("type"))
-        .with_columns(
-            pl.when(pl.col("type").arr.lengths() == 0)
-            .then(pl.lit(None))
-            .otherwise(pl.col("type"))
-            .keep_name()
-        )
-        .collect()
-    ).to_dict(False) == {"id": [1], "type": [[2, 2]]}
+        df.lazy().rename({"y": "x", "x": "y"}).select(["x", "y"]).collect()
+    ).to_dict(False) == {"x": [2], "y": [1]}
 
-    # this tests ternary with an empty argument
-    assert (
-        df.filter(pl.col("id") == 42)
-        .groupby(["id"])
-        .agg(pl.col("type"))
-        .with_columns(
-            pl.when(pl.col("type").arr.lengths() == 0)
-            .then(pl.lit(None))
-            .otherwise(pl.col("type"))
-            .keep_name()
-        )
-    ).to_dict(False) == {"id": [], "type": []}
+
+def test_all_null_cast_5826() -> None:
+    df = pl.DataFrame(data=[pl.Series("a", [None], dtype=pl.Utf8)])
+    out = df.with_columns(pl.col("a").cast(pl.Boolean))
+    assert out.dtypes == [pl.Boolean]
+    assert out.item() is None
+
+
+def test_empty_list_eval_schema_5734() -> None:
+    df = pl.DataFrame({"a": [[{"b": 1, "c": 2}]]})
+    assert df.filter(False).select(
+        pl.col("a").list.eval(pl.element().struct.field("b"))
+    ).schema == {"a": pl.List(pl.Int64)}
+
+
+def test_schema_true_divide_6643() -> None:
+    df = pl.DataFrame({"a": [1]})
+    a = pl.col("a")
+    assert df.lazy().select(a / 2).select(pl.col(pl.Int64)).collect().shape == (0, 0)
 
 
-def test_edge_cast_string_duplicates_4259() -> None:
-    # carefully constructed data.
-    # note that row 2, 3 concatenated are the same string ('5461214484')
+def test_rename_schema_order_6660() -> None:
     df = pl.DataFrame(
         {
-            "a": [99, 54612, 546121],
-            "b": [1, 14484, 4484],
+            "a": [],
+            "b": [],
+            "c": [],
+            "d": [],
         }
-    ).with_columns(pl.all().cast(pl.Utf8))
+    )
+
+    mapper = {"a": "1", "b": "2", "c": "3", "d": "4"}
+
+    renamed = df.lazy().rename(mapper)
+
+    computed = renamed.select([pl.all(), pl.col("4").alias("computed")])
+
+    assert renamed.schema == renamed.collect().schema
+    assert computed.schema == computed.collect().schema
 
-    mask = df.select(["a", "b"]).is_duplicated()
-    df_filtered = df.filter(pl.lit(mask))
 
-    assert df_filtered.shape == (0, 2)
-    assert df_filtered.rows() == []
+def test_from_dicts_all_cols_6716() -> None:
+    dicts = [{"a": None} for _ in range(20)] + [{"a": "crash"}]
 
+    with pytest.raises(
+        pl.ComputeError, match="make sure that all rows have the same schema"
+    ):
+        pl.from_dicts(dicts, infer_schema_length=20)
+    assert pl.from_dicts(dicts, infer_schema_length=None).dtypes == [pl.Utf8]
 
-def test_query_4438() -> None:
-    df = pl.DataFrame({"x": [1, 2, 3, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 1, 1]})
 
+def test_from_dicts_empty() -> None:
+    with pytest.raises(pl.NoDataError, match="No rows. Cannot infer schema."):
+        pl.from_dicts([])
+
+
+def test_duration_division_schema() -> None:
+    df = pl.DataFrame({"a": [1]})
     q = (
         df.lazy()
-        .with_columns(pl.col("x").rolling_max(window_size=3).alias("rolling_max"))
-        .fill_null(strategy="backward")
-        .with_columns(
-            pl.col("rolling_max").rolling_max(window_size=3).alias("rolling_max_2")
-        )
+        .with_columns(pl.col("a").cast(pl.Duration))
+        .select(pl.col("a") / pl.col("a"))
     )
-    assert q.collect()["rolling_max_2"].to_list() == [
-        None,
-        None,
-        3,
-        10,
-        10,
-        10,
-        10,
-        10,
-        9,
-        8,
-        7,
-        6,
-        5,
-        4,
-        3,
-    ]
 
+    assert q.schema == {"a": pl.Float64}
+    assert q.collect().to_dict(False) == {"a": [1.0]}
 
-def test_query_4538() -> None:
-    df = pl.DataFrame(
-        [
-            pl.Series("value", ["aaa", "bbb"]),
-        ]
-    )
-    assert df.select([pl.col("value").str.to_uppercase().is_in(["AAA"])])[
-        "value"
-    ].to_list() == [True, False]
 
+def test_int_operator_stability() -> None:
+    for dt in pl.datatypes.INTEGER_DTYPES:
+        s = pl.Series(values=[10], dtype=dt)
+        assert pl.select(pl.lit(s) // 2).dtypes == [dt]
+        assert pl.select(pl.lit(s) + 2).dtypes == [dt]
+        assert pl.select(pl.lit(s) - 2).dtypes == [dt]
+        assert pl.select(pl.lit(s) * 2).dtypes == [dt]
+        assert pl.select(pl.lit(s) / 2).dtypes == [pl.Float64]
 
-def test_none_comparison_4773() -> None:
-    df = pl.DataFrame(
-        {
-            "x": [0, 1, None, 2],
-            "y": [1, 2, None, 3],
-        }
-    ).filter(pl.col("x") != pl.col("y"))
-    assert df.shape == (3, 2)
-    assert df.rows() == [(0, 1), (1, 2), (2, 3)]
+
+def test_deep_subexpression_f32_schema_7129() -> None:
+    df = pl.DataFrame({"a": [1.1, 2.3, 3.4, 4.5]}, schema={"a": pl.Float32()})
+    assert df.with_columns(pl.col("a") - pl.col("a").median()).dtypes == [pl.Float32]
+    assert df.with_columns(
+        (pl.col("a") - pl.col("a").mean()) / (pl.col("a").std() + 0.001)
+    ).dtypes == [pl.Float32]
 
 
-def test_datetime_supertype_5236() -> None:
-    df = pd.DataFrame(
+def test_absence_off_null_prop_8224() -> None:
+    # a reminder to self to not do null propagation
+    # it is inconsistent and makes output dtype
+    # dependent of the data, big no!
+
+    def sub_col_min(column: str, min_column: str) -> pl.Expr:
+        return pl.col(column).sub(pl.col(min_column).min())
+
+    df = pl.DataFrame(
         {
-            "StartDateTime": [
-                pd.Timestamp(datetime.utcnow(), tz="UTC"),
-                pd.Timestamp(datetime.utcnow(), tz="UTC"),
-            ],
-            "EndDateTime": [
-                pd.Timestamp(datetime.utcnow(), tz="UTC"),
-                pd.Timestamp(datetime.utcnow(), tz="UTC"),
-            ],
+            "group": [1, 1, 2, 2],
+            "vals_num": [10.0, 11.0, 12.0, 13.0],
+            "vals_partial": [None, None, 12.0, 13.0],
+            "vals_null": [None, None, None, None],
         }
     )
-    out = pl.from_pandas(df).filter(
-        pl.col("StartDateTime")
-        < (pl.col("EndDateTime").dt.truncate("1d").max() - timedelta(days=1))
+
+    q = (
+        df.lazy()
+        .groupby("group")
+        .agg(
+            [
+                sub_col_min("vals_num", "vals_num").alias("sub_num"),
+                sub_col_min("vals_num", "vals_partial").alias("sub_partial"),
+                sub_col_min("vals_num", "vals_null").alias("sub_null"),
+            ]
+        )
     )
-    assert out.shape == (0, 2)
-    assert out.dtypes == [pl.Datetime("ns", "UTC")] * 2
+
+    assert q.collect().dtypes == [
+        pl.Int64,
+        pl.List(pl.Float64),
+        pl.List(pl.Float64),
+        pl.List(pl.Float64),
+    ]
+
+
+@typing.no_type_check
+def test_schemas() -> None:
+    # add all expression output tests here:
+    args = [
+        # coalesce
+        {
+            "data": {"x": ["x"], "y": ["y"]},
+            "expr": pl.coalesce(pl.col("x"), pl.col("y")),
+            "expected_select": {"x": pl.Utf8},
+            "expected_gb": {"x": pl.List(pl.Utf8)},
+        },
+        # boolean sum
+        {
+            "data": {"x": [True]},
+            "expr": pl.col("x").sum(),
+            "expected_select": {"x": pl.UInt32},
+            "expected_gb": {"x": pl.UInt32},
+        },
+    ]
+    for arg in args:
+        df = pl.DataFrame(arg["data"])
+
+        # test selection schema
+        schema = df.select(arg["expr"]).schema
+        for key, dtype in arg["expected_select"].items():
+            assert schema[key] == dtype
+
+        # test groupby schema
+        schema = df.groupby(pl.lit(1)).agg(arg["expr"]).schema
+        for key, dtype in arg["expected_gb"].items():
+            assert schema[key] == dtype
+
+
+def test_list_null_constructor_schema() -> None:
+    expected = pl.List(pl.Null)
+    assert pl.Series([[]]).dtype == expected
+    assert pl.Series([[]], dtype=pl.List).dtype == expected
+    assert pl.DataFrame({"a": [[]]}).dtypes[0] == expected
+    assert pl.DataFrame(schema={"a": pl.List}).dtypes[0] == expected
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_rows.py` & `polars_lts_cpu-0.18.0/tests/unit/test_rows.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_schema.py` & `polars_lts_cpu-0.18.0/tests/unit/functions/test_as_datatype.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,455 +1,458 @@
-import typing
+from datetime import date, datetime
 
 import pytest
 
 import polars as pl
-from polars.testing import assert_frame_equal
+from polars.testing import assert_frame_equal, assert_series_equal
 
 
-def test_schema_on_agg() -> None:
-    df = pl.DataFrame({"a": ["x", "x", "y", "n"], "b": [1, 2, 3, 4]})
-
-    assert (
-        df.lazy()
-        .groupby("a")
-        .agg(
-            [
-                pl.col("b").min().alias("min"),
-                pl.col("b").max().alias("max"),
-                pl.col("b").sum().alias("sum"),
-                pl.col("b").first().alias("first"),
-                pl.col("b").last().alias("last"),
-            ]
-        )
-    ).schema == {
-        "a": pl.Utf8,
-        "min": pl.Int64,
-        "max": pl.Int64,
-        "sum": pl.Int64,
-        "first": pl.Int64,
-        "last": pl.Int64,
-    }
-
-
-def test_fill_null_minimal_upcast_4056() -> None:
-    df = pl.DataFrame({"a": [-1, 2, None]})
-    df = df.with_columns(pl.col("a").cast(pl.Int8))
-    assert df.with_columns(pl.col(pl.Int8).fill_null(-1)).dtypes[0] == pl.Int8
-    assert df.with_columns(pl.col(pl.Int8).fill_null(-1000)).dtypes[0] == pl.Int32
-
-
-def test_pow_dtype() -> None:
+def test_date_datetime() -> None:
     df = pl.DataFrame(
         {
-            "foo": [1, 2, 3, 4, 5],
+            "year": [2001, 2002, 2003],
+            "month": [1, 2, 3],
+            "day": [1, 2, 3],
+            "hour": [23, 12, 8],
         }
-    ).lazy()
-
-    df = df.with_columns([pl.col("foo").cast(pl.UInt32)]).with_columns(
-        [
-            (pl.col("foo") * 2**2).alias("scaled_foo"),
-            (pl.col("foo") * 2**2.1).alias("scaled_foo2"),
-        ]
     )
-    assert df.collect().dtypes == [pl.UInt32, pl.UInt32, pl.Float64]
-
-
-def test_bool_numeric_supertype() -> None:
-    df = pl.DataFrame({"v": [1, 2, 3, 4, 5, 6]})
-    for dt in [
-        pl.UInt8,
-        pl.UInt16,
-        pl.UInt32,
-        pl.UInt64,
-        pl.Int8,
-        pl.Int16,
-        pl.Int32,
-        pl.Int64,
-    ]:
-        assert (
-            df.select([(pl.col("v") < 3).sum().cast(dt) / pl.count()]).item()
-            - 0.3333333
-            <= 0.00001
-        )
+    out = df.select(
+        pl.all(),
+        pl.datetime("year", "month", "day", "hour").dt.hour().cast(int).alias("h2"),
+        pl.date("year", "month", "day").dt.day().cast(int).alias("date"),
+    )
+    assert_series_equal(out["date"], df["day"].rename("date"))
+    assert_series_equal(out["h2"], df["hour"].rename("h2"))
 
 
-def test_with_context() -> None:
-    df_a = pl.DataFrame({"a": [1, 2, 3], "b": ["a", "c", None]}).lazy()
-    df_b = pl.DataFrame({"c": ["foo", "ham"]})
+def test_time() -> None:
+    df = pl.DataFrame(
+        {
+            "hour": [7, 14, 21],
+            "min": [10, 20, 30],
+            "sec": [15, 30, 45],
+            "micro": [123456, 555555, 987654],
+        }
+    )
+    out = df.select(
+        pl.all(),
+        pl.time("hour", "min", "sec", "micro").dt.hour().cast(int).alias("h2"),
+        pl.time("hour", "min", "sec", "micro").dt.minute().cast(int).alias("m2"),
+        pl.time("hour", "min", "sec", "micro").dt.second().cast(int).alias("s2"),
+        pl.time("hour", "min", "sec", "micro").dt.microsecond().cast(int).alias("ms2"),
+    )
+    assert_series_equal(out["h2"], df["hour"].rename("h2"))
+    assert_series_equal(out["m2"], df["min"].rename("m2"))
+    assert_series_equal(out["s2"], df["sec"].rename("s2"))
+    assert_series_equal(out["ms2"], df["micro"].rename("ms2"))
 
-    assert (
-        df_a.with_context(df_b.lazy()).select([pl.col("b") + pl.col("c").first()])
-    ).collect().to_dict(False) == {"b": ["afoo", "cfoo", None]}
 
-    with pytest.raises(pl.ComputeError):
-        (df_a.with_context(df_b.lazy()).select(["a", "c"])).collect()
+def test_empty_duration() -> None:
+    s = pl.DataFrame([], {"days": pl.Int32}).select(pl.duration(days="days"))
+    assert s.dtypes == [pl.Duration("ns")]
+    assert s.shape == (0, 1)
 
 
-def test_from_dicts_nested_nulls() -> None:
-    assert pl.from_dicts([{"a": [None, None]}, {"a": [1, 2]}]).to_dict(False) == {
-        "a": [[None, None], [1, 2]]
-    }
-
+def test_list_concat() -> None:
+    s0 = pl.Series("a", [[1, 2]])
+    s1 = pl.Series("b", [[3, 4, 5]])
+    expected = pl.Series("a", [[1, 2, 3, 4, 5]])
 
-def test_group_schema_err() -> None:
-    df = pl.DataFrame({"foo": [None, 1, 2], "bar": [1, 2, 3]}).lazy()
-    with pytest.raises(pl.ColumnNotFoundError):
-        df.groupby("not-existent").agg(pl.col("bar").max().alias("max_bar")).schema
+    out = s0.list.concat([s1])
+    assert_series_equal(out, expected)
 
+    out = s0.list.concat(s1)
+    assert_series_equal(out, expected)
 
-def test_schema_inference_from_rows() -> None:
-    # these have to upcast to float
-    assert pl.from_records([[1, 2.1, 3], [4, 5, 6.4]]).to_dict(False) == {
-        "column_0": [1.0, 2.1, 3.0],
-        "column_1": [4.0, 5.0, 6.4],
-    }
-    assert pl.from_dicts([{"a": 1, "b": 2}, {"a": 3.1, "b": 4.5}]).to_dict(False) == {
-        "a": [1.0, 3.1],
-        "b": [2.0, 4.5],
-    }
+    df = pl.DataFrame([s0, s1])
+    assert_series_equal(df.select(pl.concat_list(["a", "b"]).alias("a"))["a"], expected)
+    assert_series_equal(
+        df.select(pl.col("a").list.concat("b").alias("a"))["a"], expected
+    )
+    assert_series_equal(
+        df.select(pl.col("a").list.concat(["b"]).alias("a"))["a"], expected
+    )
 
 
-def test_lazy_map_schema() -> None:
-    df = pl.DataFrame({"a": [1, 2, 3], "b": ["a", "b", "c"]})
+def test_concat_list_with_lit() -> None:
+    df = pl.DataFrame({"a": [1, 2, 3]})
 
-    # identity
-    assert_frame_equal(df.lazy().map(lambda x: x).collect(), df)
+    assert df.select(pl.concat_list([pl.col("a"), pl.lit(1)]).alias("a")).to_dict(
+        False
+    ) == {"a": [[1, 1], [2, 1], [3, 1]]}
 
-    def custom(df: pl.DataFrame) -> pl.Series:
-        return df["a"]
+    assert df.select(pl.concat_list([pl.lit(1), pl.col("a")]).alias("a")).to_dict(
+        False
+    ) == {"a": [[1, 1], [1, 2], [1, 3]]}
 
-    with pytest.raises(
-        pl.ComputeError,
-        match="Expected 'LazyFrame.map' to return a 'DataFrame', got a",
-    ):
-        df.lazy().map(custom).collect()  # type: ignore[arg-type]
-
-    def custom2(
-        df: pl.DataFrame,
-    ) -> pl.DataFrame:
-        # changes schema
-        return df.select(pl.all().cast(pl.Utf8))
-
-    with pytest.raises(
-        pl.ComputeError,
-        match="The output schema of 'LazyFrame.map' is incorrect. Expected",
-    ):
-        df.lazy().map(custom2).collect()
 
-    assert df.lazy().map(custom2, validate_output_schema=False).collect().to_dict(
-        False
-    ) == {"a": ["1", "2", "3"], "b": ["a", "b", "c"]}
+def test_concat_list_empty_raises() -> None:
+    with pytest.raises(pl.ComputeError):
+        pl.DataFrame({"a": [1, 2, 3]}).with_columns(pl.concat_list([]))
 
 
-def test_join_as_of_by_schema() -> None:
-    a = pl.DataFrame({"a": [1], "b": [2], "c": [3]}).lazy()
-    b = pl.DataFrame({"a": [1], "b": [2], "d": [4]}).lazy()
-    q = a.join_asof(b, on=pl.col("a").set_sorted(), by="b")
-    assert q.collect().columns == q.columns
+def test_list_concat_nulls() -> None:
+    assert pl.DataFrame(
+        {
+            "a": [["a", "b"], None, ["c", "d", "e"], None],
+            "t": [["x"], ["y"], None, None],
+        }
+    ).with_columns(pl.concat_list(["a", "t"]).alias("concat"))["concat"].to_list() == [
+        ["a", "b", "x"],
+        None,
+        None,
+        None,
+    ]
 
 
-def test_unknown_apply() -> None:
-    df = pl.DataFrame(
-        {"Amount": [10, 1, 1, 5], "Flour": ["1000g", "100g", "50g", "75g"]}
-    )
+def test_concat_list_in_agg_6397() -> None:
+    df = pl.DataFrame({"group": [1, 2, 2, 3], "value": ["a", "b", "c", "d"]})
 
-    q = df.lazy().select(
+    # single list
+    assert df.groupby("group").agg(
         [
-            pl.col("Amount"),
-            pl.col("Flour").apply(lambda x: 100.0) / pl.col("Amount"),
+            # this casts every element to a list
+            pl.concat_list(pl.col("value")),
         ]
-    )
-
-    assert q.collect().to_dict(False) == {
-        "Amount": [10, 1, 1, 5],
-        "Flour": [10.0, 100.0, 100.0, 20.0],
+    ).sort("group").to_dict(False) == {
+        "group": [1, 2, 3],
+        "value": [[["a"]], [["b"], ["c"]], [["d"]]],
     }
-    assert q.dtypes == [pl.Int64, pl.Unknown]
-
 
-def test_remove_redundant_mapping_4668() -> None:
-    df = pl.DataFrame([["a"]] * 2, ["A", "B "]).lazy()
-    clean_name_dict = {x: " ".join(x.split()) for x in df.columns}
-    df = df.rename(clean_name_dict)
-    assert df.columns == ["A", "B"]
+    # nested list
+    assert df.groupby("group").agg(
+        [
+            pl.concat_list(pl.col("value").implode()).alias("result"),
+        ]
+    ).sort("group").to_dict(False) == {
+        "group": [1, 2, 3],
+        "result": [[["a"]], [["b", "c"]], [["d"]]],
+    }
 
 
-def test_fold_all_schema() -> None:
+def test_list_concat_supertype() -> None:
     df = pl.DataFrame(
-        {
-            "A": [1, 2, 3, 4, 5],
-            "fruits": ["banana", "banana", "apple", "apple", "banana"],
-            "B": [5, 4, 3, 2, 1],
-            "cars": ["beetle", "audi", "beetle", "beetle", "beetle"],
-            "optional": [28, 300, None, 2, -30],
-        }
+        [pl.Series("a", [1, 2], pl.UInt8), pl.Series("b", [10000, 20000], pl.UInt16)]
     )
-    # divide because of overflow
-    assert df.select(pl.sum(pl.all().hash(seed=1) // int(1e8))).dtypes == [pl.UInt64]
-
-
-def test_fill_null_static_schema_4843() -> None:
-    df1 = pl.DataFrame(
+    assert df.with_columns(pl.concat_list(pl.col(["a", "b"])).alias("concat_list"))[
+        "concat_list"
+    ].to_list() == [[1, 10000], [2, 20000]]
+
+
+def test_categorical_list_concat_4762() -> None:
+    df = pl.DataFrame({"x": "a"})
+    expected = {"x": [["a", "a"]]}
+
+    q = df.lazy().select([pl.concat_list([pl.col("x").cast(pl.Categorical)] * 2)])
+    with pl.StringCache():
+        assert q.collect().to_dict(False) == expected
+
+
+def test_list_concat_rolling_window() -> None:
+    # inspired by:
+    # https://stackoverflow.com/questions/70377100/use-the-rolling-function-of-polars-to-get-a-list-of-all-values-in-the-rolling-wi
+    # this tests if it works without specifically creating list dtype upfront. note that
+    # the given answer is preferred over this snippet as that reuses the list array when
+    # shifting
+    df = pl.DataFrame(
         {
-            "a": [1, 2, None],
-            "b": [1, None, 4],
+            "A": [1.0, 2.0, 9.0, 2.0, 13.0],
         }
-    ).lazy()
+    )
+    out = df.with_columns(
+        [pl.col("A").shift(i).alias(f"A_lag_{i}") for i in range(3)]
+    ).select(
+        [pl.concat_list([f"A_lag_{i}" for i in range(3)][::-1]).alias("A_rolling")]
+    )
+    assert out.shape == (5, 1)
 
-    df2 = df1.select([pl.col(pl.Int64).fill_null(0)])
-    df3 = df2.select(pl.col(pl.Int64))
-    assert df3.schema == {"a": pl.Int64, "b": pl.Int64}
-
-
-def test_shrink_dtype() -> None:
-    out = pl.DataFrame(
-        {
-            "a": [1, 2, 3],
-            "b": [1, 2, 2 << 32],
-            "c": [-1, 2, 1 << 30],
-            "d": [-112, 2, 112],
-            "e": [-112, 2, 129],
-            "f": ["a", "b", "c"],
-            "g": [0.1, 1.32, 0.12],
-            "h": [True, None, False],
-        }
-    ).select(pl.all().shrink_dtype())
-    assert out.dtypes == [
-        pl.Int8,
-        pl.Int64,
-        pl.Int32,
-        pl.Int8,
-        pl.Int16,
-        pl.Utf8,
-        pl.Float32,
-        pl.Boolean,
+    s = out.to_series()
+    assert s.dtype == pl.List
+    assert s.to_list() == [
+        [None, None, 1.0],
+        [None, 1.0, 2.0],
+        [1.0, 2.0, 9.0],
+        [2.0, 9.0, 2.0],
+        [9.0, 2.0, 13.0],
     ]
 
-    assert out.to_dict(False) == {
-        "a": [1, 2, 3],
-        "b": [1, 2, 8589934592],
-        "c": [-1, 2, 1073741824],
-        "d": [-112, 2, 112],
-        "e": [-112, 2, 129],
-        "f": ["a", "b", "c"],
-        "g": [0.10000000149011612, 1.3200000524520874, 0.11999999731779099],
-        "h": [True, None, False],
-    }
-
-
-def test_diff_duration_dtype() -> None:
-    dates = ["2022-01-01", "2022-01-02", "2022-01-03", "2022-01-03"]
-    df = pl.DataFrame({"date": pl.Series(dates).str.strptime(pl.Date, "%Y-%m-%d")})
+    # this test proper null behavior of concat list
+    out = (
+        df.with_columns(pl.col("A").reshape((-1, 1)))  # first turn into a list
+        .with_columns(
+            [
+                pl.col("A").shift(i).alias(f"A_lag_{i}")
+                for i in range(3)  # slice the lists to a lag
+            ]
+        )
+        .select(
+            [
+                pl.all(),
+                pl.concat_list([f"A_lag_{i}" for i in range(3)][::-1]).alias(
+                    "A_rolling"
+                ),
+            ]
+        )
+    )
+    assert out.shape == (5, 5)
 
-    assert df.select(pl.col("date").diff() < pl.duration(days=1))["date"].to_list() == [
-        None,
-        False,
-        False,
-        True,
-    ]
+    l64 = pl.List(pl.Float64)
+    assert out.schema == {
+        "A": l64,
+        "A_lag_0": l64,
+        "A_lag_1": l64,
+        "A_lag_2": l64,
+        "A_rolling": l64,
+    }
 
 
-def test_boolean_agg_schema() -> None:
-    df = pl.DataFrame(
-        {
-            "x": [1, 1, 1],
-            "y": [False, True, False],
-        }
-    ).lazy()
+def test_concat_list_reverse_struct_fields() -> None:
+    df = pl.DataFrame({"nums": [1, 2, 3, 4], "letters": ["a", "b", "c", "d"]}).select(
+        [
+            pl.col("nums"),
+            pl.struct(["letters", "nums"]).alias("combo"),
+            pl.struct(["nums", "letters"]).alias("reverse_combo"),
+        ]
+    )
+    result1 = df.select(pl.concat_list(["combo", "reverse_combo"]))
+    result2 = df.select(pl.concat_list(["combo", "combo"]))
+    assert_frame_equal(result1, result2)
 
-    agg_df = df.groupby("x").agg(pl.col("y").max().alias("max_y"))
 
-    for streaming in [True, False]:
-        assert (
-            agg_df.collect(streaming=streaming).schema
-            == agg_df.schema
-            == {"x": pl.Int64, "max_y": pl.Boolean}
-        )
+def test_struct_args_kwargs() -> None:
+    df = pl.DataFrame({"a": [1, 2], "b": [3, 4], "c": ["a", "b"]})
 
+    # Single input
+    result = df.select(r=pl.struct((pl.col("a") + pl.col("b")).alias("p")))
+    expected = pl.DataFrame({"r": [{"p": 4}, {"p": 6}]})
+    assert_frame_equal(result, expected)
 
-def test_schema_owned_arithmetic_5669() -> None:
-    df = (
-        pl.DataFrame({"A": [1, 2, 3]})
-        .lazy()
-        .filter(pl.col("A") >= 3)
-        .with_columns(-pl.col("A").alias("B"))
-        .collect()
-    )
-    assert df.columns == ["A", "literal"], df.columns
+    # List input
+    result = df.select(r=pl.struct([pl.col("a").alias("p"), pl.col("b").alias("q")]))
+    expected = pl.DataFrame({"r": [{"p": 1, "q": 3}, {"p": 2, "q": 4}]})
+    assert_frame_equal(result, expected)
 
+    # Positional input
+    result = df.select(r=pl.struct(pl.col("a").alias("p"), pl.col("b").alias("q")))
+    assert_frame_equal(result, expected)
 
-def test_fill_null_f32_with_lit() -> None:
-    # ensure the literal integer does not upcast the f32 to an f64
-    df = pl.DataFrame({"a": [1.1, 1.2]}, schema=[("a", pl.Float32)])
-    assert df.fill_null(value=0).dtypes == [pl.Float32]
+    # Keyword input
+    result = df.select(r=pl.struct(p="a", q="b"))
+    assert_frame_equal(result, expected)
 
 
-def test_lazy_rename() -> None:
-    df = pl.DataFrame({"x": [1], "y": [2]})
+def test_struct_with_lit() -> None:
+    expr = pl.struct([pl.col("a"), pl.lit(1).alias("b")])
 
     assert (
-        df.lazy().rename({"y": "x", "x": "y"}).select(["x", "y"]).collect()
-    ).to_dict(False) == {"x": [2], "y": [1]}
-
+        pl.DataFrame({"a": pl.Series([], dtype=pl.Int64)}).select(expr).to_dict(False)
+    ) == {"a": []}
 
-def test_all_null_cast_5826() -> None:
-    df = pl.DataFrame(data=[pl.Series("a", [None], dtype=pl.Utf8)])
-    out = df.with_columns(pl.col("a").cast(pl.Boolean))
-    assert out.dtypes == [pl.Boolean]
-    assert out.item() is None
+    assert (
+        pl.DataFrame({"a": pl.Series([1], dtype=pl.Int64)}).select(expr).to_dict(False)
+    ) == {"a": [{"a": 1, "b": 1}]}
 
+    assert (
+        pl.DataFrame({"a": pl.Series([1, 2], dtype=pl.Int64)})
+        .select(expr)
+        .to_dict(False)
+    ) == {"a": [{"a": 1, "b": 1}, {"a": 2, "b": 1}]}
 
-def test_empty_list_eval_schema_5734() -> None:
-    df = pl.DataFrame({"a": [[{"b": 1, "c": 2}]]})
-    assert df.filter(False).select(
-        pl.col("a").arr.eval(pl.element().struct.field("b"))
-    ).schema == {"a": pl.List(pl.Int64)}
 
+def test_eager_struct() -> None:
+    with pytest.raises(pl.DuplicateError, match="multiple fields with name '' found"):
+        s = pl.struct([pl.Series([1, 2, 3]), pl.Series(["a", "b", "c"])], eager=True)
 
-def test_schema_true_divide_6643() -> None:
-    df = pl.DataFrame({"a": [1]})
-    a = pl.col("a")
-    assert df.lazy().select(a / 2).select(pl.col(pl.Int64)).collect().shape == (0, 0)
+    s = pl.struct(
+        [pl.Series("a", [1, 2, 3]), pl.Series("b", ["a", "b", "c"])], eager=True
+    )
+    assert s.dtype == pl.Struct
 
 
-def test_rename_schema_order_6660() -> None:
+def test_struct_from_schema_only() -> None:
+    # we create a dataframe with default types
     df = pl.DataFrame(
         {
-            "a": [],
-            "b": [],
-            "c": [],
-            "d": [],
+            "str": ["a", "b", "c", "d", "e"],
+            "u8": [1, 2, 3, 4, 5],
+            "i32": [1, 2, 3, 4, 5],
+            "f64": [1, 2, 3, 4, 5],
+            "cat": ["a", "b", "c", "d", "e"],
+            "datetime": pl.Series(
+                [
+                    date(2023, 1, 1),
+                    date(2023, 1, 2),
+                    date(2023, 1, 3),
+                    date(2023, 1, 4),
+                    date(2023, 1, 5),
+                ]
+            ),
+            "bool": [1, 0, 1, 1, 0],
+            "list[u8]": [[1], [2], [3], [4], [5]],
         }
     )
 
-    mapper = {"a": "1", "b": "2", "c": "3", "d": "4"}
-
-    renamed = df.lazy().rename(mapper)
-
-    computed = renamed.select([pl.all(), pl.col("4").alias("computed")])
-
-    assert renamed.schema == renamed.collect().schema
-    assert computed.schema == computed.collect().schema
+    # specify a schema with specific dtypes
+    s = df.select(
+        pl.struct(
+            schema={
+                "str": pl.Utf8,
+                "u8": pl.UInt8,
+                "i32": pl.Int32,
+                "f64": pl.Float64,
+                "cat": pl.Categorical,
+                "datetime": pl.Datetime("ms"),
+                "bool": pl.Boolean,
+                "list[u8]": pl.List(pl.UInt8),
+            }
+        ).alias("s")
+    )["s"]
 
+    # check dtypes
+    assert s.dtype == pl.Struct(
+        [
+            pl.Field("str", pl.Utf8),
+            pl.Field("u8", pl.UInt8),
+            pl.Field("i32", pl.Int32),
+            pl.Field("f64", pl.Float64),
+            pl.Field("cat", pl.Categorical),
+            pl.Field("datetime", pl.Datetime("ms")),
+            pl.Field("bool", pl.Boolean),
+            pl.Field("list[u8]", pl.List(pl.UInt8)),
+        ]
+    )
 
-def test_from_dicts_all_cols_6716() -> None:
-    dicts = [{"a": None} for _ in range(20)] + [{"a": "crash"}]
+    # check values
+    assert s.to_list() == [
+        {
+            "str": "a",
+            "u8": 1,
+            "i32": 1,
+            "f64": 1.0,
+            "cat": "a",
+            "datetime": datetime(2023, 1, 1, 0, 0),
+            "bool": True,
+            "list[u8]": [1],
+        },
+        {
+            "str": "b",
+            "u8": 2,
+            "i32": 2,
+            "f64": 2.0,
+            "cat": "b",
+            "datetime": datetime(2023, 1, 2, 0, 0),
+            "bool": False,
+            "list[u8]": [2],
+        },
+        {
+            "str": "c",
+            "u8": 3,
+            "i32": 3,
+            "f64": 3.0,
+            "cat": "c",
+            "datetime": datetime(2023, 1, 3, 0, 0),
+            "bool": True,
+            "list[u8]": [3],
+        },
+        {
+            "str": "d",
+            "u8": 4,
+            "i32": 4,
+            "f64": 4.0,
+            "cat": "d",
+            "datetime": datetime(2023, 1, 4, 0, 0),
+            "bool": True,
+            "list[u8]": [4],
+        },
+        {
+            "str": "e",
+            "u8": 5,
+            "i32": 5,
+            "f64": 5.0,
+            "cat": "e",
+            "datetime": datetime(2023, 1, 5, 0, 0),
+            "bool": False,
+            "list[u8]": [5],
+        },
+    ]
 
-    with pytest.raises(
-        pl.ComputeError, match="make sure that all rows have the same schema"
-    ):
-        pl.from_dicts(dicts, infer_schema_length=20)
-    assert pl.from_dicts(dicts, infer_schema_length=None).dtypes == [pl.Utf8]
 
+def test_struct_broadcasting() -> None:
+    df = pl.DataFrame(
+        {
+            "col1": [1, 2],
+            "col2": [10, 20],
+        }
+    )
 
-def test_from_dicts_empty() -> None:
-    with pytest.raises(pl.NoDataError, match="No rows. Cannot infer schema."):
-        pl.from_dicts([])
+    assert (
+        df.select(
+            pl.struct(
+                [
+                    pl.lit("a").alias("a"),
+                    pl.col("col1").alias("col1"),
+                ]
+            ).alias("my_struct")
+        )
+    ).to_dict(False) == {"my_struct": [{"a": "a", "col1": 1}, {"a": "a", "col1": 2}]}
 
 
-def test_duration_division_schema() -> None:
-    df = pl.DataFrame({"a": [1]})
-    q = (
-        df.lazy()
-        .with_columns(pl.col("a").cast(pl.Duration))
-        .select(pl.col("a") / pl.col("a"))
+def test_struct_list_cat_8235() -> None:
+    df = pl.DataFrame(
+        {"values": [["a", "b", "c"]]}, schema={"values": pl.List(pl.Categorical)}
     )
-
-    assert q.schema == {"a": pl.Float64}
-    assert q.collect().to_dict(False) == {"a": [1.0]}
+    assert df.select(pl.struct("values")).to_dict(False) == {
+        "values": [{"values": ["a", "b", "c"]}]
+    }
 
 
-def test_int_operator_stability() -> None:
-    for dt in pl.datatypes.INTEGER_DTYPES:
-        s = pl.Series(values=[10], dtype=dt)
-        assert pl.select(pl.lit(s) // 2).dtypes == [dt]
-        assert pl.select(pl.lit(s) + 2).dtypes == [dt]
-        assert pl.select(pl.lit(s) - 2).dtypes == [dt]
-        assert pl.select(pl.lit(s) * 2).dtypes == [dt]
-        assert pl.select(pl.lit(s) / 2).dtypes == [pl.Float64]
+def test_struct_lit_cast() -> None:
+    df = pl.DataFrame({"a": [1, 2, 3]})
+    schema = {"a": pl.Int64, "b": pl.List(pl.Int64)}
+
+    for lit in [pl.lit(None), pl.lit([[]])]:
+        s = df.select(pl.struct([pl.col("a"), lit.alias("b")], schema=schema))["a"]  # type: ignore[arg-type]
+        assert s.dtype == pl.Struct(
+            [pl.Field("a", pl.Int64), pl.Field("b", pl.List(pl.Int64))]
+        )
+        assert s.to_list() == [
+            {"a": 1, "b": None},
+            {"a": 2, "b": None},
+            {"a": 3, "b": None},
+        ]
 
 
-def test_deep_subexpression_f32_schema_7129() -> None:
-    df = pl.DataFrame({"a": [1.1, 2.3, 3.4, 4.5]}, schema={"a": pl.Float32()})
-    assert df.with_columns(pl.col("a") - pl.col("a").median()).dtypes == [pl.Float32]
-    assert df.with_columns(
-        (pl.col("a") - pl.col("a").mean()) / (pl.col("a").std() + 0.001)
-    ).dtypes == [pl.Float32]
+def test_suffix_in_struct_creation() -> None:
+    assert (
+        pl.DataFrame(
+            {
+                "a": [1, 2],
+                "b": [3, 4],
+                "c": [5, 6],
+            }
+        ).select(pl.struct(pl.col(["a", "c"]).suffix("_foo")).alias("bar"))
+    ).unnest("bar").to_dict(False) == {"a_foo": [1, 2], "c_foo": [5, 6]}
 
 
-def test_absence_off_null_prop_8224() -> None:
-    # a reminder to self to not do null propagation
-    # it is inconsistent and makes output dtype
-    # dependent of the data, big no!
+def test_concat_str() -> None:
+    df = pl.DataFrame({"a": ["a", "b", "c"], "b": [1, 2, 3]})
 
-    def sub_col_min(column: str, min_column: str) -> pl.Expr:
-        return pl.col(column).sub(pl.col(min_column).min())
+    out = df.select([pl.concat_str(["a", "b"], separator="-")])
+    assert out["a"].to_list() == ["a-1", "b-2", "c-3"]
 
-    df = pl.DataFrame(
-        {
-            "group": [1, 1, 2, 2],
-            "vals_num": [10.0, 11.0, 12.0, 13.0],
-            "vals_partial": [None, None, 12.0, 13.0],
-            "vals_null": [None, None, None, None],
-        }
-    )
 
-    q = (
-        df.lazy()
-        .groupby("group")
-        .agg(
-            [
-                sub_col_min("vals_num", "vals_num").alias("sub_num"),
-                sub_col_min("vals_num", "vals_partial").alias("sub_partial"),
-                sub_col_min("vals_num", "vals_null").alias("sub_null"),
-            ]
-        )
-    )
+def test_concat_str_wildcard_expansion() -> None:
+    # one function requires wildcard expansion the other need
+    # this tests the nested behavior
+    # see: #2867
 
-    assert q.collect().dtypes == [
-        pl.Int64,
-        pl.List(pl.Float64),
-        pl.List(pl.Float64),
-        pl.List(pl.Float64),
-    ]
+    df = pl.DataFrame({"a": ["x", "Y", "z"], "b": ["S", "o", "S"]})
+    assert df.select(
+        pl.concat_str(pl.all()).str.to_lowercase()
+    ).to_series().to_list() == ["xs", "yo", "zs"]
 
 
-@typing.no_type_check
-def test_schemas() -> None:
-    # add all expression output tests here:
-    args = [
-        # coalesce
-        {
-            "data": {"x": ["x"], "y": ["y"]},
-            "expr": pl.coalesce(pl.col("x"), pl.col("y")),
-            "expected_select": {"x": pl.Utf8},
-            "expected_gb": {"x": pl.List(pl.Utf8)},
-        },
-        # boolean sum
-        {
-            "data": {"x": [True]},
-            "expr": pl.col("x").sum(),
-            "expected_select": {"x": pl.UInt32},
-            "expected_gb": {"x": pl.UInt32},
-        },
-    ]
-    for arg in args:
-        df = pl.DataFrame(arg["data"])
+def test_format() -> None:
+    df = pl.DataFrame({"a": ["a", "b", "c"], "b": [1, 2, 3]})
 
-        # test selection schema
-        schema = df.select(arg["expr"]).schema
-        for key, dtype in arg["expected_select"].items():
-            assert schema[key] == dtype
-
-        # test groupby schema
-        schema = df.groupby(pl.lit(1)).agg(arg["expr"]).schema
-        for key, dtype in arg["expected_gb"].items():
-            assert schema[key] == dtype
-
-
-def test_list_null_constructor_schema() -> None:
-    expected = pl.List(pl.Null)
-    assert pl.Series([[]]).dtype == expected
-    assert pl.Series([[]], dtype=pl.List).dtype == expected
-    assert pl.DataFrame({"a": [[]]}).dtypes[0] == expected
-    assert pl.DataFrame(schema={"a": pl.List}).dtypes[0] == expected
+    out = df.select([pl.format("foo_{}_bar_{}", pl.col("a"), "b").alias("fmt")])
+    assert out["fmt"].to_list() == ["foo_a_bar_1", "foo_b_bar_2", "foo_c_bar_3"]
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_serde.py` & `polars_lts_cpu-0.18.0/tests/unit/test_serde.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_series.py` & `polars_lts_cpu-0.18.0/tests/unit/test_series.py`

 * *Files 1% similar despite different names*

```diff
@@ -4,14 +4,15 @@
 from datetime import date, datetime, time, timedelta
 from typing import TYPE_CHECKING, Any, Iterator, cast
 
 import numpy as np
 import pandas as pd
 import pyarrow as pa
 import pytest
+from numpy.testing import assert_array_equal
 
 import polars as pl
 from polars.datatypes import (
     Date,
     Datetime,
     Field,
     Float64,
@@ -389,23 +390,23 @@
     assert_series_equal(a**2, pl.Series([1.0, 4.0], dtype=Float64))
     assert_series_equal(b**3, pl.Series([None, 8.0], dtype=Float64))
     assert_series_equal(a**a, pl.Series([1.0, 4.0], dtype=Float64))
     assert_series_equal(b**b, pl.Series([None, 4.0], dtype=Float64))
     assert_series_equal(a**b, pl.Series([None, 4.0], dtype=Float64))
     with pytest.raises(ValueError):
         c**2
-    with pytest.raises(pl.ComputeError):
+    with pytest.raises(pl.ColumnNotFoundError):
         a ** "hi"  # type: ignore[operator]
 
     # rpow
     assert_series_equal(2.0**a, pl.Series("literal", [2.0, 4.0], dtype=Float64))
     assert_series_equal(2**b, pl.Series("literal", [None, 4.0], dtype=Float64))
     with pytest.raises(ValueError):
         2**c
-    with pytest.raises(pl.ComputeError):
+    with pytest.raises(pl.ColumnNotFoundError):
         "hi" ** a
 
     # Series.pow() method
     assert_series_equal(a.pow(2), pl.Series([1.0, 4.0], dtype=Float64))
 
 
 def test_add_string() -> None:
@@ -446,15 +447,15 @@
 
 
 def test_various() -> None:
     a = pl.Series("a", [1, 2])
     assert a.is_null().sum() == 0
     assert a.name == "a"
 
-    a.rename("b", in_place=True)
+    a = a.rename("b")
     assert a.name == "b"
     assert a.len() == 2
     assert len(a) == 2
 
     a.append(a.clone())
     assert_series_equal(a, pl.Series("b", [1, 2, 1, 2]))
 
@@ -593,14 +594,28 @@
     assert len(b) == 20
 
     a = pl.Series("a", [1, None, 2])
     assert a.null_count() == 1
     assert a.to_list() == [1, None, 2]
 
 
+def test_to_struct() -> None:
+    s = pl.Series("nums", ["12 34", "56 78", "90 00"]).str.extract_all(r"\d+")
+
+    assert s.list.to_struct().struct.fields == ["field_0", "field_1"]
+    assert s.list.to_struct(fields=lambda idx: f"n{idx:02}").struct.fields == [
+        "n00",
+        "n01",
+    ]
+    assert_frame_equal(
+        s.list.to_struct(fields=["one", "two"]).struct.unnest(),
+        pl.DataFrame({"one": ["12", "56", "90"], "two": ["34", "78", "00"]}),
+    )
+
+
 def test_sort() -> None:
     a = pl.Series("a", [2, 1, 3])
     assert_series_equal(a.sort(), pl.Series("a", [1, 2, 3]))
     assert_series_equal(a.sort(descending=True), pl.Series("a", [3, 2, 1]))
 
 
 def test_rechunk() -> None:
@@ -779,14 +794,22 @@
     c2 = np.multiply(a2, 3)
     assert_series_equal(
         cast(pl.Series, c2),
         pl.Series("a", [3.0, None, 9.0, 12.0, 15.0, None]),
     )
 
 
+def test_numpy_string_array() -> None:
+    s_utf8 = pl.Series("a", ["aa", "bb", "cc", "dd"], dtype=pl.Utf8)
+    assert_array_equal(
+        np.char.capitalize(s_utf8),
+        np.array(["Aa", "Bb", "Cc", "Dd"], dtype="<U2"),
+    )
+
+
 def test_get() -> None:
     a = pl.Series("a", [1, 2, 3])
     pos_idxs = pl.Series("idxs", [2, 0, 1, 0], dtype=pl.Int8)
     neg_and_pos_idxs = pl.Series(
         "neg_and_pos_idxs", [-2, 1, 0, -1, 2, -3], dtype=pl.Int8
     )
     assert a[0] == 1
@@ -1070,52 +1093,14 @@
     vals = [[12], "foo", 9]
     a = pl.Series("a", vals)
     assert a.dtype == pl.Object
     assert a.to_list() == vals
     assert a[1] == "foo"
 
 
-def test_repeat() -> None:
-    s = pl.repeat(2**31 - 1, 3, eager=True)
-    assert s.dtype == pl.Int32
-    assert s.len() == 3
-    assert s.to_list() == [2**31 - 1] * 3
-    s = pl.repeat(-(2**31), 4, eager=True)
-    assert s.dtype == pl.Int32
-    assert s.len() == 4
-    assert s.to_list() == [-(2**31)] * 4
-    s = pl.repeat(2**31, 5, eager=True)
-    assert s.dtype == pl.Int64
-    assert s.len() == 5
-    assert s.to_list() == [2**31] * 5
-    s = pl.repeat(-(2**31) - 1, 3, eager=True)
-    assert s.dtype == pl.Int64
-    assert s.len() == 3
-    assert s.to_list() == [-(2**31) - 1] * 3
-    s = pl.repeat("foo", 2, eager=True)
-    assert s.dtype == pl.Utf8
-    assert s.len() == 2
-    assert s.to_list() == ["foo"] * 2
-    s = pl.repeat(1.0, 5, eager=True)
-    assert s.dtype == pl.Float64
-    assert s.len() == 5
-    assert s.to_list() == [1.0] * 5
-    s = pl.repeat(True, 4, eager=True)
-    assert s.dtype == pl.Boolean
-    assert s.len() == 4
-    assert s.to_list() == [True] * 4
-    s = pl.repeat(None, 7, eager=True)
-    assert s.dtype == pl.Null
-    assert s.len() == 7
-    assert s.to_list() == [None] * 7
-    s = pl.repeat(0, 0, eager=True)
-    assert s.dtype == pl.Int32
-    assert s.len() == 0
-
-
 def test_shape() -> None:
     s = pl.Series([1, 2, 3])
     assert s.shape == (3,)
 
 
 @pytest.mark.parametrize("arrow_available", [True, False])
 def test_create_list_series(arrow_available: bool, monkeypatch: Any) -> None:
@@ -1240,49 +1225,22 @@
         slice(-1, -3, -1),
         slice(-3, None, -3),
     ):
         # confirm series slice matches python slice
         assert s[py_slice].to_list() == s.to_list()[py_slice]
 
 
-def test_arange_expr() -> None:
-    df = pl.DataFrame({"a": ["foobar", "barfoo"]})
-    out = df.select([pl.arange(0, pl.col("a").count() * 10)])
-    assert out.shape == (20, 1)
-    assert out.to_series(0)[-1] == 19
-
-    # eager arange
-    out2 = pl.arange(0, 10, 2, eager=True)
-    assert out2.to_list() == [0, 2, 4, 6, 8]
-
-    out3 = pl.arange(pl.Series([0, 19]), pl.Series([3, 39]), step=2, eager=True)
-    assert out3.dtype == pl.List
-    assert out3[0].to_list() == [0, 2]
-
-    df = pl.DataFrame({"start": [1, 2, 3, 5, 5, 5], "stop": [8, 3, 12, 8, 8, 8]})
-
-    assert df.select(pl.arange(pl.lit(1), pl.col("stop") + 1).alias("test")).to_dict(
-        False
-    ) == {
-        "test": [
-            [1, 2, 3, 4, 5, 6, 7, 8],
-            [1, 2, 3],
-            [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],
-            [1, 2, 3, 4, 5, 6, 7, 8],
-            [1, 2, 3, 4, 5, 6, 7, 8],
-            [1, 2, 3, 4, 5, 6, 7, 8],
-        ]
-    }
-
-
 def test_round() -> None:
     a = pl.Series("f", [1.003, 2.003])
     b = a.round(2)
     assert b.to_list() == [1.00, 2.00]
 
+    b = a.round()
+    assert b.to_list() == [1.0, 2.0]
+
 
 def test_apply_list_out() -> None:
     s = pl.Series("count", [3, 2, 2])
     out = s.apply(lambda val: pl.repeat(val, val, eager=True))
     assert out[0].to_list() == [3, 3, 3]
     assert out[1].to_list() == [2, 2]
     assert out[2].to_list() == [2, 2]
@@ -1381,38 +1339,38 @@
     assert s.kurtosis() == pytest.approx(expected)
     df = pl.DataFrame([s])
     assert np.isclose(df.select(pl.col("a").kurtosis())["a"][0], expected)
 
 
 def test_arr_lengths() -> None:
     s = pl.Series("a", [[1, 2], [1, 2, 3]])
-    assert_series_equal(s.arr.lengths(), pl.Series("a", [2, 3], dtype=UInt32))
+    assert_series_equal(s.list.lengths(), pl.Series("a", [2, 3], dtype=UInt32))
     df = pl.DataFrame([s])
     assert_series_equal(
-        df.select(pl.col("a").arr.lengths())["a"], pl.Series("a", [2, 3], dtype=UInt32)
+        df.select(pl.col("a").list.lengths())["a"], pl.Series("a", [2, 3], dtype=UInt32)
     )
 
 
 def test_arr_arithmetic() -> None:
     s = pl.Series("a", [[1, 2], [1, 2, 3]])
-    assert_series_equal(s.arr.sum(), pl.Series("a", [3, 6]))
-    assert_series_equal(s.arr.mean(), pl.Series("a", [1.5, 2.0]))
-    assert_series_equal(s.arr.max(), pl.Series("a", [2, 3]))
-    assert_series_equal(s.arr.min(), pl.Series("a", [1, 1]))
+    assert_series_equal(s.list.sum(), pl.Series("a", [3, 6]))
+    assert_series_equal(s.list.mean(), pl.Series("a", [1.5, 2.0]))
+    assert_series_equal(s.list.max(), pl.Series("a", [2, 3]))
+    assert_series_equal(s.list.min(), pl.Series("a", [1, 1]))
 
 
 def test_arr_ordering() -> None:
     s = pl.Series("a", [[2, 1], [1, 3, 2]])
-    assert_series_equal(s.arr.sort(), pl.Series("a", [[1, 2], [1, 2, 3]]))
-    assert_series_equal(s.arr.reverse(), pl.Series("a", [[1, 2], [2, 3, 1]]))
+    assert_series_equal(s.list.sort(), pl.Series("a", [[1, 2], [1, 2, 3]]))
+    assert_series_equal(s.list.reverse(), pl.Series("a", [[1, 2], [2, 3, 1]]))
 
 
 def test_arr_unique() -> None:
     s = pl.Series("a", [[2, 1], [1, 2, 2]])
-    result = s.arr.unique()
+    result = s.list.unique()
     assert len(result) == 2
     assert sorted(result[0]) == [1, 2]
     assert sorted(result[1]) == [1, 2]
 
 
 def test_sqrt() -> None:
     s = pl.Series("a", [1, 2])
@@ -1449,35 +1407,14 @@
 def test_strict_cast() -> None:
     with pytest.raises(pl.ComputeError):
         pl.Series("a", [2**16]).cast(dtype=pl.Int16, strict=True)
     with pytest.raises(pl.ComputeError):
         pl.DataFrame({"a": [2**16]}).select([pl.col("a").cast(pl.Int16, strict=True)])
 
 
-def test_list_concat() -> None:
-    s0 = pl.Series("a", [[1, 2]])
-    s1 = pl.Series("b", [[3, 4, 5]])
-    expected = pl.Series("a", [[1, 2, 3, 4, 5]])
-
-    out = s0.arr.concat([s1])
-    assert_series_equal(out, expected)
-
-    out = s0.arr.concat(s1)
-    assert_series_equal(out, expected)
-
-    df = pl.DataFrame([s0, s1])
-    assert_series_equal(df.select(pl.concat_list(["a", "b"]).alias("a"))["a"], expected)
-    assert_series_equal(
-        df.select(pl.col("a").arr.concat("b").alias("a"))["a"], expected
-    )
-    assert_series_equal(
-        df.select(pl.col("a").arr.concat(["b"]).alias("a"))["a"], expected
-    )
-
-
 def test_floor_divide() -> None:
     s = pl.Series("a", [1, 2, 3])
     assert_series_equal(s // 2, pl.Series("a", [0, 1, 1]))
     assert_series_equal(
         pl.DataFrame([s]).select(pl.col("a") // 2)["a"], pl.Series("a", [0, 1, 1])
     )
 
@@ -1962,19 +1899,43 @@
     with pl.StringCache():
         for values in [[None], ["foo", "bar"], [None, "foo", "bar"]]:
             expected = pl.Series("a", values, dtype=pl.Utf8).cast(pl.Categorical)
             a = pl.Series("a", values, dtype=pl.Categorical)
             assert_series_equal(a, expected)
 
 
-def test_nested_list_types_preserved() -> None:
-    expected_dtype = pl.UInt32
-    srs1 = pl.Series([pl.Series([3, 4, 5, 6], dtype=expected_dtype) for _ in range(5)])
-    for srs2 in srs1:
-        assert srs2.dtype == expected_dtype
+def test_iter_nested_list() -> None:
+    elems = list(pl.Series("s", [[1, 2], [3, 4]]))
+    assert_series_equal(elems[0], pl.Series([1, 2]))
+    assert_series_equal(elems[1], pl.Series([3, 4]))
+
+
+def test_iter_nested_struct() -> None:
+    # note: this feels inconsistent with the above test for nested list, but
+    # let's ensure the behaviour is codified before potentially modifying...
+    elems = list(pl.Series("s", [{"a": 1, "b": 2}, {"a": 3, "b": 4}]))
+    assert elems[0] == {"a": 1, "b": 2}
+    assert elems[1] == {"a": 3, "b": 4}
+
+
+@pytest.mark.parametrize(
+    "dtype",
+    [
+        pl.UInt8,
+        pl.Float32,
+        pl.Int32,
+        pl.Boolean,
+        pl.List(pl.Utf8),
+        pl.Struct([pl.Field("a", pl.Int64), pl.Field("b", pl.Boolean)]),
+    ],
+)
+def test_nested_list_types_preserved(dtype: pl.DataType) -> None:
+    srs = pl.Series([pl.Series([], dtype=dtype) for _ in range(5)])
+    for srs_nested in srs:
+        assert srs_nested.dtype == dtype
 
 
 def test_log_exp() -> None:
     a = pl.Series("a", [1, 100, 1000])
     b = pl.Series("a", [0.0, 2.0, 3.0])
     assert_series_equal(a.log10(), b)
 
@@ -2004,14 +1965,24 @@
     assert_series_equal(s.to_physical(), s)
 
     # casting a date results in an Int32
     s = pl.Series("a", [date(2020, 1, 1)] * 3)
     expected = pl.Series("a", [18262] * 3, dtype=Int32)
     assert_series_equal(s.to_physical(), expected)
 
+    # casting a categorical results in a UInt32
+    s = pl.Series(["cat1"]).cast(pl.Categorical)
+    expected = pl.Series([0], dtype=UInt32)
+    assert_series_equal(s.to_physical(), expected)
+
+    # casting a List(Categorical) results in a List(UInt32)
+    s = pl.Series([["cat1"]]).cast(pl.List(pl.Categorical))
+    expected = pl.Series([[0]], dtype=pl.List(UInt32))
+    assert_series_equal(s.to_physical(), expected)
+
 
 def test_is_between_datetime() -> None:
     s = pl.Series("a", [datetime(2020, 1, 1, 10, 0, 0), datetime(2020, 1, 1, 20, 0, 0)])
     start = datetime(2020, 1, 1, 12, 0, 0)
     end = datetime(2020, 1, 1, 23, 0, 0)
     expected = pl.Series("a", [False, True])
 
@@ -2508,16 +2479,16 @@
 
     ptr2 = s2.rechunk()._get_ptr()
     assert ptr != ptr2
 
 
 def test_null_comparisons() -> None:
     s = pl.Series("s", [None, "str", "a"])
-    assert (s.shift() == s).null_count() == 0
-    assert (s.shift() != s).null_count() == 0
+    assert (s.shift() == s).null_count() == 2
+    assert (s.shift() != s).null_count() == 2
 
 
 def test_min_max_agg_on_str() -> None:
     strings = ["b", "a", "x"]
     s = pl.Series(strings)
     assert (s.min(), s.max()) == ("a", "x")
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_single.py` & `polars_lts_cpu-0.18.0/tests/unit/test_single.py`

 * *Files identical despite different names*

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_streaming.py` & `polars_lts_cpu-0.18.0/tests/unit/streaming/test_streaming.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 import time
+import typing
 from datetime import date
 from typing import Any
 
 import numpy as np
 import pytest
 
 import polars as pl
@@ -124,15 +125,19 @@
     q = df.lazy().groupby("a").agg(pl.count()).sort("a")
     assert_frame_equal(q.collect(streaming=True), q.collect())
 
     q = df.lazy().with_columns(pl.col("a").cast(pl.Utf8))
     q = q.groupby("a").agg(pl.count()).sort("a")
     assert_frame_equal(q.collect(streaming=True), q.collect())
     q = df.lazy().with_columns(pl.col("a").alias("b"))
-    q = q.groupby(["a", "b"]).agg(pl.count()).sort("a")
+    q = (
+        q.groupby(["a", "b"])
+        .agg(pl.count(), pl.col("a").sum().alias("sum_a"))
+        .sort("a")
+    )
     assert_frame_equal(q.collect(streaming=True), q.collect())
 
 
 def test_streaming_groupby_sorted_fast_path() -> None:
     a = np.random.randint(0, 20, 80)
     df = pl.DataFrame(
         {
@@ -227,15 +232,15 @@
     assert a.join(a, how="cross").head().collect(streaming=True).shape == (5, 2)
     t1 = time.time()
     assert (t1 - t0) < 0.5
 
 
 @pytest.mark.slow()
 def test_ooc_sort(monkeypatch: Any) -> None:
-    monkeypatch.setenv("POLARS_FORCE_OOC_SORT", "1")
+    monkeypatch.setenv("POLARS_FORCE_OOC", "1")
 
     s = pl.arange(0, 100_000, eager=True).rename("idx")
 
     df = s.shuffle().to_frame()
 
     for descending in [True, False]:
         out = (
@@ -340,15 +345,15 @@
     (_, err) = capfd.readouterr()
     assert "df -> re-project-sink -> sort_multiple" in err
 
 
 @pytest.mark.write_disk()
 def test_streaming_sort(monkeypatch: Any, capfd: Any) -> None:
     monkeypatch.setenv("POLARS_VERBOSE", "1")
-    monkeypatch.setenv("POLARS_FORCE_OOC_SORT", "1")
+    monkeypatch.setenv("POLARS_FORCE_OOC", "1")
     # this creates a lot of duplicate partitions and triggers: #7568
     assert (
         pl.Series(np.random.randint(0, 100, 100))
         .to_frame("s")
         .lazy()
         .sort("s")
         .collect(streaming=True)["s"]
@@ -359,15 +364,15 @@
 
 
 @pytest.mark.write_disk()
 def test_streaming_groupby_ooc(monkeypatch: Any) -> None:
     np.random.seed(1)
     s = pl.Series("a", np.random.randint(0, 10, 100))
 
-    for env in ["POLARS_FORCE_OOC_SORT", "_NO_OP"]:
+    for env in ["POLARS_FORCE_OOC", "_NO_OP"]:
         monkeypatch.setenv(env, "1")
         q = (
             s.to_frame()
             .lazy()
             .groupby("a")
             .agg(pl.first("a").alias("a_first"), pl.last("a").alias("a_last"))
             .sort("a")
@@ -423,7 +428,75 @@
     assert df1.groupby("tuples").agg(pl.count(), pl.col("B").first()).sort("B").collect(
         streaming=True
     ).to_dict(False) == {
         "tuples": [{"A": 3, "C": 4}, {"A": 1, "C": 2}, {"A": 2, "C": 3}],
         "count": [1, 1, 2],
         "B": ["apple", "google", "ms"],
     }
+
+
+@pytest.mark.slow()
+def test_streaming_groupby_all_numeric_types_stability_8570() -> None:
+    m = 1000
+    n = 1000
+
+    rng = np.random.default_rng(seed=0)
+    dfa = pl.DataFrame({"x": pl.arange(start=0, end=n, eager=True)})
+    dfb = pl.DataFrame(
+        {
+            "y": rng.integers(low=0, high=10, size=m),
+            "z": rng.integers(low=0, high=2, size=m),
+        }
+    )
+    dfc = dfa.join(dfb, how="cross")
+
+    for keys in [["x", "y"], "z"]:
+        for dtype in [pl.Boolean, *pl.INTEGER_DTYPES]:
+            # the alias checks if the schema is correctly handled
+            dfd = (
+                dfc.lazy()
+                .with_columns(pl.col("z").cast(dtype))
+                .groupby(keys)
+                .agg(pl.col("z").sum().alias("z_sum"))
+                .collect(streaming=True)
+            )
+            assert dfd["z_sum"].sum() == dfc["z"].sum()
+
+
+@typing.no_type_check
+def test_streaming_groupby_categorical_aggregate() -> None:
+    with pl.StringCache():
+        out = (
+            pl.LazyFrame(
+                {
+                    "a": pl.Series(
+                        ["a", "a", "b", "b", "c", "c", None, None], dtype=pl.Categorical
+                    ),
+                    "b": pl.Series(
+                        pl.date_range(
+                            date(2023, 4, 28),
+                            date(2023, 5, 5),
+                            eager=True,
+                        ).to_list(),
+                        dtype=pl.Date,
+                    ),
+                }
+            )
+            .groupby(["a", "b"])
+            .agg([pl.col("a").first().alias("sum")])
+            .collect(streaming=True)
+        )
+
+    assert out.sort("b").to_dict(False) == {
+        "a": ["a", "a", "b", "b", "c", "c", None, None],
+        "b": [
+            date(2023, 4, 28),
+            date(2023, 4, 29),
+            date(2023, 4, 30),
+            date(2023, 5, 1),
+            date(2023, 5, 2),
+            date(2023, 5, 3),
+            date(2023, 5, 4),
+            date(2023, 5, 5),
+        ],
+        "sum": ["a", "a", "b", "b", "c", "c", None, None],
+    }
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/test_testing.py` & `polars_lts_cpu-0.18.0/tests/unit/test_testing.py`

 * *Files 11% similar despite different names*

```diff
@@ -5,14 +5,15 @@
 
 import pytest
 
 import polars as pl
 from polars.exceptions import InvalidAssert
 from polars.testing import (
     assert_frame_equal,
+    assert_frame_not_equal,
     assert_series_equal,
     assert_series_not_equal,
 )
 
 
 def test_compare_series_value_mismatch() -> None:
     srs1 = pl.Series([1, 2, 3])
@@ -77,86 +78,97 @@
     srs6 = pl.Series([1, 2, 3, 4, None, 6])
 
     assert_series_equal(srs4, srs6, check_dtype=False)
     with pytest.raises(AssertionError):
         assert_series_equal(srs5, srs6, check_dtype=False)
     assert_series_not_equal(srs5, srs6, check_dtype=True)
 
+    # nested
+    for float_type in (pl.Float32, pl.Float64):
+        srs = pl.Series([[0.0, nan]], dtype=pl.List(float_type))
+        assert srs.dtype == pl.List(float_type)
+        assert_series_equal(srs, srs)
+
 
 def test_compare_series_nulls() -> None:
     srs1 = pl.Series([1, 2, None])
     srs2 = pl.Series([1, 2, None])
     assert_series_equal(srs1, srs2)
 
     srs1 = pl.Series([1, 2, 3])
     srs2 = pl.Series([1, None, None])
+    assert_series_not_equal(srs1, srs2)
 
-    with pytest.raises(AssertionError, match="Value mismatch"):
+    with pytest.raises(AssertionError, match="null_count is not equal"):
         assert_series_equal(srs1, srs2)
-    with pytest.raises(AssertionError, match="Exact value mismatch"):
-        assert_series_equal(srs1, srs2, check_exact=True)
 
 
 def test_series_cmp_fast_paths() -> None:
     assert (
         pl.Series([None], dtype=pl.Int32) != pl.Series([1, 2], dtype=pl.Int32)
-    ).to_list() == [True, True]
+    ).to_list() == [None, None]
     assert (
         pl.Series([None], dtype=pl.Int32) == pl.Series([1, 2], dtype=pl.Int32)
-    ).to_list() == [False, False]
+    ).to_list() == [None, None]
 
     assert (
         pl.Series([None], dtype=pl.Utf8) != pl.Series(["a", "b"], dtype=pl.Utf8)
-    ).to_list() == [True, True]
+    ).to_list() == [None, None]
     assert (
         pl.Series([None], dtype=pl.Utf8) == pl.Series(["a", "b"], dtype=pl.Utf8)
-    ).to_list() == [False, False]
+    ).to_list() == [None, None]
 
     assert (
         pl.Series([None], dtype=pl.Boolean)
         != pl.Series([True, False], dtype=pl.Boolean)
-    ).to_list() == [True, True]
+    ).to_list() == [None, None]
     assert (
         pl.Series([None], dtype=pl.Boolean)
         == pl.Series([False, False], dtype=pl.Boolean)
-    ).to_list() == [False, False]
+    ).to_list() == [None, None]
 
 
 def test_compare_series_value_mismatch_string() -> None:
     srs1 = pl.Series(["hello", "no"])
     srs2 = pl.Series(["hello", "yes"])
+
+    assert_series_not_equal(srs1, srs2)
     with pytest.raises(
         AssertionError, match="Series are different.\n\nExact value mismatch"
     ):
         assert_series_equal(srs1, srs2)
 
 
 def test_compare_series_type_mismatch() -> None:
     srs1 = pl.Series([1, 2, 3])
     srs2 = pl.DataFrame({"col1": [2, 3, 4]})
+
     with pytest.raises(
         AssertionError, match="Inputs are different.\n\nUnexpected input types"
     ):
         assert_series_equal(srs1, srs2)  # type: ignore[arg-type]
 
     srs3 = pl.Series([1.0, 2.0, 3.0])
+    assert_series_not_equal(srs1, srs3)
     with pytest.raises(AssertionError, match="Series are different.\n\nDtype mismatch"):
         assert_series_equal(srs1, srs3)
 
 
 def test_compare_series_name_mismatch() -> None:
     srs1 = pl.Series(values=[1, 2, 3], name="srs1")
     srs2 = pl.Series(values=[1, 2, 3], name="srs2")
     with pytest.raises(AssertionError, match="Series are different.\n\nName mismatch"):
         assert_series_equal(srs1, srs2)
 
 
 def test_compare_series_shape_mismatch() -> None:
     srs1 = pl.Series(values=[1, 2, 3, 4], name="srs1")
     srs2 = pl.Series(values=[1, 2, 3], name="srs2")
+
+    assert_series_not_equal(srs1, srs2)
     with pytest.raises(
         AssertionError, match="Series are different.\n\nLength mismatch"
     ):
         assert_series_equal(srs1, srs2)
 
 
 def test_compare_series_value_exact_mismatch() -> None:
@@ -165,30 +177,97 @@
     with pytest.raises(
         AssertionError, match="Series are different.\n\nExact value mismatch"
     ):
         assert_series_equal(srs1, srs2, check_exact=True)
 
 
 def test_compare_frame_equal_nans() -> None:
-    # NaN values do not _compare_ equal, but should _assert_ as equal here
     nan = float("NaN")
-
     df1 = pl.DataFrame(
         data={"x": [1.0, nan], "y": [nan, 2.0]},
         schema=[("x", pl.Float32), ("y", pl.Float64)],
     )
     assert_frame_equal(df1, df1, check_exact=True)
 
     df2 = pl.DataFrame(
         data={"x": [1.0, nan], "y": [None, 2.0]},
         schema=[("x", pl.Float32), ("y", pl.Float64)],
     )
+    assert_frame_not_equal(df1, df2)
+    with pytest.raises(AssertionError, match="Values for column 'y' are different"):
+        assert_frame_equal(df1, df2, check_exact=True)
+
+
+def test_compare_frame_equal_nested_nans() -> None:
+    nan = float("NaN")
+
+    # list dtype
+    df1 = pl.DataFrame(
+        data={"x": [[1.0, nan]], "y": [[nan, 2.0]]},
+        schema=[("x", pl.List(pl.Float32)), ("y", pl.List(pl.Float64))],
+    )
+    assert_frame_equal(df1, df1, check_exact=True)
+
+    df2 = pl.DataFrame(
+        data={"x": [[1.0, nan]], "y": [[None, 2.0]]},
+        schema=[("x", pl.List(pl.Float32)), ("y", pl.List(pl.Float64))],
+    )
+    assert_frame_not_equal(df1, df2)
     with pytest.raises(AssertionError, match="Values for column 'y' are different"):
         assert_frame_equal(df1, df2, check_exact=True)
 
+    # struct dtype
+    df3 = pl.from_dicts(
+        [
+            {
+                "id": 1,
+                "struct": [
+                    {"x": "text", "y": [0.0, nan]},
+                    {"x": "text", "y": [0.0, nan]},
+                ],
+            },
+            {
+                "id": 2,
+                "struct": [
+                    {"x": "text", "y": [1]},
+                    {"x": "text", "y": [1]},
+                ],
+            },
+        ]
+    )
+    df4 = pl.from_dicts(
+        [
+            {
+                "id": 1,
+                "struct": [
+                    {"x": "text", "y": [0.0, nan], "z": ["$"]},
+                    {"x": "text", "y": [0.0, nan], "z": ["$"]},
+                ],
+            },
+            {
+                "id": 2,
+                "struct": [
+                    {"x": "text", "y": [nan, 1], "z": ["!"]},
+                    {"x": "text", "y": [nan, 1], "z": ["?"]},
+                ],
+            },
+        ]
+    )
+
+    assert_frame_equal(df3, df3)
+    assert_frame_not_equal(df3, df3, nans_compare_equal=False)
+
+    assert_frame_equal(df4, df4)
+    assert_frame_not_equal(df4, df4, nans_compare_equal=False)
+
+    assert_frame_not_equal(df3, df4)
+    for check_dtype in (True, False):
+        with pytest.raises(AssertionError, match="mismatch|different"):
+            assert_frame_equal(df3, df4, check_dtype=check_dtype)
+
 
 def test_assert_frame_equal_pass() -> None:
     df1 = pl.DataFrame({"a": [1, 2]})
     df2 = pl.DataFrame({"a": [1, 2]})
     assert_frame_equal(df1, df2)
```

### Comparing `polars_lts_cpu-0.17.9/tests/unit/utils/test_utils.py` & `polars_lts_cpu-0.18.0/tests/unit/utils/test_utils.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,24 +1,25 @@
 from __future__ import annotations
 
 import inspect
+import warnings
 from datetime import date, datetime, time, timedelta
 from typing import TYPE_CHECKING, Any
 
 import pytest
 
 import polars as pl
 from polars.utils.convert import (
     _date_to_pl_date,
     _datetime_to_pl_timestamp,
     _time_to_pl_time,
     _timedelta_to_pl_duration,
     _timedelta_to_pl_timedelta,
 )
-from polars.utils.decorators import deprecate_nonkeyword_arguments
+from polars.utils.decorators import deprecate_nonkeyword_arguments, redirect
 from polars.utils.various import parse_version
 
 if TYPE_CHECKING:
     from polars.type_aliases import TimeUnit
 
 
 @pytest.mark.parametrize(
@@ -132,7 +133,28 @@
 def test_deprecate_nonkeyword_arguments_method_warning() -> None:
     msg = (
         r"All arguments of Foo\.bar except for \'baz\' will be keyword-only in the next breaking release."
         r" Use keyword arguments to silence this warning."
     )
     with pytest.deprecated_call(match=msg):
         Foo().bar("qux", "quox")
+
+
+def test_redirect() -> None:
+    with warnings.catch_warnings():
+        warnings.simplefilter("ignore", DeprecationWarning)
+
+        # one-to-one redirection
+        @redirect({"foo": "bar"})
+        class DemoClass1:
+            def bar(self, upper: bool = False) -> str:
+                return "BAZ" if upper else "baz"
+
+        assert DemoClass1().foo() == "baz"  # type: ignore[attr-defined]
+
+        # redirection with **kwargs
+        @redirect({"foo": ("bar", {"upper": True})})
+        class DemoClass2:
+            def bar(self, upper: bool = False) -> str:
+                return "BAZ" if upper else "baz"
+
+        assert DemoClass2().foo() == "BAZ"  # type: ignore[attr-defined]
```

### Comparing `polars_lts_cpu-0.17.9/Cargo.lock` & `polars_lts_cpu-0.18.0/Cargo.lock`

 * *Files 2% similar despite different names*

```diff
@@ -82,16 +82,17 @@
 dependencies = [
  "planus",
  "serde",
 ]
 
 [[package]]
 name = "arrow2"
-version = "0.17.0"
-source = "git+https://github.com/ritchie46/arrow2?branch=polars_2023-04-20#48c24de7ed852bfbac50d3923908043fdcde12a7"
+version = "0.17.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "15ae0428d69ab31d7b2adad22a752d6f11fef2e901d2262d0cad4f5cb08b7093"
 dependencies = [
  "ahash",
  "arrow-format",
  "avro-schema",
  "base64",
  "bytemuck",
  "chrono",
@@ -100,16 +101,14 @@
  "either",
  "ethnum",
  "fallible-streaming-iterator",
  "foreign_vec",
  "futures",
  "getrandom",
  "hash_hasher",
- "indexmap",
- "json-deserializer",
  "lexical-core",
  "lz4",
  "multiversion",
  "num-traits",
  "parquet2",
  "regex",
  "regex-syntax 0.6.29",
@@ -135,26 +134,26 @@
 name = "async-stream-impl"
 version = "0.3.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "16e62a023e7c117e27523144c5d2459f4397fcc3cab0085af8e2224f643a0193"
 dependencies = [
  "proc-macro2",
  "quote",
- "syn 2.0.15",
+ "syn 2.0.16",
 ]
 
 [[package]]
 name = "async-trait"
 version = "0.1.68"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "b9ccdd8f2a161be9bd5c023df56f1b2a0bd1d83872ae53b71a84a12c9bf6e842"
 dependencies = [
  "proc-macro2",
  "quote",
- "syn 2.0.15",
+ "syn 2.0.16",
 ]
 
 [[package]]
 name = "atoi"
 version = "2.0.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "f28d99ec8bfea296261ca1af174f24225171fea9664ba9003cbebee704810528"
@@ -180,17 +179,17 @@
  "serde",
  "serde_json",
  "snap",
 ]
 
 [[package]]
 name = "base64"
-version = "0.21.0"
+version = "0.21.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "a4a4ddaa51a5bc52a6948f74c06d20aaaddb71924eab79b8c97a8c556e942d6a"
+checksum = "3f1e31e207a6b8fb791a38ea3105e6cb541f55e4d029902d3039a4ad07cc4105"
 
 [[package]]
 name = "bitflags"
 version = "1.3.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "bef38d45163c2f1dde094a7dfd33ccf595c92905c8f8f4fdc18d06fb1037718a"
 
@@ -224,17 +223,17 @@
  "cargo-lock",
  "chrono",
  "git2",
 ]
 
 [[package]]
 name = "bumpalo"
-version = "3.12.0"
+version = "3.13.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "0d261e256854913907f67ed06efbc3338dfe6179796deefc1ff763fc1aee5535"
+checksum = "a3e2c3daef883ecc1b5d58c15adae93470a91d425f3532ba1695849656af3fc1"
 
 [[package]]
 name = "bytemuck"
 version = "1.13.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "17febce684fd15d89027105661fec94afb475cb995fbc59d2865198446ba2eea"
 dependencies = [
@@ -245,24 +244,18 @@
 name = "bytemuck_derive"
 version = "1.4.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "fdde5c9cd29ebd706ce1b35600920a33550e402fc998a2e53ad3b42c3c47a192"
 dependencies = [
  "proc-macro2",
  "quote",
- "syn 2.0.15",
+ "syn 2.0.16",
 ]
 
 [[package]]
-name = "byteorder"
-version = "1.4.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "14c189c53d098945499cdfa7ecc63567cf3886b3332b312a5b4585d8d3a6a610"
-
-[[package]]
 name = "bytes"
 version = "1.4.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "89b2fd2a0dcf38d7971e2194b6b6eebab45ae01067456a7fd93d5547a61b70be"
 
 [[package]]
 name = "cargo-lock"
@@ -327,50 +320,40 @@
  "parse-zoneinfo",
  "phf",
  "phf_codegen",
 ]
 
 [[package]]
 name = "ciborium"
-version = "0.2.0"
+version = "0.2.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "b0c137568cc60b904a7724001b35ce2630fd00d5d84805fbb608ab89509d788f"
+checksum = "effd91f6c78e5a4ace8a5d3c0b6bfaec9e2baaef55f3efc00e45fb2e477ee926"
 dependencies = [
  "ciborium-io",
  "ciborium-ll",
  "serde",
 ]
 
 [[package]]
 name = "ciborium-io"
-version = "0.2.0"
+version = "0.2.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "346de753af073cc87b52b2083a506b38ac176a44cfb05497b622e27be899b369"
+checksum = "cdf919175532b369853f5d5e20b26b43112613fd6fe7aee757e35f7a44642656"
 
 [[package]]
 name = "ciborium-ll"
-version = "0.2.0"
+version = "0.2.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "213030a2b5a4e0c0892b6652260cf6ccac84827b83a85a534e178e3906c4cf1b"
+checksum = "defaa24ecc093c77630e6c15e17c51f5e187bf35ee514f4e2d67baaa96dae22b"
 dependencies = [
  "ciborium-io",
  "half",
 ]
 
 [[package]]
-name = "codespan-reporting"
-version = "0.11.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "3538270d33cc669650c4b093848450d380def10c331d38c768e34cac80576e6e"
-dependencies = [
- "termcolor",
- "unicode-width",
-]
-
-[[package]]
 name = "comfy-table"
 version = "6.1.4"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "6e7b787b0dc42e8111badfdbe4c3059158ccb2db8780352fa1b01e8ccf45cc4d"
 dependencies = [
  "crossterm",
  "strum",
@@ -439,14 +422,24 @@
  "cfg-if",
  "crossbeam-utils",
  "memoffset",
  "scopeguard",
 ]
 
 [[package]]
+name = "crossbeam-queue"
+version = "0.3.8"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "d1cfb3ea8a53f37c40dea2c7bedcbd88bdfae54f5e2175d6ecaff1c988353add"
+dependencies = [
+ "cfg-if",
+ "crossbeam-utils",
+]
+
+[[package]]
 name = "crossbeam-utils"
 version = "0.8.15"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "3c063cd8cc95f5c377ed0d4b49a4b21f632396ff690e8470c29b3359b346984b"
 dependencies = [
  "cfg-if",
 ]
@@ -473,68 +466,14 @@
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "2ae1b35a484aa10e07fe0638d02301c5ad24de82d310ccbd2f3693da5f09bf1c"
 dependencies = [
  "winapi",
 ]
 
 [[package]]
-name = "ctor"
-version = "0.2.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "dd4056f63fce3b82d852c3da92b08ea59959890813a7f4ce9c0ff85b10cf301b"
-dependencies = [
- "quote",
- "syn 2.0.15",
-]
-
-[[package]]
-name = "cxx"
-version = "1.0.94"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "f61f1b6389c3fe1c316bf8a4dccc90a38208354b330925bce1f74a6c4756eb93"
-dependencies = [
- "cc",
- "cxxbridge-flags",
- "cxxbridge-macro",
- "link-cplusplus",
-]
-
-[[package]]
-name = "cxx-build"
-version = "1.0.94"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "12cee708e8962df2aeb38f594aae5d827c022b6460ac71a7a3e2c3c2aae5a07b"
-dependencies = [
- "cc",
- "codespan-reporting",
- "once_cell",
- "proc-macro2",
- "quote",
- "scratch",
- "syn 2.0.15",
-]
-
-[[package]]
-name = "cxxbridge-flags"
-version = "1.0.94"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "7944172ae7e4068c533afbb984114a56c46e9ccddda550499caa222902c7f7bb"
-
-[[package]]
-name = "cxxbridge-macro"
-version = "1.0.94"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "2345488264226bf682893e25de0769f3360aac9957980ec49361b083ddaa5bc5"
-dependencies = [
- "proc-macro2",
- "quote",
- "syn 2.0.15",
-]
-
-[[package]]
 name = "dyn-clone"
 version = "1.0.11"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "68b0cf012f1230e43cd00ebb729c6bb58707ecfa8ad08b52ef3a4ccd2697fc30"
 
 [[package]]
 name = "either"
@@ -570,17 +509,17 @@
 name = "fast-float"
 version = "0.2.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "95765f67b4b18863968b4a1bd5bb576f732b29a4a28c7cd84c09fa3e2875f33c"
 
 [[package]]
 name = "flate2"
-version = "1.0.25"
+version = "1.0.26"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "a8a2db397cb1c8772f31494cb8917e48cd1e64f0fa7efac59fbd741a0a8ce841"
+checksum = "3b9429470923de8e8cbd4d2dc513535400b4b3fef0319fb5c4e1f520a7bef743"
 dependencies = [
  "crc32fast",
  "miniz_oxide",
 ]
 
 [[package]]
 name = "float-cmp"
@@ -658,15 +597,15 @@
 name = "futures-macro"
 version = "0.3.28"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "89ca545a94061b6365f2c7355b4b32bd20df3ff95f02da9329b34ccc3bd6ee72"
 dependencies = [
  "proc-macro2",
  "quote",
- "syn 2.0.15",
+ "syn 2.0.16",
 ]
 
 [[package]]
 name = "futures-sink"
 version = "0.3.28"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "f43be4fe21a13b9781a69afa4985b0f6ee0e1afab2c6f454a8cf30e2b2237b6e"
@@ -692,23 +631,14 @@
  "memchr",
  "pin-project-lite",
  "pin-utils",
  "slab",
 ]
 
 [[package]]
-name = "fxhash"
-version = "0.2.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "c31b6d751ae2c7f11320402d34e41349dd1016f8d5d45e48c4312bc8625af50c"
-dependencies = [
- "byteorder",
-]
-
-[[package]]
 name = "getrandom"
 version = "0.2.9"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "c85e1d9ab2eadba7e5040d4e09cbd6d072b76a557ad64e797c2cb9d4da21d7e4"
 dependencies = [
  "cfg-if",
  "js-sys",
@@ -721,15 +651,15 @@
 name = "ghost"
 version = "0.1.9"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "e77ac7b51b8e6313251737fcef4b1c01a2ea102bde68415b62c0ee9268fec357"
 dependencies = [
  "proc-macro2",
  "quote",
- "syn 2.0.15",
+ "syn 2.0.16",
 ]
 
 [[package]]
 name = "git2"
 version = "0.16.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "ccf7f68c2995f392c49fffb4f95ae2c873297830eb25c6bc4c114ce8f4562acc"
@@ -751,19 +681,18 @@
 name = "half"
 version = "1.8.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "eabb4a44450da02c90444cf74558da904edde8fb4e9035a9a6a4e15445af0bd7"
 
 [[package]]
 name = "halfbrown"
-version = "0.1.18"
+version = "0.2.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "9e2a3c70a9c00cc1ee87b54e89f9505f73bb17d63f1b25c9a462ba8ef885444f"
+checksum = "f985624e90f861184145c13b736873a0f83cdb998a292dbb0653598ab03aecbf"
 dependencies = [
- "fxhash",
  "hashbrown 0.13.2",
  "serde",
 ]
 
 [[package]]
 name = "hash_hasher"
 version = "2.0.3"
@@ -805,19 +734,19 @@
 name = "hex"
 version = "0.4.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "7f24254aa9a54b5c858eaee2f5bccdb46aaf0e486a595ed5fd8f86ba55232a70"
 
 [[package]]
 name = "home"
-version = "0.5.4"
+version = "0.5.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "747309b4b440c06d57b0b25f2aee03ee9b5e5397d288c60e21fc709bb98a7408"
+checksum = "5444c27eef6923071f7ebcc33e3444508466a76f7a2b93da00ed6e19f30c1ddb"
 dependencies = [
- "winapi",
+ "windows-sys 0.48.0",
 ]
 
 [[package]]
 name = "iana-time-zone"
 version = "0.1.56"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "0722cd7114b7de04316e7ea5456a0bbb20e4adb46fd27a3697adb812cff0f37c"
@@ -828,20 +757,19 @@
  "js-sys",
  "wasm-bindgen",
  "windows",
 ]
 
 [[package]]
 name = "iana-time-zone-haiku"
-version = "0.1.1"
+version = "0.1.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "0703ae284fc167426161c2e3f1da3ea71d94b21bedbcc9494e92b28e334e3dca"
+checksum = "f31827a206f56af32e590ba56d5d2d085f558508192593743f16b2306495269f"
 dependencies = [
- "cxx",
- "cxx-build",
+ "cc",
 ]
 
 [[package]]
 name = "idna"
 version = "0.3.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "e14ddfc70884202db2244c223200c204c2bda1bc6e0998d11b5e024d657209e6"
@@ -874,19 +802,18 @@
 checksum = "7a5bbe824c507c5da5956355e86a746d82e0e1464f65d862cc5e71da70e94b2c"
 dependencies = [
  "cfg-if",
 ]
 
 [[package]]
 name = "inventory"
-version = "0.3.5"
+version = "0.3.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "7741301a6d6a9b28ce77c0fb77a4eb116b6bc8f3bef09923f7743d059c4157d3"
+checksum = "e0539b5de9241582ce6bd6b0ba7399313560151e58c9aaf8b74b711b1bdce644"
 dependencies = [
- "ctor",
  "ghost",
 ]
 
 [[package]]
 name = "itoa"
 version = "1.0.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
@@ -925,31 +852,22 @@
 checksum = "936cfd212a0155903bcbc060e316fb6cc7cbf2e1907329391ebadc1fe0ce77c2"
 dependencies = [
  "libc",
 ]
 
 [[package]]
 name = "js-sys"
-version = "0.3.61"
+version = "0.3.63"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "445dde2150c55e483f3d8416706b97ec8e8237c307e5b7b4b8dd15e6af2a0730"
+checksum = "2f37a4a5928311ac501dee68b3c7613a1037d0edb30c8e5427bd832d55d1b790"
 dependencies = [
  "wasm-bindgen",
 ]
 
 [[package]]
-name = "json-deserializer"
-version = "0.4.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "5f63b421e16eb4100beb677af56f0b4f3a4f08bab74ef2af079ce5bb92c2683f"
-dependencies = [
- "indexmap",
-]
-
-[[package]]
 name = "jsonpath_lib"
 version = "0.3.0"
 source = "git+https://github.com/ritchie46/jsonpath?branch=improve_compiled#24eaf0b4416edff38a4d1b6b17bc4b9f3f047b4b"
 dependencies = [
  "log",
  "serde",
  "serde_json",
@@ -1026,23 +944,23 @@
 dependencies = [
  "lexical-util",
  "static_assertions",
 ]
 
 [[package]]
 name = "libc"
-version = "0.2.142"
+version = "0.2.144"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "6a987beff54b60ffa6d51982e1aa1146bc42f19bd26be28b0586f252fccf5317"
+checksum = "2b00cc1c228a6782d0f076e7b232802e0c5689d41bb5df366f2a6b6621cfdfe1"
 
 [[package]]
 name = "libflate"
-version = "1.3.0"
+version = "1.4.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "97822bf791bd4d5b403713886a5fbe8bf49520fe78e323b0dc480ca1a03e50b0"
+checksum = "5ff4ae71b685bbad2f2f391fe74f6b7659a34871c08b210fdc039e43bee07d18"
 dependencies = [
  "adler32",
  "crc32fast",
  "libflate_lz77",
 ]
 
 [[package]]
@@ -1064,50 +982,41 @@
  "libc",
  "libz-sys",
  "pkg-config",
 ]
 
 [[package]]
 name = "libm"
-version = "0.2.6"
+version = "0.2.7"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "348108ab3fba42ec82ff6e9564fc4ca0247bdccdc68dd8af9764bbc79c3c8ffb"
+checksum = "f7012b1bbb0719e1097c47611d3898568c546d597c2e74d66f6087edd5233ff4"
 
 [[package]]
 name = "libmimalloc-sys"
-version = "0.1.32"
+version = "0.1.33"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "43a558e3d911bc3c7bfc8c78bc580b404d6e51c1cefbf656e176a94b49b0df40"
+checksum = "f4ac0e912c8ef1b735e92369695618dc5b1819f5a7bf3f167301a3ba1cea515e"
 dependencies = [
  "cc",
  "libc",
 ]
 
 [[package]]
 name = "libz-sys"
-version = "1.1.8"
+version = "1.1.9"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "9702761c3935f8cc2f101793272e202c72b99da8f4224a19ddcf1279a6450bbf"
+checksum = "56ee889ecc9568871456d42f603d6a0ce59ff328d291063a45cbdf0036baf6db"
 dependencies = [
  "cc",
  "libc",
  "pkg-config",
  "vcpkg",
 ]
 
 [[package]]
-name = "link-cplusplus"
-version = "1.0.8"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "ecd207c9c713c34f95a097a5b029ac2ce6010530c7b49d7fea24d977dede04f5"
-dependencies = [
- "cc",
-]
-
-[[package]]
 name = "lock_api"
 version = "0.4.9"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "435011366fe56583b16cf956f9df0095b405b82d76425bc8981c0e22e60ec4df"
 dependencies = [
  "autocfg",
  "scopeguard",
@@ -1140,18 +1049,19 @@
 dependencies = [
  "cc",
  "libc",
 ]
 
 [[package]]
 name = "matrixmultiply"
-version = "0.3.3"
+version = "0.3.7"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "bb99c395ae250e1bf9133673f03ca9f97b7e71b705436bf8f089453445d1e9fe"
+checksum = "090126dc04f95dc0d1c1c91f61bdd474b3930ca064c1edc8a849da2c6cbe1e77"
 dependencies = [
+ "autocfg",
  "rawpointer",
 ]
 
 [[package]]
 name = "memchr"
 version = "2.5.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
@@ -1173,57 +1083,57 @@
 checksum = "d61c719bcfbcf5d62b3a09efa6088de8c54bc0bfcd3ea7ae39fcc186108b8de1"
 dependencies = [
  "autocfg",
 ]
 
 [[package]]
 name = "mimalloc"
-version = "0.1.36"
+version = "0.1.37"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "3d88dad3f985ec267a3fcb7a1726f5cb1a7e8cad8b646e70a84f967210df23da"
+checksum = "4e2894987a3459f3ffb755608bd82188f8ed00d0ae077f1edea29c068d639d98"
 dependencies = [
  "libmimalloc-sys",
 ]
 
 [[package]]
 name = "miniz_oxide"
-version = "0.6.2"
+version = "0.7.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "b275950c28b37e794e8c55d88aeb5e139d0ce23fdbbeda68f8d7174abdf9e8fa"
+checksum = "e7810e0be55b428ada41041c41f32c9f1a42817901b4ccf45fa3d4b6561e74c7"
 dependencies = [
  "adler",
 ]
 
 [[package]]
 name = "mio"
 version = "0.8.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "5b9d9a46eff5b4ff64b45a9e316a6d1e0bc719ef429cbec4dc630684212bfdf9"
 dependencies = [
  "libc",
  "log",
  "wasi 0.11.0+wasi-snapshot-preview1",
- "windows-sys",
+ "windows-sys 0.45.0",
 ]
 
 [[package]]
 name = "multiversion"
-version = "0.7.1"
+version = "0.7.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "e6a87eede2251ca235e5573086d01d2ab6b59dfaea54c2be10f9320980f7e8f7"
+checksum = "8cda45dade5144c2c929bf2ed6c24bebbba784e9198df049ec87d722b9462bd1"
 dependencies = [
  "multiversion-macros",
  "target-features",
 ]
 
 [[package]]
 name = "multiversion-macros"
-version = "0.7.1"
+version = "0.7.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "1af1abf82261d780d114014eff4b555e47d823f3b84f893c4388572b40e089fb"
+checksum = "04bffdccbd4798b61dce08c97ce8c66a68976f95541aaf284a6e90c1d1c306e1"
 dependencies = [
  "proc-macro2",
  "quote",
  "syn 1.0.109",
  "target-features",
 ]
 
@@ -1359,15 +1269,15 @@
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "9069cbb9f99e3a5083476ccb29ceb1de18b9118cafa53e90c9551235de2b9521"
 dependencies = [
  "cfg-if",
  "libc",
  "redox_syscall",
  "smallvec",
- "windows-sys",
+ "windows-sys 0.45.0",
 ]
 
 [[package]]
 name = "parquet-format-safe"
 version = "0.2.4"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "1131c54b167dd4e4799ce762e1ab01549ebb94d5bdd13e6ec1b467491c378e1f"
@@ -1457,68 +1367,69 @@
 name = "pin-utils"
 version = "0.1.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "8b870d8c151b6f2fb93e84a13146138f05d02ed11c7e7c54f8826aaaf7c9f184"
 
 [[package]]
 name = "pkg-config"
-version = "0.3.26"
+version = "0.3.27"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "6ac9a59f73473f1b8d852421e59e64809f025994837ef743615c6d0c5b305160"
+checksum = "26072860ba924cbfa98ea39c8c19b4dd6a4a25423dbdf219c1eca91aa0cf6964"
 
 [[package]]
 name = "planus"
 version = "0.3.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "fc1691dd09e82f428ce8d6310bd6d5da2557c82ff17694d2a32cad7242aea89f"
 dependencies = [
  "array-init-cursor",
 ]
 
 [[package]]
 name = "polars"
-version = "0.28.0"
+version = "0.30.0"
 dependencies = [
  "getrandom",
  "polars-core",
  "polars-io",
  "polars-lazy",
  "polars-ops",
  "polars-sql",
  "polars-time",
  "version_check",
 ]
 
 [[package]]
 name = "polars-algo"
-version = "0.28.0"
+version = "0.30.0"
 dependencies = [
  "polars-core",
  "polars-lazy",
  "polars-ops",
 ]
 
 [[package]]
 name = "polars-arrow"
-version = "0.28.0"
+version = "0.30.0"
 dependencies = [
  "arrow2",
+ "atoi",
  "chrono",
  "chrono-tz",
  "hashbrown 0.13.2",
  "multiversion",
  "num-traits",
  "polars-error",
  "serde",
  "thiserror",
 ]
 
 [[package]]
 name = "polars-core"
-version = "0.28.0"
+version = "0.30.0"
 dependencies = [
  "ahash",
  "arrow2",
  "bitflags",
  "chrono",
  "chrono-tz",
  "comfy-table",
@@ -1543,24 +1454,24 @@
  "thiserror",
  "wasm-timer",
  "xxhash-rust",
 ]
 
 [[package]]
 name = "polars-error"
-version = "0.28.0"
+version = "0.30.0"
 dependencies = [
  "arrow2",
  "regex",
  "thiserror",
 ]
 
 [[package]]
 name = "polars-io"
-version = "0.28.0"
+version = "0.30.0"
 dependencies = [
  "ahash",
  "arrow2",
  "async-trait",
  "bytes",
  "chrono",
  "chrono-tz",
@@ -1573,70 +1484,90 @@
  "memchr",
  "memmap2",
  "num-traits",
  "once_cell",
  "polars-arrow",
  "polars-core",
  "polars-error",
+ "polars-json",
  "polars-time",
  "polars-utils",
  "rayon",
  "regex",
  "serde",
  "serde_json",
  "simd-json",
  "simdutf8",
  "tokio",
 ]
 
 [[package]]
+name = "polars-json"
+version = "0.30.0"
+dependencies = [
+ "ahash",
+ "arrow2",
+ "fallible-streaming-iterator",
+ "hashbrown 0.13.2",
+ "indexmap",
+ "num-traits",
+ "polars-arrow",
+ "polars-error",
+ "polars-utils",
+ "simd-json",
+]
+
+[[package]]
 name = "polars-lazy"
-version = "0.28.0"
+version = "0.30.0"
 dependencies = [
  "ahash",
  "bitflags",
  "glob",
  "once_cell",
  "polars-arrow",
  "polars-core",
  "polars-io",
+ "polars-json",
  "polars-ops",
  "polars-pipe",
  "polars-plan",
  "polars-time",
  "polars-utils",
  "pyo3",
  "rayon",
  "smartstring",
 ]
 
 [[package]]
 name = "polars-ops"
-version = "0.28.0"
+version = "0.30.0"
 dependencies = [
  "argminmax",
  "arrow2",
  "base64",
  "either",
  "hex",
  "jsonpath_lib",
  "memchr",
  "polars-arrow",
  "polars-core",
+ "polars-json",
  "polars-utils",
  "serde",
  "serde_json",
  "smartstring",
 ]
 
 [[package]]
 name = "polars-pipe"
-version = "0.28.0"
+version = "0.30.0"
 dependencies = [
  "crossbeam-channel",
+ "crossbeam-queue",
  "enum_dispatch",
  "hashbrown 0.13.2",
  "num-traits",
  "polars-arrow",
  "polars-core",
  "polars-io",
  "polars-ops",
@@ -1645,15 +1576,15 @@
  "polars-utils",
  "rayon",
  "smartstring",
 ]
 
 [[package]]
 name = "polars-plan"
-version = "0.28.0"
+version = "0.30.0"
 dependencies = [
  "ahash",
  "arrow2",
  "chrono",
  "chrono-tz",
  "once_cell",
  "polars-arrow",
@@ -1667,37 +1598,37 @@
  "regex",
  "serde",
  "smartstring",
 ]
 
 [[package]]
 name = "polars-row"
-version = "0.28.0"
+version = "0.30.0"
 dependencies = [
  "arrow2",
  "polars-error",
  "polars-utils",
 ]
 
 [[package]]
 name = "polars-sql"
-version = "0.28.0"
+version = "0.30.0"
 dependencies = [
  "polars-arrow",
  "polars-core",
  "polars-lazy",
  "polars-plan",
  "serde",
  "serde_json",
  "sqlparser",
 ]
 
 [[package]]
 name = "polars-time"
-version = "0.28.0"
+version = "0.30.0"
 dependencies = [
  "arrow2",
  "atoi",
  "chrono",
  "chrono-tz",
  "now",
  "once_cell",
@@ -1708,40 +1639,42 @@
  "regex",
  "serde",
  "smartstring",
 ]
 
 [[package]]
 name = "polars-utils"
-version = "0.28.0"
+version = "0.30.0"
 dependencies = [
+ "ahash",
+ "hashbrown 0.13.2",
  "once_cell",
  "rayon",
  "smartstring",
  "sysinfo",
 ]
 
 [[package]]
 name = "ppv-lite86"
 version = "0.2.17"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "5b40af805b3121feab8a3c29f04d8ad262fa8e0561883e7653e024ae4479e6de"
 
 [[package]]
 name = "proc-macro2"
-version = "1.0.56"
+version = "1.0.58"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "2b63bdb0cd06f1f4dedf69b254734f9b45af66e4a031e42a7480257d9898b435"
+checksum = "fa1fb82fc0c281dd9671101b66b771ebbe1eaf967b96ac8740dcba4b70005ca8"
 dependencies = [
  "unicode-ident",
 ]
 
 [[package]]
 name = "py-polars"
-version = "0.17.9"
+version = "0.18.0"
 dependencies = [
  "ahash",
  "built",
  "ciborium",
  "jemallocator",
  "lexical-core",
  "libc",
@@ -1825,17 +1758,17 @@
  "proc-macro2",
  "quote",
  "syn 1.0.109",
 ]
 
 [[package]]
 name = "quote"
-version = "1.0.26"
+version = "1.0.27"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "4424af4bf778aae2051a77b60283332f386554255d722233d09fbfc7e30da2fc"
+checksum = "8f4f29d145265ec1c483c7c654450edde0bfe043d3938d6972630663356d9500"
 dependencies = [
  "proc-macro2",
 ]
 
 [[package]]
 name = "rand"
 version = "0.8.5"
@@ -1911,34 +1844,34 @@
 checksum = "fb5a58c1855b4b6819d59012155603f0b22ad30cad752600aadfcb695265519a"
 dependencies = [
  "bitflags",
 ]
 
 [[package]]
 name = "regex"
-version = "1.8.0"
+version = "1.8.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "ac6cf59af1067a3fb53fbe5c88c053764e930f932be1d71d3ffe032cbe147f59"
+checksum = "d1a59b5d8e97dee33696bf13c5ba8ab85341c002922fba050069326b9c498974"
 dependencies = [
  "aho-corasick",
  "memchr",
- "regex-syntax 0.7.0",
+ "regex-syntax 0.7.2",
 ]
 
 [[package]]
 name = "regex-syntax"
 version = "0.6.29"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "f162c6dd7b008981e4d40210aca20b4bd0f9b60ca9271061b07f78537722f2e1"
 
 [[package]]
 name = "regex-syntax"
-version = "0.7.0"
+version = "0.7.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "b6868896879ba532248f33598de5181522d8b3d9d724dfd230911e1a7d4822f5"
+checksum = "436b050e76ed2903236f032a59761c1eb99e1b0aead2c257922771dab1fc8c78"
 
 [[package]]
 name = "rle-decode-fast"
 version = "1.0.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "3582f63211428f83597b51b2ddb88e2a91a9d52d12831f9d08f5e624e8977422"
 
@@ -1972,20 +1905,14 @@
 [[package]]
 name = "scopeguard"
 version = "1.1.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "d29ab0c6d3fc0ee92fe66e2d99f700eab17a8d57d1c1d3b748380fb20baa78cd"
 
 [[package]]
-name = "scratch"
-version = "1.0.5"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "1792db035ce95be60c3f8853017b3999209281c24e2ba5bc8e59bf97a0c590c1"
-
-[[package]]
 name = "semver"
 version = "1.0.17"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "bebd363326d05ec3e2f532ab7660680f3b02130d780c299bca73469d521bc0ed"
 dependencies = [
  "serde",
 ]
@@ -1994,30 +1921,30 @@
 name = "seq-macro"
 version = "0.3.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "e6b44e8fc93a14e66336d230954dda83d18b4605ccace8fe09bc7514a71ad0bc"
 
 [[package]]
 name = "serde"
-version = "1.0.160"
+version = "1.0.163"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "bb2f3770c8bce3bcda7e149193a069a0f4365bda1fa5cd88e03bca26afc1216c"
+checksum = "2113ab51b87a539ae008b5c6c02dc020ffa39afd2d83cffcb3f4eb2722cebec2"
 dependencies = [
  "serde_derive",
 ]
 
 [[package]]
 name = "serde_derive"
-version = "1.0.160"
+version = "1.0.163"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "291a097c63d8497e00160b166a967a4a79c64f3facdd01cbd7502231688d77df"
+checksum = "8c805777e3930c8883389c602315a24224bcc738b63905ef87cd1420353ea93e"
 dependencies = [
  "proc-macro2",
  "quote",
- "syn 2.0.15",
+ "syn 2.0.16",
 ]
 
 [[package]]
 name = "serde_json"
 version = "1.0.96"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "057d394a50403bcac12672b2b18fb387ab6d289d957dab67dd201875391e52f1"
@@ -2056,19 +1983,22 @@
 checksum = "d8229b473baa5980ac72ef434c4415e70c4b5e71b423043adb4ba059f89c99a1"
 dependencies = [
  "libc",
 ]
 
 [[package]]
 name = "simd-json"
-version = "0.7.0"
-source = "git+https://github.com/ritchie46/simd-json?branch=alignment#cbd37361769d900620944618a39123f37edf3d83"
+version = "0.10.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "5b001e6c10fcba79ac15990241d37c3f8c6ba4f9a14ee35fcebc0c067514b83a"
 dependencies = [
+ "ahash",
  "halfbrown",
  "lexical-core",
+ "once_cell",
  "serde",
  "serde_json",
  "simdutf8",
  "value-trait",
 ]
 
 [[package]]
@@ -2124,17 +2054,17 @@
 dependencies = [
  "libc",
  "winapi",
 ]
 
 [[package]]
 name = "sqlparser"
-version = "0.30.0"
+version = "0.34.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "db67dc6ef36edb658196c3fef0464a80b53dbbc194a904e81f9bd4190f9ecc5b"
+checksum = "37d3706eefb17039056234df6b566b0014f303f867f2656108334a55b8096f59"
 dependencies = [
  "log",
 ]
 
 [[package]]
 name = "static_assertions"
 version = "1.1.0"
@@ -2190,57 +2120,48 @@
  "proc-macro2",
  "quote",
  "unicode-ident",
 ]
 
 [[package]]
 name = "syn"
-version = "2.0.15"
+version = "2.0.16"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "a34fcf3e8b60f57e6a14301a2e916d323af98b0ea63c599441eec8558660c822"
+checksum = "a6f671d4b5ffdb8eadec19c0ae67fe2639df8684bd7bc4b83d986b8db549cf01"
 dependencies = [
  "proc-macro2",
  "quote",
  "unicode-ident",
 ]
 
 [[package]]
 name = "sysinfo"
-version = "0.28.4"
+version = "0.29.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "b4c2f3ca6693feb29a89724516f016488e9aafc7f37264f898593ee4b942f31b"
+checksum = "02f1dc6930a439cc5d154221b5387d153f8183529b07c19aca24ea31e0a167e1"
 dependencies = [
  "cfg-if",
  "core-foundation-sys",
  "libc",
  "ntapi",
  "once_cell",
  "winapi",
 ]
 
 [[package]]
 name = "target-features"
-version = "0.1.3"
+version = "0.1.4"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "24840de800c1707d75c800893dbd727a5e1501ce921944e602f0698167491e36"
+checksum = "06f6b473c37f9add4cf1df5b4d66a8ef58ab6c895f1a3b3f949cf3e21230140e"
 
 [[package]]
 name = "target-lexicon"
-version = "0.12.6"
+version = "0.12.7"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "8ae9980cab1db3fceee2f6c6f643d5d8de2997c58ee8d25fb0cc8a9e9e7348e5"
-
-[[package]]
-name = "termcolor"
-version = "1.2.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "be55cf8942feac5c765c2c993422806843c9a9a45d4d5c407ad6dd2ea95eb9b6"
-dependencies = [
- "winapi-util",
-]
+checksum = "fd1ba337640d60c3e96bc6f0638a939b9c9a7f2c316a1598c279828b3d1dc8c5"
 
 [[package]]
 name = "thiserror"
 version = "1.0.40"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "978c9a314bd8dc99be594bc3c175faaa9794be04a5a5e153caba6915336cebac"
 dependencies = [
@@ -2251,15 +2172,15 @@
 name = "thiserror-impl"
 version = "1.0.40"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "f9456a42c5b0d803c8cd86e73dd7cc9edd429499f37a3550d286d5e86720569f"
 dependencies = [
  "proc-macro2",
  "quote",
- "syn 2.0.15",
+ "syn 2.0.16",
 ]
 
 [[package]]
 name = "time"
 version = "0.1.45"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "1b797afad3f312d1c66a56d11d0316f916356d11bd158fbc6ca6389ff6bf805a"
@@ -2282,24 +2203,24 @@
 name = "tinyvec_macros"
 version = "0.1.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "1f3ccbac311fea05f86f61904b462b55fb3df8837a366dfc601a0161d0532f20"
 
 [[package]]
 name = "tokio"
-version = "1.27.0"
+version = "1.28.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "d0de47a4eecbe11f498978a9b29d792f0d2692d1dd003650c24c76510e3bc001"
+checksum = "0aa32867d44e6f2ce3385e89dceb990188b8bb0fb25b0cf576647a6f98ac5105"
 dependencies = [
  "autocfg",
  "libc",
  "mio",
  "pin-project-lite",
  "socket2",
- "windows-sys",
+ "windows-sys 0.48.0",
 ]
 
 [[package]]
 name = "toml"
 version = "0.5.11"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "f4f7f0dd8d50a853a531c426359045b1998f04219d88799810762cd4ad314234"
@@ -2349,17 +2270,17 @@
  "form_urlencoded",
  "idna",
  "percent-encoding",
 ]
 
 [[package]]
 name = "value-trait"
-version = "0.5.1"
+version = "0.6.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "995de1aa349a0dc50f4aa40870dce12961a30229027230bad09acd2843edbe9e"
+checksum = "09a5b6c8ceb01263b969cac48d4a6705134d490ded13d889e52c0cfc80c6945e"
 dependencies = [
  "float-cmp",
  "halfbrown",
  "itoa",
  "ryu",
 ]
 
@@ -2385,77 +2306,77 @@
 name = "wasi"
 version = "0.11.0+wasi-snapshot-preview1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "9c8d87e72b64a3b4db28d11ce29237c246188f4f51057d65a7eab63b7987e423"
 
 [[package]]
 name = "wasm-bindgen"
-version = "0.2.84"
+version = "0.2.86"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "31f8dcbc21f30d9b8f2ea926ecb58f6b91192c17e9d33594b3df58b2007ca53b"
+checksum = "5bba0e8cb82ba49ff4e229459ff22a191bbe9a1cb3a341610c9c33efc27ddf73"
 dependencies = [
  "cfg-if",
  "wasm-bindgen-macro",
 ]
 
 [[package]]
 name = "wasm-bindgen-backend"
-version = "0.2.84"
+version = "0.2.86"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "95ce90fd5bcc06af55a641a86428ee4229e44e07033963a2290a8e241607ccb9"
+checksum = "19b04bc93f9d6bdee709f6bd2118f57dd6679cf1176a1af464fca3ab0d66d8fb"
 dependencies = [
  "bumpalo",
  "log",
  "once_cell",
  "proc-macro2",
  "quote",
- "syn 1.0.109",
+ "syn 2.0.16",
  "wasm-bindgen-shared",
 ]
 
 [[package]]
 name = "wasm-bindgen-futures"
-version = "0.4.34"
+version = "0.4.36"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "f219e0d211ba40266969f6dbdd90636da12f75bee4fc9d6c23d1260dadb51454"
+checksum = "2d1985d03709c53167ce907ff394f5316aa22cb4e12761295c5dc57dacb6297e"
 dependencies = [
  "cfg-if",
  "js-sys",
  "wasm-bindgen",
  "web-sys",
 ]
 
 [[package]]
 name = "wasm-bindgen-macro"
-version = "0.2.84"
+version = "0.2.86"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "4c21f77c0bedc37fd5dc21f897894a5ca01e7bb159884559461862ae90c0b4c5"
+checksum = "14d6b024f1a526bb0234f52840389927257beb670610081360e5a03c5df9c258"
 dependencies = [
  "quote",
  "wasm-bindgen-macro-support",
 ]
 
 [[package]]
 name = "wasm-bindgen-macro-support"
-version = "0.2.84"
+version = "0.2.86"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "2aff81306fcac3c7515ad4e177f521b5c9a15f2b08f4e32d823066102f35a5f6"
+checksum = "e128beba882dd1eb6200e1dc92ae6c5dbaa4311aa7bb211ca035779e5efc39f8"
 dependencies = [
  "proc-macro2",
  "quote",
- "syn 1.0.109",
+ "syn 2.0.16",
  "wasm-bindgen-backend",
  "wasm-bindgen-shared",
 ]
 
 [[package]]
 name = "wasm-bindgen-shared"
-version = "0.2.84"
+version = "0.2.86"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "0046fef7e28c3804e5e38bfa31ea2a0f73905319b677e57ebe37e49358989b5d"
+checksum = "ed9d5b4305409d1fc9482fee2d7f9bcbf24b3972bf59817ef757e23982242a93"
 
 [[package]]
 name = "wasm-timer"
 version = "0.2.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "be0ecb0db480561e9a7642b5d3e4187c128914e58aa84330b9493e3eb68c5e7f"
 dependencies = [
@@ -2466,17 +2387,17 @@
  "wasm-bindgen",
  "wasm-bindgen-futures",
  "web-sys",
 ]
 
 [[package]]
 name = "web-sys"
-version = "0.3.61"
+version = "0.3.63"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "e33b99f4b23ba3eec1a53ac264e35a755f00e966e0065077d6027c0f575b0b97"
+checksum = "3bdd9ef4e984da1187bf8110c5cf5b845fbc87a23602cdf912386a76fcd3a7c2"
 dependencies = [
  "js-sys",
  "wasm-bindgen",
 ]
 
 [[package]]
 name = "winapi"
@@ -2491,23 +2412,14 @@
 [[package]]
 name = "winapi-i686-pc-windows-gnu"
 version = "0.4.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "ac3b87c63620426dd9b991e5ce0329eff545bccbbb34f3be09ff6fb6ab51b7b6"
 
 [[package]]
-name = "winapi-util"
-version = "0.1.5"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "70ec6ce85bb158151cae5e5c87f95a8e97d2c0c4b001223f33a334e3ce5de178"
-dependencies = [
- "winapi",
-]
-
-[[package]]
 name = "winapi-x86_64-pc-windows-gnu"
 version = "0.4.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "712e227841d057c1ee1cd2fb22fa7e5a5461ae8e48fa2ca79ec42cfc1931183f"
 
 [[package]]
 name = "windows"
@@ -2524,14 +2436,23 @@
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "75283be5efb2831d37ea142365f009c02ec203cd29a3ebecbc093d52315b66d0"
 dependencies = [
  "windows-targets 0.42.2",
 ]
 
 [[package]]
+name = "windows-sys"
+version = "0.48.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "677d2418bec65e3338edb076e806bc1ec15693c5d0104683f2efe857f61056a9"
+dependencies = [
+ "windows-targets 0.48.0",
+]
+
+[[package]]
 name = "windows-targets"
 version = "0.42.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "8e5180c00cd44c9b1c88adb3693291f1cd93605ded80c250a75d472756b4d071"
 dependencies = [
  "windows_aarch64_gnullvm 0.42.2",
  "windows_aarch64_msvc 0.42.2",
```

### Comparing `polars_lts_cpu-0.17.9/PKG-INFO` & `polars_lts_cpu-0.18.0/PKG-INFO`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: polars-lts-cpu
-Version: 0.17.9
+Version: 0.18.0
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Environment :: Console
 Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python
 Classifier: Programming Language :: Python :: 3
@@ -12,52 +12,52 @@
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11
 Classifier: Programming Language :: Rust
 Classifier: Topic :: Scientific/Engineering
-Requires-Dist: typing_extensions >= 4.0.1; python_version < '3.11'
-Requires-Dist: fsspec; extra == 'fsspec'
-Requires-Dist: xlsx2csv >= 0.8.0; extra == 'xlsx2csv'
-Requires-Dist: connectorx; extra == 'connectorx'
-Requires-Dist: numpy >= 1.16.0; extra == 'numpy'
+Requires-Dist: typing_extensions >= 4.0.1; python_version < '3.8'
 Requires-Dist: matplotlib; extra == 'matplotlib'
-Requires-Dist: xlsxwriter; extra == 'xlsxwriter'
 Requires-Dist: deltalake >= 0.8.0; extra == 'deltalake'
-Requires-Dist: backports.zoneinfo; (python_version < '3.9') and extra == 'timezone'
-Requires-Dist: tzdata; (platform_system == 'Windows') and extra == 'timezone'
 Requires-Dist: pyarrow>=7.0.0; extra == 'pandas'
 Requires-Dist: pandas; extra == 'pandas'
+Requires-Dist: numpy >= 1.16.0; extra == 'numpy'
+Requires-Dist: backports.zoneinfo; (python_version < '3.9') and extra == 'timezone'
+Requires-Dist: tzdata; (platform_system == 'Windows') and extra == 'timezone'
+Requires-Dist: pyarrow>=7.0.0; extra == 'pyarrow'
+Requires-Dist: fsspec; extra == 'fsspec'
 Requires-Dist: sqlalchemy; extra == 'sqlalchemy'
 Requires-Dist: pandas; extra == 'sqlalchemy'
-Requires-Dist: pyarrow>=7.0.0; extra == 'pyarrow'
 Requires-Dist: polars[pyarrow,pandas,numpy,fsspec,connectorx,xlsx2csv,deltalake,timezone,matplotlib,sqlalchemy,xlsxwriter]; extra == 'all'
-Provides-Extra: fsspec
-Provides-Extra: xlsx2csv
-Provides-Extra: connectorx
-Provides-Extra: numpy
+Requires-Dist: xlsx2csv >= 0.8.0; extra == 'xlsx2csv'
+Requires-Dist: connectorx; extra == 'connectorx'
+Requires-Dist: xlsxwriter; extra == 'xlsxwriter'
 Provides-Extra: matplotlib
-Provides-Extra: xlsxwriter
 Provides-Extra: deltalake
-Provides-Extra: timezone
 Provides-Extra: pandas
-Provides-Extra: sqlalchemy
+Provides-Extra: numpy
+Provides-Extra: timezone
 Provides-Extra: pyarrow
+Provides-Extra: fsspec
+Provides-Extra: sqlalchemy
 Provides-Extra: all
+Provides-Extra: xlsx2csv
+Provides-Extra: connectorx
+Provides-Extra: xlsxwriter
 License-File: LICENSE
 Summary: Blazingly fast DataFrame library
 Keywords: dataframe,arrow,out-of-core
 Author-email: Ritchie Vink <ritchie46@gmail.com>
 Requires-Python: >=3.7
 Description-Content-Type: text/markdown; charset=UTF-8; variant=GFM
-Project-URL: Changelog, https://github.com/pola-rs/polars/releases
-Project-URL: Homepage, https://www.pola.rs/
 Project-URL: Documentation, https://pola-rs.github.io/polars/py-polars/html/reference/index.html
+Project-URL: Changelog, https://github.com/pola-rs/polars/releases
 Project-URL: Repository, https://github.com/pola-rs/polars
+Project-URL: Homepage, https://www.pola.rs/
 
 <h1 align="center">
   <img src="https://raw.githubusercontent.com/pola-rs/polars-static/master/logos/polars_github_logo_rect_dark_name.svg">
   <br>
 </h1>
 
 <div align="center">
@@ -257,25 +257,25 @@
 ```sh
 pip install 'polars[all]'
 pip install 'polars[numpy,pandas,pyarrow]'  # install a subset of all optional dependencies
 ```
 
 You can also install the dependencies directly.
 
-| Tag        | Description                                                                                                                           |
-| ---------- | ------------------------------------------------------------------------------------------------------------------------------------- |
-| all        | Install all optional dependencies (all of the following)                                                                              |
-| pandas     | Install with Pandas for converting data to and from Pandas Dataframes/Series                                                          |
-| numpy      | Install with numpy for converting data to and from numpy arrays                                                                       |
-| pyarrow    | Reading data formats using PyArrow                                                                                                    |
-| fsspec     | Support for reading from remote file systems                                                                                          |
-| connectorx | Support for reading from SQL databases                                                                                                |
-| xlsx2csv   | Support for reading from Excel files                                                                                                  |
-| deltalake  | Support for reading from Delta Lake Tables                                                                                            |
-| timezone   | Timezone support, only needed if 1. you are on Python < 3.9 and/or 2. you are on Windows, otherwise no dependencies will be installed |
+| Tag        | Description                                                                  |
+| ---------- | ---------------------------------------------------------------------------- |
+| **all**    | Install all optional dependencies (all of the following)                     |
+| pandas     | Install with Pandas for converting data to and from Pandas Dataframes/Series |
+| numpy      | Install with numpy for converting data to and from numpy arrays              |
+| pyarrow    | Reading data formats using PyArrow                                           |
+| fsspec     | Support for reading from remote file systems                                 |
+| connectorx | Support for reading from SQL databases                                       |
+| xlsx2csv   | Support for reading from Excel files                                         |
+| deltalake  | Support for reading from Delta Lake Tables                                   |
+| timezone   | Timezone support, only needed if are on Python<3.9 or you are on Windows     |
 
 Releases happen quite often (weekly / every few days) at the moment, so updating polars regularly to get the latest bugfixes / features might not be a bad idea.
 
 ### Rust
 
 You can take latest release from `crates.io`, or if you want to use the latest features / performance improvements
 point to the `main` branch of this repo.
```

#### html2text {}

```diff
@@ -1,41 +1,41 @@
-Metadata-Version: 2.1 Name: polars-lts-cpu Version: 0.17.9 Classifier:
+Metadata-Version: 2.1 Name: polars-lts-cpu Version: 0.18.0 Classifier:
 Development Status :: 5 - Production/Stable Classifier: Environment :: Console
 Classifier: Intended Audience :: Science/Research Classifier: License :: OSI
 Approved :: MIT License Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python Classifier: Programming Language ::
 Python :: 3 Classifier: Programming Language :: Python :: 3 :: Only Classifier:
 Programming Language :: Python :: 3.7 Classifier: Programming Language ::
 Python :: 3.8 Classifier: Programming Language :: Python :: 3.9 Classifier:
 Programming Language :: Python :: 3.10 Classifier: Programming Language ::
 Python :: 3.11 Classifier: Programming Language :: Rust Classifier: Topic ::
 Scientific/Engineering Requires-Dist: typing_extensions >= 4.0.1;
-python_version < '3.11' Requires-Dist: fsspec; extra == 'fsspec' Requires-Dist:
-xlsx2csv >= 0.8.0; extra == 'xlsx2csv' Requires-Dist: connectorx; extra ==
-'connectorx' Requires-Dist: numpy >= 1.16.0; extra == 'numpy' Requires-Dist:
-matplotlib; extra == 'matplotlib' Requires-Dist: xlsxwriter; extra ==
-'xlsxwriter' Requires-Dist: deltalake >= 0.8.0; extra == 'deltalake' Requires-
-Dist: backports.zoneinfo; (python_version < '3.9') and extra == 'timezone'
-Requires-Dist: tzdata; (platform_system == 'Windows') and extra == 'timezone'
-Requires-Dist: pyarrow>=7.0.0; extra == 'pandas' Requires-Dist: pandas; extra
-== 'pandas' Requires-Dist: sqlalchemy; extra == 'sqlalchemy' Requires-Dist:
-pandas; extra == 'sqlalchemy' Requires-Dist: pyarrow>=7.0.0; extra == 'pyarrow'
-Requires-Dist: polars
+python_version < '3.8' Requires-Dist: matplotlib; extra == 'matplotlib'
+Requires-Dist: deltalake >= 0.8.0; extra == 'deltalake' Requires-Dist:
+pyarrow>=7.0.0; extra == 'pandas' Requires-Dist: pandas; extra == 'pandas'
+Requires-Dist: numpy >= 1.16.0; extra == 'numpy' Requires-Dist:
+backports.zoneinfo; (python_version < '3.9') and extra == 'timezone' Requires-
+Dist: tzdata; (platform_system == 'Windows') and extra == 'timezone' Requires-
+Dist: pyarrow>=7.0.0; extra == 'pyarrow' Requires-Dist: fsspec; extra ==
+'fsspec' Requires-Dist: sqlalchemy; extra == 'sqlalchemy' Requires-Dist:
+pandas; extra == 'sqlalchemy' Requires-Dist: polars
 [pyarrow,pandas,numpy,fsspec,connectorx,xlsx2csv,deltalake,timezone,matplotlib,sqlalchemy,xlsxwriter];
-extra == 'all' Provides-Extra: fsspec Provides-Extra: xlsx2csv Provides-Extra:
-connectorx Provides-Extra: numpy Provides-Extra: matplotlib Provides-Extra:
-xlsxwriter Provides-Extra: deltalake Provides-Extra: timezone Provides-Extra:
-pandas Provides-Extra: sqlalchemy Provides-Extra: pyarrow Provides-Extra: all
+extra == 'all' Requires-Dist: xlsx2csv >= 0.8.0; extra == 'xlsx2csv' Requires-
+Dist: connectorx; extra == 'connectorx' Requires-Dist: xlsxwriter; extra ==
+'xlsxwriter' Provides-Extra: matplotlib Provides-Extra: deltalake Provides-
+Extra: pandas Provides-Extra: numpy Provides-Extra: timezone Provides-Extra:
+pyarrow Provides-Extra: fsspec Provides-Extra: sqlalchemy Provides-Extra: all
+Provides-Extra: xlsx2csv Provides-Extra: connectorx Provides-Extra: xlsxwriter
 License-File: LICENSE Summary: Blazingly fast DataFrame library Keywords:
 dataframe,arrow,out-of-core Author-email: Ritchie Vink
 gmail.com> Requires-Python: >=3.7 Description-Content-Type: text/markdown;
-charset=UTF-8; variant=GFM Project-URL: Changelog, https://github.com/pola-rs/
-polars/releases Project-URL: Homepage, https://www.pola.rs/ Project-URL:
-Documentation, https://pola-rs.github.io/polars/py-polars/html/reference/
-index.html Project-URL: Repository, https://github.com/pola-rs/polars
+charset=UTF-8; variant=GFM Project-URL: Documentation, https://pola-
+rs.github.io/polars/py-polars/html/reference/index.html Project-URL: Changelog,
+https://github.com/pola-rs/polars/releases Project-URL: Repository, https://
+github.com/pola-rs/polars Project-URL: Homepage, https://www.pola.rs/
  ****** [https://raw.githubusercontent.com/pola-rs/polars-static/master/logos/
                     polars_github_logo_rect_dark_name.svg]
                                      ******
 [rust_docs] [Build_and_test] [https://img.shields.io/crates/v/polars.svg] [PyPi
  Latest_Release] [NPM_Latest_Release] [R-universe_Latest_Release] [DOI_Latest
                                    Release]
   Documentation: Python - Rust - Node.js - R | StackOverflow: Python - Rust -
@@ -106,28 +106,26 @@
 Install the latest polars version with: ```sh pip install polars ``` We also
 have a conda package (`conda install -c conda-forge polars`), however pip is
 the preferred way to install Polars. Install Polars with all optional
 dependencies. ```sh pip install 'polars[all]' pip install 'polars
 [numpy,pandas,pyarrow]' # install a subset of all optional dependencies ``` You
 can also install the dependencies directly. | Tag | Description | | ---------
 - | ---------------------------------------------------------------------------
----------------------------------------------------------- | | all | Install
-all optional dependencies (all of the following) | | pandas | Install with
-Pandas for converting data to and from Pandas Dataframes/Series | | numpy |
-Install with numpy for converting data to and from numpy arrays | | pyarrow |
-Reading data formats using PyArrow | | fsspec | Support for reading from remote
-file systems | | connectorx | Support for reading from SQL databases | |
-xlsx2csv | Support for reading from Excel files | | deltalake | Support for
-reading from Delta Lake Tables | | timezone | Timezone support, only needed if
-1. you are on Python < 3.9 and/or 2. you are on Windows, otherwise no
-dependencies will be installed | Releases happen quite often (weekly / every
-few days) at the moment, so updating polars regularly to get the latest
-bugfixes / features might not be a bad idea. ### Rust You can take latest
-release from `crates.io`, or if you want to use the latest features /
-performance improvements point to the `main` branch of this repo. ```toml
+- | | **all** | Install all optional dependencies (all of the following) | |
+pandas | Install with Pandas for converting data to and from Pandas Dataframes/
+Series | | numpy | Install with numpy for converting data to and from numpy
+arrays | | pyarrow | Reading data formats using PyArrow | | fsspec | Support
+for reading from remote file systems | | connectorx | Support for reading from
+SQL databases | | xlsx2csv | Support for reading from Excel files | | deltalake
+| Support for reading from Delta Lake Tables | | timezone | Timezone support,
+only needed if are on Python<3.9 or you are on Windows | Releases happen quite
+often (weekly / every few days) at the moment, so updating polars regularly to
+get the latest bugfixes / features might not be a bad idea. ### Rust You can
+take latest release from `crates.io`, or if you want to use the latest features
+/ performance improvements point to the `main` branch of this repo. ```toml
 polars = { git = "https://github.com/pola-rs/polars", rev = "" } ``` Required
 Rust version `>=1.62` ## Contributing Want to contribute? Read our
 [contribution guideline](./CONTRIBUTING.md). ## Python: compile polars from
 source If you want a bleeding edge release or maximal performance you should
 compile **polars** from source. This can be done by going through the following
 steps in sequence: 1. Install the latest [Rust compiler](https://www.rust-
 lang.org/tools/install) 2. Install [maturin](https://maturin.rs/): `pip install
```

